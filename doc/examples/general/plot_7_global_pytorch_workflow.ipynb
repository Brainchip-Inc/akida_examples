{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# PyTorch to Akida workflow\n\nThe [Global Akida workflow](../general/plot_0_global_workflow.html)_ guide\ndescribes the steps to prepare a model for Akida starting from a TF-Keras model.\nHere we will instead describe a workflow to go from a model trained in PyTorch.\n\n.. Note::\n   | This example targets users who already have a PyTorch training pipeline\n     in place, and a trained model: this workflow will allow you to rapidly convert\n     your model to Akida 2.0.\n   | Note however that this pathway offers slightly less flexibility than our default,\n     TensorFlow-based pathway - specifically, fine-tuning of the quantized model is\n     not possible when starting from PyTorch.\n   | In most cases, that won't matter, there should be almost no performance drop when\n     quantizing to 8-bit anyway.\n   | However, advanced users interested in further optimization of the original model\n     (going to 4-bit quantization for example) or users who don't yet have a\n     training pipeline in place may prefer the extra options afforded by our default,\n     TensorFlow-based [Global Akida workflow](../general/plot_0_global_workflow.html)_.\n\n\nQuantizeML natively allows the quantization and fine-tuning of TensorFlow models. While\nit does not support PyTorch quantization natively, it allows to quantize float models\nstored in the [Open Neural Network eXchange (ONNX)](https://onnx.ai)_ format. Export\nfrom PyTorch to ONNX is well supported, and so this provides a straightforward pathway to\nprepare your PyTorch model for Akida.\n\nAs a concrete example, we will prepare a PyTorch model on a simple classification task\n(MNIST). This model will then be exported to ONNX and quantized to 8-bit using QuantizeML.\nThe quantized model is then converted to Akida, and performance evaluated to show that\nthere has been no loss in accuracy.\n\nPlease refer to the [Akida user guide](../../user_guide/akida.html)_ for further information.\n\n.. Note::\n   | This example is loosely based on the PyTorch [Training a Classifier](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)_ tutorial and\n     does not aim to describe PyTorch training in detail. We assume that if you are following\n     this example, it's because you already have a trained PyTorch model.\n   | [PyTorch 2.7.0](https://github.com/pytorch/pytorch/releases/tag/v2.7.0)_ is used\n     for this example.\n\n```\npip install torch==2.7.0 torchvision\n```\n.. Warning::\n   | The MNIST example below is light enough to train on the CPU only.\n   | However, where GPU acceleration is desirable for the PyTorch training step, you may find\n     it simpler to use separate virtual environments for the PyTorch-dependent sections\n     (`1. Create and train`_ and `2. Export`_) vs the TensorFlow-dependent sections\n     (`3. Quantize`_ and `4. Convert`_).\n\n\n.. figure:: ../../img/overall_onnx_flow.png\n   :target: ../../_images/overall_onnx_flow.png\n   :alt: Overall PyTorch flow\n   :scale: 60 %\n   :align: center\n\n   PyTorch Akida workflow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Create and train\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1. Load and normalize MNIST dataset\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torchvision\nimport torchvision.transforms as transforms\n\nimport matplotlib.pyplot as plt\n\nbatch_size = 128\n\n\ndef get_dataloader(train, batch_size, num_workers=2):\n    transform = transforms.Compose([transforms.ToTensor(),\n                                    transforms.Normalize(0.5, 0.5)])\n    dataset = torchvision.datasets.MNIST(root='datasets/mnist',\n                                         train=train,\n                                         download=True,\n                                         transform=transform)\n    return torch.utils.data.DataLoader(dataset,\n                                       batch_size=batch_size,\n                                       shuffle=train,\n                                       num_workers=num_workers)\n\n\n# Load MNIST dataset and normalize between [-1, 1]\ntrainloader = get_dataloader(train=True, batch_size=batch_size)\ntestloader = get_dataloader(train=False, batch_size=batch_size)\n\n\ndef imshow(img):\n    # Unnormalize\n    img = img / 2 + 0.5\n    npimg = img.numpy()\n    plt.imshow(npimg.transpose((1, 2, 0)))\n    plt.show()\n\n\n# Get some random training images\nimages, labels = next(iter(trainloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Show images and labels\nimshow(torchvision.utils.make_grid(images, nrow=8))\nprint(\"Labels:\\n\", labels.reshape((-1, 8)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2. Model definition\n\nNote that at this stage, there is nothing specific to the Akida IP.\nThe model constructed below uses the [torch.nn.Sequential](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html#nn-sequential)_\nmodule to define a standard CNN.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_torch = torch.nn.Sequential(torch.nn.Conv2d(1, 32, 5, padding=(2, 2)),\n                                  torch.nn.ReLU6(),\n                                  torch.nn.MaxPool2d(kernel_size=2),\n                                  torch.nn.Conv2d(32, 64, 3, stride=2),\n                                  torch.nn.ReLU(),\n                                  torch.nn.Dropout(0.25),\n                                  torch.nn.Flatten(),\n                                  torch.nn.Linear(2304, 512),\n                                  torch.nn.ReLU(),\n                                  torch.nn.Dropout(0.5),\n                                  torch.nn.Linear(512, 10))\nprint(model_torch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3. Model training\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Define training rules\noptimizer = torch.optim.Adam(model_torch.parameters(), lr=1e-4)\ncriterion = torch.nn.CrossEntropyLoss()\nepochs = 10\n\n# Loop over the dataset multiple times\nfor epoch in range(epochs):\n    running_loss = 0.0\n    for i, data in enumerate(trainloader, 0):\n        # Get the inputs and labels\n        inputs, labels = data\n\n        # Zero the parameter gradients\n        optimizer.zero_grad()\n\n        # Forward + Backward + Optimize\n        outputs = model_torch(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        # Print statistics\n        running_loss += loss.detach().item()\n        if (i + 1) % 100 == 0:\n            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n            running_loss = 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.4. Model testing\n\nEvaluate the model performance on the test set. It should achieve an accuracy over 98%.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "correct = 0\ntotal = 0\nwith torch.no_grad():\n    for data in testloader:\n        inputs, labels = data\n        # Calculate outputs by running images through the network\n        outputs = model_torch(inputs)\n        # The class with the highest score is the prediction\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nassert correct / total >= 0.98\nprint(f'Test accuracy: {100 * correct // total} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Export\n\nPyTorch models are not directly compatible with the [QuantizeML quantization\ntool](../../api_reference/quantizeml_apis.html)_, it is therefore necessary\nto use an intermediate format. Like many other machine learning frameworks,\nPyTorch has tools to export modules in the [ONNX](https://onnx.ai)_ format.\n\nTherefore, the model is exported by the following code:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sample, _ = next(iter(trainloader))\ntorch.onnx.export(model_torch,\n                  sample,\n                  f=\"mnist_cnn.onnx\",\n                  input_names=[\"inputs\"],\n                  output_names=[\"outputs\"],\n                  dynamic_axes={'inputs': {0: 'batch_size'}, 'outputs': {0: 'batch_size'}})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. Note::\n Find more information about how to export PyTorch models in ONNX at\n [](https://pytorch.org/docs/stable/onnx.html).\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Quantize\n\nAn Akida accelerator processes integer activations and weights. Therefore, the floating\npoint model must be quantized in preparation to run on an Akida accelerator.\n\nThe [QuantizeML quantize()](../../api_reference/quantizeml_apis.html#quantizeml.models.quantize)_\nfunction recognizes [ModelProto](https://onnx.ai/onnx/api/classes.html#modelproto)_ objects\nand can quantize them for Akida. The result is another ``ModelProto``, compatible with the\n[CNN2SNN Toolkit](../../user_guide/cnn2snn.html)_.\n\n.. Warning::\n ONNX and PyTorch offer their own quantization methods. You should not use those when preparing\n your model for Akida. Only the [QuantizeML quantize()](../../api_reference/quantizeml_apis.html#quantizeml.models.quantize)_ function\n can be used to generate a quantized model ready for conversion to Akida.\n\n.. Note::\n For this simple model, using random samples for calibration is sufficient, as\n shown in the following steps.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import onnx\nfrom quantizeml.models import quantize\n\n# Read the exported ONNX model\nmodel_onnx = onnx.load_model(\"mnist_cnn.onnx\")\n\n# Extract a batch of train samples for calibration\ncalib_samples = next(iter(trainloader))[0].numpy()\n\n# Quantize\nmodel_quantized = quantize(model_onnx, samples=calib_samples)\nprint(onnx.helper.printable_graph(model_quantized.graph))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Convert\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Convert to Akida model\n\nThe quantized model can now be converted to the native Akida format.\nThe [convert()](../../api_reference/cnn2snn_apis.html#cnn2snn.convert)_\nfunction returns a model in Akida format ready for inference.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from cnn2snn import convert\n\nmodel_akida = convert(model_quantized)\nmodel_akida.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2. Check performance\n\nNative PyTorch data must be presented in a different format to perform\nthe evaluation in Akida models. Specifically:\n\n1. images must be numpy-raw, with an 8-bit unsigned integer data type and\n2. the channel dimension must be in the last dimension.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\n# Read raw data and convert it into numpy\nx_test = testloader.dataset.data.numpy()\ny_test = testloader.dataset.targets.numpy()\n\n# Add a channel dimension to the image sets as Akida expects 4-D inputs corresponding to\n# (num_samples, width, height, channels). Note: MNIST is a grayscale dataset and is unusual\n# in this respect - most image data already includes a channel dimension, and this step will\n# not be necessary.\nx_test = np.expand_dims(x_test, axis=1)\n\naccuracy = model_akida.evaluate(x_test, y_test.astype(np.int32))\nprint('Test accuracy after conversion:', accuracy)\n\n# For non-regression purposes\nassert accuracy > 0.96"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Show predictions for a single image\n\nDisplay one of the test images, such as the first image in the aforementioned\ndataset, to visualize the output of the model.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Test a single example\nsample_image = 0\nimage = np.expand_dims(x_test[sample_image], axis=0)\noutputs = model_akida.predict(image)\n\nplt.imshow(x_test[sample_image].reshape((28, 28)), cmap=\"Greys\")\nprint('Input Label:', y_test[sample_image].item())\nprint('Prediction Label:', outputs.squeeze().argmax())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}