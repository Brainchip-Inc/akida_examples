
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "examples/general/plot_2_ds_cnn_kws.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_examples_general_plot_2_ds_cnn_kws.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_examples_general_plot_2_ds_cnn_kws.py:


DS-CNN/KWS inference
=======================

This tutorial illustrates the process of developing an Akida-compatible speech recognition
model that can identify thirty-two different keywords.

Initially, the model is defined as a CNN in TF-Keras and trained regularly. Next, it undergoes
quantization using `QuantizeML <../../user_guide/quantizeml.html>`__ and finally converted
to Akida using `CNN2SNN <../../user_guide/cnn2snn.html>`__.

This example uses a Keyword Spotting Dataset prepared using **TensorFlow** `audio recognition
example <https://www.tensorflow.org/tutorials/audio/simple_audio>`__ utils.

.. GENERATED FROM PYTHON SOURCE LINES 18-34

1. Load the preprocessed dataset
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The TensorFlow `speech_commands <https://www.tensorflow.org/datasets/catalog/speech_commands>`__
dataset is used for training and validation. All keywords except "backward",
"follow" and "forward", are retrieved. These three words are kept to
illustrate the edge learning in this
`edge example <../edge/plot_1_edge_learning_kws.html>`__.

The words to recognize have been converted to `spectrogram images
<https://github.com/tensorflow/docs/blob/master/site/en/r1/tutorials/sequences/audio_recognition.md#how-does-this-model-work>`__
that allows us to use a model architecture that is typically used for image recognition tasks.
The raw audio data have been preprocessed, transforming the audio files into MFCC features,
well-suited for CNN networks.
A pickle file containing the preprocessed data is available on Brainchip data server.


.. GENERATED FROM PYTHON SOURCE LINES 34-51

.. code-block:: Python

    import pickle

    from akida_models import fetch_file

    # Fetch pre-processed data for 32 keywords
    fname = fetch_file(
        fname='kws_preprocessed_all_words_except_backward_follow_forward.pkl',
        origin="https://data.brainchip.com/dataset-mirror/kws/kws_preprocessed_all_words_except_backward_follow_forward.pkl",
        cache_subdir='datasets/kws')
    with open(fname, 'rb') as f:
        [_, _, x_valid, y_valid, _, _, word_to_index, _] = pickle.load(f)

    # Preprocessed dataset parameters
    num_classes = len(word_to_index)

    print("Wanted words and labels:\n", word_to_index)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Downloading data from https://data.brainchip.com/dataset-mirror/kws/kws_preprocessed_all_words_except_backward_follow_forward.pkl.
           0/62628765 [..............................] - ETA: 0s      147456/62628765 [..............................] - ETA: 21s      851968/62628765 [..............................] - ETA: 7s      1802240/62628765 [..............................] - ETA: 5s     2850816/62628765 [>.............................] - ETA: 4s     3940352/62628765 [>.............................] - ETA: 3s     5013504/62628765 [=>............................] - ETA: 3s     5914624/62628765 [=>............................] - ETA: 3s     7241728/62628765 [==>...........................] - ETA: 3s     8183808/62628765 [==>...........................] - ETA: 3s     9437184/62628765 [===>..........................] - ETA: 2s    10846208/62628765 [====>.........................] - ETA: 2s    11960320/62628765 [====>.........................] - ETA: 2s    13164544/62628765 [=====>........................] - ETA: 2s    14499840/62628765 [=====>........................] - ETA: 2s    15040512/62628765 [======>.......................] - ETA: 2s    16916480/62628765 [=======>......................] - ETA: 2s    18481152/62628765 [=======>......................] - ETA: 2s    19644416/62628765 [========>.....................] - ETA: 2s    20824064/62628765 [========>.....................] - ETA: 1s    21954560/62628765 [=========>....................] - ETA: 1s    22749184/62628765 [=========>....................] - ETA: 1s    23945216/62628765 [==========>...................] - ETA: 1s    25026560/62628765 [==========>...................] - ETA: 1s    25911296/62628765 [===========>..................] - ETA: 1s    27058176/62628765 [===========>..................] - ETA: 1s    28286976/62628765 [============>.................] - ETA: 1s    29188096/62628765 [============>.................] - ETA: 1s    30302208/62628765 [=============>................] - ETA: 1s    31563776/62628765 [==============>...............] - ETA: 1s    32595968/62628765 [==============>...............] - ETA: 1s    33595392/62628765 [===============>..............] - ETA: 1s    34824192/62628765 [===============>..............] - ETA: 1s    36085760/62628765 [================>.............] - ETA: 1s    37003264/62628765 [================>.............] - ETA: 1s    38133760/62628765 [=================>............] - ETA: 1s    39395328/62628765 [=================>............] - ETA: 1s    40493056/62628765 [==================>...........] - ETA: 1s    41459712/62628765 [==================>...........] - ETA: 1s    42721280/62628765 [===================>..........] - ETA: 0s    43859968/62628765 [====================>.........] - ETA: 0s    44818432/62628765 [====================>.........] - ETA: 0s    46112768/62628765 [=====================>........] - ETA: 0s    47194112/62628765 [=====================>........] - ETA: 0s    48193536/62628765 [======================>.......] - ETA: 0s    49242112/62628765 [======================>.......] - ETA: 0s    50241536/62628765 [=======================>......] - ETA: 0s    51568640/62628765 [=======================>......] - ETA: 0s    52592640/62628765 [========================>.....] - ETA: 0s    53714944/62628765 [========================>.....] - ETA: 0s    54878208/62628765 [=========================>....] - ETA: 0s    55779328/62628765 [=========================>....] - ETA: 0s    57032704/62628765 [==========================>...] - ETA: 0s    57876480/62628765 [==========================>...] - ETA: 0s    59138048/62628765 [===========================>..] - ETA: 0s    59973632/62628765 [===========================>..] - ETA: 0s    61267968/62628765 [============================>.] - ETA: 0s    62398464/62628765 [============================>.] - ETA: 0s    62628765/62628765 [==============================] - 3s 0us/step
    Download complete.
    Wanted words and labels:
     {'six': 23, 'three': 25, 'seven': 21, 'bed': 1, 'eight': 6, 'yes': 31, 'cat': 3, 'on': 18, 'one': 19, 'stop': 24, 'two': 27, 'house': 11, 'five': 7, 'down': 5, 'four': 8, 'go': 9, 'up': 28, 'learn': 12, 'no': 16, 'bird': 2, 'zero': 32, 'nine': 15, 'visual': 29, 'wow': 30, 'sheila': 22, 'marvin': 14, 'off': 17, 'right': 20, 'left': 13, 'happy': 10, 'dog': 4, 'tree': 26, '_silence_': 0}




.. GENERATED FROM PYTHON SOURCE LINES 52-64

2. Load a pre-trained native TF-Keras model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The model consists of:

* a first convolutional layer accepting dense inputs (images),
* several separable convolutional layers preserving spatial dimensions,
* a global pooling reducing the spatial dimensions to a single pixel,
* a final dense layer to classify words.

All layers are followed by a batch normalization and a ReLU activation.


.. GENERATED FROM PYTHON SOURCE LINES 64-76

.. code-block:: Python


    from tf_keras.models import load_model

    # Retrieve the model file from the BrainChip data server
    model_file = fetch_file(fname="ds_cnn_kws.h5",
                            origin="https://data.brainchip.com/models/AkidaV2/ds_cnn/ds_cnn_kws.h5",
                            cache_subdir='models')

    # Load the native TF-Keras pre-trained model
    model_keras = load_model(model_file)
    model_keras.summary()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Downloading data from https://data.brainchip.com/models/AkidaV2/ds_cnn/ds_cnn_kws.h5.
         0/170496 [..............................] - ETA: 0s    170496/170496 [==============================] - 0s 0us/step
    Download complete.
    Model: "ds_cnn_kws"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     input (InputLayer)          [(None, 49, 10, 1)]       0         
                                                                 
     rescaling (Rescaling)       (None, 49, 10, 1)         0         
                                                                 
     conv_0 (Conv2D)             (None, 25, 5, 64)         1600      
                                                                 
     conv_0/BN (BatchNormalizat  (None, 25, 5, 64)         256       
     ion)                                                            
                                                                 
     conv_0/relu (ReLU)          (None, 25, 5, 64)         0         
                                                                 
     dw_separable_1 (DepthwiseC  (None, 25, 5, 64)         576       
     onv2D)                                                          
                                                                 
     pw_separable_1 (Conv2D)     (None, 25, 5, 64)         4096      
                                                                 
     pw_separable_1/BN (BatchNo  (None, 25, 5, 64)         256       
     rmalization)                                                    
                                                                 
     pw_separable_1/relu (ReLU)  (None, 25, 5, 64)         0         
                                                                 
     dw_separable_2 (DepthwiseC  (None, 25, 5, 64)         576       
     onv2D)                                                          
                                                                 
     pw_separable_2 (Conv2D)     (None, 25, 5, 64)         4096      
                                                                 
     pw_separable_2/BN (BatchNo  (None, 25, 5, 64)         256       
     rmalization)                                                    
                                                                 
     pw_separable_2/relu (ReLU)  (None, 25, 5, 64)         0         
                                                                 
     dw_separable_3 (DepthwiseC  (None, 25, 5, 64)         576       
     onv2D)                                                          
                                                                 
     pw_separable_3 (Conv2D)     (None, 25, 5, 64)         4096      
                                                                 
     pw_separable_3/BN (BatchNo  (None, 25, 5, 64)         256       
     rmalization)                                                    
                                                                 
     pw_separable_3/relu (ReLU)  (None, 25, 5, 64)         0         
                                                                 
     dw_separable_4 (DepthwiseC  (None, 25, 5, 64)         576       
     onv2D)                                                          
                                                                 
     pw_separable_4 (Conv2D)     (None, 25, 5, 64)         4096      
                                                                 
     pw_separable_4/BN (BatchNo  (None, 25, 5, 64)         256       
     rmalization)                                                    
                                                                 
     pw_separable_4/relu (ReLU)  (None, 25, 5, 64)         0         
                                                                 
     pw_separable_4/global_avg   (None, 64)                0         
     (GlobalAveragePooling2D)                                        
                                                                 
     dense_5 (Dense)             (None, 33)                2145      
                                                                 
     act_softmax (Activation)    (None, 33)                0         
                                                                 
    =================================================================
    Total params: 23713 (92.63 KB)
    Trainable params: 23073 (90.13 KB)
    Non-trainable params: 640 (2.50 KB)
    _________________________________________________________________




.. GENERATED FROM PYTHON SOURCE LINES 77-89

.. code-block:: Python


    import numpy as np

    from sklearn.metrics import accuracy_score

    # Check TF-Keras Model performance
    potentials_keras = model_keras.predict(x_valid)
    preds_keras = np.squeeze(np.argmax(potentials_keras, 1))

    accuracy = accuracy_score(y_valid, preds_keras)
    print("Accuracy: " + "{0:.2f}".format(100 * accuracy) + "%")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      1/308 [..............................] - ETA: 36s     44/308 [===>..........................] - ETA: 0s      88/308 [=======>......................] - ETA: 0s    133/308 [===========>..................] - ETA: 0s    177/308 [================>.............] - ETA: 0s    221/308 [====================>.........] - ETA: 0s    265/308 [========================>.....] - ETA: 0s    308/308 [==============================] - ETA: 0s    308/308 [==============================] - 0s 1ms/step
    Accuracy: 93.09%




.. GENERATED FROM PYTHON SOURCE LINES 90-96

3. Load a pre-trained quantized TF-Keras model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The above native TF-Keras model has been quantized to 8-bit. Note that
a 4-bit version is also available from the `model zoo <../../model_zoo_performance.html#id10>`_.


.. GENERATED FROM PYTHON SOURCE LINES 96-114

.. code-block:: Python


    from quantizeml import load_model

    # Load the pre-trained quantized model
    model_file = fetch_file(
        fname="ds_cnn_kws_i8_w8_a8.h5",
        origin="https://data.brainchip.com/models/AkidaV2/ds_cnn/ds_cnn_kws_i8_w8_a8.h5",
        cache_subdir='models')
    model_keras_quantized = load_model(model_file)
    model_keras_quantized.summary()

    # Check Model performance
    potentials_keras_q = model_keras_quantized.predict(x_valid)
    preds_keras_q = np.squeeze(np.argmax(potentials_keras_q, 1))

    accuracy_q = accuracy_score(y_valid, preds_keras_q)
    print("Accuracy: " + "{0:.2f}".format(100 * accuracy_q) + "%")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Downloading data from https://data.brainchip.com/models/AkidaV2/ds_cnn/ds_cnn_kws_i8_w8_a8.h5.
         0/176200 [..............................] - ETA: 0s    176200/176200 [==============================] - 0s 0us/step
    Download complete.
    Model: "ds_cnn_kws"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     input (InputLayer)          [(None, 49, 10, 1)]       0         
                                                                 
     rescaling (QuantizedRescal  (None, 49, 10, 1)         0         
     ing)                                                            
                                                                 
     conv_0 (QuantizedConv2D)    (None, 25, 5, 64)         1664      
                                                                 
     conv_0/relu (QuantizedReLU  (None, 25, 5, 64)         128       
     )                                                               
                                                                 
     dw_separable_1 (QuantizedD  (None, 25, 5, 64)         704       
     epthwiseConv2D)                                                 
                                                                 
     pw_separable_1 (QuantizedC  (None, 25, 5, 64)         4160      
     onv2D)                                                          
                                                                 
     pw_separable_1/relu (Quant  (None, 25, 5, 64)         128       
     izedReLU)                                                       
                                                                 
     dw_separable_2 (QuantizedD  (None, 25, 5, 64)         704       
     epthwiseConv2D)                                                 
                                                                 
     pw_separable_2 (QuantizedC  (None, 25, 5, 64)         4160      
     onv2D)                                                          
                                                                 
     pw_separable_2/relu (Quant  (None, 25, 5, 64)         128       
     izedReLU)                                                       
                                                                 
     dw_separable_3 (QuantizedD  (None, 25, 5, 64)         704       
     epthwiseConv2D)                                                 
                                                                 
     pw_separable_3 (QuantizedC  (None, 25, 5, 64)         4160      
     onv2D)                                                          
                                                                 
     pw_separable_3/relu (Quant  (None, 25, 5, 64)         128       
     izedReLU)                                                       
                                                                 
     dw_separable_4 (QuantizedD  (None, 25, 5, 64)         704       
     epthwiseConv2D)                                                 
                                                                 
     pw_separable_4 (QuantizedC  (None, 25, 5, 64)         4160      
     onv2D)                                                          
                                                                 
     pw_separable_4/relu (Quant  (None, 25, 5, 64)         0         
     izedReLU)                                                       
                                                                 
     pw_separable_4/global_avg   (None, 64)                2         
     (QuantizedGlobalAveragePoo                                      
     ling2D)                                                         
                                                                 
     dense_5 (QuantizedDense)    (None, 33)                2145      
                                                                 
     dense_5/dequantizer (Dequa  (None, 33)                0         
     ntizer)                                                         
                                                                 
     act_softmax (Activation)    (None, 33)                0         
                                                                 
    =================================================================
    Total params: 23779 (92.89 KB)
    Trainable params: 22753 (88.88 KB)
    Non-trainable params: 1026 (4.01 KB)
    _________________________________________________________________
      1/308 [..............................] - ETA: 12:05      7/308 [..............................] - ETA: 2s        13/308 [>.............................] - ETA: 2s     19/308 [>.............................] - ETA: 2s     25/308 [=>............................] - ETA: 2s     31/308 [==>...........................] - ETA: 2s     37/308 [==>...........................] - ETA: 2s     43/308 [===>..........................] - ETA: 2s     49/308 [===>..........................] - ETA: 2s     55/308 [====>.........................] - ETA: 2s     61/308 [====>.........................] - ETA: 2s     67/308 [=====>........................] - ETA: 2s     73/308 [======>.......................] - ETA: 2s     79/308 [======>.......................] - ETA: 2s     85/308 [=======>......................] - ETA: 1s     91/308 [=======>......................] - ETA: 1s     97/308 [========>.....................] - ETA: 1s    103/308 [=========>....................] - ETA: 1s    109/308 [=========>....................] - ETA: 1s    115/308 [==========>...................] - ETA: 1s    121/308 [==========>...................] - ETA: 1s    127/308 [===========>..................] - ETA: 1s    133/308 [===========>..................] - ETA: 1s    139/308 [============>.................] - ETA: 1s    145/308 [=============>................] - ETA: 1s    151/308 [=============>................] - ETA: 1s    157/308 [==============>...............] - ETA: 1s    163/308 [==============>...............] - ETA: 1s    169/308 [===============>..............] - ETA: 1s    175/308 [================>.............] - ETA: 1s    181/308 [================>.............] - ETA: 1s    187/308 [=================>............] - ETA: 1s    193/308 [=================>............] - ETA: 0s    199/308 [==================>...........] - ETA: 0s    205/308 [==================>...........] - ETA: 0s    211/308 [===================>..........] - ETA: 0s    217/308 [====================>.........] - ETA: 0s    223/308 [====================>.........] - ETA: 0s    229/308 [=====================>........] - ETA: 0s    235/308 [=====================>........] - ETA: 0s    241/308 [======================>.......] - ETA: 0s    247/308 [=======================>......] - ETA: 0s    253/308 [=======================>......] - ETA: 0s    259/308 [========================>.....] - ETA: 0s    265/308 [========================>.....] - ETA: 0s    271/308 [=========================>....] - ETA: 0s    277/308 [=========================>....] - ETA: 0s    283/308 [==========================>...] - ETA: 0s    289/308 [===========================>..] - ETA: 0s    295/308 [===========================>..] - ETA: 0s    301/308 [============================>.] - ETA: 0s    307/308 [============================>.] - ETA: 0s    308/308 [==============================] - 5s 9ms/step
    Accuracy: 92.87%




.. GENERATED FROM PYTHON SOURCE LINES 115-121

4. Conversion to Akida
~~~~~~~~~~~~~~~~~~~~~~

The converted model is Akida 2.0 compatible and its performance
evaluation is done using the Akida simulator.


.. GENERATED FROM PYTHON SOURCE LINES 121-128

.. code-block:: Python


    from cnn2snn import convert

    # Convert the model
    model_akida = convert(model_keras_quantized)
    model_akida.summary()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /usr/local/lib/python3.11/dist-packages/cnn2snn/quantizeml/blocks.py:157: UserWarning: Conversion stops at layer dense_5 because of a dequantizer. The end of the model is ignored:
    ___________________________________________________
    Layer (type)
    ===================================================
    act_softmax (Activation)
    ===================================================

      warnings.warn("Conversion stops" + stop_layer_msg + " because of a dequantizer. "
                    Model Summary                 
    ______________________________________________
    Input shape  Output shape  Sequences  Layers
    ==============================================
    [49, 10, 1]  [1, 1, 33]    1          11    
    ______________________________________________

    _________________________________________________________________
    Layer (type)                       Output shape  Kernel shape  

    ============ SW/conv_0-dense_5/dequantizer (Software) ===========

    conv_0 (InputConv2D)               [25, 5, 64]   (5, 5, 1, 64) 
    _________________________________________________________________
    dw_separable_1 (DepthwiseConv2D)   [25, 5, 64]   (3, 3, 64, 1) 
    _________________________________________________________________
    pw_separable_1 (Conv2D)            [25, 5, 64]   (1, 1, 64, 64)
    _________________________________________________________________
    dw_separable_2 (DepthwiseConv2D)   [25, 5, 64]   (3, 3, 64, 1) 
    _________________________________________________________________
    pw_separable_2 (Conv2D)            [25, 5, 64]   (1, 1, 64, 64)
    _________________________________________________________________
    dw_separable_3 (DepthwiseConv2D)   [25, 5, 64]   (3, 3, 64, 1) 
    _________________________________________________________________
    pw_separable_3 (Conv2D)            [25, 5, 64]   (1, 1, 64, 64)
    _________________________________________________________________
    dw_separable_4 (DepthwiseConv2D)   [25, 5, 64]   (3, 3, 64, 1) 
    _________________________________________________________________
    pw_separable_4 (Conv2D)            [1, 1, 64]    (1, 1, 64, 64)
    _________________________________________________________________
    dense_5 (Dense1D)                  [1, 1, 33]    (64, 33)      
    _________________________________________________________________
    dense_5/dequantizer (Dequantizer)  [1, 1, 33]    N/A           
    _________________________________________________________________




.. GENERATED FROM PYTHON SOURCE LINES 129-139

.. code-block:: Python


    # Check Akida model performance
    preds_akida = model_akida.predict_classes(x_valid, num_classes=num_classes)

    accuracy = accuracy_score(y_valid, preds_akida)
    print("Accuracy: " + "{0:.2f}".format(100 * accuracy) + "%")

    # For non-regression purposes
    assert accuracy > 0.9





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Accuracy: 92.87%




.. GENERATED FROM PYTHON SOURCE LINES 140-154

5. Confusion matrix
~~~~~~~~~~~~~~~~~~~

The confusion matrix provides a good summary of what mistakes the
network is making.

Per scikit-learn convention it displays the true class in each row (ie
on each row you can see what the network predicted for the corresponding
word).

Please refer to the Tensorflow `audio
recognition <https://github.com/tensorflow/docs/blob/master/site/en/r1/tutorials/sequences/audio_recognition.md#confusion-matrix>`__
example for a detailed explanation of the confusion matrix.


.. GENERATED FROM PYTHON SOURCE LINES 154-192

.. code-block:: Python

    import itertools
    import matplotlib.pyplot as plt

    from sklearn.metrics import confusion_matrix

    # Create confusion matrix
    cm = confusion_matrix(y_valid, preds_akida,
                          labels=list(word_to_index.values()))

    # Normalize
    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    # Display confusion matrix
    plt.rcParams["figure.figsize"] = (16, 16)
    plt.figure()

    title = 'Confusion matrix'
    cmap = plt.cm.Blues

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(word_to_index))
    plt.xticks(tick_marks, word_to_index, rotation=45)
    plt.yticks(tick_marks, word_to_index)

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j,
                 i,
                 format(cm[i, j], '.2f'),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.autoscale()
    plt.show()



.. image-sg:: /examples/general/images/sphx_glr_plot_2_ds_cnn_kws_001.png
   :alt: Confusion matrix
   :srcset: /examples/general/images/sphx_glr_plot_2_ds_cnn_kws_001.png
   :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 23.397 seconds)


.. _sphx_glr_download_examples_general_plot_2_ds_cnn_kws.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_2_ds_cnn_kws.ipynb <plot_2_ds_cnn_kws.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_2_ds_cnn_kws.py <plot_2_ds_cnn_kws.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_2_ds_cnn_kws.zip <plot_2_ds_cnn_kws.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
