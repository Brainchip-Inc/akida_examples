
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "examples/general/plot_3_regression.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_examples_general_plot_3_regression.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_examples_general_plot_3_regression.py:


Age estimation (regression) example
==================================================

This tutorial aims to demonstrate the comparable accuracy of the Akida-compatible
model to the traditional TF-Keras model in performing an age estimation task.

It uses the `UTKFace dataset <https://susanqq.github.io/UTKFace/>`__, which
includes images of faces and age labels, to showcase how well akida compatible
model can predict the ages of individuals based on their facial features.

.. GENERATED FROM PYTHON SOURCE LINES 15-25

1. Load the UTKFace Dataset
~~~~~~~~~~~~~~~~~~~~~~~~~~~

The UTKFace dataset has 20,000+ diverse face images spanning 0 to 116 years.
It includes age, gender, ethnicity annotations. This dataset is useful for
various tasks like age estimation, face detection, and more.

Load the dataset from Brainchip data server using the `load_data
<../../api_reference/akida_models_apis.html#akida_models.utk_face.preprocessing.load_data>`__
helper (decode JPEG images and load the associated labels).

.. GENERATED FROM PYTHON SOURCE LINES 25-31

.. code-block:: Python


    from akida_models.utk_face.preprocessing import load_data

    # Load the dataset
    x_train, y_train, x_test, y_test = load_data()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Downloading data from https://data.brainchip.com/dataset-mirror/utk_face/UTKFace_preprocessed.tar.gz.
           0/48742400 [..............................] - ETA: 0s       49152/48742400 [..............................] - ETA: 58s      122880/48742400 [..............................] - ETA: 43s      368640/48742400 [..............................] - ETA: 21s      843776/48742400 [..............................] - ETA: 12s     2744320/48742400 [>.............................] - ETA: 4s      4612096/48742400 [=>............................] - ETA: 3s     7921664/48742400 [===>..........................] - ETA: 2s    10182656/48742400 [=====>........................] - ETA: 1s    11968512/48742400 [======>.......................] - ETA: 1s    14032896/48742400 [=======>......................] - ETA: 1s    15933440/48742400 [========>.....................] - ETA: 1s    17784832/48742400 [=========>....................] - ETA: 1s    19849216/48742400 [===========>..................] - ETA: 1s    21569536/48742400 [============>.................] - ETA: 0s    23617536/48742400 [=============>................] - ETA: 0s    25223168/48742400 [==============>...............] - ETA: 0s    27287552/48742400 [===============>..............] - ETA: 0s    28827648/48742400 [================>.............] - ETA: 0s    30351360/48742400 [=================>............] - ETA: 0s    32153600/48742400 [==================>...........] - ETA: 0s    33464320/48742400 [===================>..........] - ETA: 0s    34840576/48742400 [====================>.........] - ETA: 0s    36798464/48742400 [=====================>........] - ETA: 0s    38297600/48742400 [======================>.......] - ETA: 0s    39608320/48742400 [=======================>......] - ETA: 0s    41312256/48742400 [========================>.....] - ETA: 0s    43098112/48742400 [=========================>....] - ETA: 0s    44376064/48742400 [==========================>...] - ETA: 0s    45637632/48742400 [===========================>..] - ETA: 0s    46882816/48742400 [===========================>..] - ETA: 0s    48701440/48742400 [============================>.] - ETA: 0s    48742400/48742400 [==============================] - 2s 0us/step
    Download complete.




.. GENERATED FROM PYTHON SOURCE LINES 32-34

Akida models accept only `uint8 tensors <../../api_reference/akida_apis.html?highlight=uint8#akida.Model>`_
as inputs. Use uint8 raw data to train the model and for Akida performance evaluation.

.. GENERATED FROM PYTHON SOURCE LINES 34-38

.. code-block:: Python

    x_train = x_train.astype('uint8')
    x_test = x_test.astype('uint8')









.. GENERATED FROM PYTHON SOURCE LINES 39-52

2. Load a pre-trained native TF-Keras model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The model is a simplified version inspired from `VGG <https://arxiv.org/abs/1409.1556>`__
architecture. It consists of a succession of convolutional and pooling layers
and ends with two dense layers that outputs a single value
corresponding to the estimated age.

The performance of the model is evaluated using the "Mean Absolute Error"
(MAE). The MAE, used as a metric in regression problem, is calculated as an
average of absolute differences between the target values and the predictions.
The MAE is a linear score, i.e. all the individual differences are equally
weighted in the average.

.. GENERATED FROM PYTHON SOURCE LINES 52-65

.. code-block:: Python


    from akida_models import fetch_file
    from tf_keras.models import load_model

    # Retrieve the model file from the BrainChip data server
    model_file = fetch_file(fname="vgg_utk_face.h5",
                            origin="https://data.brainchip.com/models/AkidaV2/vgg/vgg_utk_face.h5",
                            cache_subdir='models')

    # Load the native TF-Keras pre-trained model
    model_keras = load_model(model_file)
    model_keras.summary()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Downloading data from https://data.brainchip.com/models/AkidaV2/vgg/vgg_utk_face.h5.
         0/559792 [..............................] - ETA: 0s    237568/559792 [===========>..................] - ETA: 0s    559792/559792 [==============================] - 0s 0us/step
    Download complete.
    Model: "vgg_utk_face"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     input (InputLayer)          [(None, 32, 32, 3)]       0         
                                                                 
     rescaling (Rescaling)       (None, 32, 32, 3)         0         
                                                                 
     conv_0 (Conv2D)             (None, 30, 30, 32)        864       
                                                                 
     conv_0/BN (BatchNormalizat  (None, 30, 30, 32)        128       
     ion)                                                            
                                                                 
     conv_0/relu (ReLU)          (None, 30, 30, 32)        0         
                                                                 
     conv_1 (Conv2D)             (None, 30, 30, 32)        9216      
                                                                 
     conv_1/maxpool (MaxPooling  (None, 15, 15, 32)        0         
     2D)                                                             
                                                                 
     conv_1/BN (BatchNormalizat  (None, 15, 15, 32)        128       
     ion)                                                            
                                                                 
     conv_1/relu (ReLU)          (None, 15, 15, 32)        0         
                                                                 
     dropout_3 (Dropout)         (None, 15, 15, 32)        0         
                                                                 
     conv_2 (Conv2D)             (None, 15, 15, 64)        18432     
                                                                 
     conv_2/BN (BatchNormalizat  (None, 15, 15, 64)        256       
     ion)                                                            
                                                                 
     conv_2/relu (ReLU)          (None, 15, 15, 64)        0         
                                                                 
     conv_3 (Conv2D)             (None, 15, 15, 64)        36864     
                                                                 
     conv_3/maxpool (MaxPooling  (None, 8, 8, 64)          0         
     2D)                                                             
                                                                 
     conv_3/BN (BatchNormalizat  (None, 8, 8, 64)          256       
     ion)                                                            
                                                                 
     conv_3/relu (ReLU)          (None, 8, 8, 64)          0         
                                                                 
     dropout_4 (Dropout)         (None, 8, 8, 64)          0         
                                                                 
     conv_4 (Conv2D)             (None, 8, 8, 84)          48384     
                                                                 
     conv_4/BN (BatchNormalizat  (None, 8, 8, 84)          336       
     ion)                                                            
                                                                 
     conv_4/relu (ReLU)          (None, 8, 8, 84)          0         
                                                                 
     conv_4/global_avg (GlobalA  (None, 84)                0         
     veragePooling2D)                                                
                                                                 
     dropout_5 (Dropout)         (None, 84)                0         
                                                                 
     dense_1 (Dense)             (None, 64)                5376      
                                                                 
     dense_1/BN (BatchNormaliza  (None, 64)                256       
     tion)                                                           
                                                                 
     dense_1/relu (ReLU)         (None, 64)                0         
                                                                 
     dense_2 (Dense)             (None, 1)                 65        
                                                                 
    =================================================================
    Total params: 120561 (470.94 KB)
    Trainable params: 119881 (468.29 KB)
    Non-trainable params: 680 (2.66 KB)
    _________________________________________________________________




.. GENERATED FROM PYTHON SOURCE LINES 66-75

.. code-block:: Python


    # Compile the native TF-Keras model (required to evaluate the MAE)
    model_keras.compile(optimizer='Adam', loss='mae')

    # Check Keras model performance
    mae_keras = model_keras.evaluate(x_test, y_test, verbose=0)

    print("TF-Keras MAE: {0:.4f}".format(mae_keras))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    TF-Keras MAE: 6.0806




.. GENERATED FROM PYTHON SOURCE LINES 76-81

3. Load a pre-trained quantized TF-Keras model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The above native TF-Keras model is quantized and fine-tuned (QAT). All weights and activations are
quantized to 8-bit.

.. GENERATED FROM PYTHON SOURCE LINES 81-87

.. code-block:: Python

    from akida_models import vgg_utk_face_pretrained

    # Load the pre-trained quantized model
    model_quantized_keras = vgg_utk_face_pretrained()
    model_quantized_keras.summary()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Downloading data from https://data.brainchip.com/models/AkidaV2/vgg/vgg_utk_face_i8_w8_a8.h5.
         0/554712 [..............................] - ETA: 0s    212992/554712 [==========>...................] - ETA: 0s    554712/554712 [==============================] - 0s 0us/step
    Download complete.
    Model: "vgg_utk_face"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     input (InputLayer)          [(None, 32, 32, 3)]       0         
                                                                 
     rescaling (QuantizedRescal  (None, 32, 32, 3)         0         
     ing)                                                            
                                                                 
     conv_0 (QuantizedConv2D)    (None, 30, 30, 32)        896       
                                                                 
     conv_0/relu (QuantizedReLU  (None, 30, 30, 32)        64        
     )                                                               
                                                                 
     conv_1 (QuantizedConv2D)    (None, 30, 30, 32)        9248      
                                                                 
     conv_1/maxpool (QuantizedM  (None, 15, 15, 32)        0         
     axPool2D)                                                       
                                                                 
     conv_1/relu (QuantizedReLU  (None, 15, 15, 32)        64        
     )                                                               
                                                                 
     dropout_3 (QuantizedDropou  (None, 15, 15, 32)        0         
     t)                                                              
                                                                 
     conv_2 (QuantizedConv2D)    (None, 15, 15, 64)        18496     
                                                                 
     conv_2/relu (QuantizedReLU  (None, 15, 15, 64)        128       
     )                                                               
                                                                 
     conv_3 (QuantizedConv2D)    (None, 15, 15, 64)        36928     
                                                                 
     conv_3/maxpool (QuantizedM  (None, 8, 8, 64)          0         
     axPool2D)                                                       
                                                                 
     conv_3/relu (QuantizedReLU  (None, 8, 8, 64)          128       
     )                                                               
                                                                 
     dropout_4 (QuantizedDropou  (None, 8, 8, 64)          0         
     t)                                                              
                                                                 
     conv_4 (QuantizedConv2D)    (None, 8, 8, 84)          48468     
                                                                 
     conv_4/relu (QuantizedReLU  (None, 8, 8, 84)          0         
     )                                                               
                                                                 
     conv_4/global_avg (Quantiz  (None, 84)                2         
     edGlobalAveragePooling2D)                                       
                                                                 
     dropout_5 (QuantizedDropou  (None, 84)                0         
     t)                                                              
                                                                 
     dense_1 (QuantizedDense)    (None, 64)                5440      
                                                                 
     dense_1/relu (QuantizedReL  (None, 64)                2         
     U)                                                              
                                                                 
     dense_2 (QuantizedDense)    (None, 1)                 65        
                                                                 
     dequantizer (Dequantizer)   (None, 1)                 0         
                                                                 
    =================================================================
    Total params: 119929 (468.47 KB)
    Trainable params: 119541 (466.96 KB)
    Non-trainable params: 388 (1.52 KB)
    _________________________________________________________________




.. GENERATED FROM PYTHON SOURCE LINES 88-97

.. code-block:: Python


    # Compile the quantized TF-Keras model (required to evaluate the MAE)
    model_quantized_keras.compile(optimizer='Adam', loss='mae')

    # Check Keras model performance
    mae_quant = model_quantized_keras.evaluate(x_test, y_test, verbose=0)

    print("Keras MAE: {0:.4f}".format(mae_quant))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Keras MAE: 6.0300




.. GENERATED FROM PYTHON SOURCE LINES 98-104

4. Conversion to Akida
~~~~~~~~~~~~~~~~~~~~~~

The quantized TF-Keras model is now converted into an Akida model. After conversion, we evaluate
the performance on the UTKFace dataset.


.. GENERATED FROM PYTHON SOURCE LINES 104-111

.. code-block:: Python


    from cnn2snn import convert

    # Convert the model
    model_akida = convert(model_quantized_keras)
    model_akida.summary()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

                    Model Summary                 
    ______________________________________________
    Input shape  Output shape  Sequences  Layers
    ==============================================
    [32, 32, 3]  [1, 1, 1]     1          8     
    ______________________________________________

    _________________________________________________________
    Layer (type)               Output shape  Kernel shape  

    ============ SW/conv_0-dequantizer (Software) ===========

    conv_0 (InputConv2D)       [30, 30, 32]  (3, 3, 3, 32) 
    _________________________________________________________
    conv_1 (Conv2D)            [15, 15, 32]  (3, 3, 32, 32)
    _________________________________________________________
    conv_2 (Conv2D)            [15, 15, 64]  (3, 3, 32, 64)
    _________________________________________________________
    conv_3 (Conv2D)            [8, 8, 64]    (3, 3, 64, 64)
    _________________________________________________________
    conv_4 (Conv2D)            [1, 1, 84]    (3, 3, 64, 84)
    _________________________________________________________
    dense_1 (Dense1D)          [1, 1, 64]    (84, 64)      
    _________________________________________________________
    dense_2 (Dense1D)          [1, 1, 1]     (64, 1)       
    _________________________________________________________
    dequantizer (Dequantizer)  [1, 1, 1]     N/A           
    _________________________________________________________




.. GENERATED FROM PYTHON SOURCE LINES 112-125

.. code-block:: Python


    import numpy as np

    # Check Akida model performance
    y_akida = model_akida.predict(x_test)

    # Compute and display the MAE
    mae_akida = np.sum(np.abs(y_test.squeeze() - y_akida.squeeze())) / len(y_test)
    print("Akida MAE: {0:.4f}".format(mae_akida))

    # For non-regression purposes
    assert abs(mae_keras - mae_akida) < 0.5





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Akida MAE: 6.0299




.. GENERATED FROM PYTHON SOURCE LINES 126-131

5. Estimate age on a single image
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Select a random image from the test set for age estimation.
Print the TF-Keras model's age prediction using the ``model_keras.predict()`` function.
Print the Akida model's estimated age and the actual age associated with the image.

.. GENERATED FROM PYTHON SOURCE LINES 131-146

.. code-block:: Python


    import matplotlib.pyplot as plt

    # Estimate age on a random single image and display TF-Keras and Akida outputs
    rng = np.random.default_rng()
    id = rng.integers(0, len(y_test))
    age_keras = model_keras.predict(x_test[np.newaxis, id])

    plt.imshow(x_test[id], interpolation='bicubic')
    plt.xticks([]), plt.yticks([])
    plt.show()

    print("TF-Keras estimated age: {0:.1f}".format(age_keras.squeeze()))
    print("Akida estimated age: {0:.1f}".format(y_akida[id].squeeze()))
    print(f"Actual age: {y_test[id].squeeze()}")



.. image-sg:: /examples/general/images/sphx_glr_plot_3_regression_001.png
   :alt: plot 3 regression
   :srcset: /examples/general/images/sphx_glr_plot_3_regression_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    1/1 [==============================] - ETA: 0s    1/1 [==============================] - 0s 94ms/step
    TF-Keras estimated age: 44.5
    Akida estimated age: 44.2
    Actual age: 52





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 35.807 seconds)


.. _sphx_glr_download_examples_general_plot_3_regression.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_3_regression.ipynb <plot_3_regression.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_3_regression.py <plot_3_regression.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_3_regression.zip <plot_3_regression.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
