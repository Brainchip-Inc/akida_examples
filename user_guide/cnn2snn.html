

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>CNN2SNN toolkit &mdash; Akida Examples  documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery-dataframe.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Hardware constraints" href="hw_constraints.html" />
    <link rel="prev" title="Akida user guide" href="aee.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #3f51b5" >
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/akida.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                Akida 1.8.7
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../index.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../installation.html#requirements">Requirements</a></li>
<li class="toctree-l2"><a class="reference internal" href="../installation.html#quick-installation">Quick installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../installation.html#running-examples">Running examples</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="user_guide.html">User guide</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="getting_started.html">Getting started</a><ul>
<li class="toctree-l3"><a class="reference internal" href="getting_started.html#for-beginners">For beginners</a></li>
<li class="toctree-l3"><a class="reference internal" href="getting_started.html#for-users-familiar-with-deep-learning">For users familiar with deep-learning</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="aee.html">Akida user guide</a><ul>
<li class="toctree-l3"><a class="reference internal" href="aee.html#the-akida-execution-engine">The Akida Execution Engine</a><ul>
<li class="toctree-l4"><a class="reference internal" href="aee.html#id1">1. The Spiking Neural Network model</a></li>
<li class="toctree-l4"><a class="reference internal" href="aee.html#id2">2. Input data format</a></li>
<li class="toctree-l4"><a class="reference internal" href="aee.html#id3">3. Determine training mode</a></li>
<li class="toctree-l4"><a class="reference internal" href="aee.html#id4">4. Interpreting outputs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="aee.html#neural-network-model">Neural Network model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="aee.html#specifying-the-neural-network-model">Specifying the Neural Network model</a></li>
<li class="toctree-l4"><a class="reference internal" href="aee.html#saving-and-loading">Saving and loading</a></li>
<li class="toctree-l4"><a class="reference internal" href="aee.html#input-layer-types">Input layer types</a></li>
<li class="toctree-l4"><a class="reference internal" href="aee.html#data-processing-layer-types">Data-Processing layer types</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="aee.html#id5">Using Akida Unsupervised Learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="aee.html#learning-constraints">Learning constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="aee.html#compiling-a-layer">Compiling a layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="aee.html#id6">Learning parameters</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">CNN2SNN toolkit</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#conversion-workflow">Conversion workflow</a></li>
<li class="toctree-l4"><a class="reference internal" href="#typical-training-scenario">Typical training scenario</a></li>
<li class="toctree-l4"><a class="reference internal" href="#design-compatibility-constraints">Design compatibility constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="#quantization-compatibility-constraints">Quantization compatibility constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="#command-line-interface">Command-line interface</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#layers-considerations">Layers Considerations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#supported-layer-types">Supported layer types</a></li>
<li class="toctree-l4"><a class="reference internal" href="#cnn2snn-quantization-aware-layers">CNN2SNN Quantization-aware layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="#training-only-layers">Training-Only Layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="#first-layers">First Layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id6">Final Layers</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#layer-blocks">Layer Blocks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id8">Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="#conv-block"><code class="docutils literal notranslate"><span class="pre">conv_block</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#dense-block"><code class="docutils literal notranslate"><span class="pre">dense_block</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#separable-conv-block"><code class="docutils literal notranslate"><span class="pre">separable_conv_block</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#tips-and-tricks">Tips and Tricks</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="hw_constraints.html">Hardware constraints</a><ul>
<li class="toctree-l3"><a class="reference internal" href="hw_constraints.html#input-layer">Input layer</a></li>
<li class="toctree-l3"><a class="reference internal" href="hw_constraints.html#data-processing-layers">Data-Processing layers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="hw_constraints.html#convolutional-layer">Convolutional layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="hw_constraints.html#fully-connected-layer">Fully connected layer</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../api_reference/api_reference.html">API reference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../api_reference/aee_apis.html">Akida Execution Engine</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#model">Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#layer">Layer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#layerstatistics">LayerStatistics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#observer">Observer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#inputdata">InputData</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#inputconvolutional">InputConvolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#fullyconnected">FullyConnected</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#convolutional">Convolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#separableconvolutional">SeparableConvolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#dense">Dense</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#sparse">Sparse</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#coords-to-sparse">coords_to_sparse</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#dense-to-sparse">dense_to_sparse</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#backendtype">BackendType</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#convolutionmode">ConvolutionMode</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#poolingtype">PoolingType</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#learningtype">LearningType</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api_reference/cnn2snn_apis.html">CNN2SNN</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#quantize">quantize</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#convert">convert</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#check-model-compatibility">check_model_compatibility</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#weightquantizer">WeightQuantizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#trainableweightquantizer">TrainableWeightQuantizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#weightfloat">WeightFloat</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#quantizedconv2d">QuantizedConv2D</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#quantizeddepthwiseconv2d">QuantizedDepthwiseConv2D</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#quantizeddense">QuantizedDense</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#quantizedseparableconv2d">QuantizedSeparableConv2D</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#activationdiscreterelu">ActivationDiscreteRelu</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#quantizedrelu">QuantizedReLU</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api_reference/akida_models_apis.html">Akida models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_models_apis.html#quantization-blocks">Quantization blocks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#conv-block">conv_block</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#separable-conv-block">separable_conv_block</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#dense-block">dense_block</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_models_apis.html#model-zoo">Model zoo</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#mobilenet">Mobilenet</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#ds-cnn">DS-CNN</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#vgg">VGG</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../examples/index.html">Examples</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../examples/plot_gxnor_mnist.html">GXNOR/MNIST inference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_gxnor_mnist.html#loading-the-mnist-dataset">1. Loading the MNIST dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_gxnor_mnist.html#look-at-some-images-from-the-test-dataset">2. Look at some images from the test dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_gxnor_mnist.html#load-the-pre-trained-akida-model">3. Load the pre-trained Akida model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_gxnor_mnist.html#classify-a-single-image">4. Classify a single image</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_gxnor_mnist.html#check-performance-across-a-number-of-samples">5. Check performance across a number of samples</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/plot_regression.html">Regression tutorial</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_regression.html#load-dependencies">1. Load dependencies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_regression.html#load-the-dataset">2. Load the dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_regression.html#create-a-keras-model-satisfying-akida-nsoc-requirements">3. Create a Keras model satisfying Akida NSoC requirements</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_regression.html#check-performance">4. Check performance</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_regression.html#conversion-to-akida">5. Conversion to Akida</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_regression.html#convert-the-trained-keras-model-to-akida">5.1 Convert the trained Keras model to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_regression.html#check-akida-model-accuracy">5.2 Check Akida model accuracy</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_regression.html#estimate-age-on-a-single-image">6. Estimate age on a single image</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/plot_ds_cnn_kws.html">DS-CNN/KWS inference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_ds_cnn_kws.html#load-cnn2snn-tool-dependencies">1. Load CNN2SNN tool dependencies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_ds_cnn_kws.html#load-the-preprocessed-dataset">2. Load the preprocessed dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_ds_cnn_kws.html#create-a-keras-model-satisfying-akida-nsoc-requirements">3. Create a Keras model satisfying Akida NSoC requirements</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_ds_cnn_kws.html#check-performance">4. Check performance</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_ds_cnn_kws.html#conversion-to-akida">5. Conversion to Akida</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_ds_cnn_kws.html#convert-the-trained-keras-model-to-akida">5.1 Convert the trained Keras model to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_ds_cnn_kws.html#check-prediction-accuracy">5.2 Check prediction accuracy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_ds_cnn_kws.html#confusion-matrix">5.3 Confusion matrix</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/plot_vgg_ds_cnn_cifar10.html">VGG and DS-CNN/CIFAR10 inference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_vgg_ds_cnn_cifar10.html#load-cnn2snn-tool-dependencies">1. Load CNN2SNN tool dependencies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_vgg_ds_cnn_cifar10.html#load-and-reshape-cifar10-dataset">2. Load and reshape CIFAR10 dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_vgg_ds_cnn_cifar10.html#create-a-quantized-keras-vgg-model">3. Create a quantized Keras VGG model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_vgg_ds_cnn_cifar10.html#a-instantiate-keras-model">3.A Instantiate Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_vgg_ds_cnn_cifar10.html#b-check-performance">3.B Check performance</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_vgg_ds_cnn_cifar10.html#conversion-to-akida">4. Conversion to Akida</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_vgg_ds_cnn_cifar10.html#a-convert-to-akida-model">4.A Convert to Akida model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_vgg_ds_cnn_cifar10.html#b-check-hardware-compliancy">4.B Check hardware compliancy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_vgg_ds_cnn_cifar10.html#c-check-performance">4.C Check performance</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_vgg_ds_cnn_cifar10.html#create-a-quantized-keras-ds-cnn-model">5. Create a quantized Keras DS-CNN model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_vgg_ds_cnn_cifar10.html#a-instantiate-keras-ds-cnn-model">5.A Instantiate Keras DS-CNN model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_vgg_ds_cnn_cifar10.html#id1">5.B Check performance</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_vgg_ds_cnn_cifar10.html#id2">6. Conversion to Akida</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_vgg_ds_cnn_cifar10.html#id3">6.A Convert to Akida model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_vgg_ds_cnn_cifar10.html#id4">6.B Check hardware compliancy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_vgg_ds_cnn_cifar10.html#id5">6.C Check performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_vgg_ds_cnn_cifar10.html#d-show-predictions-for-a-random-image">6D. Show predictions for a random image</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/plot_transfer_learning.html">Transfer learning with MobileNet for cats vs. dogs</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_transfer_learning.html#transfer-learning-process">1. Transfer learning process</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_transfer_learning.html#load-and-preprocess-data">2. Load and preprocess data</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_transfer_learning.html#a-load-and-split-data">2.A - Load and split data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_transfer_learning.html#b-preprocess-the-test-set">2.B - Preprocess the test set</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_transfer_learning.html#c-get-labels">2.C - Get labels</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_transfer_learning.html#convert-a-quantized-keras-model-to-akida">3. Convert a quantized Keras model to Akida</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_transfer_learning.html#a-instantiate-a-keras-base-model">3.A - Instantiate a Keras base model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_transfer_learning.html#b-modify-the-network-and-load-pre-trained-weights">3.B - Modify the network and load pre-trained weights</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_transfer_learning.html#c-convert-to-akida">3.C - Convert to Akida</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_transfer_learning.html#classify-test-images">4. Classify test images</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_transfer_learning.html#a-classify-test-images">4.A Classify test images</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_transfer_learning.html#b-compare-results">4.B Compare results</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/plot_mobilenet_imagenet.html">MobileNet/ImageNet inference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_mobilenet_imagenet.html#load-cnn2snn-tool-dependencies">1. Load CNN2SNN tool dependencies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_mobilenet_imagenet.html#load-test-images-from-imagenet">2. Load test images from ImageNet</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_mobilenet_imagenet.html#load-test-images-and-preprocess-test-images">2.1 Load test images and preprocess test images</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_mobilenet_imagenet.html#load-labels">2.2 Load labels</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_mobilenet_imagenet.html#create-a-quantized-keras-model">3. Create a quantized Keras model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_mobilenet_imagenet.html#instantiate-keras-model">3.1 Instantiate Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_mobilenet_imagenet.html#check-performance-of-the-keras-model">3.2 Check performance of the Keras model</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_mobilenet_imagenet.html#convert-keras-model-for-akida-nsoc">4. Convert Keras model for Akida NSoC</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_mobilenet_imagenet.html#convert-keras-model-to-an-akida-compatible-model">4.1 Convert Keras model to an Akida compatible model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_mobilenet_imagenet.html#test-performance-of-the-akida-model">4.2 Test performance of the Akida model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_mobilenet_imagenet.html#show-predictions-for-a-random-test-image">4.3 Show predictions for a random test image</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/plot_cnn_flow.html">CNN conversion flow tutorial</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_cnn_flow.html#system-configuration">1. System configuration</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_cnn_flow.html#load-cnn2snn-tool-dependencies">1.1 Load CNN2SNN tool dependencies</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_cnn_flow.html#load-and-reshape-mnist-dataset">1.2 Load and reshape MNIST dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_cnn_flow.html#set-training-parameters">1.3 Set training parameters</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_cnn_flow.html#model-creation-and-performance-check">2. Model creation and performance check</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_cnn_flow.html#model-creation">2.1 Model creation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_cnn_flow.html#performance-check">2.2 Performance check</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_cnn_flow.html#model-akida-compatibility-check-and-changes">3. Model Akida-compatibility check and changes</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_cnn_flow.html#compatibility-check">3.1 Compatibility check</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_cnn_flow.html#model-adaptation">3.2 Model adaptation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_cnn_flow.html#id1">3.3 Performance check</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_cnn_flow.html#model-quantization-and-training">4. Model quantization and training</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_cnn_flow.html#quantize-the-model">4.1 Quantize the model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_cnn_flow.html#id2">4.2 Performance check</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_cnn_flow.html#convert-trained-model-for-akida-and-test">5. Convert trained model for Akida and test</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_cnn_flow.html#final-conversion">5.1 Final conversion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_cnn_flow.html#performances-check-with-the-akida-execution-engine">5.2 Performances check with the Akida Execution Engine</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/Brainchip-Inc/akida_examples/releases">Changelog</a></li>
<li class="toctree-l1"><a class="reference external" href="https://support.brainchip.com/portal/home">Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Akida Examples</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="user_guide.html">User guide</a> &raquo;</li>
        
      <li>CNN2SNN toolkit</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="cnn2snn-toolkit">
<h1>CNN2SNN toolkit<a class="headerlink" href="#cnn2snn-toolkit" title="Permalink to this headline">¶</a></h1>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>The Brainchip CNN2SNN toolkit provides means to convert Convolutional Neural
Networks (CNN) that were trained using Deep Learning methods to a low-latency
and low-power Spiking Neural Network (SNN) for use with the Akida Execution
Engine. This document is a guide to that process.</p>
<p>The Akida Execution Engine provides Spiking Neural Networks (SNN) in which
communications between neurons take the form of “spikes” or impulses that are
generated when a neuron exceeds a threshold level of activation. Neurons that
do not cross the threshold generate no output and contribute no further
computational cost downstream. This feature is key to Akida hardware efficiency.
The Akida Execution Engine further extends this efficiency by operating with low
bitwidth “synapses” or weights of connections between neurons.</p>
<p>Despite the apparent fundamental differences between SNNs and CNNs, the
underlying mathematical operations performed by each may be rendered identical.
Consequently, the trained parameters of a CNN can be converted to be
Akida-compatible, given only a small number of constraints. By careful
attention to specifics in the architecture and training of the CNN, an overly
complex conversion step from CNN to SNN can be avoided. The CNN2SNN toolkit
comprises a set of functions designed for the popular <a class="reference external" href="https://www.tensorflow.org/guide/keras">Tensorflow Keras</a> framework, making it easy to train a
SNN-compatible network.</p>
<div class="section" id="conversion-workflow">
<h3>Conversion workflow<a class="headerlink" href="#conversion-workflow" title="Permalink to this headline">¶</a></h3>
<a class="reference external image-reference" href="../_images/CNN2SNN_Flow.png"><img alt="CNN2SNN Flow" src="../_images/CNN2SNN_Flow.png" style="width: 610.8px; height: 498.59999999999997px;" /></a>
</div>
<div class="section" id="typical-training-scenario">
<h3>Typical training scenario<a class="headerlink" href="#typical-training-scenario" title="Permalink to this headline">¶</a></h3>
<p>The first step in the conversion workflow is to train a standard Keras model.
This trained model is the starting point for the quantization stage. Once it is
established that the overall model configuration prior to quantization yields a
satisfactory performance on the task, we can proceed with quantization.</p>
<p>The CNN2SNN toolkit offers a turnkey solution to quantize a model:
the <a class="reference external" href="../api_reference/cnn2snn_apis.html#quantize">quantize</a> function. It
replaces the neural Keras layers (Conv2D, SeparableConv2D and Dense) and
the ReLU layers with custom CNN2SNN layers, which are quantization-aware
derived versions of the base Keras layer types. The obtained quantized model is
still a Keras model with a mix of CNN2SNN quantized layers (QuantizedReLU,
QuantizedDense, etc.) and standard Keras layers (BatchNormalization, MaxPool2D,
etc.).</p>
<p>Direct quantization of a standard Keras model (also called post-training
quantization) generally introduces a drop in performance. This drop is usually
small for 8-bit or even 4-bit quantization of simple models, but it can be very
significant for low quantization bitwidth and complex models.</p>
<p>If the quantized model offers acceptable performance, it can be directly
converted into an Akida model, ready to be loaded on the Akida NSoC (see the
<a class="reference external" href="../api_reference/cnn2snn_apis.html#convert">convert</a> function).</p>
<p>However, if the performance drop is too high, a quantization-aware training is
required to recover the performance prior to quantization. Since the quantized
model is a Keras model, it can then be trained using the standard Keras API.</p>
<p>Note that quantizing directly to the target bitwidth is not mandatory: it is
possible to proceed with quantization in a serie of smaller steps.
For example, it may be beneficial to keep float weights and only quantize
activations, retrain, and then, quantize weights.</p>
</div>
<div class="section" id="design-compatibility-constraints">
<h3>Design compatibility constraints<a class="headerlink" href="#design-compatibility-constraints" title="Permalink to this headline">¶</a></h3>
<p>When designing a tf.keras model, consider design compatibility at these
distinct levels before the quantization stage:</p>
<ul class="simple">
<li><p>Only serial and feedforward arrangements can be converted<a class="footnote-reference brackets" href="#fn-2" id="id1">2</a>.</p></li>
<li><p>Supported Keras layers are listed <a class="reference external" href="#supported-layer-types">below</a>.</p></li>
<li><p>Order of the layers is important, e.g. a BatchNormalization layer
must be placed before the activation, and not after.</p></li>
<li><p>Some constraints are needed about layer’s parameters, e.g. a MaxPool2D layer
must have the same padding as its corresponding convolutional layer.</p></li>
</ul>
<p>All these design compatibility constraints are summarized in the CNN2SNN
<a class="reference external" href="../api_reference/cnn2snn_apis.html#check-model-compatibility">check_model_compatibility</a>
function. A good practice is to check model compatibility before going through
the training process <a class="footnote-reference brackets" href="#fn-3" id="id2">3</a>.</p>
<p>Helpers (see <a class="reference internal" href="#layer-blocks"><span class="std std-ref">Layer Blocks</span></a>) are available in the <code class="docutils literal notranslate"><span class="pre">akida_models</span></code>
PyPi package to easily create a compatible Keras model from scratch.</p>
</div>
<div class="section" id="quantization-compatibility-constraints">
<h3>Quantization compatibility constraints<a class="headerlink" href="#quantization-compatibility-constraints" title="Permalink to this headline">¶</a></h3>
<p>In addition to the model design constraints, the Akida NSoC has several
quantization constraints:</p>
<ul class="simple">
<li><p>Weights of the neural layers must be quantized using
<a class="reference external" href="hw_constraints.html">1, 2, 3, 4 or 8 bits</a>.</p></li>
<li><p>Activations should be quantized too using 1, 2 or 4 bits, with maximum spike
value set to 15<a class="footnote-reference brackets" href="#fn-4" id="id3">4</a>.</p></li>
<li><p>Every neural layer accepts inputs with different quantization parameters,
e.g. a quantized Dense layer can only accept 1-bit or 2-bit inputs.</p></li>
</ul>
<p>Please refer to the <a class="reference external" href="hw_constraints.html">Hardware constraints</a> page for full
details.</p>
</div>
<div class="section" id="command-line-interface">
<h3>Command-line interface<a class="headerlink" href="#command-line-interface" title="Permalink to this headline">¶</a></h3>
<p>In addition to the cnn2snn programming API, the CNN2SNN toolkit also provides a
command-line interface to perform quantization and conversion to an Akida NSoC
compatible model.</p>
<p>Quantizing a standard Keras model or a CNN2SNN quantized model using the CLI
makes use of the <code class="docutils literal notranslate"><span class="pre">cnn2snn.quantize</span></code> Python function. The same arguments, i.e.
the quantization bitwidths for weights and activations, are required.</p>
<p><strong>Examples</strong></p>
<p>Quantize a standard Keras model with 4-bit weights and activations and 8-bit
input weights:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>cnn2snn -m model_keras.h5 quantize -wq <span class="m">4</span> -aq <span class="m">4</span> -iq <span class="m">8</span>
</pre></div>
</div>
<p>The quantized model is automatically saved to <code class="docutils literal notranslate"><span class="pre">model_keras_iq8_wq4_aq4.h5</span></code>.</p>
<p>Quantize an already quantized model with different quantization bitwidths:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>cnn2snn -m model_keras_iq8_wq4_aq4.h5 -wq <span class="m">2</span> -aq <span class="m">2</span>
</pre></div>
</div>
<p>A new quantized model named <code class="docutils literal notranslate"><span class="pre">model_keras_iq2_wq2_aq2.h5</span></code> is saved.</p>
<p>Converting a CNN2SNN quantized model into an Akida model using the CLI makes use
of the <code class="docutils literal notranslate"><span class="pre">cnn2snn.convert</span></code> Python function. The same arguments, i.e.
the input scaling and whether the inputs are sparse, are required.</p>
<p><strong>Examples</strong></p>
<p>Convert a quantized model without input scaling and with image inputs:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>cnn2snn -m model_keras_iq2_wq2_aq2.h5 convert
</pre></div>
</div>
<p>An Akida .fbz model named <code class="docutils literal notranslate"><span class="pre">model_keras_iq2_wq2_aq2.fbz</span></code> is then saved.</p>
<p>Convert a quantized model with input scaling of (255, 0) and with sparse inputs:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>cnn2snn -m model_keras_iq2_wq2_aq2.h5 convert -sc <span class="m">255</span> -sh <span class="m">0</span> -sp True
</pre></div>
</div>
</div>
</div>
<div class="section" id="layers-considerations">
<h2>Layers Considerations<a class="headerlink" href="#layers-considerations" title="Permalink to this headline">¶</a></h2>
<div class="section" id="supported-layer-types">
<h3>Supported layer types<a class="headerlink" href="#supported-layer-types" title="Permalink to this headline">¶</a></h3>
<p>The CNN2SNN toolkit provides quantization of Keras models with the following
Keras layer types:</p>
<ul class="simple">
<li><p><strong>Core Neural Layers</strong>:</p>
<ul>
<li><p>tf.keras <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense">Dense</a></p></li>
<li><p>tf.keras <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D">Conv2D</a></p></li>
</ul>
</li>
<li><p><strong>Specialized Layers</strong>:</p>
<ul>
<li><p>tf.keras <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/SeparableConv2D">SeparableConv2D</a></p></li>
</ul>
</li>
<li><p><strong>Other Layers (from tf.keras)</strong>:</p>
<ul>
<li><p>ReLU</p></li>
<li><p>BatchNormalization</p></li>
<li><p>MaxPooling2D</p></li>
<li><p>GlobalAveragePooling2D</p></li>
<li><p>Dropout</p></li>
<li><p>Flatten</p></li>
<li><p>Reshape</p></li>
<li><p>Input</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="cnn2snn-quantization-aware-layers">
<h3>CNN2SNN Quantization-aware layers<a class="headerlink" href="#cnn2snn-quantization-aware-layers" title="Permalink to this headline">¶</a></h3>
<p>Several articles have reported<a class="footnote-reference brackets" href="#fn-5" id="id4">5</a> that the quantization of a pre-trained
float Keras model using 8-bit precision can be performed with a minimal loss
of accuracy for simple models, but that for lower bitwidth or complex models a
quantization-aware re-training of the quantized model may be required.</p>
<p>The CNN2SNN toolkit therefore includes quantization-aware versions of the base
Keras layers.</p>
<p>These layers are produced when quantizing a standard Keras model using the
<code class="docutils literal notranslate"><span class="pre">quantize</span></code> function: it replaces the base Keras layers with their quantization-aware
counterparts (see the <a class="reference external" href="../api_reference/cnn2snn_apis.html#quantize">quantize</a> function).</p>
<p>Quantization-aware training simulates the effect of quantization in the forward
pass, yet using a straight-through estimator for the quantization gradient in
the backward pass.
For the stochastic gradient descent to be efficient, the weights are stored as
float values and updated with high precision during back propagation.
This ensures sufficient precision in accumulating tiny weights adjustments.</p>
<p>The CNN2SNN toolkit includes two classes of quantization-aware layers:</p>
<ul class="simple">
<li><p><strong>quantized processing layers</strong>:</p>
<ul>
<li><p><a class="reference external" href="../api_reference/cnn2snn_apis.html#quantizeddense">QuantizedDense</a>,</p></li>
<li><p><a class="reference external" href="../api_reference/cnn2snn_apis.html#quantizedconv2d">QuantizedConv2D</a>,</p></li>
<li><p><a class="reference external" href="../api_reference/cnn2snn_apis.html#quantizedseparableconv2d">QuantizedSeparableConv2D</a></p></li>
</ul>
</li>
<li><p><strong>quantized activation layers</strong>:</p>
<ul>
<li><p><a class="reference external" href="../api_reference/cnn2snn_apis.html#quantizedrelu">QuantizedReLU</a></p></li>
</ul>
</li>
</ul>
<p>Most of the parameters for the quantized processing layers are identical to
those used when defining a model using standard Keras layers. However, each of
these layers also includes a <code class="docutils literal notranslate"><span class="pre">quantizer</span></code> parameter that specifies the
<a class="reference external" href="../api_reference/cnn2snn_apis.html#weightquantizer">WeightQuantizer</a>
object to use during the quantization-aware training.</p>
<p>The quantized ReLU takes a single parameter corresponding to the
bitwidth of the quantized activations.</p>
</div>
<div class="section" id="training-only-layers">
<h3>Training-Only Layers<a class="headerlink" href="#training-only-layers" title="Permalink to this headline">¶</a></h3>
<p>The Akida Execution Engine is used in CNN conversion for inference only.
Training is done within the Keras environment and training-only layers may be
added at will, such as BatchNormalization or Dropout layers. These are handled
fully by Keras during the training and do not need to be modified to be
Akida-compatible for inference.</p>
<p>As regards the implementation within the Akida Execution Engine: it may be
helpful to understand that the associated scaling operations (multiplication and
shift) are never performed during inference. The computational cost is reduced
by wrapping the (optional) batch normalization function and quantized activation
function into the spike generating thresholds and other parameters of the Akida
SNN.
That process is completely transparent to the user. It does, however, have an
important consequence for the output of the final layer of the model; see
<a class="reference external" href="#id6">Final Layers</a> below.</p>
</div>
<div class="section" id="first-layers">
<h3>First Layers<a class="headerlink" href="#first-layers" title="Permalink to this headline">¶</a></h3>
<p>Most layers of an Akida model only accept sparse inputs.
In order to support the most common classes of models in computer vision, a
special layer (<a class="reference external" href="../api_reference/aee_apis.html#inputconvolutional">InputConvolutional</a>)
is however able to receive image data (8-bit grayscale or RGB). See the
<a class="reference external" href="aee.html">Akida user guide</a> for further details.</p>
<p>The CNN2SNN toolkit supports any quantization-aware training layer as the first
layer in the model. The type of input accepted by that layer can be specified
during conversion, but only models starting with a QuantizedConv2D layer will
accept dense inputs, thanks to the special <a class="reference external" href="../api_reference/aee_apis.html#inputconvolutional">InputConvolutional</a>
layer.</p>
<div class="section" id="input-scaling">
<h4>Input Scaling<a class="headerlink" href="#input-scaling" title="Permalink to this headline">¶</a></h4>
<p>The <a class="reference external" href="../api_reference/aee_apis.html#inputconvolutional">InputConvolutional</a>
layer only receives 8-bit input values:</p>
<ul class="simple">
<li><p>if the data is already in 8-bit format it can be sent to the Akida inputs
without rescaling.</p></li>
<li><p>if the data has been scaled to ease training, it is necessary to provide the
scaling coefficients at model conversion.</p></li>
</ul>
<p>This applies to the common case where input data are natively 8-bit. If input
data are not 8-bit, the process is more complex, and we recommend applying
rescaling in two steps:</p>
<ol class="arabic simple">
<li><p>Taking the data to an 8-bit unsigned integer format suitable for Akida
architecture. Apply this step both for training and inference data.</p></li>
<li><p>Rescaling the 8-bit values to some unit or zero centered range suitable for
CNN training, as above. This step should only be applied for the CNN training.
Also, remember to provide those scaling coefficients when converting the
trained model to an Akida-compatible format.</p></li>
</ol>
</div>
</div>
<div class="section" id="id6">
<h3>Final Layers<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h3>
<p>As is typical for CNNs, the final layer of a model does not include the
standard activation nonlinearity. If that is the case, once converted to Akida
hardware, the model will give the potentials levels and in most cases, taking the
maximum among these values is sufficient to obtain the correct response from
the model.
However, if there is a difference in performance between the Keras and the
Akida-compatible implementations of the model, it is likely be at this step.</p>
</div>
</div>
<div class="section" id="layer-blocks">
<span id="id7"></span><h2>Layer Blocks<a class="headerlink" href="#layer-blocks" title="Permalink to this headline">¶</a></h2>
<p>Ensuring that the design of a Keras model is compatible for conversion into
an Akida model can be tricky. Therefore, a higher-level interface is proposed
with the use of layer blocks. These blocks are available in the
<code class="docutils literal notranslate"><span class="pre">akida_models</span></code> PyPi package:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">akida_models.layer_blocks</span>
</pre></div>
</div>
<div class="section" id="id8">
<h3>Overview<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h3>
<p>In Keras, when adding a core layer type (<code class="docutils literal notranslate"><span class="pre">Dense</span></code> or <code class="docutils literal notranslate"><span class="pre">Conv2D</span></code>) to a
model, an activation function is typically included:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p>or the equivalent, explicitly adding the activation function separately:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p>It is very common for other functions to be included in this arrangement, e.g.,
a normalization of values before applying the activation function:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p>This particular arrangement of layers is important for conversion and is
therefore reflected in the blocks API.</p>
<p>For instance, the following code snippet sets up the same trio of layers as
those above:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">dense_block</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">add_batchnorm</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">dense_block</span></code> function will produce a group of layers that we call a
“block”.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To avoid adding the activation layer, add the parameter
<code class="docutils literal notranslate"><span class="pre">add_activation</span> <span class="pre">=</span> <span class="pre">False</span></code> to the block.</p>
</div>
<p>The option of including pooling, batchnorm layers or activation is directly
built into the provided block modules.
The layer block functions provided are:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">conv_block</span></code>,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">separable_conv_block</span></code>,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dense_block</span></code>.</p></li>
</ul>
<p>Most of the parameters for these blocks are identical to those passed to the
corresponding inner processing layers, such as strides and bias.</p>
</div>
<div class="section" id="conv-block">
<h3><code class="docutils literal notranslate"><span class="pre">conv_block</span></code><a class="headerlink" href="#conv-block" title="Permalink to this headline">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">conv_block</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span>
               <span class="n">filters</span><span class="p">,</span>
               <span class="n">kernel_size</span><span class="p">,</span>
               <span class="n">pooling</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
               <span class="n">add_batchnorm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
               <span class="n">add_activation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
               <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</pre></div>
</div>
</div>
<div class="section" id="dense-block">
<h3><code class="docutils literal notranslate"><span class="pre">dense_block</span></code><a class="headerlink" href="#dense-block" title="Permalink to this headline">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">dense_block</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span>
                <span class="n">units</span><span class="p">,</span>
                <span class="n">add_batchnorm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">add_activation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="separable-conv-block">
<h3><code class="docutils literal notranslate"><span class="pre">separable_conv_block</span></code><a class="headerlink" href="#separable-conv-block" title="Permalink to this headline">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">separable_conv_block</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span>
                         <span class="n">filters</span><span class="p">,</span>
                         <span class="n">kernel_size</span><span class="p">,</span>
                         <span class="n">pooling</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                         <span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
                         <span class="n">add_batchnorm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                         <span class="n">add_activation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="tips-and-tricks">
<h2>Tips and Tricks<a class="headerlink" href="#tips-and-tricks" title="Permalink to this headline">¶</a></h2>
<p>In some cases, it may be useful to adapt existing CNN models in order to
simplify or enhance the converted SNN. Here’s a short list of some possible
substitutions that might come in handy:</p>
<ul class="simple">
<li><p><a class="reference external" href="http://cs231n.github.io/convolutional-networks/#convert">Substitute a fully connected layer with a convolutional layer</a>.</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1412.6806">Substitute a convolutional layer with stride 2 with a convolutional layer
with stride 1 in combination with an additional pooling layer</a>.</p></li>
<li><p><a class="reference external" href="http://cs231n.github.io/convolutional-networks/">Substitute a convolutional layer that has 1 large filter with multiple
convolutional layers that contain smaller filters</a>.</p></li>
</ul>
<hr class="docutils" />
<dl class="footnote brackets">
<dt class="label" id="fn-1"><span class="brackets">1</span></dt>
<dd><p>Sparsity refers to the fraction of both weights and activations with
value zero.</p>
</dd>
<dt class="label" id="fn-2"><span class="brackets"><a class="fn-backref" href="#id1">2</a></span></dt>
<dd><p>Parallel layers and “residual” connections are currently not
supported.</p>
</dd>
<dt class="label" id="fn-3"><span class="brackets"><a class="fn-backref" href="#id2">3</a></span></dt>
<dd><p>Check model compatibility must be applied on a quantized model. It
then requires to quantize the model first.</p>
</dd>
<dt class="label" id="fn-4"><span class="brackets"><a class="fn-backref" href="#id3">4</a></span></dt>
<dd><p>The spike value depends on the intensity of the potential, see the
<a class="reference external" href="aee.html">Akida documentation</a> for details on the activation.</p>
</dd>
<dt class="label" id="fn-5"><span class="brackets"><a class="fn-backref" href="#id4">5</a></span></dt>
<dd><p>See for instance <a class="reference external" href="https://arxiv.org/pdf/1806.08342.pdf">“Quantizing deep convolutional networks for
efficient inference: A whitepaper”</a>
- Raghuraman Krishnamoorthi, 2018</p>
</dd>
</dl>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="hw_constraints.html" class="btn btn-neutral float-right" title="Hardware constraints" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="aee.html" class="btn btn-neutral float-left" title="Akida user guide" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright Copyright 2020, BrainChip Holdings Ltd. All Rights Reserved.

    </p>
  </div> 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>