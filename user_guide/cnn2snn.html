<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>CNN2SNN toolkit &mdash; Akida Examples  documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery-binder.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery-dataframe.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery-rendered-html.css" type="text/css" />
      <link rel="stylesheet" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" type="text/css" />
      <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/design-tabs.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Akida models zoo" href="akida_models.html" />
    <link rel="prev" title="QuantizeML toolkit" href="quantizeml.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #989898" >

          
          
          <a href="../index.html">
            
              <img src="../_static/MetaTF_logo.png" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                Akida, 2nd Generation
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../index.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../installation.html#supported-configurations">Supported configurations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../installation.html#quick-installation">Quick installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../installation.html#running-examples">Running examples</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="user_guide.html">User guide</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="akida.html">Akida user guide</a><ul>
<li class="toctree-l3"><a class="reference internal" href="akida.html#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="akida.html#programming-interface">Programming interface</a><ul>
<li class="toctree-l4"><a class="reference internal" href="akida.html#the-akida-model">The Akida Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida.html#akida-layers">Akida layers</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="akida.html#model-hardware-mapping">Model Hardware Mapping</a><ul>
<li class="toctree-l4"><a class="reference internal" href="akida.html#devices">Devices</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida.html#model-mapping">Model mapping</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida.html#advanced-mapping-details-and-hardware-devices-usage">Advanced Mapping Details and Hardware Devices Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida.html#performance-measurement">Performance measurement</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="akida.html#using-akida-edge-learning">Using Akida Edge learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="akida.html#learning-constraints">Learning constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida.html#compiling-a-layer">Compiling a layer</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="quantizeml.html">QuantizeML toolkit</a><ul>
<li class="toctree-l3"><a class="reference internal" href="quantizeml.html#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="quantizeml.html#the-fixedpoint-representation">The FixedPoint representation</a></li>
<li class="toctree-l3"><a class="reference internal" href="quantizeml.html#quantization-flow">Quantization flow</a><ul>
<li class="toctree-l4"><a class="reference internal" href="quantizeml.html#compatibility-constraints">Compatibility constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="quantizeml.html#model-loading">Model loading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="quantizeml.html#command-line-interface">Command line interface</a><ul>
<li class="toctree-l4"><a class="reference internal" href="quantizeml.html#quantize-cli">quantize CLI</a></li>
<li class="toctree-l4"><a class="reference internal" href="quantizeml.html#config-cli">config CLI</a></li>
<li class="toctree-l4"><a class="reference internal" href="quantizeml.html#check-cli">check CLI</a></li>
<li class="toctree-l4"><a class="reference internal" href="quantizeml.html#insert-rescaling-cli">insert_rescaling CLI</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="quantizeml.html#supported-layer-types">Supported layer types</a></li>
</ul>
</li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">CNN2SNN toolkit</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#conversion-flow">Conversion flow</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#conversion-compatibility">Conversion compatibility</a></li>
<li class="toctree-l4"><a class="reference internal" href="#command-line-interface">Command-line interface</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#handling-akida-1-0-and-akida-2-0-specificities">Handling Akida 1.0 and Akida 2.0 specificities</a></li>
<li class="toctree-l3"><a class="reference internal" href="#legacy-quantization-api">Legacy quantization API</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#typical-quantization-scenario">Typical quantization scenario</a></li>
<li class="toctree-l4"><a class="reference internal" href="#design-compatibility-constraints">Design compatibility constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id3">Command-line interface</a></li>
<li class="toctree-l4"><a class="reference internal" href="#layers-considerations">Layers Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="#tips-and-tricks">Tips and Tricks</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="akida_models.html">Akida models zoo</a><ul>
<li class="toctree-l3"><a class="reference internal" href="akida_models.html#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="akida_models.html#command-line-interface-for-model-creation">Command-line interface for model creation</a></li>
<li class="toctree-l3"><a class="reference internal" href="akida_models.html#command-line-interface-for-model-training">Command-line interface for model training</a><ul>
<li class="toctree-l4"><a class="reference internal" href="akida_models.html#kws-training">KWS training</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida_models.html#akidanet-training">AkidaNet training</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="akida_models.html#command-line-interface-for-model-evaluation">Command-line interface for model evaluation</a></li>
<li class="toctree-l3"><a class="reference internal" href="akida_models.html#command-line-interface-to-evaluate-model-macs">Command-line interface to evaluate model MACS</a></li>
<li class="toctree-l3"><a class="reference internal" href="akida_models.html#id1">Layer Blocks</a></li>
<li class="toctree-l3"><a class="reference internal" href="akida_models.html#handling-akida-1-0-and-akida-2-0-specificities">Handling Akida 1.0 and Akida 2.0 specificities</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="hw_constraints.html">Hardware constraints</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../api_reference/api_reference.html">API reference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../api_reference/akida_apis.html">Akida runtime</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_apis.html#model">Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_apis.html#layer">Layer</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_apis.html#id1">Layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_apis.html#mapping">Mapping</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_apis.html#akida-v1-layers">Akida V1 layers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_apis.html#akida-v2-layers">Akida V2 layers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_apis.html#layer-parameters">Layer parameters</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_apis.html#layertype">LayerType</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_apis.html#padding">Padding</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_apis.html#pooltype">PoolType</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_apis.html#optimizers">Optimizers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_apis.html#sequence">Sequence</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_apis.html#id2">Sequence</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_apis.html#backendtype">BackendType</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_apis.html#pass">Pass</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_apis.html#device">Device</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_apis.html#id3">Device</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_apis.html#hwversion">HwVersion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_apis.html#hwdevice">HWDevice</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_apis.html#id4">HWDevice</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_apis.html#socdriver">SocDriver</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_apis.html#clockmode">ClockMode</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_apis.html#powermeter">PowerMeter</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_apis.html#np">NP</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_apis.html#tools">Tools</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_apis.html#sparsity">Sparsity</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api_reference/cnn2snn_apis.html">CNN2SNN</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#akida-version">Akida version</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#conversion">Conversion</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#legacy-quantization-api">Legacy quantization API</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#utils">Utils</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#calibration">Calibration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#transforms">Transforms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#constraint">Constraint</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#quantization">Quantization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#quantizers">Quantizers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#quantized-layers">Quantized layers</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api_reference/quantizeml_apis.html">QuantizeML</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/quantizeml_apis.html#layers">Layers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#reshaping">Reshaping</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#activations">Activations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#attention">Attention</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#normalization">Normalization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#convolution">Convolution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#depthwise-convolution">Depthwise convolution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#separable-convolution">Separable convolution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#dense">Dense</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#skip-connection">Skip connection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#pooling">Pooling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#shiftmax">Shiftmax</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#transformers">Transformers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#rescaling">Rescaling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#dropout">Dropout</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#quantizers">Quantizers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#quantization-parameters">Quantization parameters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#calibration">Calibration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#recording">Recording</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/quantizeml_apis.html#models">Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#transforms">Transforms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#quantization">Quantization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#id1">Calibration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#utils">Utils</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/quantizeml_apis.html#tensors">Tensors</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#qtensor">QTensor</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#fixedpoint">FixedPoint</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#qfloat">QFloat</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api_reference/akida_models_apis.html">Akida models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_models_apis.html#layer-blocks">Layer blocks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#cnn-blocks">CNN blocks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#transformers-blocks">Transformers blocks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#transposed-blocks">Transposed blocks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#detection-block">Detection block</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_models_apis.html#helpers">Helpers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#gamma-constraint">Gamma constraint</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#unfusing-separableconvolutional">Unfusing SeparableConvolutional</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#extract-samples">Extract samples</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_models_apis.html#knowledge-distillation">Knowledge distillation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_models_apis.html#macs">MACS</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_models_apis.html#model-i-o">Model I/O</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_models_apis.html#utils">Utils</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_models_apis.html#model-zoo">Model zoo</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#akidanet">AkidaNet</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#mobilenet">Mobilenet</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#ds-cnn">DS-CNN</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#vgg">VGG</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#yolo">YOLO</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#pointnet">PointNet++</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#gxnor">GXNOR</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#centernet">CenterNet</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#akidaunet">AkidaUNet</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#transformers">Transformers</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../examples/index.html">Examples</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../examples/index.html#general-examples">General examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_0_global_workflow.html">Global Akida workflow</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_0_global_workflow.html#create-and-train">1. Create and train</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_0_global_workflow.html#quantize">2. Quantize</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_0_global_workflow.html#convert">3. Convert</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_0_global_workflow.html#gxnor-mnist">4. GXNOR/MNIST</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_1_akidanet_imagenet.html">AkidaNet/ImageNet inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_1_akidanet_imagenet.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_1_akidanet_imagenet.html#pretrained-quantized-model">2. Pretrained quantized model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_1_akidanet_imagenet.html#conversion-to-akida">3. Conversion to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_1_akidanet_imagenet.html#hardware-mapping-and-performance">4. Hardware mapping and performance</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_2_ds_cnn_kws.html">DS-CNN/KWS inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_2_ds_cnn_kws.html#load-the-preprocessed-dataset">1. Load the preprocessed dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_2_ds_cnn_kws.html#load-a-pre-trained-native-keras-model">2. Load a pre-trained native Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_2_ds_cnn_kws.html#load-a-pre-trained-quantized-keras-model">3. Load a pre-trained quantized Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_2_ds_cnn_kws.html#conversion-to-akida">4. Conversion to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_2_ds_cnn_kws.html#confusion-matrix">5. Confusion matrix</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_3_regression.html">Age estimation (regression) example</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_3_regression.html#load-the-utkface-dataset">1. Load the UTKFace Dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_3_regression.html#load-a-pre-trained-native-keras-model">2. Load a pre-trained native Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_3_regression.html#load-a-pre-trained-quantized-keras-model">3. Load a pre-trained quantized Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_3_regression.html#conversion-to-akida">4. Conversion to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_3_regression.html#estimate-age-on-a-single-image">5. Estimate age on a single image</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_4_transfer_learning.html">Transfer learning with AkidaNet for PlantVillage</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_transfer_learning.html#transfer-learning-process">Transfer learning process</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_transfer_learning.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_transfer_learning.html#get-a-trained-akidanet-base-model">2. Get a trained AkidaNet base model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_transfer_learning.html#add-a-classification-head-to-the-model">3. Add a classification head to the model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_transfer_learning.html#freeze-the-base-model">4. Freeze the base model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_transfer_learning.html#train-for-a-few-epochs">5. Train for a few epochs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_transfer_learning.html#quantize-the-model">6. Quantize the model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_transfer_learning.html#compute-accuracy">7. Compute accuracy</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_5_voc_yolo_detection.html">YOLO/PASCAL-VOC detection tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_5_voc_yolo_detection.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_5_voc_yolo_detection.html#preprocessing-tools">2. Preprocessing tools</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_5_voc_yolo_detection.html#model-architecture">3. Model architecture</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_5_voc_yolo_detection.html#training">4. Training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_5_voc_yolo_detection.html#performance">5. Performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_5_voc_yolo_detection.html#conversion-to-akida">6. Conversion to Akida</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_6_segmentation.html">Segmentation tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_6_segmentation.html#load-the-dataset">1. Load the dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_6_segmentation.html#load-a-pre-trained-native-keras-model">2. Load a pre-trained native Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_6_segmentation.html#load-a-pre-trained-quantized-keras-model">3. Load a pre-trained quantized Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_6_segmentation.html#conversion-to-akida">4. Conversion to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_6_segmentation.html#segment-a-single-image">5. Segment a single image</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_7_vision_transformer.html">Build Vision Transformers for Akida</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_7_vision_transformer.html#model-selection">1. Model selection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_7_vision_transformer.html#model-optimization-for-akida-hardware">2. Model optimization for Akida hardware</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_7_vision_transformer.html#model-training">3. Model Training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_7_vision_transformer.html#model-quantization">4. Model quantization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_7_vision_transformer.html#conversion-to-akida">5. Conversion to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_7_vision_transformer.html#displaying-results-attention-maps">6. Displaying results Attention Maps</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/index.html#quantization">Quantization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/quantization/plot_0_advanced_quantizeml.html">Advanced QuantizeML tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/quantization/plot_0_advanced_quantizeml.html#defining-a-quantization-scheme">1. Defining a quantization scheme</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/quantization/plot_0_advanced_quantizeml.html#calibration">2. Calibration</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/quantization/plot_1_upgrading_to_2.0.html">Upgrading to Akida 2.0</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/quantization/plot_1_upgrading_to_2.0.html#workflow-differences">1. Workflow differences</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/quantization/plot_1_upgrading_to_2.0.html#models-architecture-differences">2. Models architecture differences</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/quantization/plot_1_upgrading_to_2.0.html#using-akidaversion">3. Using <code class="docutils literal notranslate"><span class="pre">AkidaVersion</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/index.html#edge-examples">Edge examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/edge/plot_0_edge_learning_vision.html">Akida vision edge learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_0_edge_learning_vision.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_0_edge_learning_vision.html#prepare-akida-model-for-learning">2. Prepare Akida model for learning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_0_edge_learning_vision.html#edge-learning-with-akida">3. Edge learning with Akida</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/edge/plot_1_edge_learning_kws.html">Akida edge learning for keyword spotting</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_1_edge_learning_kws.html#edge-learning-process">1. Edge learning process</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_1_edge_learning_kws.html#dataset-preparation">2. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_1_edge_learning_kws.html#prepare-akida-model-for-learning">3. Prepare Akida model for learning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_1_edge_learning_kws.html#learn-with-akida-using-the-training-set">4. Learn with Akida using the training set</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_1_edge_learning_kws.html#edge-learning">5. Edge learning</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/edge/plot_2_edge_learning_parameters.html">Tips to set Akida learning parameters</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_2_edge_learning_parameters.html#akida-learning-parameters">1. Akida learning parameters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_2_edge_learning_parameters.html#create-akida-model">2. Create Akida model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_2_edge_learning_parameters.html#estimate-the-required-number-of-weights-of-the-trainable-layer">3. Estimate the required number of weights of the trainable layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_2_edge_learning_parameters.html#estimate-the-number-of-neurons-per-class">4. Estimate the number of neurons per class</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/index.html#deprecated-cnn2snn-tutorials">[Deprecated] CNN2SNN tutorials</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/cnn2snn/plot_1_advanced_cnn2snn.html">Advanced CNN2SNN tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_1_advanced_cnn2snn.html#design-a-cnn2snn-quantized-model">1. Design a CNN2SNN quantized model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_1_advanced_cnn2snn.html#weight-quantizer-details">2. Weight Quantizer Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_1_advanced_cnn2snn.html#understanding-quantized-activation">3. Understanding quantized activation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_1_advanced_cnn2snn.html#how-to-deal-with-too-high-scale-factors">4. How to deal with too high scale factors</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../model_zoo_performance.html">Model zoo performance</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../model_zoo_performance.html#akida-1-0-models">Akida 1.0 models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../model_zoo_performance.html#image-icon-ref-image-domain"> Image domain</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../model_zoo_performance.html#classification">Classification</a></li>
<li class="toctree-l4"><a class="reference internal" href="../model_zoo_performance.html#object-detection">Object detection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../model_zoo_performance.html#regression">Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="../model_zoo_performance.html#face-recognition">Face recognition</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../model_zoo_performance.html#audio-icon-ref-audio-domain"> Audio domain</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../model_zoo_performance.html#keyword-spotting">Keyword spotting</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../model_zoo_performance.html#pointcloud-icon-ref-point-cloud"> Point cloud</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../model_zoo_performance.html#id1">Classification</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../model_zoo_performance.html#akida-2-0-models">Akida 2.0 models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../model_zoo_performance.html#id2"> Image domain</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../model_zoo_performance.html#id3">Classification</a></li>
<li class="toctree-l4"><a class="reference internal" href="../model_zoo_performance.html#id6">Object detection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../model_zoo_performance.html#id7">Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="../model_zoo_performance.html#id8">Face recognition</a></li>
<li class="toctree-l4"><a class="reference internal" href="../model_zoo_performance.html#segmentation">Segmentation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../model_zoo_performance.html#id10"> Audio domain</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../model_zoo_performance.html#id11">Keyword spotting</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../model_zoo_performance.html#id12"> Point cloud</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../model_zoo_performance.html#id13">Classification</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../changelog.html">Changelog</a></li>
<li class="toctree-l1"><a class="reference external" href="https://support.brainchip.com/portal/home">Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #989898" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Akida Examples</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="user_guide.html">User guide</a></li>
      <li class="breadcrumb-item active">CNN2SNN toolkit</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="cnn2snn-toolkit">
<h1>CNN2SNN toolkit<a class="headerlink" href="#cnn2snn-toolkit" title="Permalink to this headline"></a></h1>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>The Brainchip CNN2SNN toolkit provides a means to convert a quantized model obtained using
QuantizeML to a low-latency and low-power network for use with the Akida runtime.</p>
</section>
<section id="conversion-flow">
<h2>Conversion flow<a class="headerlink" href="#conversion-flow" title="Permalink to this headline"></a></h2>
<p>CNN2SNN offers a simple <a class="reference external" href="../api_reference/cnn2snn_apis.html#cnn2snn.convert">convert</a> function
that takes a quantized model as input and converts it into an Akida runtime compatible network.</p>
<p>Let’s take the <a class="reference external" href="../api_reference/akida_models_apis.html#ds-cnn">DS-CNN</a> model from our zoo that
targets KWS task as an example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">akida_models</span> <span class="kn">import</span> <span class="n">ds_cnn_kws_pretrained</span>
<span class="kn">from</span> <span class="nn">cnn2snn</span> <span class="kn">import</span> <span class="n">convert</span>

<span class="c1"># Load a pretrained 8/4/4 quantized model</span>
<span class="n">quantized_model</span> <span class="o">=</span> <span class="n">ds_cnn_kws_pretrained</span><span class="p">()</span>
<span class="n">model_akida</span> <span class="o">=</span> <span class="n">convert</span><span class="p">(</span><span class="n">quantized_model</span><span class="p">)</span>
</pre></div>
</div>
<section id="conversion-compatibility">
<h3>Conversion compatibility<a class="headerlink" href="#conversion-compatibility" title="Permalink to this headline"></a></h3>
<p>It is possible to check if a quantized model is compatible with Akida conversion using the
<a class="reference external" href="../api_reference/cnn2snn_apis.html#cnn2snn.quantizeml.compatibility_checks.check_model_compatibility">check_model_compatibility</a>
helper. Note that this helper will not convert the model or check compatibility with an Akida
hardware, it only checks that the model quantization scheme is allowed and that building blocks are
compatible with Akida layers blocks.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference external" href="../api_reference/cnn2snn_apis.html#cnn2snn.quantizeml.compatibility_checks.check_model_compatibility">cnn2snn.quantizeml.compatibility_checks.check_model_compatibility</a>
only applies to a model quantized via QuantizeML. For a CNN2SNN-quantized model, instead use the
deprecated
<a class="reference external" href="../api_reference/cnn2snn_apis.html#cnn2snn.check_model_compatibility">cnn2snn.check_model_compatibility</a>.</p>
</div>
</section>
<section id="command-line-interface">
<h3>Command-line interface<a class="headerlink" href="#command-line-interface" title="Permalink to this headline"></a></h3>
<p>In addition to the CNN2SNN programming API, the CNN2SNN toolkit provides a command-line interface to
perform conversion to an Akida runtime compatible model. Converting a quantized model into an Akida
model using the CLI makes use of the
<a class="reference external" href="../api_reference/cnn2snn_apis.html#cnn2snn.convert">convert</a> function.</p>
<p><strong>Examples</strong></p>
<p>Convert the DS-CNN/KWS 8/4/4 quantized model:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>wget<span class="w"> </span>https://data.brainchip.com/models/AkidaV2/ds_cnn/ds_cnn_kws_i8_w4_a4.h5

cnn2snn<span class="w"> </span>convert<span class="w"> </span>-m<span class="w"> </span>ds_cnn_kws_i8_w4_a4.h5
</pre></div>
</div>
<p>An Akida <code class="docutils literal notranslate"><span class="pre">.fbz</span></code> model named <code class="docutils literal notranslate"><span class="pre">ds_cnn_kws_i8_w4_a4.fbz</span></code> is then saved. This model can be loaded
back into an <a class="reference external" href="../api_reference/akida_apis.html#akida.Model">akida.Model</a> and run on Akida runtime.</p>
<section id="deprecated-cli-actions">
<h4>Deprecated CLI actions<a class="headerlink" href="#deprecated-cli-actions" title="Permalink to this headline"></a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">scale</span></code> and <code class="docutils literal notranslate"><span class="pre">shift</span></code> options of the <code class="docutils literal notranslate"><span class="pre">convert</span></code> CLI action that were used to set input
scaling parameters are now deprecated.</p>
<p>CNN2SNN CLI comes with additional actions that are also deprecated and should no longer be used:
<code class="docutils literal notranslate"><span class="pre">quantize</span></code>, <code class="docutils literal notranslate"><span class="pre">reshape</span></code> and  <code class="docutils literal notranslate"><span class="pre">calibrate</span></code>. You should rather use
<a class="reference external" href="quantizeml.html#">QuantizeML</a> tool to perform the same operations.</p>
</section>
</section>
</section>
<section id="handling-akida-1-0-and-akida-2-0-specificities">
<h2>Handling Akida 1.0 and Akida 2.0 specificities<a class="headerlink" href="#handling-akida-1-0-and-akida-2-0-specificities" title="Permalink to this headline"></a></h2>
<p>Conversion towards Akida 1.0 or Akida 2.0 is inherently different because the targeted SoC or IP
comes with different features. By default, a model is converted towards Akida 2.0. It is however
possible to convert towards Akida 1.0.</p>
<p>Using the programming interface:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">akida_models</span> <span class="kn">import</span> <span class="n">ds_cnn_kws_pretrained</span>
<span class="kn">from</span> <span class="nn">cnn2snn</span> <span class="kn">import</span> <span class="n">convert</span><span class="p">,</span> <span class="n">set_akida_version</span><span class="p">,</span> <span class="n">AkidaVersion</span>

<span class="k">with</span> <span class="n">set_akida_version</span><span class="p">(</span><span class="n">AkidaVersion</span><span class="o">.</span><span class="n">v1</span><span class="p">):</span>
    <span class="n">quantized_model</span> <span class="o">=</span> <span class="n">ds_cnn_kws_pretrained</span><span class="p">()</span>
    <span class="n">model_akida</span> <span class="o">=</span> <span class="n">convert</span><span class="p">(</span><span class="n">quantized_model</span><span class="p">)</span>
</pre></div>
</div>
<p>Using the CLI interface:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>wget<span class="w"> </span>https://data.brainchip.com/models/AkidaV1/ds_cnn/ds_cnn_kws_iq8_wq4_aq4_laq1.h5

<span class="nv">CNN2SNN_TARGET_AKIDA_VERSION</span><span class="o">=</span>v1<span class="w"> </span>cnn2snn<span class="w"> </span>convert<span class="w"> </span>-m<span class="w"> </span>ds_cnn_kws_iq8_wq4_aq4_laq1.h5
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>converting a model <a class="reference external" href="quantizeml.html">quantized with QuantizeML</a> will use the contextual
<a class="reference external" href="../api_reference/cnn2snn_apis.html#cnn2snn.AkidaVersion">AkidaVersion</a> to target either
1.0 or 2.0.</p></li>
<li><p>converting a model <a class="reference external" href="cnn2snn.html#legacy-quantization-api">quantized with CNN2SNN</a>
(deprecated path) will always target 1.0.</p></li>
</ul>
</div>
</section>
<section id="legacy-quantization-api">
<h2>Legacy quantization API<a class="headerlink" href="#legacy-quantization-api" title="Permalink to this headline"></a></h2>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>While it is possible to quantize Akida 1.0 models using cnn2snn legacy quantization blocks, such
usage is deprecated. You should rather use <a class="reference external" href="../user_guide/quantizeml.html#">QuantizeML</a> tool
to quantize a model whenever possible.</p>
</div>
<section id="typical-quantization-scenario">
<h3>Typical quantization scenario<a class="headerlink" href="#typical-quantization-scenario" title="Permalink to this headline"></a></h3>
<p>The CNN2SNN toolkit offers a turnkey solution to quantize a model:
the <a class="reference external" href="../api_reference/cnn2snn_apis.html#cnn2snn.quantize">quantize</a> function. It
replaces the neural Keras layers (Conv2D, SeparableConv2D and Dense) and
the ReLU layers with custom CNN2SNN layers, which are Quantization Aware
derived versions of the base Keras layer types. The obtained quantized model is
still a Keras model with a mix of CNN2SNN quantized layers (QuantizedReLU,
QuantizedDense, etc.) and standard Keras layers (BatchNormalization, MaxPool2D,
etc.).</p>
<p>Direct quantization of a standard Keras model (also called post-training
quantization) generally introduces a drop in performance. This drop is usually
small for 8-bit or even 4-bit quantization of simple models, but it can be very
significant for low quantization bitwidth and complex models.</p>
<p>If the quantized model offers acceptable performance, it can be directly
converted into an Akida model, ready to be loaded on the Akida NSoC (see the
<a class="reference external" href="../api_reference/cnn2snn_apis.html#cnn2snn.convert">convert</a> function).</p>
<p>However, if the performance drop is too high, a Quantization Aware Training is
required to recover the performance prior to quantization. Since the quantized
model is a Keras model, it can then be trained using the standard Keras API.</p>
<p>Note that quantizing directly to the target bitwidth is not mandatory: it is
possible to proceed with quantization in a serie of smaller steps.
For example, it may be beneficial to keep float weights and only quantize
activations, retrain, and then, quantize weights.</p>
</section>
<section id="design-compatibility-constraints">
<h3>Design compatibility constraints<a class="headerlink" href="#design-compatibility-constraints" title="Permalink to this headline"></a></h3>
<p>When designing a tf.keras model, consider design compatibility at these
distinct levels before the quantization stage:</p>
<ul class="simple">
<li><p>Only serial and feedforward arrangements can be converted<a class="footnote-reference brackets" href="#fn-1" id="id1">1</a>.</p></li>
<li><p>Supported Keras layers are listed <a class="reference external" href="#supported-layer-types">below</a>.</p></li>
<li><p>Order of the layers is important, e.g. a BatchNormalization layer
must be placed before the activation, and not after.</p></li>
<li><p>Some constraints are needed about layer’s parameters, e.g. a MaxPool2D layer
must have the same padding as its corresponding convolutional layer.</p></li>
</ul>
<p>All these design compatibility constraints are summarized in the CNN2SNN
<a class="reference external" href="../api_reference/cnn2snn_apis.html#cnn2snn.check_model_compatibility">check_model_compatibility</a>
function. A good practice is to check model compatibility before going through
the training process <a class="footnote-reference brackets" href="#fn-2" id="id2">2</a>.</p>
<p>Helpers (see <a class="reference external" href="../api_reference/akida_models_apis.html#layer-blocks">Layer Blocks</a>) are available
in the <code class="docutils literal notranslate"><span class="pre">akida_models</span></code> PyPI package to easily create a compatible model from scratch.</p>
</section>
<section id="id3">
<h3>Command-line interface<a class="headerlink" href="#id3" title="Permalink to this headline"></a></h3>
<p>In addition to the cnn2snn programming API, the CNN2SNN toolkit also provides a
command-line interface to perform quantization, conversion to an Akida NSoC
compatible model or model reshape.</p>
<p>Quantizing a standard Keras model or a CNN2SNN quantized model using the CLI
makes use of the <code class="docutils literal notranslate"><span class="pre">cnn2snn.quantize</span></code> Python function. The same arguments, i.e.
the quantization bitwidths for weights and activations, are required.</p>
<p><strong>Examples</strong></p>
<p>Quantize a standard Keras model with 4-bit weights and activations and 8-bit input weights:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>cnn2snn<span class="w"> </span>quantize<span class="w"> </span>-m<span class="w"> </span>model_keras.h5<span class="w"> </span>-wq<span class="w"> </span><span class="m">4</span><span class="w"> </span>-aq<span class="w"> </span><span class="m">4</span><span class="w"> </span>-iq<span class="w"> </span><span class="m">8</span>
</pre></div>
</div>
<p>The quantized model is automatically saved to <code class="docutils literal notranslate"><span class="pre">model_keras_iq8_wq4_aq4.h5</span></code>.</p>
<p>Quantize an already quantized model with different quantization bitwidths:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>cnn2snn<span class="w"> </span>quantize<span class="w"> </span>-m<span class="w"> </span>model_keras_iq8_wq4_aq4.h5<span class="w"> </span>-wq<span class="w"> </span><span class="m">2</span><span class="w"> </span>-aq<span class="w"> </span><span class="m">2</span>
</pre></div>
</div>
<p>A new quantized model named <code class="docutils literal notranslate"><span class="pre">model_keras_iq2_wq2_aq2.h5</span></code> is saved.</p>
<p>A model can be reshaped (change of input shape) using CNN2SNN CLI that makes
use of the <code class="docutils literal notranslate"><span class="pre">cnn2snn.transforms.reshape</span></code> function. This will only apply to
Sequential models, a <a class="reference external" href="../api_reference/cnn2snn_apis.html#cnn2snn.transforms.sequentialize">sequentialize helper</a> is
provided for convenience.</p>
<p><strong>Examples</strong></p>
<p>Reshape a model to 160x96:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>cnn2snn<span class="w"> </span>reshape<span class="w"> </span>-m<span class="w"> </span>model_keras.h5<span class="w"> </span>-iw<span class="w"> </span><span class="m">160</span><span class="w"> </span>-ih<span class="w"> </span><span class="m">96</span>
</pre></div>
</div>
<p>A reshaped model will be saved as <code class="docutils literal notranslate"><span class="pre">model_keras_160_96.h5</span></code>.</p>
</section>
<section id="layers-considerations">
<h3>Layers Considerations<a class="headerlink" href="#layers-considerations" title="Permalink to this headline"></a></h3>
<section id="supported-layer-types">
<h4>Supported layer types<a class="headerlink" href="#supported-layer-types" title="Permalink to this headline"></a></h4>
<p>The CNN2SNN toolkit provides quantization of Keras models with the following
Keras layer types:</p>
<ul class="simple">
<li><p><strong>Core Neural Layers</strong>:</p>
<ul>
<li><p>tf.keras <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense">Dense</a></p></li>
<li><p>tf.keras <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D">Conv2D</a></p></li>
</ul>
</li>
<li><p><strong>Specialized Layers</strong>:</p>
<ul>
<li><p>tf.keras <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/SeparableConv2D">SeparableConv2D</a></p></li>
</ul>
</li>
<li><p><strong>Other Layers (from tf.keras)</strong>:</p>
<ul>
<li><p>ReLU</p></li>
<li><p>BatchNormalization</p></li>
<li><p>MaxPooling2D</p></li>
<li><p>GlobalAveragePooling2D</p></li>
<li><p>Dropout</p></li>
<li><p>Flatten</p></li>
<li><p>Reshape</p></li>
<li><p>Input</p></li>
</ul>
</li>
</ul>
</section>
<section id="cnn2snn-quantization-aware-layers">
<h4>CNN2SNN Quantization Aware layers<a class="headerlink" href="#cnn2snn-quantization-aware-layers" title="Permalink to this headline"></a></h4>
<p>Several articles have reported<a class="footnote-reference brackets" href="#fn-4" id="id4">4</a> that the quantization of a pre-trained
float Keras model using 8-bit precision can be performed with a minimal loss
of accuracy for simple models, but that for lower bitwidth or complex models a
Quantization Aware Training of the quantized model may be required.</p>
<p>The CNN2SNN toolkit therefore includes Quantization Aware versions of the base
Keras layers.</p>
<p>These layers are produced when quantizing a standard Keras model using the
<code class="docutils literal notranslate"><span class="pre">quantize</span></code> function: it replaces the base Keras layers with their Quantization Aware
counterparts (see the <a class="reference external" href="../api_reference/cnn2snn_apis.html#cnn2snn.quantize">quantize</a> function).</p>
<p>Quantization Aware Training simulates the effect of quantization in the forward
pass, yet using a straight-through estimator for the quantization gradient in
the backward pass.
For the stochastic gradient descent to be efficient, the weights are stored as
float values and updated with high precision during back propagation.
This ensures sufficient precision in accumulating tiny weights adjustments.</p>
<p>The CNN2SNN toolkit includes two classes of Quantization Aware layers:</p>
<ul class="simple">
<li><p><strong>quantized processing layers</strong>:</p>
<ul>
<li><p><a class="reference external" href="../api_reference/cnn2snn_apis.html#quantizeddense">QuantizedDense</a>,</p></li>
<li><p><a class="reference external" href="../api_reference/cnn2snn_apis.html#quantizedconv2d">QuantizedConv2D</a>,</p></li>
<li><p><a class="reference external" href="../api_reference/cnn2snn_apis.html#quantizedseparableconv2d">QuantizedSeparableConv2D</a></p></li>
</ul>
</li>
<li><p><strong>quantized activation layers</strong>:</p>
<ul>
<li><p><a class="reference external" href="../api_reference/cnn2snn_apis.html#quantizedrelu">QuantizedReLU</a></p></li>
</ul>
</li>
</ul>
<p>Most of the parameters for the quantized processing layers are identical to
those used when defining a model using standard Keras layers. However, each of
these layers also includes a <code class="docutils literal notranslate"><span class="pre">quantizer</span></code> parameter that specifies the
<a class="reference external" href="../api_reference/cnn2snn_apis.html#weightquantizer">WeightQuantizer</a>
object to use during the Quantization Aware Training.</p>
<p>The quantized ReLU takes a single parameter corresponding to the
bitwidth of the quantized activations.</p>
</section>
<section id="training-only-layers">
<h4>Training-Only Layers<a class="headerlink" href="#training-only-layers" title="Permalink to this headline"></a></h4>
<p>Training is done within the Keras environment and training-only layers may be
added at will, such as BatchNormalization or Dropout layers. These are handled
fully by Keras during the training and do not need to be modified to be
Akida-compatible for inference.</p>
<p>As regards the implementation within the Akida neuromorphic IP: it may be
helpful to understand that the associated scaling operations (multiplication and
shift) are never performed during inference. The computational cost is reduced
by wrapping the (optional) batch normalization function and quantized activation
function into the spike generating thresholds and other parameters of the Akida model.
That process is completely transparent to the user. It does, however, have an
important consequence for the output of the final layer of the model; see
<a class="reference external" href="#id6">Final Layers</a> below.</p>
</section>
<section id="first-layers">
<h4>First Layers<a class="headerlink" href="#first-layers" title="Permalink to this headline"></a></h4>
<p>Most layers of an Akida model only accept sparse inputs.
In order to support the most common classes of models in computer vision, a
special layer (<a class="reference external" href="../api_reference/akida_apis.html#akida.InputConvolutional">InputConvolutional</a>)
is however able to receive image data (8-bit grayscale or RGB). See the
<a class="reference external" href="akida.html">Akida user guide</a> for further details.</p>
<p>The CNN2SNN toolkit supports any Quantization Aware Training layer as the first
layer in the model. The type of input accepted by that layer can be specified
during conversion, but only models starting with a QuantizedConv2D layer will
accept dense inputs, thanks to the special
<a class="reference external" href="../api_reference/akida_apis.html#akida.InputConvolutional">InputConvolutional</a> layer.</p>
<section id="input-scaling">
<h5>Input Scaling<a class="headerlink" href="#input-scaling" title="Permalink to this headline"></a></h5>
<p>The <a class="reference external" href="../api_reference/akida_apis.html#akida.InputConvolutional">InputConvolutional</a>
layer only receives 8-bit input values:</p>
<ul class="simple">
<li><p>if the data is already in 8-bit format it can be sent to the Akida inputs
without rescaling.</p></li>
<li><p>if the data has been scaled to ease training, it is necessary to provide the
scaling coefficients at model conversion.</p></li>
</ul>
<p>This applies to the common case where input data are natively 8-bit. If input
data are not 8-bit, the process is more complex, and we recommend applying
rescaling in two steps:</p>
<ol class="arabic simple">
<li><p>Taking the data to an 8-bit unsigned integer format suitable for Akida
architecture. Apply this step both for training and inference data.</p></li>
<li><p>Rescaling the 8-bit values to some unit or zero centered range suitable for
CNN training, as above. This step should only be applied for the CNN training.
Also, remember to provide those scaling coefficients when converting the
trained model to an Akida-compatible format.</p></li>
</ol>
</section>
</section>
<section id="id6">
<h4>Final Layers<a class="headerlink" href="#id6" title="Permalink to this headline"></a></h4>
<p>As is typical for CNNs, the final layer of a model does not include the
standard activation nonlinearity. If that is the case, once converted to Akida
hardware, the model will give the potentials levels and in most cases, taking the
maximum among these values is sufficient to obtain the correct response from
the model.
However, if there is a difference in performance between the Keras and the
Akida-compatible implementations of the model, it is likely be at this step.</p>
</section>
</section>
<section id="tips-and-tricks">
<h3>Tips and Tricks<a class="headerlink" href="#tips-and-tricks" title="Permalink to this headline"></a></h3>
<p>In some cases, it may be useful to adapt existing CNN models in order to
simplify or enhance the converted model. Here’s a short list of some possible
substitutions that might come in handy:</p>
<ul class="simple">
<li><p><a class="reference external" href="http://cs231n.github.io/convolutional-networks/#convert">Substitute a fully connected layer with a convolutional layer</a>.</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1412.6806">Substitute a convolutional layer with stride 2 with a convolutional layer
with stride 1 in combination with an additional pooling layer</a>.</p></li>
<li><p><a class="reference external" href="http://cs231n.github.io/convolutional-networks/">Substitute a convolutional layer that has 1 large filter with multiple
convolutional layers that contain smaller filters</a>.</p></li>
</ul>
<hr class="docutils" />
<dl class="footnote brackets">
<dt class="label" id="fn-1"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Parallel layers and “residual” connections are currently not
supported.</p>
</dd>
<dt class="label" id="fn-2"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>Check model compatibility must be applied on a quantized model. It
then requires to quantize the model first.</p>
</dd>
<dt class="label" id="fn-3"><span class="brackets">3</span></dt>
<dd><p>The spike value depends on the intensity of the potential, see the
<a class="reference external" href="akida.html">Akida documentation</a> for details on the activation.</p>
</dd>
<dt class="label" id="fn-4"><span class="brackets"><a class="fn-backref" href="#id4">4</a></span></dt>
<dd><p>See for instance <a class="reference external" href="https://arxiv.org/pdf/1806.08342.pdf">“Quantizing deep convolutional networks for
efficient inference: A whitepaper”</a>
- Raghuraman Krishnamoorthi, 2018</p>
</dd>
</dl>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="quantizeml.html" class="btn btn-neutral float-left" title="QuantizeML toolkit" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="akida_models.html" class="btn btn-neutral float-right" title="Akida models zoo" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, BrainChip Holdings Ltd. All Rights Reserved.</p>
  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>