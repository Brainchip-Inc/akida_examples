<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>QuantizeML toolkit &mdash; Akida Examples  documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery-binder.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery-dataframe.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery-rendered-html.css" type="text/css" />
      <link rel="stylesheet" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" type="text/css" />
      <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/design-tabs.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="CNN2SNN toolkit" href="cnn2snn.html" />
    <link rel="prev" title="Akida user guide" href="akida.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #989898" >

          
          
          <a href="../index.html">
            
              <img src="../_static/MetaTF_logo.png" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                Akida, 2nd Generation
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../index.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../installation.html#supported-configurations">Supported configurations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../installation.html#quick-installation">Quick installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../installation.html#running-examples">Running examples</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="user_guide.html">User guide</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="akida.html">Akida user guide</a><ul>
<li class="toctree-l3"><a class="reference internal" href="akida.html#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="akida.html#programming-interface">Programming interface</a><ul>
<li class="toctree-l4"><a class="reference internal" href="akida.html#the-akida-model">The Akida Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida.html#akida-layers">Akida layers</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="akida.html#model-hardware-mapping">Model Hardware Mapping</a><ul>
<li class="toctree-l4"><a class="reference internal" href="akida.html#devices">Devices</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida.html#model-mapping">Model mapping</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida.html#advanced-mapping-details-and-hardware-devices-usage">Advanced Mapping Details and Hardware Devices Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida.html#performance-measurement">Performance measurement</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida.html#command-line-interface-for-model-evaluation">Command-line interface for model evaluation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="akida.html#using-akida-edge-learning">Using Akida Edge learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="akida.html#learning-constraints">Learning constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida.html#compiling-a-layer">Compiling a layer</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">QuantizeML toolkit</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#the-fixedpoint-representation">The FixedPoint representation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#quantization-flow">Quantization flow</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#compatibility-constraints">Compatibility constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="#model-loading">Model loading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#command-line-interface">Command line interface</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#quantize-cli">quantize CLI</a></li>
<li class="toctree-l4"><a class="reference internal" href="#config-cli">config CLI</a></li>
<li class="toctree-l4"><a class="reference internal" href="#check-cli">check CLI</a></li>
<li class="toctree-l4"><a class="reference internal" href="#insert-rescaling-cli">insert_rescaling CLI</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#supported-layer-types">Supported layer types</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#keras-support">Keras support</a></li>
<li class="toctree-l4"><a class="reference internal" href="#onnx-support">ONNX support</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="cnn2snn.html">CNN2SNN toolkit</a><ul>
<li class="toctree-l3"><a class="reference internal" href="cnn2snn.html#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="cnn2snn.html#conversion-flow">Conversion flow</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn.html#conversion-compatibility">Conversion compatibility</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn.html#command-line-interface">Command-line interface</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cnn2snn.html#handling-akida-1-0-and-akida-2-0-specificities">Handling Akida 1.0 and Akida 2.0 specificities</a></li>
<li class="toctree-l3"><a class="reference internal" href="cnn2snn.html#legacy-quantization-api">Legacy quantization API</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn.html#typical-quantization-scenario">Typical quantization scenario</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn.html#design-compatibility-constraints">Design compatibility constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn.html#id3">Command-line interface</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn.html#layers-considerations">Layers Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn.html#tips-and-tricks">Tips and Tricks</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="akida_models.html">Akida models zoo</a><ul>
<li class="toctree-l3"><a class="reference internal" href="akida_models.html#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="akida_models.html#command-line-interface-for-model-creation">Command-line interface for model creation</a></li>
<li class="toctree-l3"><a class="reference internal" href="akida_models.html#command-line-interface-for-model-training">Command-line interface for model training</a><ul>
<li class="toctree-l4"><a class="reference internal" href="akida_models.html#kws-training">KWS training</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida_models.html#akidanet-training">AkidaNet training</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="akida_models.html#command-line-interface-for-model-evaluation">Command-line interface for model evaluation</a></li>
<li class="toctree-l3"><a class="reference internal" href="akida_models.html#command-line-interface-to-evaluate-model-macs">Command-line interface to evaluate model MACS</a></li>
<li class="toctree-l3"><a class="reference internal" href="akida_models.html#command-line-interface-to-display-summary">Command-line interface to display summary</a></li>
<li class="toctree-l3"><a class="reference internal" href="akida_models.html#id1">Layer Blocks</a></li>
<li class="toctree-l3"><a class="reference internal" href="akida_models.html#handling-akida-1-0-and-akida-2-0-specificities">Handling Akida 1.0 and Akida 2.0 specificities</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="engine.html">Akida Engine</a><ul>
<li class="toctree-l3"><a class="reference internal" href="engine.html#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="engine.html#engine-directory-structure">Engine directory structure</a></li>
<li class="toctree-l3"><a class="reference internal" href="engine.html#engine-api-overview">Engine API overview</a><ul>
<li class="toctree-l4"><a class="reference internal" href="engine.html#hardwaredriver">HardwareDriver</a></li>
<li class="toctree-l4"><a class="reference internal" href="engine.html#hardwaredevice">HardwareDevice</a></li>
<li class="toctree-l4"><a class="reference internal" href="engine.html#dense">Dense</a></li>
<li class="toctree-l4"><a class="reference internal" href="engine.html#shape">Shape</a></li>
<li class="toctree-l4"><a class="reference internal" href="engine.html#hwversion">HwVersion</a></li>
<li class="toctree-l4"><a class="reference internal" href="engine.html#sparse-and-input-conversion-functions">Sparse and Input conversion functions</a></li>
<li class="toctree-l4"><a class="reference internal" href="engine.html#other-headers-in-the-api">Other headers in the API</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../api_reference/api_reference.html">API reference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../api_reference/akida_apis.html">Akida runtime</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_apis.html#model">Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_apis.html#layer">Layer</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_apis.html#id1">Layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_apis.html#mapping">Mapping</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_apis.html#akida-layers">Akida layers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_apis.html#akida-v1-layers">Akida V1 layers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_apis.html#akida-v2-layers">Akida V2 layers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_apis.html#layer-parameters">Layer parameters</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_apis.html#layertype">LayerType</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_apis.html#padding">Padding</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_apis.html#pooltype">PoolType</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_apis.html#optimizers">Optimizers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_apis.html#sequence">Sequence</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_apis.html#id2">Sequence</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_apis.html#backendtype">BackendType</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_apis.html#pass">Pass</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_apis.html#device">Device</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_apis.html#id3">Device</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_apis.html#hwversion">HwVersion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_apis.html#hwdevice">HWDevice</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_apis.html#id4">HWDevice</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_apis.html#socdriver">SocDriver</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_apis.html#clockmode">ClockMode</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_apis.html#powermeter">PowerMeter</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_apis.html#np">NP</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_apis.html#tools">Tools</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_apis.html#sparsity">Sparsity</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api_reference/cnn2snn_apis.html">CNN2SNN</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#akida-version">Akida version</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#conversion">Conversion</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#legacy-quantization-api">Legacy quantization API</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#utils">Utils</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#calibration">Calibration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#transforms">Transforms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#constraint">Constraint</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#quantization">Quantization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#quantizers">Quantizers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#quantized-layers">Quantized layers</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api_reference/quantizeml_apis.html">QuantizeML</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/quantizeml_apis.html#layers">Layers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#reshaping">Reshaping</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#activations">Activations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#attention">Attention</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#normalization">Normalization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#convolution">Convolution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#depthwise-convolution">Depthwise convolution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#separable-convolution">Separable convolution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#dense">Dense</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#skip-connection">Skip connection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#pooling">Pooling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#shiftmax">Shiftmax</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#transformers">Transformers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#rescaling">Rescaling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#dropout">Dropout</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#quantizers">Quantizers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#calibration">Calibration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#recording">Recording</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/quantizeml_apis.html#models">Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#transforms">Transforms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#quantization">Quantization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#quantization-parameters">Quantization parameters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#id1">Calibration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#utils">Utils</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/quantizeml_apis.html#tensors">Tensors</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#qtensor">QTensor</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#fixedpoint">FixedPoint</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#qfloat">QFloat</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/quantizeml_apis.html#onnx-support">ONNX support</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#id2">Layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#custom-patterns">Custom patterns</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api_reference/akida_models_apis.html">Akida models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_models_apis.html#layer-blocks">Layer blocks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#cnn-blocks">CNN blocks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#transformers-blocks">Transformers blocks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#transposed-blocks">Transposed blocks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#detection-block">Detection block</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_models_apis.html#helpers">Helpers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#gamma-constraint">Gamma constraint</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#unfusing-separableconvolutional">Unfusing SeparableConvolutional</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#extract-samples">Extract samples</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_models_apis.html#knowledge-distillation">Knowledge distillation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_models_apis.html#macs">MACS</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_models_apis.html#model-i-o">Model I/O</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_models_apis.html#utils">Utils</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_models_apis.html#model-zoo">Model zoo</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#akidanet">AkidaNet</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#mobilenet">Mobilenet</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#ds-cnn">DS-CNN</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#vgg">VGG</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#yolo">YOLO</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#pointnet">PointNet++</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#gxnor">GXNOR</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#centernet">CenterNet</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#akidaunet">AkidaUNet</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#transformers">Transformers</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../examples/index.html">Examples</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../examples/index.html#general-examples">General examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_0_global_workflow.html">Global Akida workflow</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_0_global_workflow.html#create-and-train">1. Create and train</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_0_global_workflow.html#quantize">2. Quantize</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_0_global_workflow.html#convert">3. Convert</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_1_akidanet_imagenet.html">AkidaNet/ImageNet inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_1_akidanet_imagenet.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_1_akidanet_imagenet.html#pretrained-quantized-model">2. Pretrained quantized model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_1_akidanet_imagenet.html#conversion-to-akida">3. Conversion to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_1_akidanet_imagenet.html#hardware-mapping-and-performance">4. Hardware mapping and performance</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_2_ds_cnn_kws.html">DS-CNN/KWS inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_2_ds_cnn_kws.html#load-the-preprocessed-dataset">1. Load the preprocessed dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_2_ds_cnn_kws.html#load-a-pre-trained-native-keras-model">2. Load a pre-trained native Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_2_ds_cnn_kws.html#load-a-pre-trained-quantized-keras-model">3. Load a pre-trained quantized Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_2_ds_cnn_kws.html#conversion-to-akida">4. Conversion to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_2_ds_cnn_kws.html#confusion-matrix">5. Confusion matrix</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_3_regression.html">Age estimation (regression) example</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_3_regression.html#load-the-utkface-dataset">1. Load the UTKFace Dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_3_regression.html#load-a-pre-trained-native-keras-model">2. Load a pre-trained native Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_3_regression.html#load-a-pre-trained-quantized-keras-model">3. Load a pre-trained quantized Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_3_regression.html#conversion-to-akida">4. Conversion to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_3_regression.html#estimate-age-on-a-single-image">5. Estimate age on a single image</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_4_transfer_learning.html">Transfer learning with AkidaNet for PlantVillage</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_transfer_learning.html#transfer-learning-process">Transfer learning process</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_transfer_learning.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_transfer_learning.html#get-a-trained-akidanet-base-model">2. Get a trained AkidaNet base model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_transfer_learning.html#add-a-classification-head-to-the-model">3. Add a classification head to the model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_transfer_learning.html#train-for-a-few-epochs">4. Train for a few epochs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_transfer_learning.html#quantize-the-model">5. Quantize the model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_transfer_learning.html#compute-accuracy">6. Compute accuracy</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_5_voc_yolo_detection.html">YOLO/PASCAL-VOC detection tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_5_voc_yolo_detection.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_5_voc_yolo_detection.html#preprocessing-tools">2. Preprocessing tools</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_5_voc_yolo_detection.html#model-architecture">3. Model architecture</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_5_voc_yolo_detection.html#training">4. Training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_5_voc_yolo_detection.html#performance">5. Performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_5_voc_yolo_detection.html#conversion-to-akida">6. Conversion to Akida</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_6_segmentation.html">Segmentation tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_6_segmentation.html#load-the-dataset">1. Load the dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_6_segmentation.html#load-a-pre-trained-native-keras-model">2. Load a pre-trained native Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_6_segmentation.html#load-a-pre-trained-quantized-keras-model">3. Load a pre-trained quantized Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_6_segmentation.html#conversion-to-akida">4. Conversion to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_6_segmentation.html#segment-a-single-image">5. Segment a single image</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_7_vision_transformer.html">Build Vision Transformers for Akida</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_7_vision_transformer.html#model-selection">1. Model selection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_7_vision_transformer.html#model-optimization-for-akida-hardware">2. Model optimization for Akida hardware</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_7_vision_transformer.html#model-training">3. Model Training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_7_vision_transformer.html#model-quantization">4. Model quantization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_7_vision_transformer.html#conversion-to-akida">5. Conversion to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_7_vision_transformer.html#displaying-results-attention-maps">6. Displaying results Attention Maps</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_8_global_pytorch_workflow.html">PyTorch to Akida workflow</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_8_global_pytorch_workflow.html#create-and-train">1. Create and train</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_8_global_pytorch_workflow.html#export">2. Export</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_8_global_pytorch_workflow.html#quantize">3. Quantize</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_8_global_pytorch_workflow.html#convert">4. Convert</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/index.html#quantization">Quantization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/quantization/plot_0_advanced_quantizeml.html">Advanced QuantizeML tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/quantization/plot_0_advanced_quantizeml.html#defining-a-quantization-scheme">1. Defining a quantization scheme</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/quantization/plot_0_advanced_quantizeml.html#calibration">2. Calibration</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/quantization/plot_1_upgrading_to_2.0.html">Upgrading to Akida 2.0</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/quantization/plot_1_upgrading_to_2.0.html#workflow-differences">1. Workflow differences</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/quantization/plot_1_upgrading_to_2.0.html#models-architecture-differences">2. Models architecture differences</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/quantization/plot_1_upgrading_to_2.0.html#using-akidaversion">3. Using <code class="docutils literal notranslate"><span class="pre">AkidaVersion</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/quantization/plot_2_off_the_shelf_quantization.html">Off-the-shelf models quantization</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/quantization/plot_2_off_the_shelf_quantization.html#workflow-overview">1. Workflow overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/quantization/plot_2_off_the_shelf_quantization.html#data-preparation">2. Data preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/quantization/plot_2_off_the_shelf_quantization.html#download-and-export">3. Download and export</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/quantization/plot_2_off_the_shelf_quantization.html#quantize">4. Quantize</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/quantization/plot_3_custom_patterns.html">Advanced ONNX models quantization</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/quantization/plot_3_custom_patterns.html#get-model-and-data">1. Get model and data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/quantization/plot_3_custom_patterns.html#quantize">2. Quantize</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/quantization/plot_3_custom_patterns.html#conversion">3. Conversion</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/index.html#edge-examples">Edge examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/edge/plot_0_edge_learning_vision.html">Akida vision edge learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_0_edge_learning_vision.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_0_edge_learning_vision.html#prepare-akida-model-for-learning">2. Prepare Akida model for learning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_0_edge_learning_vision.html#edge-learning-with-akida">3. Edge learning with Akida</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/edge/plot_1_edge_learning_kws.html">Akida edge learning for keyword spotting</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_1_edge_learning_kws.html#edge-learning-process">1. Edge learning process</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_1_edge_learning_kws.html#dataset-preparation">2. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_1_edge_learning_kws.html#prepare-akida-model-for-learning">3. Prepare Akida model for learning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_1_edge_learning_kws.html#learn-with-akida-using-the-training-set">4. Learn with Akida using the training set</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_1_edge_learning_kws.html#edge-learning">5. Edge learning</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/edge/plot_2_edge_learning_parameters.html">Tips to set Akida edge learning parameters</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_2_edge_learning_parameters.html#akida-learning-parameters">1. Akida learning parameters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_2_edge_learning_parameters.html#create-akida-model">2. Create Akida model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_2_edge_learning_parameters.html#estimate-the-required-number-of-weights-of-the-trainable-layer">3. Estimate the required number of weights of the trainable layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_2_edge_learning_parameters.html#estimate-the-number-of-neurons-per-class">4. Estimate the number of neurons per class</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/index.html#deprecated-cnn2snn-tutorials">[Deprecated] CNN2SNN tutorials</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/cnn2snn/plot_1_advanced_cnn2snn.html">Advanced CNN2SNN tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_1_advanced_cnn2snn.html#design-a-cnn2snn-quantized-model">1. Design a CNN2SNN quantized model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_1_advanced_cnn2snn.html#weight-quantizer-details">2. Weight Quantizer Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_1_advanced_cnn2snn.html#understanding-quantized-activation">3. Understanding quantized activation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_1_advanced_cnn2snn.html#how-to-deal-with-too-high-scale-factors">4. How to deal with too high scale factors</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../model_zoo_performance.html">Model zoo performance</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../model_zoo_performance.html#akida-1-0-models">Akida 1.0 models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../model_zoo_performance.html#image-icon-ref-image-domain"> Image domain</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../model_zoo_performance.html#classification">Classification</a></li>
<li class="toctree-l4"><a class="reference internal" href="../model_zoo_performance.html#object-detection">Object detection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../model_zoo_performance.html#regression">Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="../model_zoo_performance.html#face-recognition">Face recognition</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../model_zoo_performance.html#audio-icon-ref-audio-domain"> Audio domain</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../model_zoo_performance.html#keyword-spotting">Keyword spotting</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../model_zoo_performance.html#pointcloud-icon-ref-point-cloud"> Point cloud</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../model_zoo_performance.html#id1">Classification</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../model_zoo_performance.html#akida-2-0-models">Akida 2.0 models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../model_zoo_performance.html#id2"> Image domain</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../model_zoo_performance.html#id3">Classification</a></li>
<li class="toctree-l4"><a class="reference internal" href="../model_zoo_performance.html#id6">Object detection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../model_zoo_performance.html#id7">Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="../model_zoo_performance.html#id8">Face recognition</a></li>
<li class="toctree-l4"><a class="reference internal" href="../model_zoo_performance.html#segmentation">Segmentation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../model_zoo_performance.html#id10"> Audio domain</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../model_zoo_performance.html#id11">Keyword spotting</a></li>
<li class="toctree-l4"><a class="reference internal" href="../model_zoo_performance.html#id12">Classification</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../model_zoo_performance.html#id14"> Point cloud</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../model_zoo_performance.html#id15">Classification</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../changelog.html">Changelog</a></li>
<li class="toctree-l1"><a class="reference external" href="https://support.brainchip.com/portal/home">Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #989898" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Akida Examples</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="user_guide.html">User guide</a></li>
      <li class="breadcrumb-item active">QuantizeML toolkit</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="quantizeml-toolkit">
<h1>QuantizeML toolkit<a class="headerlink" href="#quantizeml-toolkit" title="Permalink to this headline"></a></h1>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>QuantizeML package provides base layers and quantization tools for deep-learning models. It allows
the quantization of CNN and Vision Transformer models using low-bitwidth weights and outputs. Once
quantized with the provided tools, CNN2SNN toolkit will be able to convert the model and execute it
with Akida runtime.</p>
</section>
<section id="the-fixedpoint-representation">
<h2>The FixedPoint representation<a class="headerlink" href="#the-fixedpoint-representation" title="Permalink to this headline"></a></h2>
<p>QuantizeML uses a FixedPoint representation in place of float values for layers inputs, outputs and
weights.</p>
<p>FixedPoint numbers are actually integers with a static number of fractional bits so that:</p>
<div class="math notranslate nohighlight">
\[x_{float} \approx x_{int}.2^{-x_{frac\_bits}}\]</div>
<p>The precision of the representation is directly related to the number of fractional bits. For
example, representing PI using an 8-bit FixedPoint with varying fractional bits:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 35%" />
<col style="width: 23%" />
<col style="width: 42%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>frac_bits</p></th>
<th class="head"><p>x_int</p></th>
<th class="head"><p>float value</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>6</p></td>
<td><p>3.0</p></td>
</tr>
<tr class="row-odd"><td><p>3</p></td>
<td><p>25</p></td>
<td><p>3.125</p></td>
</tr>
<tr class="row-even"><td><p>6</p></td>
<td><p>201</p></td>
<td><p>3.140625</p></td>
</tr>
</tbody>
</table>
<p>Further details are available in the
<a class="reference external" href="../api_reference/quantizeml_apis.html#fixedpoint">FixedPoint API</a> documentation.</p>
<p>Thanks to the FixedPoint representation, all operations within layers are implemented as integer
only operations <a class="footnote-reference brackets" href="#fn-1" id="id1">1</a>.</p>
</section>
<section id="quantization-flow">
<h2>Quantization flow<a class="headerlink" href="#quantization-flow" title="Permalink to this headline"></a></h2>
<p>The first step in the workflow is to train a model. The trained model is the starting point for the
quantization stage. Once it is established that the overall model configuration prior to
quantization yields a satisfactory performance on the task, one can proceed with quantization.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For simplicity, the following leverages the Keras API to define a model, but QuantizeML
also comes with ONNX support, see the <a class="reference external" href="../examples/general/plot_8_global_pytorch_workflow.html#sphx-glr-examples-general-plot-8-global-pytorch-workflow-py">PyTorch to Akida</a>
or <a class="reference external" href="../examples/quantization/plot_2_off_the_shelf_quantization.html#sphx-glr-examples-quantization-plot-2-off-the-shelf-quantization-py">off-the-shelf models</a>
examples for more information.</p>
</div>
<p>Let’s take the <a class="reference external" href="../api_reference/akida_models_apis.html#ds-cnn">DS-CNN</a> model from our zoo that
targets KWS task as an example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">akida_models</span> <span class="kn">import</span> <span class="n">fetch_file</span>
<span class="kn">from</span> <span class="nn">quantizeml.models</span> <span class="kn">import</span> <span class="n">load_model</span>
<span class="n">model_file</span> <span class="o">=</span> <span class="n">fetch_file</span><span class="p">(</span><span class="s2">&quot;https://data.brainchip.com/models/AkidaV2/ds_cnn/ds_cnn_kws.h5&quot;</span><span class="p">,</span>
                        <span class="n">fname</span><span class="o">=</span><span class="s2">&quot;ds_cnn_kws.h5&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="n">model_file</span><span class="p">)</span>
</pre></div>
</div>
<p>The QuantizeML toolkit offers a turnkey solution to quantize a model: the
<a class="reference external" href="../api_reference/quantizeml_apis.html#quantizeml.models.quantize">quantize</a> function. It
replaces the Keras layers (or custom QuantizeML layers) with quantized, integer only layers. The
obtained quantized model is still a Keras model that can be evaluated with a standard Keras
pipeline.</p>
<p>The quantization scheme used by
<a class="reference external" href="../api_reference/quantizeml_apis.html#quantizeml.models.quantize">quantize</a> can be configured
using
<a class="reference external" href="../api_reference/quantizeml_apis.html#quantizeml.models.QuantizationParams">QuantizationParams</a>.
If none is given, an 8-bit configuration scheme will be selected.</p>
<p>Here’s an example for 8-bit quantization:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">quantizeml.models</span> <span class="kn">import</span> <span class="n">QuantizationParams</span>
<span class="n">qparams8</span> <span class="o">=</span> <span class="n">QuantizationParams</span><span class="p">(</span><span class="n">input_weight_bits</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">weight_bits</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">activation_bits</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
</pre></div>
</div>
<p>Here’s an example for 4-bit quantization (with first layer weights set to 8-bit):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">quantizeml.models</span> <span class="kn">import</span> <span class="n">QuantizationParams</span>
<span class="n">qparams4</span> <span class="o">=</span> <span class="n">QuantizationParams</span><span class="p">(</span><span class="n">input_weight_bits</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">weight_bits</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">activation_bits</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that quantizating the first weights to 8-bit helps preserving accuracy.</p>
<p>QuantizeML uses a uniform quantization scheme centered on zero. During quantization, the floating
point values are mapped to a given bitwidth quantization space of the form:</p>
<div class="math notranslate nohighlight">
\[data_{float32} = data_{fixed\_point} * scales\]</div>
<p><cite>scales</cite> is a real number used to map the FixedPoint numbers to a quantization space. It is
calculated as follows:</p>
<div class="math notranslate nohighlight">
\[scales = \frac {max(abs(data))}{2^{bitwidth} - 1}\]</div>
<p>Inputs, weights and outputs scales are folded into a single output scale vector.</p>
<p>To avoid saturation in downstream operations throughout a model graph, the bitwidth of intermediary
results is decreased using
<a class="reference external" href="../api_reference/quantizeml_apis.html#quantizeml.layers.OutputQuantizer">OutputQuantizer</a>. The
<a class="reference external" href="../api_reference/quantizeml_apis.html#quantizeml.models.quantize">quantize</a> function has
built-in rules to automatically isolate building blocks of layers after which such quantization is
required and will insert the
<a class="reference external" href="../api_reference/quantizeml_apis.html#quantizeml.layers.OutputQuantizer">OutputQuantizer</a>
objects during the quantization process.</p>
<p>To properly operate, an
<a class="reference external" href="../api_reference/quantizeml_apis.html#quantizeml.layers.OutputQuantizer">OutputQuantizer</a> must
be calibrated so that it determines an adequate quantization range. Calibration will determine the
quantization range statistically. It is possible to pass down samples to the
<a class="reference external" href="../api_reference/quantizeml_apis.html#quantizeml.models.quantize">quantize</a> function so that
calibration and quantization are performed simultaneously.</p>
<p>Calibration samples are available on
<a class="reference external" href="https://data.brainchip.com/dataset-mirror/samples/">Brainchip data server</a> for datasets used in
our zoo. They must be downloaded and deserialized before being used for calibration.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">akida_models</span> <span class="kn">import</span> <span class="n">fetch_file</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">fetch_file</span><span class="p">(</span><span class="s2">&quot;https://data.brainchip.com/dataset-mirror/samples/kws/kws_batch1024.npz&quot;</span><span class="p">,</span>
                     <span class="n">fname</span><span class="o">=</span><span class="s2">&quot;kws_batch1024.npz&quot;</span><span class="p">)</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">samples</span><span class="p">[</span><span class="n">item</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">samples</span><span class="o">.</span><span class="n">files</span><span class="p">])</span>
</pre></div>
</div>
<p>Quantizing the DS-CNN model to 8-bit is then done with:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">quantizeml.models</span> <span class="kn">import</span> <span class="n">quantize</span>
<span class="n">quantized_model</span> <span class="o">=</span> <span class="n">quantize</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">qparams</span><span class="o">=</span><span class="n">qparams8</span><span class="p">,</span> <span class="n">samples</span><span class="o">=</span><span class="n">samples</span><span class="p">)</span>
</pre></div>
</div>
<p>Please refer to <a class="reference external" href="../api_reference/quantizeml_apis.html#quantizeml.models.calibrate">calibrate</a>
for more details on calibration.</p>
<p>Direct quantization of a standard Keras model (also called Post Training Quantization, PTQ)
generally introduces a drop in performance. This drop is usually small for 8-bit or even 4-bit
quantization of simple models, but it can be very significant for low quantization bitwidth and
complex models (<a class="reference external" href="../api_reference/akida_models_apis.html#akida_models.akidanet_imagenet">AkidaNet</a>
or <a class="reference external" href="../api_reference/akida_models_apis.html#transformers">transformers</a> architectures).</p>
<p>If the quantized model offers acceptable performance, it can be directly converted into an Akida
model (see the <a class="reference external" href="../api_reference/cnn2snn_apis.html#cnn2snn.convert">convert</a> function).</p>
<p>However, if the performance drop is too high, a Quantization Aware Training (QAT) step is required
to recover the performance prior to quantization. Since the quantized model is a Keras model, it can
then be trained using the standard Keras API.</p>
<p>Check out the <a class="reference external" href="../examples/index.html">examples section</a> for tutorials on quantization, PTQ and
QAT.</p>
<section id="compatibility-constraints">
<h3>Compatibility constraints<a class="headerlink" href="#compatibility-constraints" title="Permalink to this headline"></a></h3>
<p>The tookit supports a wide range of layers (see the
<a class="reference external" href="quantizeml.html#supported-layer-types">supported type section</a>). When hitting a non-compatible
layer, QuantizeML will simply stop the quantization before this layer and add a
<a class="reference external" href="../api_reference/quantizeml_apis.html#quantizeml.layers.Dequantizer">Dequantizer</a> before it so
that inference is still possible. When such an event occurs, a warning is raised to the user with the
faulty layer name.</p>
<p>While quantization comes with some restrictions on layer order (e.g. MaxPool2D operation should be
placed before ReLU activation), the
<a class="reference external" href="../api_reference/quantizeml_apis.html#quantizeml.models.transforms.sanitize">sanitize</a> helper is
called before quantization to deal with such restrictions and edit the model accordingly.
<a class="reference external" href="../api_reference/quantizeml_apis.html#quantizeml.models.transforms.sanitize">sanitize</a> will also
handle some layers that are not in the
<a class="reference external" href="quantizeml.html#supported-layer-types">supported layer types</a> such as:</p>
<ul class="simple">
<li><p>ZeroPadding2D which is replaced with ‘same’ padding convolution when possible</p></li>
<li><dl class="simple">
<dt>Lambda layers:</dt><dd><ul>
<li><p>Lambda(relu) or Activation(‘relu’) → ReLU,</p></li>
<li><p>Lambda(transpose) → Permute,</p></li>
<li><p>Lambda(reshape) → Reshape,</p></li>
<li><p>Lambda(add) → Add.</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</section>
<section id="model-loading">
<h3>Model loading<a class="headerlink" href="#model-loading" title="Permalink to this headline"></a></h3>
<p>The toolkit offers a
<a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/saving/load_model">keras.models.load_model</a>
wrapper that allows to load models with quantized layers:
<a class="reference external" href="../api_reference/quantizeml_apis.html#quantizeml.models.load_model">quantizeml.models.load_model</a></p>
</section>
</section>
<section id="command-line-interface">
<h2>Command line interface<a class="headerlink" href="#command-line-interface" title="Permalink to this headline"></a></h2>
<p>In addition to the programming interface, QuantizeML toolkit also provides a command-line interface
to perform quantization, dump a quantized model configuration, check a quantized model and insert a
rescaling layer.</p>
<section id="quantize-cli">
<h3>quantize CLI<a class="headerlink" href="#quantize-cli" title="Permalink to this headline"></a></h3>
<p>Quantizing a model through the CLI uses almost the same arguments as the programming interface but
the quantization parameters are split into the parameters: input weight quantization with “-i”,
weight bitwidth with “-w” and activation bitwidth with the “-a” options.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>quantizeml<span class="w"> </span>quantize<span class="w"> </span>-m<span class="w"> </span>model_keras.h5<span class="w"> </span>-i<span class="w"> </span><span class="m">8</span><span class="w"> </span>-w<span class="w"> </span><span class="m">8</span><span class="w"> </span>-a<span class="w"> </span><span class="m">8</span>
</pre></div>
</div>
<p>Note that without calibration options explicitly given, calibration will happen with 1024 randomly
generated samples. It is generally advised to use real samples serialized in a numpy <cite>.npz</cite> file.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>quantizeml<span class="w"> </span>quantize<span class="w"> </span>-m<span class="w"> </span>model_keras.h5<span class="w"> </span>-i<span class="w"> </span><span class="m">8</span><span class="w"> </span>-w<span class="w"> </span><span class="m">8</span><span class="w"> </span>-a<span class="w"> </span><span class="m">8</span><span class="w"> </span>-sa<span class="w"> </span>some_samples.npz<span class="w"> </span>-bs<span class="w"> </span><span class="m">128</span><span class="w"> </span>-e<span class="w"> </span><span class="m">2</span>
</pre></div>
</div>
<p>For akida 1.0 compatibility, it is mandatory to have activations quantized per-tensor instead of
the default per-axis quantization:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>quantizeml<span class="w"> </span>quantize<span class="w"> </span>-m<span class="w"> </span>model_keras.h5<span class="w"> </span>-i<span class="w"> </span><span class="m">8</span><span class="w"> </span>-w<span class="w"> </span><span class="m">4</span><span class="w"> </span>-a<span class="w"> </span><span class="m">4</span><span class="w"> </span>--per_tensor_activations
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The quantize CLI is the same for Keras and ONNX models.</p>
</div>
</section>
<section id="config-cli">
<h3>config CLI<a class="headerlink" href="#config-cli" title="Permalink to this headline"></a></h3>
<p>Advanced users might want to customize the default quantization pattern and this is made possible by
dumping a quantized model configuration to a <cite>.json</cite> file and quantizing again using the “-c”
option.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>quantizeml<span class="w"> </span>config<span class="w"> </span>-m<span class="w"> </span>model_keras_i8_w8_a8.h5<span class="w"> </span>-o<span class="w"> </span>config.json

...<span class="w"> </span>manual<span class="w"> </span>configuration<span class="w"> </span>changes<span class="w"> </span>...

quantizeml<span class="w"> </span>quantize<span class="w"> </span>-m<span class="w"> </span>model_keras.h5<span class="w"> </span>-c<span class="w"> </span>config.json
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Editing a model configuration can be complicated and might have negative effects on quantized
accuracy or even model graph. This should be reserved to users deeply familiar with QuantizeML
concepts.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This is only available for Keras models.</p>
</div>
</section>
<section id="check-cli">
<h3>check CLI<a class="headerlink" href="#check-cli" title="Permalink to this headline"></a></h3>
<p>It is possible to check for quantization errors using the <cite>check</cite> CLI that will report inaccurate
weight scales quantization or saturation in integer operations.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>quantizeml<span class="w"> </span>check<span class="w"> </span>-m<span class="w"> </span>model_keras_i8_w8_a8.h5
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This is only available for Keras models.</p>
</div>
</section>
<section id="insert-rescaling-cli">
<h3>insert_rescaling CLI<a class="headerlink" href="#insert-rescaling-cli" title="Permalink to this headline"></a></h3>
<p>Some models might not include a Rescaling layer in their architecture and have a separated
preprocessing pipeline (ie. moving from [0, 255] images to a [-1, 1] normalized representation). As
having a rescaling layer might be useful, QuantizeML offers the <cite>insert_rescaling</cite> CLI that will add
a Rescaling layer at the beginning of a given model.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>quantizeml<span class="w"> </span>insert_rescaling<span class="w"> </span>-m<span class="w"> </span>model_keras.h5<span class="w"> </span>-s<span class="w"> </span><span class="m">0</span>.007843<span class="w"> </span>-o<span class="w"> </span>-1<span class="w"> </span>-d<span class="w"> </span>model_updated.h5
</pre></div>
</div>
<p>where <span class="math notranslate nohighlight">\(0.007843 = 1/127.5\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This is only available for Keras models.</p>
</div>
</section>
</section>
<section id="supported-layer-types">
<h2>Supported layer types<a class="headerlink" href="#supported-layer-types" title="Permalink to this headline"></a></h2>
<section id="keras-support">
<h3>Keras support<a class="headerlink" href="#keras-support" title="Permalink to this headline"></a></h3>
<p>The QuantizeML toolkit provides quantization of the following layer types which are standard Keras
layers for most part and custom QuantizeML layers for some of them:</p>
<ul class="simple">
<li><dl class="simple">
<dt>Neural layers</dt><dd><ul>
<li><p><a class="reference external" href="../api_reference/quantizeml_apis.html#quantizeml.layers.QuantizedConv2D">Conv2D</a></p></li>
<li><p><a class="reference external" href="../api_reference/quantizeml_apis.html#quantizeml.layers.QuantizedConv2DTranspose">Conv2DTranspose</a></p></li>
<li><p><a class="reference external" href="../api_reference/quantizeml_apis.html#quantizeml.layers.QuantizedDepthwiseConv2D">DepthwiseConv2D</a></p></li>
<li><p><a class="reference external" href="../api_reference/quantizeml_apis.html#quantizeml.layers.QuantizedDepthwiseConv2DTranspose">DepthwiseConv2DTranspose</a>
(custom QuantizeML layer)</p></li>
<li><p><a class="reference external" href="../api_reference/quantizeml_apis.html#quantizeml.layers.QuantizedSeparableConv2D">SeparableConv2D</a></p></li>
<li><p><a class="reference external" href="../api_reference/quantizeml_apis.html#quantizeml.layers.QuantizedDense">Dense</a></p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Transformers</dt><dd><ul>
<li><p><a class="reference external" href="../api_reference/quantizeml_apis.html#quantizeml.layers.QuantizedAttention">Attention</a>
(custom QuantizeML layer)</p></li>
<li><p><a class="reference external" href="../api_reference/quantizeml_apis.html#quantizeml.layers.QuantizedClassToken">ClassToken</a>
(custom QuantizeML layer)</p></li>
<li><p><a class="reference external" href="../api_reference/quantizeml_apis.html#quantizeml.layers.QuantizedAddPositionEmbs">AddPositionEmbs</a>
(custom QuantizeML layer)</p></li>
<li><p><a class="reference external" href="../api_reference/quantizeml_apis.html#quantizeml.layers.QuantizedExtractToken">ExtractToken</a>
(custom QuantizeML layer)</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Skip connections</dt><dd><ul>
<li><p><a class="reference external" href="../api_reference/quantizeml_apis.html#quantizeml.layers.QuantizedAdd">Add</a></p></li>
<li><p><a class="reference external" href="../api_reference/quantizeml_apis.html#quantizeml.layers.QuantizedConcatenate">Concatenate</a></p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Normalization</dt><dd><ul>
<li><p><a class="reference external" href="../api_reference/quantizeml_apis.html#quantizeml.layers.QuantizedBatchNormalization">BatchNormalization</a></p></li>
<li><p><a class="reference external" href="../api_reference/quantizeml_apis.html#quantizeml.layers.QuantizedLayerNormalization">LayerMadNormalization</a>
(custom QuantizeML layer)</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Activations</dt><dd><ul>
<li><p><a class="reference external" href="../api_reference/quantizeml_apis.html#quantizeml.layers.QuantizedReLU">ReLU</a>
(both unbounded and with a max value)</p></li>
<li><p><a class="reference external" href="../api_reference/quantizeml_apis.html#quantizeml.layers.QuantizedShiftmax">Shiftmax</a>
(custom QuantizeML layer)</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Pooling</dt><dd><ul>
<li><p><a class="reference external" href="../api_reference/quantizeml_apis.html#quantizeml.layers.QuantizedMaxPool2D">MaxPool2D</a></p></li>
<li><p><a class="reference external" href="../api_reference/quantizeml_apis.html#quantizeml.layers.QuantizedGlobalAveragePooling2D">GlobalAveragePooling2D</a></p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Reshaping</dt><dd><ul>
<li><p><a class="reference external" href="../api_reference/quantizeml_apis.html#quantizeml.layers.QuantizedFlatten">Flatten</a></p></li>
<li><p><a class="reference external" href="../api_reference/quantizeml_apis.html#quantizeml.layers.QuantizedPermute">Permute</a></p></li>
<li><p><a class="reference external" href="../api_reference/quantizeml_apis.html#quantizeml.layers.QuantizedReshape">Reshape</a></p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Others</dt><dd><ul>
<li><p><a class="reference external" href="../api_reference/quantizeml_apis.html#quantizeml.layers.QuantizedRescaling">Rescaling</a></p></li>
<li><p><a class="reference external" href="../api_reference/quantizeml_apis.html#quantizeml.layers.QuantizedDropout">Dropout</a></p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</section>
<section id="onnx-support">
<h3>ONNX support<a class="headerlink" href="#onnx-support" title="Permalink to this headline"></a></h3>
<p>The QuantizeML toolkit will identify groups of ONNX operations, or ‘patterns’ and quantize towards:</p>
<ul>
<li><p><a class="reference external" href="../api_reference/quantizeml_apis.html#quantizeml.onnx_support.layers.QuantizedConv2D">QuantizedConv2D</a>
when the pattern is:</p>
<blockquote>
<div><ul class="simple">
<li><p>&lt;Conv, Relu, GlobalAveragePool&gt;</p></li>
<li><p>&lt;Conv, Relu, MaxPool&gt;</p></li>
<li><p>&lt;Conv, GlobalAveragePool&gt;</p></li>
<li><p>&lt;Conv, Relu&gt;</p></li>
<li><p>&lt;Conv&gt;</p></li>
</ul>
</div></blockquote>
</li>
<li><p><a class="reference external" href="../api_reference/quantizeml_apis.html#quantizeml.onnx_support.layers.QuantizedDepthwise2D">QuantizedDepthwise2D</a>
when the pattern is:</p>
<blockquote>
<div><ul class="simple">
<li><p>&lt;DepthwiseConv, Relu&gt;</p></li>
<li><p>&lt;DepthwiseConv&gt;</p></li>
</ul>
</div></blockquote>
</li>
<li><p><a class="reference external" href="../api_reference/quantizeml_apis.html#quantizeml.onnx_support.layers.QuantizedDense1D">QuantizedDense1D</a>
when the pattern is:</p>
<blockquote>
<div><ul class="simple">
<li><p>&lt;Flatten, Gemm, Relu&gt;</p></li>
<li><p>&lt;Flatten, Gemm&gt;</p></li>
<li><p>&lt;Gemm, Relu&gt;</p></li>
<li><p>&lt;Gemm&gt;</p></li>
</ul>
</div></blockquote>
</li>
<li><p><a class="reference external" href="../api_reference/quantizeml_apis.html#quantizeml.onnx_support.layers.QuantizedAdd">QuantizedAdd</a>
when the pattern is:</p>
<blockquote>
<div><ul class="simple">
<li><p>&lt;Add&gt;</p></li>
</ul>
</div></blockquote>
</li>
</ul>
<p>While Akida directly supports the most important models, it is not feasible to support all
possibilities. There might occasionally be models which are nearly compatible with Akida but which
will fail to quantize due to just a few incompatibilities. The <a class="reference external" href="../api_reference/quantizeml_apis.html#quantizeml.onnx_support.quantization.custom_pattern_scope">custom pattern feature</a>
allows to handle such models as illustrated in <a class="reference external" href="../examples/quantization/plot_3_custom_patterns.html#sphx-glr-examples-quantization-plot-3-custom-patterns-py">the dedicated advanced example</a>.</p>
<hr class="docutils" />
<dl class="footnote brackets">
<dt class="label" id="fn-1"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>See <a class="reference external" href="https://en.wikipedia.org/wiki/Fixed-point_arithmetic">https://en.wikipedia.org/wiki/Fixed-point_arithmetic</a> for more details on the
arithmetics.</p>
</dd>
</dl>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="akida.html" class="btn btn-neutral float-left" title="Akida user guide" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="cnn2snn.html" class="btn btn-neutral float-right" title="CNN2SNN toolkit" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, BrainChip Holdings Ltd. All Rights Reserved.</p>
  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>