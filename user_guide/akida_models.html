<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Akida models zoo &mdash; Akida Examples  documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery-binder.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery-dataframe.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery-rendered-html.css" type="text/css" />
      <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Hardware constraints" href="hw_constraints.html" />
    <link rel="prev" title="CNN2SNN toolkit" href="cnn2snn.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #989898" >

          
          
          <a href="../index.html">
            
              <img src="../_static/MetaTF_logo.png" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                Akida, 2nd Generation
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../index.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../installation.html#supported-configurations">Supported configurations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../installation.html#quick-installation">Quick installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../installation.html#running-examples">Running examples</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="user_guide.html">User guide</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="akida.html">Akida user guide</a><ul>
<li class="toctree-l3"><a class="reference internal" href="akida.html#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="akida.html#programming-interface">Programming interface</a><ul>
<li class="toctree-l4"><a class="reference internal" href="akida.html#the-akida-model">The Akida Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida.html#akida-layers">Akida layers</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="akida.html#model-hardware-mapping">Model Hardware Mapping</a><ul>
<li class="toctree-l4"><a class="reference internal" href="akida.html#devices">Devices</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida.html#model-mapping">Model mapping</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida.html#advanced-mapping-details-and-hardware-devices-usage">Advanced Mapping Details and Hardware Devices Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida.html#performance-measurement">Performance measurement</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="akida.html#using-akida-edge-learning">Using Akida Edge learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="akida.html#learning-constraints">Learning constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida.html#compiling-a-layer">Compiling a layer</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="quantizeml.html">QuantizeML toolkit</a><ul>
<li class="toctree-l3"><a class="reference internal" href="quantizeml.html#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="quantizeml.html#the-fixedpoint-representation">The FixedPoint representation</a></li>
<li class="toctree-l3"><a class="reference internal" href="quantizeml.html#quantization-flow">Quantization flow</a><ul>
<li class="toctree-l4"><a class="reference internal" href="quantizeml.html#compatibility-constraints">Compatibility constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="quantizeml.html#model-loading">Model loading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="quantizeml.html#command-line-interface">Command line interface</a><ul>
<li class="toctree-l4"><a class="reference internal" href="quantizeml.html#quantize-cli">quantize CLI</a></li>
<li class="toctree-l4"><a class="reference internal" href="quantizeml.html#config-cli">config CLI</a></li>
<li class="toctree-l4"><a class="reference internal" href="quantizeml.html#check-cli">check CLI</a></li>
<li class="toctree-l4"><a class="reference internal" href="quantizeml.html#insert-rescaling-cli">insert_rescaling CLI</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="quantizeml.html#supported-layer-types">Supported layer types</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="cnn2snn.html">CNN2SNN toolkit</a><ul>
<li class="toctree-l3"><a class="reference internal" href="cnn2snn.html#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="cnn2snn.html#conversion-flow">Conversion flow</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn.html#conversion-compatibility">Conversion compatibility</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn.html#command-line-interface">Command-line interface</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cnn2snn.html#handling-akida-1-0-and-akida-2-0-specificities">Handling Akida 1.0 and Akida 2.0 specificities</a></li>
<li class="toctree-l3"><a class="reference internal" href="cnn2snn.html#legacy-quantization-api">Legacy quantization API</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn.html#typical-quantization-scenario">Typical quantization scenario</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn.html#design-compatibility-constraints">Design compatibility constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn.html#id3">Command-line interface</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn.html#layers-considerations">Layers Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn.html#tips-and-tricks">Tips and Tricks</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Akida models zoo</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#command-line-interface-for-model-creation">Command-line interface for model creation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#command-line-interface-for-model-training">Command-line interface for model training</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#kws-training">KWS training</a></li>
<li class="toctree-l4"><a class="reference internal" href="#akidanet-training">AkidaNet training</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#command-line-interface-for-model-evaluation">Command-line interface for model evaluation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#command-line-interface-to-evaluate-model-macs">Command-line interface to evaluate model MACS</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id1">Layer Blocks</a></li>
<li class="toctree-l3"><a class="reference internal" href="#handling-akida-1-0-and-akida-2-0-specificities">Handling Akida 1.0 and Akida 2.0 specificities</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="hw_constraints.html">Hardware constraints</a><ul>
<li class="toctree-l3"><a class="reference internal" href="hw_constraints.html#inputconvolutional">InputConvolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="hw_constraints.html#convolutional">Convolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="hw_constraints.html#separableconvolutional">SeparableConvolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="hw_constraints.html#fullyconnected">FullyConnected</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../api_reference/api_reference.html">API reference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../api_reference/akida_apis.html">Akida runtime</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_apis.html#model">Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_apis.html#layer">Layer</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_apis.html#id1">Layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_apis.html#mapping">Mapping</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_apis.html#akida-v1-layers">Akida V1 layers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_apis.html#akida-v2-layers">Akida V2 layers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_apis.html#layer-parameters">Layer parameters</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_apis.html#layertype">LayerType</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_apis.html#padding">Padding</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_apis.html#pooltype">PoolType</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_apis.html#optimizers">Optimizers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_apis.html#sequence">Sequence</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_apis.html#id2">Sequence</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_apis.html#backendtype">BackendType</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_apis.html#pass">Pass</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_apis.html#device">Device</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_apis.html#id3">Device</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_apis.html#hwversion">HwVersion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_apis.html#hwdevice">HWDevice</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_apis.html#id4">HWDevice</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_apis.html#socdriver">SocDriver</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_apis.html#clockmode">ClockMode</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_apis.html#powermeter">PowerMeter</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_apis.html#np">NP</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_apis.html#tools">Tools</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_apis.html#sparsity">Sparsity</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_apis.html#compatibility">Compatibility</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api_reference/cnn2snn_apis.html">CNN2SNN</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#akida-version">Akida version</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#conversion">Conversion</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#legacy-quantization-api">Legacy quantization API</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#utils">Utils</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#calibration">Calibration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#transforms">Transforms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#constraint">Constraint</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#quantization">Quantization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#quantizers">Quantizers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#quantized-layers">Quantized layers</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api_reference/quantizeml_apis.html">QuantizeML</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/quantizeml_apis.html#layers">Layers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#reshaping">Reshaping</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#activations">Activations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#attention">Attention</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#normalization">Normalization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#convolution">Convolution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#depthwise-convolution">Depthwise convolution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#separable-convolution">Separable convolution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#dense">Dense</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#skip-connection">Skip connection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#pooling">Pooling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#shiftmax">Shiftmax</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#transformers">Transformers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#rescaling">Rescaling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#dropout">Dropout</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#quantizers">Quantizers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#quantization-parameters">Quantization parameters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#calibration">Calibration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#recording">Recording</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/quantizeml_apis.html#models">Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#transforms">Transforms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#quantization">Quantization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#id1">Calibration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#utils">Utils</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/quantizeml_apis.html#tensors">Tensors</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#qtensor">QTensor</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#fixedpoint">FixedPoint</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/quantizeml_apis.html#qfloat">QFloat</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api_reference/akida_models_apis.html">Akida models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_models_apis.html#layer-blocks">Layer blocks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#cnn-blocks">CNN blocks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#transformers-blocks">Transformers blocks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#transposed-blocks">Transposed blocks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#detection-block">Detection block</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_models_apis.html#helpers">Helpers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#gamma-constraint">Gamma constraint</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#unfusing-separableconvolutional">Unfusing SeparableConvolutional</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#extract-samples">Extract samples</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_models_apis.html#knowledge-distillation">Knowledge distillation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_models_apis.html#training">Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_models_apis.html#macs">MACS</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_models_apis.html#model-i-o">Model I/O</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_models_apis.html#utils">Utils</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_models_apis.html#model-zoo">Model zoo</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#akidanet">AkidaNet</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#mobilenet">Mobilenet</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#ds-cnn">DS-CNN</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#vgg">VGG</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#yolo">YOLO</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#pointnet">PointNet++</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#gxnor">GXNOR</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#centernet">CenterNet</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#akidaunet">AkidaUNet</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#transformers">Transformers</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../examples/index.html">Examples</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../examples/index.html#general-examples">General examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_0_global_workflow.html">Global Akida workflow tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_0_global_workflow.html#create-and-train">1. Create and train</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_0_global_workflow.html#quantize">2. Quantize</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_0_global_workflow.html#convert">3. Convert</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_0_global_workflow.html#gxnor-mnist">4. GXNOR/MNIST</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_1_akidanet_imagenet.html">AkidaNet/ImageNet inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_1_akidanet_imagenet.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_1_akidanet_imagenet.html#create-a-keras-akidanet-model">2. Create a Keras AkidaNet model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_1_akidanet_imagenet.html#quantized-model">3. Quantized model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_1_akidanet_imagenet.html#pretrained-quantized-model">4. Pretrained quantized model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_1_akidanet_imagenet.html#conversion-to-akida">5. Conversion to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_1_akidanet_imagenet.html#hardware-mapping-and-performance">6. Hardware mapping and performance</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_2_ds_cnn_kws.html">DS-CNN/KWS inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_2_ds_cnn_kws.html#load-the-preprocessed-dataset">1. Load the preprocessed dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_2_ds_cnn_kws.html#load-a-pre-trained-native-keras-model">2. Load a pre-trained native Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_2_ds_cnn_kws.html#load-a-pre-trained-quantized-keras-model">3. Load a pre-trained quantized Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_2_ds_cnn_kws.html#conversion-to-akida">4. Conversion to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_2_ds_cnn_kws.html#confusion-matrix">5. Confusion matrix</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_3_regression.html">Regression tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_3_regression.html#load-the-dataset">1. Load the dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_3_regression.html#load-a-pre-trained-native-keras-model">2. Load a pre-trained native Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_3_regression.html#load-a-pre-trained-quantized-keras-model">3. Load a pre-trained quantized Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_3_regression.html#conversion-to-akida">4. Conversion to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_3_regression.html#estimate-age-on-a-single-image">5. Estimate age on a single image</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_4_transfer_learning.html">Transfer learning with AkidaNet for PlantVillage</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_transfer_learning.html#transfer-learning-process">Transfer learning process</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_transfer_learning.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_transfer_learning.html#get-a-trained-akidanet-base-model">2. Get a trained AkidaNet base model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_transfer_learning.html#add-a-classification-head-to-the-model">3. Add a classification head to the model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_transfer_learning.html#freeze-the-base-model">4. Freeze the base model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_transfer_learning.html#train-for-a-few-epochs">5. Train for a few epochs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_transfer_learning.html#quantize-the-model">6. Quantize the model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_transfer_learning.html#compute-accuracy">7. Compute accuracy</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_5_voc_yolo_detection.html">YOLO/PASCAL-VOC detection tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_5_voc_yolo_detection.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_5_voc_yolo_detection.html#preprocessing-tools">2. Preprocessing tools</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_5_voc_yolo_detection.html#model-architecture">3. Model architecture</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_5_voc_yolo_detection.html#training">4. Training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_5_voc_yolo_detection.html#performance">5. Performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_5_voc_yolo_detection.html#conversion-to-akida">6. Conversion to Akida</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/index.html#edge-examples">Edge examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/edge/plot_0_edge_learning_vision.html">Akida vision edge learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_0_edge_learning_vision.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_0_edge_learning_vision.html#prepare-akida-model-for-learning">2. Prepare Akida model for learning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_0_edge_learning_vision.html#edge-learning-with-akida">3. Edge learning with Akida</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/edge/plot_1_edge_learning_kws.html">Akida edge learning for keyword spotting</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_1_edge_learning_kws.html#edge-learning-process">1. Edge learning process</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_1_edge_learning_kws.html#dataset-preparation">2. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_1_edge_learning_kws.html#prepare-akida-model-for-learning">3. Prepare Akida model for learning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_1_edge_learning_kws.html#learn-with-akida-using-the-training-set">4. Learn with Akida using the training set</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_1_edge_learning_kws.html#edge-learning">5. Edge learning</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/edge/plot_2_edge_learning_parameters.html">Tips to set Akida learning parameters</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_2_edge_learning_parameters.html#akida-learning-parameters">1. Akida learning parameters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_2_edge_learning_parameters.html#create-akida-model">2. Create Akida model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_2_edge_learning_parameters.html#estimate-the-required-number-of-weights-of-the-trainable-layer">3. Estimate the required number of weights of the trainable layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_2_edge_learning_parameters.html#estimate-the-number-of-neurons-per-class">4. Estimate the number of neurons per class</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/index.html#deprecated-cnn2snn-tutorials">[Deprecated] CNN2SNN tutorials</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/cnn2snn/plot_1_advanced_cnn2snn.html">Advanced CNN2SNN tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_1_advanced_cnn2snn.html#design-a-cnn2snn-quantized-model">1. Design a CNN2SNN quantized model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_1_advanced_cnn2snn.html#weight-quantizer-details">2. Weight Quantizer Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_1_advanced_cnn2snn.html#understanding-quantized-activation">3. Understanding quantized activation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_1_advanced_cnn2snn.html#how-to-deal-with-too-high-scale-factors">4. How to deal with too high scale factors</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../zoo_performances.html">Model zoo performances</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../zoo_performances.html#akida-1-0-models">Akida 1.0 models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../zoo_performances.html#image-icon-ref-image-domain"> Image domain</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../zoo_performances.html#classification">Classification</a></li>
<li class="toctree-l4"><a class="reference internal" href="../zoo_performances.html#object-detection">Object detection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../zoo_performances.html#regression">Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="../zoo_performances.html#face-recognition">Face recognition</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../zoo_performances.html#audio-icon-ref-audio-domain"> Audio domain</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../zoo_performances.html#keyword-spotting">Keyword spotting</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../zoo_performances.html#pointcloud-icon-ref-point-cloud"> Point cloud</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../zoo_performances.html#id1">Classification</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../zoo_performances.html#akida-2-0-models">Akida 2.0 models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../zoo_performances.html#id2"> Image domain</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../zoo_performances.html#id3">Classification</a></li>
<li class="toctree-l4"><a class="reference internal" href="../zoo_performances.html#id6">Object detection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../zoo_performances.html#id7">Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="../zoo_performances.html#id8">Face recognition</a></li>
<li class="toctree-l4"><a class="reference internal" href="../zoo_performances.html#segmentation">Segmentation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../zoo_performances.html#id10"> Audio domain</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../zoo_performances.html#id11">Keyword spotting</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../zoo_performances.html#id12"> Point cloud</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../zoo_performances.html#id13">Classification</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/Brainchip-Inc/akida_examples/releases">Changelog</a></li>
<li class="toctree-l1"><a class="reference external" href="https://support.brainchip.com/portal/home">Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #989898" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Akida Examples</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="user_guide.html">User guide</a></li>
      <li class="breadcrumb-item active">Akida models zoo</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="akida-models-zoo">
<h1>Akida models zoo<a class="headerlink" href="#akida-models-zoo" title="Permalink to this headline"></a></h1>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>Brainchip Akida Models package is a model zoo that offers a set of pre-built akida compatible
models (e.g MobileNet, AkidaNet, ViT), pretrained weights for those models and training scripts.
See the <a class="reference external" href="../api_reference/akida_models_apis.html#model-zoo">model zoo API reference</a> for a
complete list of the available models. The performance of all models from the zoo are reported for
both Akida 1.0 and Akida 2.0 in the <a class="reference external" href="../zoo_performances.html">zoo performances page</a>.
Akida Models also contains a set of
<a class="reference external" href="../api_reference/akida_models_apis.html#layer-blocks">layer blocks</a> that are used to define the
above models.</p>
</section>
<section id="command-line-interface-for-model-creation">
<h2>Command-line interface for model creation<a class="headerlink" href="#command-line-interface-for-model-creation" title="Permalink to this headline"></a></h2>
<p>In addition to <a class="reference external" href="../api_reference/akida_models_apis.html">the programming API</a>,
the Akida Models toolkit provides a command-line interface to instantiate and
save models from the zoo.</p>
<p>Instantiating models using the CLI makes use of the model definitions from the
programming interface with default values. To quantize a given model, the
<a class="reference external" href="quantizeml.html#command-line-interface">QuantizeML quantize CLI</a> should be used.</p>
<p><strong>Examples</strong></p>
<p>Instantiate a DS-CNN network for KWS (keyword spotting):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>akida_models<span class="w"> </span>create<span class="w"> </span>ds_cnn_kws
</pre></div>
</div>
<p>The model is automatically saved to <code class="docutils literal notranslate"><span class="pre">ds_cnn_kws.h5</span></code>.</p>
<p>Some models come with additional parameters that allow a deeper configuration. Examples are given
below.</p>
<p>To build an AkidaNet model with a 64x64 input size, alpha parameter (model
width) equal to 0.35 and 250 classes:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>akida_models<span class="w"> </span>create<span class="w"> </span>akidanet_imagenet<span class="w"> </span>-i<span class="w"> </span><span class="m">64</span><span class="w"> </span>-a<span class="w"> </span><span class="m">0</span>.35<span class="w"> </span>-c<span class="w"> </span><span class="m">250</span>
</pre></div>
</div>
<p>To create a YOLO model with 20 classes, 5 anchors and a model width of 0.5:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>akida_models<span class="w"> </span>create<span class="w"> </span>yolo_base<span class="w"> </span>-c<span class="w"> </span><span class="m">20</span><span class="w"> </span>-na<span class="w"> </span><span class="m">5</span><span class="w"> </span>-a<span class="w"> </span><span class="m">0</span>.5
</pre></div>
</div>
<p>The full parameter list with description can be obtained using the  <code class="docutils literal notranslate"><span class="pre">-h</span></code> or
<code class="docutils literal notranslate"><span class="pre">--help</span></code> argument for each model:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>akida_models<span class="w"> </span>create<span class="w"> </span>akidanet_imagenet<span class="w"> </span>-h

usage:<span class="w"> </span>akida_models<span class="w"> </span>create<span class="w"> </span>akidanet_imagenet<span class="w"> </span><span class="o">[</span>-h<span class="o">]</span>
<span class="w">                                             </span><span class="o">[</span>-i<span class="w"> </span><span class="o">{</span><span class="m">32</span>,64,96,128,160,192,224<span class="o">}]</span>
<span class="w">                                             </span><span class="o">[</span>-a<span class="w"> </span>ALPHA<span class="o">]</span><span class="w"> </span><span class="o">[</span>-c<span class="w"> </span>CLASSES<span class="o">]</span>

optional<span class="w"> </span>arguments:
<span class="w">    </span>-h,<span class="w"> </span>--help<span class="w">            </span>show<span class="w"> </span>this<span class="w"> </span><span class="nb">help</span><span class="w"> </span>message<span class="w"> </span>and<span class="w"> </span><span class="nb">exit</span>
<span class="w">     </span>-c<span class="w"> </span>CLASSES,<span class="w"> </span>--classes<span class="w"> </span>CLASSES
<span class="w">                          </span>The<span class="w"> </span>number<span class="w"> </span>of<span class="w"> </span>classes,<span class="w"> </span>by<span class="w"> </span>default<span class="w"> </span><span class="m">1000</span>.
<span class="w">     </span>-i<span class="w"> </span><span class="o">{</span><span class="m">32</span>,64,96,128,160,192,224<span class="o">}</span>,<span class="w"> </span>--image_size<span class="w"> </span><span class="o">{</span><span class="m">32</span>,64,96,128,160,192,224<span class="o">}</span>
<span class="w">                          </span>The<span class="w"> </span>square<span class="w"> </span>input<span class="w"> </span>image<span class="w"> </span>size,<span class="w"> </span>by<span class="w"> </span>default<span class="w"> </span><span class="m">224</span>.
<span class="w">     </span>-a<span class="w"> </span>ALPHA,<span class="w"> </span>--alpha<span class="w"> </span>ALPHA
<span class="w">                          </span>The<span class="w"> </span>width<span class="w"> </span>of<span class="w"> </span>the<span class="w"> </span>model,<span class="w"> </span>by<span class="w"> </span>default<span class="w"> </span><span class="m">1</span>.0.
</pre></div>
</div>
<p>Current available models for creation are:</p>
<blockquote>
<div><ul class="simple">
<li><p>vgg_utk_face</p></li>
<li><p>convtiny_dvs_handy</p></li>
<li><p>convtiny_dvs_gesture</p></li>
<li><p>ds_cnn_kws</p></li>
<li><p>pointnet_plus_modelnet40</p></li>
<li><p>mobilenet_imagenet</p></li>
<li><p>akidanet_imagenet</p></li>
<li><p>akidanet_imagenet_edge</p></li>
<li><p>akidanet18_imagenet</p></li>
<li><p>yolo_base</p></li>
<li><p>centernet</p></li>
<li><p>gxnor_mnist</p></li>
<li><p>akida_unet_portrait128</p></li>
<li><p>vit_ti16</p></li>
<li><p>bc_vit_ti16</p></li>
<li><p>deit_ti16</p></li>
<li><p>bc_deit_ti16</p></li>
</ul>
</div></blockquote>
</section>
<section id="command-line-interface-for-model-training">
<h2>Command-line interface for model training<a class="headerlink" href="#command-line-interface-for-model-training" title="Permalink to this headline"></a></h2>
<p>The package also comes with a CLI to train models from the zoo.</p>
<p>Training models first requires that a model is created and saved using the CLI described above. Once
a model is ready, training will use dedicated scripts to load and preprocess a dataset and perform
training.</p>
<p>As shown in the examples below, the training CLI should be used along with <code class="docutils literal notranslate"><span class="pre">akida_models</span> <span class="pre">create</span></code>
and <code class="docutils literal notranslate"><span class="pre">quantizeml</span> <span class="pre">quantize</span></code>.</p>
<p>If the quantized model offers acceptable performance, it can be converted into an Akida model,
ready to be loaded on the Akida NSoC using the
<a class="reference external" href="cnn2snn.html#command-line-interface">CNN2SNN convert CLI</a>.</p>
<section id="kws-training">
<h3>KWS training<a class="headerlink" href="#kws-training" title="Permalink to this headline"></a></h3>
<p>KWS training pipeline uses the <code class="docutils literal notranslate"><span class="pre">ds_cnn_kws</span></code> model and the QuantizeML <code class="docutils literal notranslate"><span class="pre">quantize</span></code> CLI. Dataset
loading and preprocessing is done within the training script called by the <code class="docutils literal notranslate"><span class="pre">kws_train</span></code> CLI.</p>
<p><strong>Example</strong></p>
<p>Create a DS-CNN model for KWS, train it over 16 epochs, then quantize it to 4-bit weights and
activations (using a set of samples for calibration only), perform a 16 epochs QAT to recover
accuracy and evaluate.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>akida_models<span class="w"> </span>create<span class="w"> </span>-s<span class="w"> </span>ds_cnn_kws.h5<span class="w"> </span>ds_cnn_kws
kws_train<span class="w"> </span>train<span class="w"> </span>-m<span class="w"> </span>ds_cnn_kws.h5<span class="w"> </span>-s<span class="w"> </span>ds_cnn_kws.h5<span class="w"> </span>-e<span class="w"> </span><span class="m">16</span>

wget<span class="w"> </span>https://data.brainchip.com/dataset-mirror/samples/kws/kws_batch1024.npz
quantizeml<span class="w"> </span>quantize<span class="w"> </span>-m<span class="w"> </span>ds_cnn_kws.h5<span class="w"> </span>-w<span class="w"> </span><span class="m">4</span><span class="w"> </span>-a<span class="w"> </span><span class="m">4</span><span class="w"> </span>-e<span class="w"> </span><span class="m">2</span><span class="w"> </span>-bs<span class="w"> </span><span class="m">100</span><span class="w"> </span>-sa<span class="w"> </span>kws_batch1024.npz
kws_train<span class="w"> </span>train<span class="w"> </span>-m<span class="w"> </span>ds_cnn_kws_i8_w4_a4.h5<span class="w"> </span>-e<span class="w"> </span><span class="m">16</span><span class="w"> </span>-s<span class="w"> </span>ds_cnn_kws_i8_w4_a4.h5
kws_train<span class="w"> </span><span class="nb">eval</span><span class="w"> </span>-m<span class="w"> </span>ds_cnn_kws_i8_w4_a4.h5
</pre></div>
</div>
</section>
<section id="akidanet-training">
<h3>AkidaNet training<a class="headerlink" href="#akidanet-training" title="Permalink to this headline"></a></h3>
<p>AkidaNet training pipeline uses the <code class="docutils literal notranslate"><span class="pre">akidanet_imagenet</span></code> model and the QuantizeML <code class="docutils literal notranslate"><span class="pre">quantize</span></code> CLI.
Dataset loading and preprocessing is done within the training script called by the
<code class="docutils literal notranslate"><span class="pre">imagenet_train</span></code> CLI. Note that ImageNet data must be downloaded from
<a class="reference external" href="https://www.image-net.org/">https://www.image-net.org/</a> first.</p>
<p><strong>Example</strong></p>
<p>Create an AkidaNet 0.5 with resolution 160, train it for 90 epochs then quantize to 4-bit weights
and activations, perform a 10 epochs QAT to recover accuracy, upscale to resolution 224 and
evaluate.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>akida_models<span class="w"> </span>create<span class="w"> </span>-s<span class="w"> </span>akidanet_imagenet_160_alpha_0.5.h5<span class="w"> </span>akidanet_imagenet<span class="w"> </span>-a<span class="w"> </span><span class="m">0</span>.5<span class="w"> </span>-i<span class="w"> </span><span class="m">160</span>
imagenet_train<span class="w"> </span>train<span class="w"> </span>-d<span class="w"> </span>path/to/imagenet/<span class="w"> </span>-e<span class="w"> </span><span class="m">90</span><span class="w"> </span>-m<span class="w"> </span>akidanet_imagenet_160_alpha_0.5.h5<span class="w"> </span><span class="se">\</span>
<span class="w">                     </span>-s<span class="w"> </span>akidanet_imagenet_160_alpha_0.5.h5

wget<span class="w"> </span>https://data.brainchip.com/dataset-mirror/samples/imagenet/imagenet_batch1024_160.npz
quantizeml<span class="w"> </span>quantize<span class="w"> </span>-m<span class="w"> </span>akidanet_imagenet_160_alpha_0.5.h5<span class="w"> </span>-w<span class="w"> </span><span class="m">4</span><span class="w"> </span>-a<span class="w"> </span><span class="m">4</span><span class="w"> </span>-e<span class="w"> </span><span class="m">2</span><span class="w"> </span>-bs<span class="w"> </span><span class="m">100</span><span class="w"> </span><span class="se">\</span>
<span class="w">                     </span>-sa<span class="w"> </span>imagenet_batch1024_160.npz
imagenet_train<span class="w"> </span>tune<span class="w"> </span>-d<span class="w"> </span>path/to/imagenet/<span class="w"> </span>-e<span class="w"> </span><span class="m">10</span><span class="w"> </span>-m<span class="w"> </span>akidanet_imagenet_160_alpha_0.5_i8_w4_a4.h5<span class="w"> </span><span class="se">\</span>
<span class="w">                    </span>-s<span class="w"> </span>akidanet_imagenet_160_alpha_50_i8_w4_a4.h5
imagenet_train<span class="w"> </span>rescale<span class="w"> </span>-i<span class="w"> </span><span class="m">224</span><span class="w"> </span>-m<span class="w"> </span>akidanet_imagenet_160_alpha_0.5_i8_w4_a4.h5<span class="w"> </span><span class="se">\</span>
<span class="w">                       </span>-s<span class="w"> </span>akidanet_imagenet_224_alpha_0.5_i8_w4_a4.h5
imagenet_train<span class="w"> </span><span class="nb">eval</span><span class="w"> </span>-d<span class="w"> </span>path/to/imagenet/<span class="w"> </span>-m<span class="w"> </span>akidanet_imagenet_224_alpha_0.5_i8_w4_a4.h5
</pre></div>
</div>
<p>Current training pipelines available are:</p>
<ul class="simple">
<li><p>utk_face_train</p></li>
<li><p>kws_train</p></li>
<li><p>modelnet40_train</p></li>
<li><p>yolo_train</p></li>
<li><p>dvs_train</p></li>
<li><p>mnist_train</p></li>
<li><p>imagenet_train</p></li>
<li><p>portrait128_train</p></li>
<li><p>centernet_train</p></li>
</ul>
</section>
</section>
<section id="command-line-interface-for-model-evaluation">
<h2>Command-line interface for model evaluation<a class="headerlink" href="#command-line-interface-for-model-evaluation" title="Permalink to this headline"></a></h2>
<p>The CLI also comes with an <code class="docutils literal notranslate"><span class="pre">eval</span></code> action that allows to evaluate model performance, the <code class="docutils literal notranslate"><span class="pre">-ak</span></code>
or <code class="docutils literal notranslate"><span class="pre">--akida</span></code> option allows to convert to Akida then evaluate the model.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>kws_train<span class="w"> </span><span class="nb">eval</span><span class="w"> </span>-m<span class="w"> </span>ds_cnn_kws_i8_w4_a4.h5

kws_train<span class="w"> </span><span class="nb">eval</span><span class="w"> </span>-m<span class="w"> </span>ds_cnn_kws_i8_w4_a4.h5<span class="w"> </span>-ak
</pre></div>
</div>
</section>
<section id="command-line-interface-to-evaluate-model-macs">
<h2>Command-line interface to evaluate model MACS<a class="headerlink" href="#command-line-interface-to-evaluate-model-macs" title="Permalink to this headline"></a></h2>
<p>CLI comes with a <code class="docutils literal notranslate"><span class="pre">macs</span></code> action that allows to compute the number of multiply and accumulate (MACS)
in a model.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>akida_models<span class="w"> </span>macs<span class="w"> </span>-m<span class="w"> </span>akidanet_imagenet_224_alpha_0.5.h5<span class="w"> </span>-v
</pre></div>
</div>
</section>
<section id="id1">
<h2>Layer Blocks<a class="headerlink" href="#id1" title="Permalink to this headline"></a></h2>
<p>In Keras, it is very common for activations or other functions to be defined along with the
processing layer, e.g.:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;relu&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p>In order to ease the design of a Keras model compatible for conversion into an Akida model, a
higher-level interface is proposed with the use of layer blocks. These blocks are available
in the package through:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">akida_models.layer_blocks</span>
</pre></div>
</div>
<p>For instance, the following code snippet sets up the same trio of layers as
those above:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">dense_block</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">add_batchnorm</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">relu_activation</span><span class="o">=</span><span class="s1">&#39;ReLU&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">dense_block</span></code> function will produce a group of layers that we call a “block”.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>To avoid adding the activation layer, add the parameter <code class="docutils literal notranslate"><span class="pre">relu_activation</span> <span class="pre">=</span> <span class="pre">False</span></code> to the
block.</p></li>
<li><p>The ReLU activation max_value can be set in the parameter using a string expression, that is
<code class="docutils literal notranslate"><span class="pre">relu_activation='ReLU6'</span></code> will create a ReLU activation with max_value set to 6.</p></li>
<li><p>The ReLu activation can also be defined as unbounded, that is <code class="docutils literal notranslate"><span class="pre">relu_activation='ReLU'</span></code> (only
supported for models targeting Akida 2.0)</p></li>
</ul>
</div>
<p>Separable layers can be defined as <code class="docutils literal notranslate"><span class="pre">fused</span></code> (Akida 1.0) or <code class="docutils literal notranslate"><span class="pre">unfused</span></code> (Akida 2.0):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">separable_conv_block</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">add_batchnorm</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">relu_activation</span><span class="o">=</span><span class="s1">&#39;ReLU6&#39;</span><span class="p">,</span> <span class="n">fused</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>Placement of the GlobalAveragePooling (GAP) operation is also configurable in layer blocks so that
it comes before the activation (<code class="docutils literal notranslate"><span class="pre">post_relu_gap=False</span></code> for Akida 1.0) or after
(<code class="docutils literal notranslate"><span class="pre">post_relu_gap=True</span></code> for Akida 2.0):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">conv_block</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">relu_activation</span><span class="o">=</span><span class="s1">&#39;ReLU&#39;</span><span class="p">,</span> <span class="n">post_relu_gap</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>The option of including pooling, BatchNormalization layers or activation is directly built into the
provided block modules.</p>
<p>The layer block functions provided are:</p>
<ul class="simple">
<li><p><a class="reference external" href="../api_reference/akida_models_apis.html#akida_models.layer_blocks.conv_block">conv_block</a></p></li>
<li><p><a class="reference external" href="../api_reference/akida_models_apis.html#akida_models.layer_blocks.separable_conv_block">separable_conv_block</a></p></li>
<li><p><a class="reference external" href="../api_reference/akida_models_apis.html#akida_models.layer_blocks.dense_block">dense_block</a></p></li>
<li><p><a class="reference external" href="../api_reference/akida_models_apis.html#akida_models.layer_blocks.mlp_block">mlp_block</a></p></li>
<li><p><a class="reference external" href="../api_reference/akida_models_apis.html#akida_models.layer_blocks.multi_head_attention">multi_head_attention</a></p></li>
<li><p><a class="reference external" href="../api_reference/akida_models_apis.html#akida_models.layer_blocks.transformer_block">transformer_block</a></p></li>
<li><p><a class="reference external" href="../api_reference/akida_models_apis.html#akida_models.layer_blocks.conv_transpose_block">conv_transpose_block</a></p></li>
<li><p><a class="reference external" href="../api_reference/akida_models_apis.html#akida_models.layer_blocks.sepconv_transpose_block">sepconv_transpose_block</a></p></li>
<li><p><a class="reference external" href="../api_reference/akida_models_apis.html#akida_models.layer_blocks.yolo_head_block">yolo_head_block</a></p></li>
</ul>
<p>Most of the parameters for these blocks are identical to those passed to the
corresponding inner processing layers, such as strides and bias. The detailed API is given in the
<a class="reference external" href="../api_reference/akida_models_apis.html#layer-blocks">dedicated section</a>.</p>
</section>
<section id="handling-akida-1-0-and-akida-2-0-specificities">
<h2>Handling Akida 1.0 and Akida 2.0 specificities<a class="headerlink" href="#handling-akida-1-0-and-akida-2-0-specificities" title="Permalink to this headline"></a></h2>
<p>Akida 1.0 and 2.0 specific model architecture requirements are embedded in the returned models
(pretrained or not). By default, the returned models and pretrained model target Akida 2.0. It is
however possible to build and instantiate Akida 1.0 models.</p>
<p>Using the programming interface:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">akida_models</span> <span class="kn">import</span> <span class="n">ds_cnn_kws</span><span class="p">,</span> <span class="n">ds_cnn_kws_pretrained</span>
<span class="kn">from</span> <span class="nn">cnn2snn</span> <span class="kn">import</span> <span class="n">set_akida_version</span><span class="p">,</span> <span class="n">AkidaVersion</span>

<span class="k">with</span> <span class="n">set_akida_version</span><span class="p">(</span><span class="n">AkidaVersion</span><span class="o">.</span><span class="n">v1</span><span class="p">):</span>
   <span class="n">model</span> <span class="o">=</span> <span class="n">ds_cnn_kws</span><span class="p">()</span>
   <span class="n">pretrained</span> <span class="o">=</span> <span class="n">ds_cnn_kws_pretrained</span><span class="p">()</span>
</pre></div>
</div>
<p>Using the CLI interface:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">CNN2SNN_TARGET_AKIDA_VERSION</span><span class="o">=</span>v1<span class="w"> </span>akida_models<span class="w"> </span>create<span class="w"> </span>ds_cnn_kws
</pre></div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="cnn2snn.html" class="btn btn-neutral float-left" title="CNN2SNN toolkit" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="hw_constraints.html" class="btn btn-neutral float-right" title="Hardware constraints" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, BrainChip Holdings Ltd. All Rights Reserved.</p>
  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>