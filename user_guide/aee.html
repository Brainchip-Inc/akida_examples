

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Akida user guide &mdash; Akida Examples  documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery-dataframe.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="CNN2SNN toolkit" href="cnn2snn.html" />
    <link rel="prev" title="Getting started" href="getting_started.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #3f51b5" >
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/akida.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                Akida 1.8.10
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../index.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../installation.html#requirements">Requirements</a></li>
<li class="toctree-l2"><a class="reference internal" href="../installation.html#quick-installation">Quick installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../installation.html#running-examples">Running examples</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="user_guide.html">User guide</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="getting_started.html">Getting started</a><ul>
<li class="toctree-l3"><a class="reference internal" href="getting_started.html#for-beginners">For beginners</a></li>
<li class="toctree-l3"><a class="reference internal" href="getting_started.html#for-users-familiar-with-deep-learning">For users familiar with deep-learning</a></li>
</ul>
</li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Akida user guide</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#the-akida-execution-engine">The Akida Execution Engine</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id1">1. The Spiking Neural Network model</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id2">2. Input data format</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id3">3. Determine training mode</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id4">4. Interpreting outputs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#neural-network-model">Neural Network model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#specifying-the-neural-network-model">Specifying the Neural Network model</a></li>
<li class="toctree-l4"><a class="reference internal" href="#saving-and-loading">Saving and loading</a></li>
<li class="toctree-l4"><a class="reference internal" href="#input-layer-types">Input layer types</a></li>
<li class="toctree-l4"><a class="reference internal" href="#data-processing-layer-types">Data-Processing layer types</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id5">Using Akida Unsupervised Learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#learning-constraints">Learning constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="#compiling-a-layer">Compiling a layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id6">Learning parameters</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="cnn2snn.html">CNN2SNN toolkit</a><ul>
<li class="toctree-l3"><a class="reference internal" href="cnn2snn.html#overview">Overview</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn.html#conversion-workflow">Conversion workflow</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn.html#typical-training-scenario">Typical training scenario</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn.html#design-compatibility-constraints">Design compatibility constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn.html#quantization-compatibility-constraints">Quantization compatibility constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn.html#command-line-interface">Command-line interface</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cnn2snn.html#layers-considerations">Layers Considerations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn.html#supported-layer-types">Supported layer types</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn.html#cnn2snn-quantization-aware-layers">CNN2SNN Quantization-aware layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn.html#training-only-layers">Training-Only Layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn.html#first-layers">First Layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn.html#id6">Final Layers</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cnn2snn.html#tips-and-tricks">Tips and Tricks</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="akida_models.html">Akida models zoo</a><ul>
<li class="toctree-l3"><a class="reference internal" href="akida_models.html#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="akida_models.html#command-line-interface-for-model-creation">Command-line interface for model creation</a></li>
<li class="toctree-l3"><a class="reference internal" href="akida_models.html#command-line-interface-for-model-training">Command-line interface for model training</a><ul>
<li class="toctree-l4"><a class="reference internal" href="akida_models.html#cifar10-training-and-tuning">CIFAR10 training and tuning</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida_models.html#utk-face-training">UTK Face training</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida_models.html#kws-training">KWS training</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida_models.html#yolo-training">YOLO training</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="akida_models.html#id1">Layer Blocks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="akida_models.html#conv-block"><code class="docutils literal notranslate"><span class="pre">conv_block</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="akida_models.html#dense-block"><code class="docutils literal notranslate"><span class="pre">dense_block</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="akida_models.html#separable-conv-block"><code class="docutils literal notranslate"><span class="pre">separable_conv_block</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="hw_constraints.html">Hardware constraints</a><ul>
<li class="toctree-l3"><a class="reference internal" href="hw_constraints.html#input-layer">Input layer</a></li>
<li class="toctree-l3"><a class="reference internal" href="hw_constraints.html#data-processing-layers">Data-Processing layers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="hw_constraints.html#convolutional-layer">Convolutional layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="hw_constraints.html#fully-connected-layer">Fully connected layer</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../api_reference/api_reference.html">API reference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../api_reference/aee_apis.html">Akida Execution Engine</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#model">Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#layer">Layer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#layerstatistics">LayerStatistics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#observer">Observer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#inputdata">InputData</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#inputconvolutional">InputConvolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#fullyconnected">FullyConnected</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#convolutional">Convolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#separableconvolutional">SeparableConvolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#dense">Dense</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#sparse">Sparse</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#coords-to-sparse">coords_to_sparse</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#dense-to-sparse">dense_to_sparse</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#packetize">packetize</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#backend">Backend</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#convolutionmode">ConvolutionMode</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#poolingtype">PoolingType</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#learningtype">LearningType</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#compatibility">Compatibility</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api_reference/cnn2snn_apis.html">CNN2SNN</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#quantize">quantize</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#quantize-layer">quantize_layer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#convert">convert</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#check-model-compatibility">check_model_compatibility</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#weightquantizer">WeightQuantizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#trainableweightquantizer">TrainableWeightQuantizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#maxquantizer">MaxQuantizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#maxperaxisquantizer">MaxPerAxisQuantizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#weightfloat">WeightFloat</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#quantizedconv2d">QuantizedConv2D</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#quantizeddepthwiseconv2d">QuantizedDepthwiseConv2D</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#quantizeddense">QuantizedDense</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#quantizedseparableconv2d">QuantizedSeparableConv2D</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#activationdiscreterelu">ActivationDiscreteRelu</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#quantizedrelu">QuantizedReLU</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api_reference/akida_models_apis.html">Akida models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_models_apis.html#quantization-blocks">Quantization blocks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#conv-block">conv_block</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#separable-conv-block">separable_conv_block</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#dense-block">dense_block</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_models_apis.html#layer-blocks">Layer blocks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#id1">conv_block</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#id2">separable_conv_block</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#id3">dense_block</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_models_apis.html#model-zoo">Model zoo</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#mobilenet">Mobilenet</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#ds-cnn">DS-CNN</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#vgg">VGG</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#yolo">YOLO</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../examples/index.html">Examples</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../examples/plot_gxnor_mnist.html">GXNOR/MNIST inference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_gxnor_mnist.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_gxnor_mnist.html#load-the-pre-trained-akida-model">2. Load the pre-trained Akida model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_gxnor_mnist.html#show-predictions-for-a-single-image">3. Show predictions for a single image</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_gxnor_mnist.html#check-performance">4. Check performance</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/plot_regression.html">Regression tutorial</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_regression.html#load-the-dataset">1. Load the dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_regression.html#load-a-pre-trained-native-keras-model">2. Load a pre-trained native Keras model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_regression.html#load-a-pre-trained-quantized-keras-model-satisfying-akida-nsoc-requirements">3. Load a pre-trained quantized Keras model satisfying Akida NSoC requirements</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_regression.html#conversion-to-akida">4. Conversion to Akida</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_regression.html#estimate-age-on-a-single-image">5. Estimate age on a single image</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/plot_cnn_flow.html">CNN conversion flow tutorial</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_cnn_flow.html#load-and-reshape-mnist-dataset">1. Load and reshape MNIST dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_cnn_flow.html#model-definition">2. Model definition</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_cnn_flow.html#model-training">3. Model training</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_cnn_flow.html#model-quantization">4. Model quantization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_cnn_flow.html#model-fine-tuning-quantization-aware-training">5. Model fine tuning (quantization-aware training)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_cnn_flow.html#model-conversion">6. Model conversion</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/plot_ds_cnn_kws.html">DS-CNN/KWS inference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_ds_cnn_kws.html#load-the-preprocessed-dataset">1. Load the preprocessed dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_ds_cnn_kws.html#load-a-pre-trained-native-keras-model">2. Load a pre-trained native Keras model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_ds_cnn_kws.html#load-a-pre-trained-quantized-keras-model-satisfying-akida-nsoc-requirements">3. Load a pre-trained quantized Keras model satisfying Akida NSoC requirements</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_ds_cnn_kws.html#conversion-to-akida">4. Conversion to Akida</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_ds_cnn_kws.html#confusion-matrix">5. Confusion matrix</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/plot_ds_cnn_cifar10.html">DS-CNN CIFAR10 inference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_ds_cnn_cifar10.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_ds_cnn_cifar10.html#create-a-keras-ds-cnn-model">2. Create a Keras DS-CNN model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_ds_cnn_cifar10.html#quantized-model">3. Quantized model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_ds_cnn_cifar10.html#pretrained-quantized-model">4. Pretrained quantized model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_ds_cnn_cifar10.html#conversion-to-akida">5. Conversion to Akida</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_ds_cnn_cifar10.html#convert-to-akida-model">5.1 Convert to Akida model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_ds_cnn_cifar10.html#check-hardware-compliancy">5.2 Check hardware compliancy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_ds_cnn_cifar10.html#check-performance">5.3 Check performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_ds_cnn_cifar10.html#show-predictions-for-a-random-image">5.4 Show predictions for a random image</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/plot_voc_yolo_detection.html">YOLO/PASCAL-VOC detection tutorial</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_voc_yolo_detection.html#introduction">1. Introduction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_voc_yolo_detection.html#object-detection">1.1 Object detection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_voc_yolo_detection.html#yolo-key-concepts">1.2 YOLO key concepts</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_voc_yolo_detection.html#preprocessing-tools">2. Preprocessing tools</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_voc_yolo_detection.html#model-architecture">3. Model architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_voc_yolo_detection.html#training">4. Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_voc_yolo_detection.html#performance">5. Performance</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_voc_yolo_detection.html#conversion-to-akida">6. Conversion to Akida</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_voc_yolo_detection.html#convert-to-akida-model">6.1 Convert to Akida model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_voc_yolo_detection.html#check-performance">6.1 Check performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_voc_yolo_detection.html#show-predictions-for-a-random-image">6.2 Show predictions for a random image</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/plot_transfer_learning.html">Transfer learning with MobileNet for cats vs. dogs</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_transfer_learning.html#transfer-learning-process">Transfer learning process</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_transfer_learning.html#load-and-preprocess-data">1. Load and preprocess data</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_transfer_learning.html#a-load-and-split-data">1.A - Load and split data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_transfer_learning.html#b-preprocess-the-test-set">1.B - Preprocess the test set</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_transfer_learning.html#c-get-labels">1.C - Get labels</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_transfer_learning.html#modify-a-pre-trained-base-keras-model">2. Modify a pre-trained base Keras model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_transfer_learning.html#a-instantiate-a-keras-base-model">2.A - Instantiate a Keras base model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_transfer_learning.html#b-modify-the-network-for-the-new-task">2.B - Modify the network for the new task</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_transfer_learning.html#train-the-transferred-model-for-the-new-task">3. Train the transferred model for the new task</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_transfer_learning.html#quantize-the-top-layer">4 Quantize the top layer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_transfer_learning.html#convert-to-akida">5. Convert to Akida</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_transfer_learning.html#plot-confusion-matrix">6. Plot confusion matrix</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/plot_mobilenet_imagenet.html">MobileNet/ImageNet inference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_mobilenet_imagenet.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_mobilenet_imagenet.html#create-a-keras-mobilenet-model">2. Create a Keras MobileNet model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_mobilenet_imagenet.html#quantized-model">3. Quantized model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_mobilenet_imagenet.html#pretrained-quantized-model">4. Pretrained quantized model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_mobilenet_imagenet.html#conversion-to-akida">5. Conversion to Akida</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_mobilenet_imagenet.html#convert-to-akida-model">5.1 Convert to Akida model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_mobilenet_imagenet.html#check-hardware-compliancy">5.2 Check hardware compliancy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_mobilenet_imagenet.html#check-performance">5.3 Check performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_mobilenet_imagenet.html#show-predictions-for-a-random-image">5.4 Show predictions for a random image</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/Brainchip-Inc/akida_examples/releases">Changelog</a></li>
<li class="toctree-l1"><a class="reference external" href="https://support.brainchip.com/portal/home">Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Akida Examples</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="user_guide.html">User guide</a> &raquo;</li>
        
      <li>Akida user guide</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="akida-user-guide">
<h1>Akida user guide<a class="headerlink" href="#akida-user-guide" title="Permalink to this headline">¶</a></h1>
<div class="section" id="the-akida-execution-engine">
<h2>The Akida Execution Engine<a class="headerlink" href="#the-akida-execution-engine" title="Permalink to this headline">¶</a></h2>
<p>Performing basic tasks with the Akida Execution Engine is quite straightforward.
However, in fine tuning or for more complex tasks, an understanding of Spiking
Neural Networks, the Akida Neuron Model, and the parameters that define it, is
necessary.</p>
<p>Main steps to use the Akida Execution Engine:</p>
<ol class="arabic simple">
<li><p><a class="reference external" href="#id1">The Spiking Neural Network model</a> – what layer types?</p></li>
<li><p><a class="reference external" href="#id2">Input data format</a></p></li>
<li><p><a class="reference external" href="#id3">Determine training mode</a></p></li>
<li><p><a class="reference external" href="#id4">Interpreting outputs</a></p></li>
</ol>
<div class="section" id="id1">
<h3>1. The Spiking Neural Network model<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<p>All Akida Spiking Networks require an input layer and an additional layer. Other
layers may be required to optimize each Spiking Neural Network model, depending
on the given task. Currently, the Akida Execution Engine supports serial
feedforward architectures.</p>
<p>Three principal layer types are available:</p>
<ul class="simple">
<li><p><a class="reference external" href="../api_reference/aee_apis.html#fullyconnected">FullyConnected</a>
– sometimes described as ‘dense’</p></li>
<li><p><a class="reference external" href="../api_reference/aee_apis.html#convolutional">Convolutional</a>
– or ‘weight-sharing’</p></li>
<li><p><a class="reference external" href="../api_reference/aee_apis.html#separableconvolutional">SeparableConvolutional</a></p></li>
</ul>
<p>These three layer types may be combined as required: a typical model for
processing images like the MNIST dataset, might include the following series of layers:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>InputData ➤ Convolutional ➤ Convolutional ➤ FullyConnected ➤ FullyConnected
</pre></div>
</div>
</div>
<div class="section" id="id2">
<h3>2. Input data format<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<p>There are two initial considerations regarding the nature of the data sent to
the Akida Execution Engine:</p>
<ul class="simple">
<li><p>Data format: events or images?</p></li>
<li><p>Buffering: continuous data or frames?</p></li>
</ul>
<div class="section" id="data-format">
<h4>Data format<a class="headerlink" href="#data-format" title="Permalink to this headline">¶</a></h4>
<p>The Akida Execution Engine processes spikes natively, so for many types of input
data, the User will need to select a method of conversion for the input signal
into an event-based representation, known as: data-to-spike conversion.
The Akida Execution Engine also has built-in data-to-spike converters that will
be available in the Akida NSoC. Currently, data-to-spike converters for
pixel-based and event-based data are available; while other converters will be
added in subsequent releases.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When sending (event-based) pre-generated spikes, the first Akida layer should
be of type: <a class="reference external" href="../api_reference/aee_apis.html#inputdata">InputData</a>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The Akida integrated spike generator for image data (8-bit grayscale or RGB)
is set up by defining the first layer as type:
<a class="reference external" href="../api_reference/aee_apis.html#inputconvolutional">InputConvolutional</a>.
This layer is configurable, either using trained-weights imported from a CNN
or by setting its kernels through the Akida API.
A special <a class="reference external" href="../api_reference/aee_apis.html#inputconvolutional">InputConvolutional</a>
layer with pre-defined kernels is included in the Akida Execution Engine:
<a class="reference external" href="../api_reference/aee_apis.html#inputbcspike">InputBCSpike</a>.
It contains orientation filters to detect edges.</p>
</div>
</div>
<div class="section" id="input-events-buffering">
<h4>Input events buffering<a class="headerlink" href="#input-events-buffering" title="Permalink to this headline">¶</a></h4>
<p>The Akida Execution Engine processes many packets of events at a time through
its <a class="reference external" href="../api_reference/aee_apis.html#inputdata">InputData</a> layer – from
tens to hundreds. This requires buffering the inputs to a layer which, depending
on the nature of the data, is achieved in one of two ways:</p>
<ul class="simple">
<li><p>By default, data is processed in the form of discrete samples: the events
pertaining to a single subject should be grouped and passed to the
Akida Execution Engine as a whole. In that case, no matter how many events are
sent at once they are immediately processed as a single packet.</p></li>
<li><p>For continuous or ongoing types of data, like the stream of data events
generated by a DVS camera, the <a class="reference external" href="../api_reference/aee_apis.html#inputdata">InputData</a>
layer offers a packetizing feature. When packetizing is active, any number of
events can be sent to the Akida Execution Engine one at a time, or in very
large bursts. In either case, when packetizing is enabled, the
<a class="reference external" href="../api_reference/aee_apis.html#inputdata">InputData</a> layer takes events in
the order received until it reaches a predefined <code class="docutils literal notranslate"><span class="pre">packet_size</span></code>, and then
sends that packet for processing.</p></li>
</ul>
</div>
<div class="section" id="data-frames-parallelization">
<h4>Data frames parallelization<a class="headerlink" href="#data-frames-parallelization" title="Permalink to this headline">¶</a></h4>
<p>The Akida Execution Engine can process several image data frames in parallel
through its <a class="reference external" href="../api_reference/aee_apis.html#inputconvolutional">InputConvolutional</a> layer (or its
<a class="reference external" href="../api_reference/aee_apis.html#inputbcspike">InputBCSpike</a> variant). All
image frames should be grouped together inside a 4-dimensional array.</p>
</div>
</div>
<div class="section" id="id3">
<h3>3. Determine training mode<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<p>The Akida NSoC supports for each layer loading pre-trained weights and
parameters. <a class="reference external" href="../api_reference/aee_apis.html#convolutional">Convolutional</a>
and <a class="reference external" href="../api_reference/aee_apis.html#fullyconnected">FullyConnected</a> layers
support online Akida unsupervised learning of new weights from pre-defined
parameters.</p>
<p>The choice between the two modes of operation for each layer depends first on
the input type for that layer: Akida unsupervised learning can only be performed
on input features in the form of binary events.</p>
<p>It also depends on the model architecture: Akida unsupervised learning is more
efficient on shallow models.</p>
<p>For deep models where online learning is required, it is therefore recommended
to import the weights of early layers from a pre-trained CNN, and to apply Akida
unsupervised learning only on the last layer.</p>
<div class="section" id="akida-unsupervised-learning">
<h4>Akida Unsupervised learning<a class="headerlink" href="#akida-unsupervised-learning" title="Permalink to this headline">¶</a></h4>
<p>In this mode, an Akida Layer will typically be compiled with specific learning
parameters, and then undergo a period of feed-forward training by letting it
learn from inputs generated by previous layers from a relevant dataset.</p>
<p>Please refer to <a class="reference external" href="aee.html#id5">Using Akida Unsupervised Learning</a> for details.</p>
</div>
<div class="section" id="cnn-to-snn-conversion">
<h4>CNN to SNN conversion<a class="headerlink" href="#cnn-to-snn-conversion" title="Permalink to this headline">¶</a></h4>
<p>The <a class="reference external" href="cnn2snn.html">CNN2SNN toolkit</a> provides a means to convert a Convolutional
Neural Network (CNN) that was trained using Deep Learning methods to an Akida
<code class="docutils literal notranslate"><span class="pre">Model</span></code> that can later be extended by adding additional Akida <code class="docutils literal notranslate"><span class="pre">Layer</span></code> types.</p>
</div>
</div>
<div class="section" id="id4">
<h3>4. Interpreting outputs<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<p>Note that the Akida Execution Engine has an unsupervised learning mode.
Consequently, while it can learn to respond differently to different classes
of input, by definition it will never attach any meaning or “result” to its
outputs. Therefore, many tasks will require a classification stage. Either
simple labeling of neurons according to the classes that drive them best or,
highly sophisticated classification schemes. The Akida Execution Engine also
includes the possibility of (pseudo) supervised classification.
To summarize, if a training sample has an attached label, it is possible to
define a layer so that, during the training phase, neurons will only be allowed
to learn a representation when their target class is presented. During
inference, the output of the layer will then supply the class label directly.</p>
</div>
</div>
<div class="section" id="neural-network-model">
<h2>Neural Network model<a class="headerlink" href="#neural-network-model" title="Permalink to this headline">¶</a></h2>
<div class="section" id="specifying-the-neural-network-model">
<h3>Specifying the Neural Network model<a class="headerlink" href="#specifying-the-neural-network-model" title="Permalink to this headline">¶</a></h3>
<p>Akida Neural Network models are defined using the sequential API. This comprises
creating a <code class="docutils literal notranslate"><span class="pre">Model</span></code> object and adding layers to it using the
<a class="reference external" href="../api_reference/aee_apis.html#akida.Model.add">.add()</a>
method. The available layers are <a class="reference external" href="../api_reference/aee_apis.html#inputdata">InputData</a>,
<a class="reference external" href="../api_reference/aee_apis.html#inputconvolutional">InputConvolutional</a>,
<a class="reference external" href="../api_reference/aee_apis.html#inputbcspike">InputBCSpike</a>,
<a class="reference external" href="../api_reference/aee_apis.html#fullyconnected">FullyConnected</a>,
<a class="reference external" href="../api_reference/aee_apis.html#convolutional">Convolutional</a> and
<a class="reference external" href="../api_reference/aee_apis.html#separableconvolutional">SeparableConvolutional</a>.
Layers are built with a name and a list of named parameters that are described
in the sections below.</p>
<p>Example of sequential definition of a model:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">akida</span> <span class="kn">import</span> <span class="n">Model</span><span class="p">,</span> <span class="n">InputData</span><span class="p">,</span> <span class="n">FullyConnected</span><span class="p">,</span> <span class="n">LearningType</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">InputData</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;input&quot;</span><span class="p">,</span>
                    <span class="n">input_width</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                    <span class="n">input_height</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                    <span class="n">input_features</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">FullyConnected</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;fully&quot;</span><span class="p">,</span>
                         <span class="n">num_neurons</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                         <span class="n">threshold_fire</span><span class="o">=</span><span class="mi">40</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="saving-and-loading">
<h3>Saving and loading<a class="headerlink" href="#saving-and-loading" title="Permalink to this headline">¶</a></h3>
<p>A <code class="docutils literal notranslate"><span class="pre">Model</span></code> object can be saved to disk for future use with the
<a class="reference external" href="../api_reference/aee_apis.html#akida.Model.save">.save()</a>
method that needs a path for the model. The model will be saved as a file,
typically .fb or .fbz (compressed), that describes its architecture and weights.
A saved model can be reloaded using the <code class="docutils literal notranslate"><span class="pre">Model</span></code> object constructor with the
full path of saved file as a string argument. This will automatically load the
weights associated to the model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;demo_CharacterDVS.fbz&quot;</span><span class="p">)</span>
<span class="n">loaded_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="s2">&quot;demo_CharacterDVS.fbz&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="input-layer-types">
<h3>Input layer types<a class="headerlink" href="#input-layer-types" title="Permalink to this headline">¶</a></h3>
<p>The first layer of a neural network must be one of three possible input layer
types:</p>
<ul class="simple">
<li><p><a class="reference external" href="../api_reference/aee_apis.html#inputdata">InputData</a> – universal
input layer type. The User must apply their own event-generating
transformation on the data – except where the data is already in an
appropriate event-based format, e.g., the output from a neuromorphic camera.</p></li>
<li><p><a class="reference external" href="../api_reference/aee_apis.html#inputconvolutional">InputConvolutional</a>
- image-specific input layer, taking either RGB or grayscale pixel input.</p></li>
<li><p><a class="reference external" href="../api_reference/aee_apis.html#inputbcspike">InputBCSpike</a> – an
<a class="reference external" href="../api_reference/aee_apis.html#inputconvolutional">InputConvolutional</a>
layer with pre-defined weights to detect edges in images.</p></li>
</ul>
</div>
<div class="section" id="data-processing-layer-types">
<h3>Data-Processing layer types<a class="headerlink" href="#data-processing-layer-types" title="Permalink to this headline">¶</a></h3>
<p>After the input layer all subsequent layers will be data-processing layers.</p>
<p>As mentioned before, these layers do not process events in isolation, but rather
as groups of events – typically tens to hundreds of events together.</p>
<p>Each layer contains several neurons that are connected to the layer inputs
according to different topologies defined by the layer type. A weight is
assigned to each connection, and that weight is combined with the input
to modify the neuron potential.</p>
<p>When the neuron potentials have been evaluated, the layer feeds them to an
activation function that may or may not emit a spike.</p>
<p>A data-processing layer can be one of three types:</p>
<ul class="simple">
<li><p><a class="reference external" href="../api_reference/aee_apis.html#fullyconnected">FullyConnected</a> –
each neuron is connected to members of the full set of possible inputs –
hence ‘fully connected’, even though a much smaller number of connections
are likely to be non-zero.</p></li>
<li><p><a class="reference external" href="../api_reference/aee_apis.html#convolutional">Convolutional</a> –
each neuron’s connection weights express a localized filter – typically a
region that is a small fraction of the input’s height and width. This filter
is tested across all x and y positions.</p></li>
<li><p><a class="reference external" href="../api_reference/aee_apis.html#separableconvolutional">SeparableConvolutional</a>
- a variant of the <a class="reference external" href="../api_reference/aee_apis.html#convolutional">Convolutional</a>
layer that is less computationally intensive due to simplified filters.</p></li>
</ul>
<p>Both the <a class="reference external" href="../api_reference/aee_apis.html#fullyconnected">FullyConnected</a>
and <a class="reference external" href="../api_reference/aee_apis.html#convolutional">Convolutional</a>
layer types can be trained using the Akida training algorithm.</p>
<div class="section" id="activation-parameters">
<h4>Activation parameters<a class="headerlink" href="#activation-parameters" title="Permalink to this headline">¶</a></h4>
<p>The Akida activation function uses a quantization scheme to evaluate the neuron
response when its potential goes beyond its firing threshold.
The intensity of the response is measured by dividing the difference between the
potential and the threshold in several quantization intervals that correspond to
a set of quantized spike values. The default quantization scheme is <code class="docutils literal notranslate"><span class="pre">binary</span></code> :
whenever the neuron potential is above the threshold, a spike with a value of
one is emitted.</p>
<p>More generally, if we denote:</p>
<ul class="simple">
<li><p>T the threshold,</p></li>
<li><p>s the length of a quantization interval,</p></li>
<li><p>p the neuron potential,</p></li>
<li><p>Q the quantized activation values.</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">T</span> <span class="pre">+</span> <span class="pre">n</span> <span class="pre">*</span> <span class="pre">s</span> <span class="pre">&lt;</span> <span class="pre">p</span> <span class="pre">&lt;=</span> <span class="pre">T</span> <span class="pre">+</span> <span class="pre">(n</span> <span class="pre">+</span> <span class="pre">1)*s</span> <span class="pre">=&gt;</span> <span class="pre">response</span> <span class="pre">=</span> <span class="pre">Q[n]</span></code></p>
<p>All data-processing layers share the following activation parameters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">threshold_fire</span></code>: integer value which defines the threshold for neurons to
fire or generate an event. When using binary weights and activations, the
activation level of neurons cannot exceed the <code class="docutils literal notranslate"><span class="pre">num_weights</span></code> value.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">threshold_fire_bits</span></code>: &lt; one of <code class="docutils literal notranslate"><span class="pre">[1,</span> <span class="pre">2,</span> <span class="pre">4]</span></code>&gt; Defines the number of
bits used to quantize the neuron response (defaults to one bit for binary).
Quantized activations are integers in the range <code class="docutils literal notranslate"><span class="pre">[1,</span> <span class="pre">2^(weights_bits)</span> <span class="pre">-1]</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">threshold_fire_step</span></code>: a float value, defining the length of the potential
quantization intervals for threshold_fire_bits = 4. For 2 bits, this is 1/4 of
the length of the potentials intervals and it is not relevant for 1 bit.</p></li>
</ul>
</div>
<div class="section" id="pooling-parameters">
<h4>Pooling parameters<a class="headerlink" href="#pooling-parameters" title="Permalink to this headline">¶</a></h4>
<p>The <a class="reference external" href="../api_reference/aee_apis.html#inputconvolutional">InputConvolutional</a>,
<a class="reference external" href="../api_reference/aee_apis.html#convolutional">Convolutional</a> and
<a class="reference external" href="../api_reference/aee_apis.html#separableconvolutional">SeparableConvolutional</a>
layer types share the following pooling parameters:</p>
<ul class="simple">
<li><p>[optional if <code class="docutils literal notranslate"><span class="pre">pooling_type</span> <span class="pre">=</span> <span class="pre">Average</span></code>] <code class="docutils literal notranslate"><span class="pre">pooling_width</span></code> , <code class="docutils literal notranslate"><span class="pre">pooling_height</span></code>:
integer values, sets the width and height of the patch used to perform the
pooling. If not specified it performs a global pooling.</p></li>
<li><p>[optional] <cite>pooling_type</cite>: <a class="reference external" href="../api_reference/aee_apis.html#poolingtype">PoolingType</a>
Sets the effective pooling type (defaults to <cite>NoPooling</cite>):</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">NoPooling</span></code> – no pooling.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Max</span></code> – computing the maximum of each region.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Average</span></code> – computing the average values of each region.</p></li>
</ul>
</li>
<li><p>[optional] <code class="docutils literal notranslate"><span class="pre">pooling_stride_x</span></code>, <code class="docutils literal notranslate"><span class="pre">pooling_stride_y</span></code>: integer values,
set the horizontal and vertical strides applied when sliding the pooling
patches. If not specified, a stride of <code class="docutils literal notranslate"><span class="pre">pooling_width</span></code> or <code class="docutils literal notranslate"><span class="pre">pooling_height</span></code>
is applied.</p></li>
</ul>
</div>
</div>
</div>
<div class="section" id="id5">
<h2>Using Akida Unsupervised Learning<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h2>
<p>The Akida Unsupervised Learning is a unique feature of the Akida NSoC.</p>
<p>In this mode, an Akida Layer will typically be compiled with specific <a class="reference external" href="aee.html#id7">learning
parameters</a> and then undergo a period of feed-forward
unsupervised or semi-supervised training by letting it process inputs generated
by previous layers from a relevant dataset.</p>
<p>Once a layer has been compiled, new learning episodes can be resumed at any
time, even after the model has been saved and reloaded.</p>
<div class="section" id="learning-constraints">
<h3>Learning constraints<a class="headerlink" href="#learning-constraints" title="Permalink to this headline">¶</a></h3>
<p>The following restrictions apply to Akida Unsupervised Learning:</p>
<ul class="simple">
<li><p>Only <a class="reference external" href="../api_reference/aee_apis.html#fullyconnected">FullyConnected</a> and <a class="reference external" href="../api_reference/aee_apis.html#convolutional">Convolutional</a> layers can be trained,</p></li>
<li><p>Only layers with binary weights can be trained,</p></li>
<li><p>Only layers receiving binary inputs can be trained.</p></li>
</ul>
</div>
<div class="section" id="compiling-a-layer">
<h3>Compiling a layer<a class="headerlink" href="#compiling-a-layer" title="Permalink to this headline">¶</a></h3>
<p>For a layer to learn using Akida Unsupervised Learning, it must first be compiled
with specific <a class="reference external" href="aee.html#id7">learning parameters</a>.</p>
<p>The only mandatory parameter is the number of active (non-zero) connections that
each of the layer neurons has with the previous layer, expressed as the number
of active <code class="docutils literal notranslate"><span class="pre">weights</span></code> for each neuron.</p>
<p>Optimizing this value is key to achieving high accuracy in the Akida NSoC.
Broadly speaking, the number of weights should be related to the number of
events expected to compose the items’ or item’s sub-features of interest.</p>
<p>For example, in the MNIST dataset, sample images comprise a 28x28 pixel squares,
with substantial area of blank space, and a number of dark pixels composing the
characters. If only the x-y locations of the dark pixels were sent as events to
the neural network model, each sample would comprise of a few hundred events.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This case is only given as an example – there are better ways of encoding
image data as events.</p>
</div>
<p>To train the MNIST dataset, using a very simple neural network model
configuration with a single <a class="reference external" href="../api_reference/aee_apis.html#fullyconnected">FullyConnected</a>
type layer, it would be acceptable to set the number of weights per neuron in
that range (in this case perhaps: 300). If neurons have more weights than
required, they will acquire some ‘generalization’ that is: tolerance to slightly
different forms of the pattern they are intended to detect – but, will also lose
some ‘specificity’, or become more responsive to members of different classes.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">num_weights</span></code> parameter has other dependencies, such as an
event-generating threshold or <code class="docutils literal notranslate"><span class="pre">threshold_fire</span></code> and the number of events
processed at a time, defined either by the size of an input image, the number
of input events, or the <code class="docutils literal notranslate"><span class="pre">packet_size</span></code> of an <a class="reference external" href="../api_reference/aee_apis.html#inputdata">InputData</a>
layer.</p>
</div>
<div class="section" id="id6">
<h3>Learning parameters<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h3>
<p>The following learning parameters can be specified when compiling a layer:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">num_weights</span></code>: integer value which defines the number of connections for
each neuron and is constant across neurons. When determining a value for
<code class="docutils literal notranslate"><span class="pre">num_weights</span></code> note that the total number of available connections for a
<a class="reference external" href="../api_reference/aee_apis.html#convolutional">Convolutional</a>
layer is not set by the dimensions of the input to the layer, but by the
dimensions of the kernel. Total connections = <code class="docutils literal notranslate"><span class="pre">kernel_height</span></code> x
<code class="docutils literal notranslate"><span class="pre">kernel_width</span></code> x <code class="docutils literal notranslate"><span class="pre">num_features</span></code> , where <code class="docutils literal notranslate"><span class="pre">num_features</span></code> is typically the
<code class="docutils literal notranslate"><span class="pre">num_neurons</span></code> of the preceding layer. <code class="docutils literal notranslate"><span class="pre">num_weights</span></code> should be much smaller
than this value – not more than half, and often much less.</p></li>
<li><p>[optional] <code class="docutils literal notranslate"><span class="pre">num_classes</span></code>: integer value, representing the number of
classes in the dataset. Defining this value sets the learning to a ‘labeled’
mode, when the layer is initialized. The neurons are divided into groups of
equal size, one for each input data class. When an input packet is sent with a
label included, only the neurons corresponding to that input class are allowed
to learn.</p></li>
<li><p>[optional] <code class="docutils literal notranslate"><span class="pre">initial_plasticity</span></code>: floating point value, range 0–1 inclusive
(defaults to 1). It defines the initial plasticity of each neuron’s
connections or how easily the weights will change when learning occurs;
similar in some ways to a learning rate. Typically, this can be set to 1,
especially if the model is initialized with random weights. Plasticity can
only decrease over time, never increase; if set to 0 learning will never occur
in the model.</p></li>
<li><p>[optional] <code class="docutils literal notranslate"><span class="pre">min_plasticity</span></code>: floating point value, range 0–1 inclusive
(defaults to 0.1). It defines the minimum level to which plasticity will decay.</p></li>
<li><p>[optional] <code class="docutils literal notranslate"><span class="pre">plasticity_decay</span></code>: floating point value, range 0–1 inclusive
(defaults to 0.25). It defines the decay of plasticity with each learning
step, relative to the <code class="docutils literal notranslate"><span class="pre">initial_plasticity</span></code>.</p></li>
<li><p>[optional] <code class="docutils literal notranslate"><span class="pre">learning_competition</span></code>: floating point value, range 0–1 inclusive
(defaults to 0). It controls competition between neurons. This is a rather
subtle parameter since there is always substantial competition in learning
between neurons. This parameter controls the competition from neurons that
have already learned – when set to zero, a neuron that has already learned a
given feature will not prevent other neurons from learning similar features.
As <code class="docutils literal notranslate"><span class="pre">learning_competition</span></code> increases such neurons will exert more
competition. This parameter can, however, have serious unintended consequences
for learning stability; we recommend that it should be kept low, and probably
never exceed 0.5.</p></li>
</ul>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="cnn2snn.html" class="btn btn-neutral float-right" title="CNN2SNN toolkit" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="getting_started.html" class="btn btn-neutral float-left" title="Getting started" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright Copyright 2020, BrainChip Holdings Ltd. All Rights Reserved.

    </p>
  </div> 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>