<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Akida user guide &mdash; Akida Examples  documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery-binder.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery-dataframe.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery-rendered-html.css" type="text/css" />
      <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="CNN2SNN toolkit" href="cnn2snn.html" />
    <link rel="prev" title="Getting started" href="getting_started.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #78b3ff" >
            <a href="../index.html">
            <img src="../_static/akida.svg" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                MetaTF 2.0.4
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../index.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../installation.html#requirements">Requirements</a></li>
<li class="toctree-l2"><a class="reference internal" href="../installation.html#quick-installation">Quick installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../installation.html#running-examples">Running examples</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="user_guide.html">User guide</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="getting_started.html">Getting started</a><ul>
<li class="toctree-l3"><a class="reference internal" href="getting_started.html#for-beginners">For beginners</a></li>
<li class="toctree-l3"><a class="reference internal" href="getting_started.html#for-users-familiar-with-deep-learning">For users familiar with deep-learning</a></li>
</ul>
</li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Akida user guide</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#introduction">Introduction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#akida-layers">Akida layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="#input-format">Input Format</a></li>
<li class="toctree-l4"><a class="reference internal" href="#a-versatile-machine-learning-framework">A versatile machine learning framework</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#the-sequential-model">The Sequential model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#specifying-the-model">Specifying the model</a></li>
<li class="toctree-l4"><a class="reference internal" href="#accessing-layer-parameters-and-weights">Accessing layer parameters and weights</a></li>
<li class="toctree-l4"><a class="reference internal" href="#inference">Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="#saving-and-loading">Saving and loading</a></li>
<li class="toctree-l4"><a class="reference internal" href="#input-layer-types">Input layer types</a></li>
<li class="toctree-l4"><a class="reference internal" href="#data-processing-layer-types">Data-Processing layer types</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#model-hardware-mapping">Model Hardware Mapping</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#devices">Devices</a></li>
<li class="toctree-l4"><a class="reference internal" href="#model-mapping">Model mapping</a></li>
<li class="toctree-l4"><a class="reference internal" href="#advanced-mapping-details-and-hardware-devices-usage">Advanced Mapping Details and Hardware Devices Usage</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id1">Using Akida Edge learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#learning-constraints">Learning constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="#compiling-a-layer">Compiling a layer</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="cnn2snn.html">CNN2SNN toolkit</a><ul>
<li class="toctree-l3"><a class="reference internal" href="cnn2snn.html#overview">Overview</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn.html#conversion-workflow">Conversion workflow</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn.html#typical-training-scenario">Typical training scenario</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn.html#design-compatibility-constraints">Design compatibility constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn.html#quantization-compatibility-constraints">Quantization compatibility constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn.html#command-line-interface">Command-line interface</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cnn2snn.html#layers-considerations">Layers Considerations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn.html#supported-layer-types">Supported layer types</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn.html#cnn2snn-quantization-aware-layers">CNN2SNN Quantization-aware layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn.html#training-only-layers">Training-Only Layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn.html#first-layers">First Layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn.html#id6">Final Layers</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cnn2snn.html#tips-and-tricks">Tips and Tricks</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="akida_models.html">Akida models zoo</a><ul>
<li class="toctree-l3"><a class="reference internal" href="akida_models.html#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="akida_models.html#command-line-interface-for-model-creation">Command-line interface for model creation</a></li>
<li class="toctree-l3"><a class="reference internal" href="akida_models.html#command-line-interface-for-model-training">Command-line interface for model training</a><ul>
<li class="toctree-l4"><a class="reference internal" href="akida_models.html#cifar10-training-and-tuning">CIFAR10 training and tuning</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida_models.html#utk-face-training">UTK Face training</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida_models.html#kws-training">KWS training</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida_models.html#yolo-training">YOLO training</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="akida_models.html#command-line-interface-for-model-evaluation">Command-line interface for model evaluation</a></li>
<li class="toctree-l3"><a class="reference internal" href="akida_models.html#id1">Layer Blocks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="akida_models.html#conv-block"><code class="docutils literal notranslate"><span class="pre">conv_block</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="akida_models.html#dense-block"><code class="docutils literal notranslate"><span class="pre">dense_block</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="akida_models.html#separable-conv-block"><code class="docutils literal notranslate"><span class="pre">separable_conv_block</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="hw_constraints.html">Hardware constraints</a><ul>
<li class="toctree-l3"><a class="reference internal" href="hw_constraints.html#akida-nsoc-pre-production">Akida NSoC (Pre-production)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="hw_constraints.html#inputconvolutional">InputConvolutional</a></li>
<li class="toctree-l4"><a class="reference internal" href="hw_constraints.html#convolutional">Convolutional</a></li>
<li class="toctree-l4"><a class="reference internal" href="hw_constraints.html#separableconvolutional">SeparableConvolutional</a></li>
<li class="toctree-l4"><a class="reference internal" href="hw_constraints.html#fullyconnected">FullyConnected</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="hw_constraints.html#akida-nsoc-production">Akida NSoC (Production)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="hw_constraints.html#id1">InputConvolutional</a></li>
<li class="toctree-l4"><a class="reference internal" href="hw_constraints.html#id2">Convolutional</a></li>
<li class="toctree-l4"><a class="reference internal" href="hw_constraints.html#id3">SeparableConvolutional</a></li>
<li class="toctree-l4"><a class="reference internal" href="hw_constraints.html#id4">FullyConnected</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="compatibility.html">Akida versions compatibility</a><ul>
<li class="toctree-l3"><a class="reference internal" href="compatibility.html#upgrading-models-with-legacy-quantizers">Upgrading models with legacy quantizers</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../api_reference/api_reference.html">API reference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../api_reference/aee_apis.html">Akida Execution Engine</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#model">Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#layer">Layer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#layerstatistics">LayerStatistics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#inputdata">InputData</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#inputconvolutional">InputConvolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#fullyconnected">FullyConnected</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#convolutional">Convolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#separableconvolutional">SeparableConvolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#concat">Concat</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#backendtype">BackendType</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#padding">Padding</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#pooltype">PoolType</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#learningtype">LearningType</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#hwversion">HwVersion</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#compatibility">Compatibility</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#device">Device</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#hwdevice">HWDevice</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#socdriver">SocDriver</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#sequence">Sequence</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#program">Program</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#np">NP</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#soc">soc</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#powermeter">PowerMeter</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api_reference/cnn2snn_apis.html">CNN2SNN</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#tool-functions">Tool functions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#quantize">quantize</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#quantize-layer">quantize_layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#convert">convert</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#check-model-compatibility">check_model_compatibility</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#load-quantized-model">load_quantized_model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#load-partial-weights">load_partial_weights</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#quantizers">Quantizers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#weightquantizer">WeightQuantizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#linearweightquantizer">LinearWeightQuantizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#stdweightquantizer">StdWeightQuantizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#trainablestdweightquantizer">TrainableStdWeightQuantizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#maxquantizer">MaxQuantizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#maxperaxisquantizer">MaxPerAxisQuantizer</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#quantized-layers">Quantized layers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#quantizedconv2d">QuantizedConv2D</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#quantizeddepthwiseconv2d">QuantizedDepthwiseConv2D</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#quantizeddense">QuantizedDense</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#quantizedseparableconv2d">QuantizedSeparableConv2D</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#quantizedactivation">QuantizedActivation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#activationdiscreterelu">ActivationDiscreteRelu</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#quantizedrelu">QuantizedReLU</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api_reference/akida_models_apis.html">Akida models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_models_apis.html#layer-blocks">Layer blocks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#conv-block">conv_block</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#separable-conv-block">separable_conv_block</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#dense-block">dense_block</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_models_apis.html#helpers">Helpers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#batchnormalization-gamma-constraint">BatchNormalization gamma constraint</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_models_apis.html#knowledge-distillation">Knowledge distillation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_models_apis.html#pruning">Pruning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_models_apis.html#model-zoo">Model zoo</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#mobilenet">Mobilenet</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#ds-cnn">DS-CNN</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#vgg">VGG</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#yolo">YOLO</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#convtiny">ConvTiny</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#pointnet">PointNet++</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#gxnor">GXNOR</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../examples/index.html">Examples</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../examples/index.html#general-examples">General examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_0_gxnor_mnist.html">GXNOR/MNIST inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_0_gxnor_mnist.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_0_gxnor_mnist.html#create-a-keras-gxnor-model">2. Create a Keras GXNOR model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_0_gxnor_mnist.html#conversion-to-akida">3. Conversion to Akida</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_1_ds_cnn_cifar10.html">DS-CNN CIFAR10 inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_1_ds_cnn_cifar10.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_1_ds_cnn_cifar10.html#create-a-keras-ds-cnn-model">2. Create a Keras DS-CNN model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_1_ds_cnn_cifar10.html#quantized-model">3. Quantized model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_1_ds_cnn_cifar10.html#pretrained-quantized-model">4. Pretrained quantized model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_1_ds_cnn_cifar10.html#conversion-to-akida">5. Conversion to Akida</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_2_mobilenet_imagenet.html">MobileNet/ImageNet inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_2_mobilenet_imagenet.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_2_mobilenet_imagenet.html#create-a-keras-mobilenet-model">2. Create a Keras MobileNet model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_2_mobilenet_imagenet.html#quantized-model">3. Quantized model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_2_mobilenet_imagenet.html#pretrained-quantized-model">4. Pretrained quantized model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_2_mobilenet_imagenet.html#conversion-to-akida">5. Conversion to Akida</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_3_ds_cnn_kws.html">DS-CNN/KWS inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_3_ds_cnn_kws.html#load-the-preprocessed-dataset">1. Load the preprocessed dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_3_ds_cnn_kws.html#load-a-pre-trained-native-keras-model">2. Load a pre-trained native Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_3_ds_cnn_kws.html#load-a-pre-trained-quantized-keras-model-satisfying-akida-nsoc-requirements">3. Load a pre-trained quantized Keras model satisfying Akida NSoC requirements</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_3_ds_cnn_kws.html#conversion-to-akida">4. Conversion to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_3_ds_cnn_kws.html#confusion-matrix">5. Confusion matrix</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_4_regression.html">Regression tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_regression.html#load-the-dataset">1. Load the dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_regression.html#load-a-pre-trained-native-keras-model">2. Load a pre-trained native Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_regression.html#load-a-pre-trained-quantized-keras-model-satisfying-akida-nsoc-requirements">3. Load a pre-trained quantized Keras model satisfying Akida NSoC requirements</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_regression.html#conversion-to-akida">4. Conversion to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_regression.html#estimate-age-on-a-single-image">5. Estimate age on a single image</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_5_transfer_learning.html">Transfer learning with MobileNet for cats vs. dogs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_5_transfer_learning.html#transfer-learning-process">Transfer learning process</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_5_transfer_learning.html#load-and-preprocess-data">1. Load and preprocess data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_5_transfer_learning.html#modify-a-pre-trained-base-keras-model">2. Modify a pre-trained base Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_5_transfer_learning.html#train-the-transferred-model-for-the-new-task">3. Train the transferred model for the new task</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_5_transfer_learning.html#quantize-the-top-layer">4 Quantize the top layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_5_transfer_learning.html#convert-to-akida">5. Convert to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_5_transfer_learning.html#plot-confusion-matrix">6. Plot confusion matrix</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_6_voc_yolo_detection.html">YOLO/PASCAL-VOC detection tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_6_voc_yolo_detection.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_6_voc_yolo_detection.html#preprocessing-tools">2. Preprocessing tools</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_6_voc_yolo_detection.html#model-architecture">3. Model architecture</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_6_voc_yolo_detection.html#training">4. Training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_6_voc_yolo_detection.html#performance">5. Performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_6_voc_yolo_detection.html#conversion-to-akida">6. Conversion to Akida</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/index.html#cnn2snn-tutorials">CNN2SNN tutorials</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/cnn2snn/plot_0_cnn_flow.html">CNN conversion flow tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_0_cnn_flow.html#load-and-reshape-mnist-dataset">1. Load and reshape MNIST dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_0_cnn_flow.html#model-definition">2. Model definition</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_0_cnn_flow.html#model-training">3. Model training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_0_cnn_flow.html#model-quantization">4. Model quantization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_0_cnn_flow.html#model-fine-tuning-quantization-aware-training">5. Model fine tuning (quantization-aware training)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_0_cnn_flow.html#model-conversion">6. Model conversion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/cnn2snn/plot_1_advanced_cnn2snn.html">Advanced CNN2SNN tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_1_advanced_cnn2snn.html#design-a-cnn2snn-quantized-model">1. Design a CNN2SNN quantized model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_1_advanced_cnn2snn.html#weight-quantizer-details">2. Weight Quantizer Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_1_advanced_cnn2snn.html#quantized-activation-layer-details">3. Quantized Activation Layer Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_1_advanced_cnn2snn.html#how-to-deal-with-too-high-scale-factors">4. How to deal with too high scale factors</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/index.html#edge-examples">Edge examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/edge/plot_0_edge_learning_vision.html">Akida vision edge learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_0_edge_learning_vision.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_0_edge_learning_vision.html#prepare-akida-model-for-learning">2. Prepare Akida model for learning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_0_edge_learning_vision.html#edge-learning-with-akida">3. Edge learning with Akida</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/edge/plot_1_edge_learning_kws.html">Akida edge learning for keyword spotting</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_1_edge_learning_kws.html#edge-learning-process">1. Edge learning process</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_1_edge_learning_kws.html#dataset-preparation">2. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_1_edge_learning_kws.html#prepare-akida-model-for-learning">3. Prepare Akida model for learning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_1_edge_learning_kws.html#learn-with-akida-using-the-training-set">4. Learn with Akida using the training set</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_1_edge_learning_kws.html#edge-learning">5. Edge learning</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/edge/plot_2_edge_learning_parameters.html">Tips to set Akida learning parameters</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_2_edge_learning_parameters.html#akida-learning-parameters">1. Akida learning parameters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_2_edge_learning_parameters.html#create-akida-model">2. Create Akida model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_2_edge_learning_parameters.html#estimate-the-required-number-of-weights-of-the-trainable-layer">3. Estimate the required number of weights of the trainable layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_2_edge_learning_parameters.html#estimate-the-number-of-neurons-per-class">4. Estimate the number of neurons per class</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../zoo_performances.html">Model zoo performances</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../zoo_performances.html#image-icon-ref-image-domain"> Image domain</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../zoo_performances.html#classification">Classification</a></li>
<li class="toctree-l3"><a class="reference internal" href="../zoo_performances.html#object-detection">Object detection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../zoo_performances.html#regression">Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="../zoo_performances.html#face-recognition">Face recognition</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../zoo_performances.html#audio-icon-ref-audio-domain"> Audio domain</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../zoo_performances.html#keyword-spotting">Keyword spotting</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../zoo_performances.html#time-icon-ref-time-domain"> Time domain</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../zoo_performances.html#fault-detection">Fault detection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../zoo_performances.html#id1">Classification</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../zoo_performances.html#pointcloud-icon-ref-point-cloud"> Point cloud</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../zoo_performances.html#id2">Classification</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/Brainchip-Inc/akida_examples/releases">Changelog</a></li>
<li class="toctree-l1"><a class="reference external" href="https://support.brainchip.com/portal/home">Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #78b3ff" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Akida Examples</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="user_guide.html">User guide</a> &raquo;</li>
      <li>Akida user guide</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="akida-user-guide">
<h1>Akida user guide<a class="headerlink" href="#akida-user-guide" title="Permalink to this headline"></a></h1>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline"></a></h2>
<p>Like many other machine learning frameworks, the core data structures of Akida
are layers and models, and users familiar with Keras, Tensorflow or Pytorch
should be on familiar grounds.</p>
<p>The main difference between Akida and other machine learning framework is that
instead of modeling traditional Artificial Neural Networks, Akida models aim at
representing <a class="reference external" href="https://en.wikipedia.org/wiki/Spiking_neural_network">Spiking Neural Networks</a>,
i.e. interconnected graphs of neurons that <em>fire</em> when their potential reaches
a predefined <em>threshold</em>.</p>
<p>On another note, unlike other frameworks, Akida layers only use integer inputs,
outputs and weights.</p>
<section id="akida-layers">
<h3>Akida layers<a class="headerlink" href="#akida-layers" title="Permalink to this headline"></a></h3>
<p>Concretely, Akida layers can be represented by the combination of standard
machine learning layers into computation <em>blocks</em>:</p>
<ul class="simple">
<li><p>a Convolutional or Dense layer to evaluate the Spiking Neuron Potential,</p></li>
<li><p>the addition of an inverted bias to represent the firing <em>threshold</em>,</p></li>
<li><p>a ReLu activation to represent the neuron <em>spike</em>.</p></li>
</ul>
<p>Three principal layer types are available:</p>
<ul class="simple">
<li><p><a class="reference external" href="../api_reference/aee_apis.html#fullyconnected">FullyConnected</a>
– sometimes described as ‘dense’</p></li>
<li><p><a class="reference external" href="../api_reference/aee_apis.html#convolutional">Convolutional</a>
– or ‘weight-sharing’</p></li>
<li><p><a class="reference external" href="../api_reference/aee_apis.html#separableconvolutional">SeparableConvolutional</a>,
- a less computationally intensive convolutional layer.</p></li>
</ul>
<p>The weights of Akida layers are N-bit integer: please refer to the <a class="reference external" href="./hw_constraints.html">hardware
constraints</a> for details of the supported bitwidth for
each layer.</p>
</section>
<section id="input-format">
<h3>Input Format<a class="headerlink" href="#input-format" title="Permalink to this headline"></a></h3>
<p>Akida inputs and outputs are 4-dimensional tensors whose first dimension is the
index of a specific sample.</p>
<p>The inputs of Akida layers are N-bit integer: please refer to the <a class="reference external" href="./hw_constraints.html">hardware
constraints</a> for details of the supported bitwidth for
each layer.</p>
</section>
<section id="a-versatile-machine-learning-framework">
<h3>A versatile machine learning framework<a class="headerlink" href="#a-versatile-machine-learning-framework" title="Permalink to this headline"></a></h3>
<p>The Akida machine learning framework supports two main types of models:</p>
<ul class="simple">
<li><p>native SNN models and,</p></li>
<li><p>deep-learning SNN models.</p></li>
</ul>
<section id="native-spiking-neural-networks">
<h4>Native Spiking Neural Networks<a class="headerlink" href="#native-spiking-neural-networks" title="Permalink to this headline"></a></h4>
<p>Native SNN models are typically composed of a few Dense layers.
They require most of the time a specific feature extractor to feed the Dense
layers.
This feature extractor can either be completely external to the model, or be
a deep-learning SNN submodel as defined below.</p>
<p>The last <a class="reference external" href="../api_reference/aee_apis.html#fullyconnected">FullyConnected</a> layer
of a native SNN model can be trained online from individual samples using Akida
edge learning algorithm.
Please refer to <a class="reference external" href="aee.html#id1">Using Akida Edge Learning</a> for details.</p>
</section>
<section id="deep-learning-spiking-neural-networks">
<h4>Deep-learning Spiking Neural Networks<a class="headerlink" href="#deep-learning-spiking-neural-networks" title="Permalink to this headline"></a></h4>
<p>Deep-learning SNN models are genuine CNN models converted to Akida SNN models.</p>
<p>As a consequence, deep-learning professionals do not need to learn any new
framework to start using Akida: they can simply craft their models in
TensorFlow/Keras and convert them to Akida SNN models using the <a class="reference external" href="./cnn2snn.html">CNN2SNN</a>
seamless conversion tool.</p>
<p>Unlike genuine CNN, deep-learning SNN cannot be trained online using
back-propagation: for deep models where online learning is required, it is
therefore recommended to import the weights of early layers from a pre-trained
CNN, and to apply Akida Edge learning only on the last layer.</p>
</section>
</section>
</section>
<section id="the-sequential-model">
<h2>The Sequential model<a class="headerlink" href="#the-sequential-model" title="Permalink to this headline"></a></h2>
<section id="specifying-the-model">
<h3>Specifying the model<a class="headerlink" href="#specifying-the-model" title="Permalink to this headline"></a></h3>
<p>Akida models are defined using the sequential API.</p>
<p>This comprises creating a <code class="docutils literal notranslate"><span class="pre">Model</span></code> object and adding layers to it using the
<a class="reference external" href="../api_reference/aee_apis.html#akida.Model.add">.add()</a> method.</p>
<p>The available layers are <a class="reference external" href="../api_reference/aee_apis.html#inputdata">InputData</a>,
<a class="reference external" href="../api_reference/aee_apis.html#inputconvolutional">InputConvolutional</a>,
<a class="reference external" href="../api_reference/aee_apis.html#fullyconnected">FullyConnected</a>,
<a class="reference external" href="../api_reference/aee_apis.html#convolutional">Convolutional</a>,
<a class="reference external" href="../api_reference/aee_apis.html#separableconvolutional">SeparableConvolutional</a>
and <a class="reference external" href="../api_reference/aee_apis.html#concat">Concat</a>.</p>
<p>Layers are built with a name and a list of named parameters that are described
in the sections below.</p>
<p>Example of sequential definition of a model:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">akida</span> <span class="kn">import</span> <span class="n">Model</span><span class="p">,</span> <span class="n">InputData</span><span class="p">,</span> <span class="n">FullyConnected</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">InputData</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">FullyConnected</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;fully&quot;</span><span class="p">,</span> <span class="n">units</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mi">40</span><span class="p">))</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">Model</span></code> <a class="reference external" href="../api_reference/aee_apis.html#akida.Model.summary">.summary()</a>
method prints a description of the model architecture.</p>
</section>
<section id="accessing-layer-parameters-and-weights">
<h3>Accessing layer parameters and weights<a class="headerlink" href="#accessing-layer-parameters-and-weights" title="Permalink to this headline"></a></h3>
<p>The layers of a <code class="docutils literal notranslate"><span class="pre">Model</span></code> can be accessed either by their index or by their
name.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">first_layer</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">fc</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_layer</span><span class="p">(</span><span class="s2">&quot;fully&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Each layer type has a different set of attributes, available through the <code class="docutils literal notranslate"><span class="pre">Layer</span></code>
<a class="reference external" href="../api_reference/aee_apis.html#akida.Layer.parameters">.parameters</a> member:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">fc</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_layer</span><span class="p">(</span><span class="s2">&quot;fully&quot;</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">fc</span><span class="o">.</span><span class="n">parameters</span><span class="o">.</span><span class="n">units</span>
<span class="n">fc</span><span class="o">.</span><span class="n">parameters</span><span class="o">.</span><span class="n">weights_bits</span> <span class="o">=</span> <span class="mi">2</span>
</pre></div>
</div>
<p>Some layer types also have variables containing weights and thresholds:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">fc</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_layer</span><span class="p">(</span><span class="s2">&quot;fully&quot;</span><span class="p">)</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">fc</span><span class="o">.</span><span class="n">variables</span><span class="p">[</span><span class="s2">&quot;weights&quot;</span><span class="p">]</span>
<span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">fc</span><span class="o">.</span><span class="n">variables</span><span class="p">[</span><span class="s2">&quot;weights&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">weights</span>
</pre></div>
</div>
</section>
<section id="inference">
<h3>Inference<a class="headerlink" href="#inference" title="Permalink to this headline"></a></h3>
<p>The Akida <code class="docutils literal notranslate"><span class="pre">Model</span></code> <a class="reference external" href="../api_reference/aee_apis.html#akida.Model.forward">.forward</a>
method allows to infer the outputs of a specific set of inputs.</p>
<p>Like inference methods in other machine learning frameworks, it simply returns
the integer potentials or activations of the last layer.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="o">...</span>

<span class="c1"># Prepare one sample</span>
<span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">input_shape</span><span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
<span class="c1"># Inference</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">Model</span></code> <a class="reference external" href="../api_reference/aee_apis.html#akida.Model.evaluate">.evaluate</a>
method is very similar to the forward method, but is specifically designed to
replicate the float outputs of a converted CNN: instead of the integer potentials,
it returns float values representing the integer potentials shifted and rescaled using
per-axis constants evaluated during the CNN conversion.</p>
<p>After an inference, the <code class="docutils literal notranslate"><span class="pre">Model</span></code> <a class="reference external" href="../api_reference/aee_apis.html#akida.Model.statistics">.statistics</a> member provides relevant inference statistics.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="o">...</span>

<span class="c1"># Prepare one sample</span>
<span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">+</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">input_shape</span><span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
<span class="c1"># Inference</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">outputs</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span>
</pre></div>
</div>
</section>
<section id="saving-and-loading">
<h3>Saving and loading<a class="headerlink" href="#saving-and-loading" title="Permalink to this headline"></a></h3>
<p>A <code class="docutils literal notranslate"><span class="pre">Model</span></code> object can be saved to disk for future use with the
<a class="reference external" href="../api_reference/aee_apis.html#akida.Model.save">.save()</a>
method that needs a path for the model.</p>
<p>The model will be saved as a file with an .fbz extension that describes its
architecture and weights.</p>
<p>A saved model can be reloaded using the <code class="docutils literal notranslate"><span class="pre">Model</span></code> object constructor with the
full path of saved file as a string argument. This will automatically load the
weights associated to the model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;demo_CharacterDVS.fbz&quot;</span><span class="p">)</span>
<span class="n">loaded_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="s2">&quot;demo_CharacterDVS.fbz&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="input-layer-types">
<h3>Input layer types<a class="headerlink" href="#input-layer-types" title="Permalink to this headline"></a></h3>
<p>The first layer of a model must be one of two possible input layer
types:</p>
<ul class="simple">
<li><p><a class="reference external" href="../api_reference/aee_apis.html#inputdata">InputData</a> – universal
input layer type.</p></li>
<li><p><a class="reference external" href="../api_reference/aee_apis.html#inputconvolutional">InputConvolutional</a>
- image-specific input layer, taking either RGB or grayscale pixel input.</p></li>
</ul>
</section>
<section id="data-processing-layer-types">
<h3>Data-Processing layer types<a class="headerlink" href="#data-processing-layer-types" title="Permalink to this headline"></a></h3>
<p>After the input layer all subsequent layers will be data-processing layers.</p>
<p>Each layer contains several neurons that are connected to the layer inputs
according to different topologies defined by the layer type. A weight is
assigned to each connection, and that weight is combined with the input
to modify the neuron potential.</p>
<p>When the neuron potentials have been evaluated, the layer feeds them to an
activation function that may or may not emit a spike.</p>
<p>A data-processing layer can be one of three types:</p>
<ul class="simple">
<li><p><a class="reference external" href="../api_reference/aee_apis.html#fullyconnected">FullyConnected</a> –
each neuron is connected to members of the full set of possible inputs –
hence ‘fully connected’, even though a much smaller number of connections
are likely to be non-zero.</p></li>
<li><p><a class="reference external" href="../api_reference/aee_apis.html#convolutional">Convolutional</a> –
each neuron’s connection weights express a localized filter – typically a
region that is a small fraction of the input’s height and width. This filter
is tested across all x and y positions.</p></li>
<li><p><a class="reference external" href="../api_reference/aee_apis.html#separableconvolutional">SeparableConvolutional</a>
- a variant of the <a class="reference external" href="../api_reference/aee_apis.html#convolutional">Convolutional</a>
layer that is less computationally intensive due to simplified filters.</p></li>
</ul>
<p>The <a class="reference external" href="../api_reference/aee_apis.html#fullyconnected">FullyConnected</a>
layers can be trained using the Akida Edge learning algorithm if they are the
last layer of a model.</p>
<section id="activation-parameters">
<h4>Activation parameters<a class="headerlink" href="#activation-parameters" title="Permalink to this headline"></a></h4>
<p>The Akida activation function uses a quantization scheme to evaluate the neuron
response when its potential goes beyond its firing threshold.
The intensity of the response is measured by dividing the difference between the
potential and the threshold in several quantization intervals that correspond to
a set of quantized spike values. The default quantization scheme is <code class="docutils literal notranslate"><span class="pre">binary</span></code> :
whenever the neuron potential is above the threshold, a spike with a value of
one is emitted.</p>
<p>More generally, if we denote:</p>
<ul class="simple">
<li><p>T the threshold,</p></li>
<li><p>s the length of a quantization interval,</p></li>
<li><p>p the neuron potential,</p></li>
<li><p>Q the quantized activation values.</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">T</span> <span class="pre">+</span> <span class="pre">n</span> <span class="pre">*</span> <span class="pre">s</span> <span class="pre">&lt;</span> <span class="pre">p</span> <span class="pre">&lt;=</span> <span class="pre">T</span> <span class="pre">+</span> <span class="pre">(n</span> <span class="pre">+</span> <span class="pre">1)*s</span> <span class="pre">=&gt;</span> <span class="pre">response</span> <span class="pre">=</span> <span class="pre">Q[n]</span></code></p>
<p>All data-processing layers share the following activation parameters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">threshold</span></code>: integer value which defines the threshold for neurons to
fire or generate an event. When using binary weights and activations, the
activation level of neurons cannot exceed the <code class="docutils literal notranslate"><span class="pre">num_weights</span></code> value.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">act_bits</span></code>: &lt; one of <code class="docutils literal notranslate"><span class="pre">[1,</span> <span class="pre">2,</span> <span class="pre">4]</span></code>&gt; Defines the number of
bits used to quantize the neuron response (defaults to one bit for binary).
Quantized activations are integers in the range <code class="docutils literal notranslate"><span class="pre">[1,</span> <span class="pre">2^(weights_bits)</span> <span class="pre">-1]</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">act_step</span></code>: a float value, defining the length of the potential
quantization intervals for act_bits = 4. For 2 bits, this is 1/4 of
the length of the potentials intervals and it is not relevant for 1 bit.</p></li>
</ul>
</section>
<section id="pooling-parameters">
<h4>Pooling parameters<a class="headerlink" href="#pooling-parameters" title="Permalink to this headline"></a></h4>
<p>The <a class="reference external" href="../api_reference/aee_apis.html#inputconvolutional">InputConvolutional</a>,
<a class="reference external" href="../api_reference/aee_apis.html#convolutional">Convolutional</a> and
<a class="reference external" href="../api_reference/aee_apis.html#separableconvolutional">SeparableConvolutional</a>
layer types share the following pooling parameters:</p>
<ul class="simple">
<li><p>[optional if <code class="docutils literal notranslate"><span class="pre">pool_type</span> <span class="pre">=</span> <span class="pre">Average</span></code>] <code class="docutils literal notranslate"><span class="pre">pool_size</span></code>: tuple of integer values,
sets the width and height of the patch used to perform the pooling. If not
specified it performs a global pooling.</p></li>
<li><p>[optional] <cite>pool_type</cite>: <a class="reference external" href="../api_reference/aee_apis.html#pooltype">PoolType</a>
Sets the effective pooling type (defaults to <cite>NoPooling</cite>):</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">NoPooling</span></code> – no pooling.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Max</span></code> – computing the maximum of each region.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Average</span></code> – computing the average values of each region.</p></li>
</ul>
</li>
<li><p>[optional] <code class="docutils literal notranslate"><span class="pre">pool_stride</span></code>: tuple of integer values, sets the horizontal
and vertical strides applied when sliding the pooling patches. If not
specified, a stride of <code class="docutils literal notranslate"><span class="pre">pool_size</span></code> is applied.</p></li>
</ul>
</section>
</section>
</section>
<section id="model-hardware-mapping">
<h2>Model Hardware Mapping<a class="headerlink" href="#model-hardware-mapping" title="Permalink to this headline"></a></h2>
<p>By default, Akida models are implicitly mapped on a software backend: in other
words, their inference is computed on the host CPU.</p>
<section id="devices">
<h3>Devices<a class="headerlink" href="#devices" title="Permalink to this headline"></a></h3>
<p>In order to perform the inference of a model on hardware, the corresponding
<code class="docutils literal notranslate"><span class="pre">Model</span></code> object must first be mapped on a specific <code class="docutils literal notranslate"><span class="pre">Device</span></code>.</p>
<p>The Akida <code class="docutils literal notranslate"><span class="pre">Device</span></code> object represents an Akida device, which is entirely
characterized by:</p>
<ul class="simple">
<li><p>its <a class="reference external" href="../api_reference/aee_apis.html#hwversion">hardware version</a>,</p></li>
<li><p>the description of its <a class="reference external" href="../api_reference/aee_apis.html#akida.NP.Mesh">mesh</a> of
processing nodes.</p></li>
</ul>
<section id="discovering-hardware-devices">
<h4>Discovering Hardware Devices<a class="headerlink" href="#discovering-hardware-devices" title="Permalink to this headline"></a></h4>
<p>The list of hardware devices detected on a specific host is available using the
<a class="reference external" href="../api_reference/aee_apis.html#akida.devices">devices()</a> method.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">akida</span> <span class="kn">import</span> <span class="n">devices</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">devices</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">device</span><span class="o">.</span><span class="n">version</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="virtual-devices">
<h4>Virtual Devices<a class="headerlink" href="#virtual-devices" title="Permalink to this headline"></a></h4>
<p>Most of the time, <code class="docutils literal notranslate"><span class="pre">Device</span></code> objects are real hardware devices, but virtual
devices can also be created to allow the mapping of a <code class="docutils literal notranslate"><span class="pre">Model</span></code> on a host that is
not connected to a hardware device.</p>
<p>Virtual devices are simply created by specifying their hardware revision and mesh
topology:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">akida</span> <span class="kn">import</span> <span class="n">Device</span><span class="p">,</span> <span class="n">NSoC_v2</span>

<span class="c1"># Assuming mesh has been defined above</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">Device</span><span class="p">(</span><span class="n">NSoC_v2</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>
</pre></div>
</div>
<p>It is possible to build a virtual device for known hardware devices, by calling
functions <a class="reference external" href="../api_reference/aee_apis.html#akida.AKD1000">AKD1000()</a> and
<a class="reference external" href="../api_reference/aee_apis.html#akida.TwoNodesIP">TwoNodesIP()</a>.</p>
</section>
</section>
<section id="model-mapping">
<h3>Model mapping<a class="headerlink" href="#model-mapping" title="Permalink to this headline"></a></h3>
<p>Mapping a model on a specific device is as simple as calling the <code class="docutils literal notranslate"><span class="pre">Model</span></code>
<a class="reference external" href="../api_reference/aee_apis.html#akida.Model.map">.map()</a> method.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<p>When mapping a model on a device, the information related to the layers and related
variables are processed in such way that the selected device can perform an inference.
If the Model contains layers that are not hardware compatible or is too big to fit on
the device, it will be split in multiple sequences.</p>
<p>The number of sequences, program size for each and how they are mapped are included in
the <code class="docutils literal notranslate"><span class="pre">Model</span></code> <a class="reference external" href="../api_reference/aee_apis.html#akida.Model.summary">.summary()</a> output
after it has been mapped on a device.</p>
</section>
<section id="advanced-mapping-details-and-hardware-devices-usage">
<h3>Advanced Mapping Details and Hardware Devices Usage<a class="headerlink" href="#advanced-mapping-details-and-hardware-devices-usage" title="Permalink to this headline"></a></h3>
<p>Calling <code class="docutils literal notranslate"><span class="pre">Model</span></code> <a class="reference external" href="../api_reference/aee_apis.html#akida.Model.map">.map()</a> might create more
than one “sequence”. In this case, when inference methods are used, each sequence will be chain
loaded on the device to process the given input. Sequences can be obtained using the <code class="docutils literal notranslate"><span class="pre">Model</span></code>
<a class="reference external" href="../api_reference/aee_apis.html#akida.Model.sequences">.sequences()</a>
property, that will return a list of sequence objects. The program used to load
one sequence can be obtained programmatically.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">sequences</span><span class="p">))</span>
<span class="c1"># Assume there is at least one sequence.</span>
<span class="n">sequence</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">sequences</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="c1"># Check which layers have been mapped in this sequence</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sequence</span><span class="o">.</span><span class="n">layers</span><span class="p">)</span>
<span class="c1"># Sequence program can be saved to a bytes object</span>
<span class="n">sequence_program</span> <span class="o">=</span> <span class="n">sequence</span><span class="o">.</span><span class="n">program</span><span class="o">.</span><span class="n">to_buffer</span><span class="p">()</span>
<span class="c1"># Check program size</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sequence_program</span><span class="p">))</span>
</pre></div>
</div>
<p>The information found in the <code class="docutils literal notranslate"><span class="pre">Model</span></code> <a class="reference external" href="../api_reference/aee_apis.html#akida.Model.summary">.summary()</a> can be used to
modify a model to make it fit into less sequences, and program size can be
used to estimate the flash and memory usage on an embedded system that would
use the device.</p>
<p>Once the model has been mapped, the sequences mapped in the Hardware run on the device,
and the sequences mapped in the Software run on the CPU.</p>
<p>One can also force the model to be mapped as one sequence in the hardware device
only by setting the parameter <code class="docutils literal notranslate"><span class="pre">hw_only</span></code> to True (by default the value is False).
See the <a class="reference external" href="../api_reference/aee_apis.html#akida.Model.map">.map()</a> method API for more details.</p>
<p>Note: an exception will be raised if the Model cannot be mapped entirely on the device.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">hw_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>Once the model has been mapped, the inference happens only on the device, and not on the host
CPU except for passing inputs and fetching outputs.</p>
</section>
</section>
<section id="id1">
<h2>Using Akida Edge learning<a class="headerlink" href="#id1" title="Permalink to this headline"></a></h2>
<p>The Akida Edge learning is a unique feature of the Akida IP.</p>
<p>In this mode, an Akida Layer will typically be compiled with specific learning
parameters and then undergo a period of feed-forward unsupervised or
semi-supervised training by letting it process inputs generated by previous
layers from a relevant dataset.</p>
<p>Once a layer has been compiled, new learning episodes can be resumed at any
time, even after the model has been saved and reloaded.</p>
<section id="learning-constraints">
<h3>Learning constraints<a class="headerlink" href="#learning-constraints" title="Permalink to this headline"></a></h3>
<p>Only the last layer of a model can be trained with Akida Edge Learning and must
fulfill the following constraints:</p>
<ul class="simple">
<li><p>must be of type <a class="reference external" href="../api_reference/aee_apis.html#fullyconnected">FullyConnected</a>
or <a class="reference external" href="../api_reference/aee_apis.html#convolutional">Convolutional</a>,</p></li>
<li><p>must have binary weight,</p></li>
<li><p>must receive binary inputs.</p></li>
</ul>
</section>
<section id="compiling-a-layer">
<h3>Compiling a layer<a class="headerlink" href="#compiling-a-layer" title="Permalink to this headline"></a></h3>
<p>For a layer to learn using Akida Edge Learning, it must first be compiled using
the <code class="docutils literal notranslate"><span class="pre">Model</span></code> <a class="reference external" href="../api_reference/aee_apis.html#akida.Model.compile">.compile</a> method.</p>
<p>The following learning parameters can be specified when compiling a layer:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">num_weights</span></code>: integer value which defines the number of connections for
each neuron and is constant across neurons. When determining a value for
<code class="docutils literal notranslate"><span class="pre">num_weights</span></code> note that the total number of available connections for a
<a class="reference external" href="../api_reference/aee_apis.html#convolutional">Convolutional</a>
layer is not set by the dimensions of the input to the layer, but by the
dimensions of the kernel. Total connections = <code class="docutils literal notranslate"><span class="pre">kernel_size</span></code> x
<code class="docutils literal notranslate"><span class="pre">num_features</span></code>, where <code class="docutils literal notranslate"><span class="pre">num_features</span></code> is typically the <code class="docutils literal notranslate"><span class="pre">filters</span></code> or
<code class="docutils literal notranslate"><span class="pre">units</span></code> of the preceding layer. <code class="docutils literal notranslate"><span class="pre">num_weights</span></code> should be much smaller
than this value – not more than half, and often much less.</p></li>
<li><p>[optional] <code class="docutils literal notranslate"><span class="pre">num_classes</span></code>: integer value, representing the number of
classes in the dataset. Defining this value sets the learning to a ‘labeled’
mode, when the layer is initialized. The neurons are divided into groups of
equal size, one for each input data class. When an input packet is sent with a
label included, only the neurons corresponding to that input class are allowed
to learn.</p></li>
<li><p>[optional] <code class="docutils literal notranslate"><span class="pre">initial_plasticity</span></code>: floating point value, range 0–1 inclusive
(defaults to 1). It defines the initial plasticity of each neuron’s
connections or how easily the weights will change when learning occurs;
similar in some ways to a learning rate. Typically, this can be set to 1,
especially if the model is initialized with random weights. Plasticity can
only decrease over time, never increase; if set to 0 learning will never occur
in the model.</p></li>
<li><p>[optional] <code class="docutils literal notranslate"><span class="pre">min_plasticity</span></code>: floating point value, range 0–1 inclusive
(defaults to 0.1). It defines the minimum level to which plasticity will decay.</p></li>
<li><p>[optional] <code class="docutils literal notranslate"><span class="pre">plasticity_decay</span></code>: floating point value, range 0–1 inclusive
(defaults to 0.25). It defines the decay of plasticity with each learning
step, relative to the <code class="docutils literal notranslate"><span class="pre">initial_plasticity</span></code>.</p></li>
<li><p>[optional] <code class="docutils literal notranslate"><span class="pre">learning_competition</span></code>: floating point value, range 0–1 inclusive
(defaults to 0). It controls competition between neurons. This is a rather
subtle parameter since there is always substantial competition in learning
between neurons. This parameter controls the competition from neurons that
have already learned – when set to zero, a neuron that has already learned a
given feature will not prevent other neurons from learning similar features.
As <code class="docutils literal notranslate"><span class="pre">learning_competition</span></code> increases such neurons will exert more
competition. This parameter can, however, have serious unintended consequences
for learning stability; we recommend that it should be kept low, and probably
never exceed 0.5.</p></li>
</ul>
<p>The only mandatory parameter is the number of active (non-zero) connections that
each of the layer neurons has with the previous layer, expressed as the number
of active <code class="docutils literal notranslate"><span class="pre">weights</span></code> for each neuron.</p>
<p>Optimizing this value is key to achieving high accuracy in the Akida NSoC.
Broadly speaking, the number of weights should be related to the number of
events expected to compose the items’ or item’s sub-features of interest.</p>
<p>Tips to set Akida learning parameters are detailed in <a class="reference external" href="../examples/edge/plot_2_edge_learning_parameters.html">the dedicated example</a>.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="getting_started.html" class="btn btn-neutral float-left" title="Getting started" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="cnn2snn.html" class="btn btn-neutral float-right" title="CNN2SNN toolkit" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, BrainChip Holdings Ltd. All Rights Reserved.</p>
  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>