<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Model zoo performances &mdash; Akida Examples  documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/sg_gallery.css" type="text/css" />
      <link rel="stylesheet" href="_static/sg_gallery-binder.css" type="text/css" />
      <link rel="stylesheet" href="_static/sg_gallery-dataframe.css" type="text/css" />
      <link rel="stylesheet" href="_static/sg_gallery-rendered-html.css" type="text/css" />
      <link rel="stylesheet" href="_static/custom.css" type="text/css" />
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="License" href="license.html" />
    <link rel="prev" title="Tips to set Akida learning parameters" href="examples/edge/plot_2_edge_learning_parameters.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #989898" >

          
          
          <a href="index.html">
            
              <img src="_static/MetaTF_logo.png" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                Akida, 2nd Generation
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="index.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="installation.html#supported-configurations">Supported configurations</a></li>
<li class="toctree-l2"><a class="reference internal" href="installation.html#quick-installation">Quick installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="installation.html#running-examples">Running examples</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="user_guide/user_guide.html">User guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="user_guide/getting_started.html">Getting started</a><ul>
<li class="toctree-l3"><a class="reference internal" href="user_guide/getting_started.html#for-beginners">For beginners</a></li>
<li class="toctree-l3"><a class="reference internal" href="user_guide/getting_started.html#for-users-familiar-with-deep-learning">For users familiar with deep-learning</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="user_guide/akida.html">Akida user guide</a><ul>
<li class="toctree-l3"><a class="reference internal" href="user_guide/akida.html#introduction">Introduction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="user_guide/akida.html#akida-layers">Akida layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="user_guide/akida.html#input-format">Input Format</a></li>
<li class="toctree-l4"><a class="reference internal" href="user_guide/akida.html#a-versatile-machine-learning-framework">A versatile machine learning framework</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="user_guide/akida.html#the-sequential-model">The Sequential model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="user_guide/akida.html#specifying-the-model">Specifying the model</a></li>
<li class="toctree-l4"><a class="reference internal" href="user_guide/akida.html#accessing-layer-parameters-and-weights">Accessing layer parameters and weights</a></li>
<li class="toctree-l4"><a class="reference internal" href="user_guide/akida.html#inference">Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="user_guide/akida.html#saving-and-loading">Saving and loading</a></li>
<li class="toctree-l4"><a class="reference internal" href="user_guide/akida.html#input-layer-types">Input layer types</a></li>
<li class="toctree-l4"><a class="reference internal" href="user_guide/akida.html#data-processing-layer-types">Data-Processing layer types</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="user_guide/akida.html#model-hardware-mapping">Model Hardware Mapping</a><ul>
<li class="toctree-l4"><a class="reference internal" href="user_guide/akida.html#devices">Devices</a></li>
<li class="toctree-l4"><a class="reference internal" href="user_guide/akida.html#model-mapping">Model mapping</a></li>
<li class="toctree-l4"><a class="reference internal" href="user_guide/akida.html#advanced-mapping-details-and-hardware-devices-usage">Advanced Mapping Details and Hardware Devices Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="user_guide/akida.html#performances-measurement">Performances measurement</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="user_guide/akida.html#id1">Using Akida Edge learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="user_guide/akida.html#learning-constraints">Learning constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="user_guide/akida.html#compiling-a-layer">Compiling a layer</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="user_guide/quantizeml.html">QuantizeML toolkit</a><ul>
<li class="toctree-l3"><a class="reference internal" href="user_guide/quantizeml.html#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="user_guide/quantizeml.html#the-fixedpoint-representation">The FixedPoint representation</a></li>
<li class="toctree-l3"><a class="reference internal" href="user_guide/quantizeml.html#quantization-flow">Quantization flow</a><ul>
<li class="toctree-l4"><a class="reference internal" href="user_guide/quantizeml.html#compatibility-constraints">Compatibility constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="user_guide/quantizeml.html#model-loading">Model loading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="user_guide/quantizeml.html#command-line-interface">Command line interface</a><ul>
<li class="toctree-l4"><a class="reference internal" href="user_guide/quantizeml.html#quantize-cli">quantize CLI</a></li>
<li class="toctree-l4"><a class="reference internal" href="user_guide/quantizeml.html#config-cli">config CLI</a></li>
<li class="toctree-l4"><a class="reference internal" href="user_guide/quantizeml.html#check-cli">check CLI</a></li>
<li class="toctree-l4"><a class="reference internal" href="user_guide/quantizeml.html#insert-rescaling-cli">insert_rescaling CLI</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="user_guide/quantizeml.html#supported-layer-types">Supported layer types</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="user_guide/cnn2snn.html">CNN2SNN toolkit</a><ul>
<li class="toctree-l3"><a class="reference internal" href="user_guide/cnn2snn.html#overview">Overview</a><ul>
<li class="toctree-l4"><a class="reference internal" href="user_guide/cnn2snn.html#conversion-workflow">Conversion workflow</a></li>
<li class="toctree-l4"><a class="reference internal" href="user_guide/cnn2snn.html#typical-training-scenario">Typical training scenario</a></li>
<li class="toctree-l4"><a class="reference internal" href="user_guide/cnn2snn.html#design-compatibility-constraints">Design compatibility constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="user_guide/cnn2snn.html#quantization-compatibility-constraints">Quantization compatibility constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="user_guide/cnn2snn.html#command-line-interface">Command-line interface</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="user_guide/cnn2snn.html#layers-considerations">Layers Considerations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="user_guide/cnn2snn.html#supported-layer-types">Supported layer types</a></li>
<li class="toctree-l4"><a class="reference internal" href="user_guide/cnn2snn.html#cnn2snn-quantization-aware-layers">CNN2SNN Quantization-aware layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="user_guide/cnn2snn.html#training-only-layers">Training-Only Layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="user_guide/cnn2snn.html#first-layers">First Layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="user_guide/cnn2snn.html#id6">Final Layers</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="user_guide/cnn2snn.html#tips-and-tricks">Tips and Tricks</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="user_guide/akida_models.html">Akida models zoo</a><ul>
<li class="toctree-l3"><a class="reference internal" href="user_guide/akida_models.html#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="user_guide/akida_models.html#command-line-interface-for-model-creation">Command-line interface for model creation</a></li>
<li class="toctree-l3"><a class="reference internal" href="user_guide/akida_models.html#command-line-interface-for-model-training">Command-line interface for model training</a><ul>
<li class="toctree-l4"><a class="reference internal" href="user_guide/akida_models.html#utk-face-training">UTK Face training</a></li>
<li class="toctree-l4"><a class="reference internal" href="user_guide/akida_models.html#kws-training">KWS training</a></li>
<li class="toctree-l4"><a class="reference internal" href="user_guide/akida_models.html#yolo-training">YOLO training</a></li>
<li class="toctree-l4"><a class="reference internal" href="user_guide/akida_models.html#akidanet-training">AkidaNet training</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="user_guide/akida_models.html#command-line-interface-for-model-evaluation">Command-line interface for model evaluation</a></li>
<li class="toctree-l3"><a class="reference internal" href="user_guide/akida_models.html#command-line-interface-to-evaluate-model-macs">Command-line interface to evaluate model MACS</a></li>
<li class="toctree-l3"><a class="reference internal" href="user_guide/akida_models.html#id1">Layer Blocks</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="user_guide/hw_constraints.html">Hardware constraints</a><ul>
<li class="toctree-l3"><a class="reference internal" href="user_guide/hw_constraints.html#inputconvolutional">InputConvolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="user_guide/hw_constraints.html#convolutional">Convolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="user_guide/hw_constraints.html#separableconvolutional">SeparableConvolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="user_guide/hw_constraints.html#fullyconnected">FullyConnected</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="api_reference/api_reference.html">API reference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="api_reference/akida_apis.html">Akida runtime</a><ul>
<li class="toctree-l3"><a class="reference internal" href="api_reference/akida_apis.html#model">Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="api_reference/akida_apis.html#layer">Layer</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api_reference/akida_apis.html#id1">Layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="api_reference/akida_apis.html#mapping">Mapping</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="api_reference/akida_apis.html#akida-v1-layers">Akida V1 layers</a></li>
<li class="toctree-l3"><a class="reference internal" href="api_reference/akida_apis.html#akida-v2-layers">Akida V2 layers</a></li>
<li class="toctree-l3"><a class="reference internal" href="api_reference/akida_apis.html#layer-parameters">Layer parameters</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api_reference/akida_apis.html#layertype">LayerType</a></li>
<li class="toctree-l4"><a class="reference internal" href="api_reference/akida_apis.html#padding">Padding</a></li>
<li class="toctree-l4"><a class="reference internal" href="api_reference/akida_apis.html#pooltype">PoolType</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="api_reference/akida_apis.html#optimizers">Optimizers</a></li>
<li class="toctree-l3"><a class="reference internal" href="api_reference/akida_apis.html#sequence">Sequence</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api_reference/akida_apis.html#id2">Sequence</a></li>
<li class="toctree-l4"><a class="reference internal" href="api_reference/akida_apis.html#backendtype">BackendType</a></li>
<li class="toctree-l4"><a class="reference internal" href="api_reference/akida_apis.html#pass">Pass</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="api_reference/akida_apis.html#device">Device</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api_reference/akida_apis.html#id3">Device</a></li>
<li class="toctree-l4"><a class="reference internal" href="api_reference/akida_apis.html#hwversion">HwVersion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="api_reference/akida_apis.html#hwdevice">HWDevice</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api_reference/akida_apis.html#id4">HWDevice</a></li>
<li class="toctree-l4"><a class="reference internal" href="api_reference/akida_apis.html#socdriver">SocDriver</a></li>
<li class="toctree-l4"><a class="reference internal" href="api_reference/akida_apis.html#clockmode">ClockMode</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="api_reference/akida_apis.html#powermeter">PowerMeter</a></li>
<li class="toctree-l3"><a class="reference internal" href="api_reference/akida_apis.html#np">NP</a></li>
<li class="toctree-l3"><a class="reference internal" href="api_reference/akida_apis.html#tools">Tools</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api_reference/akida_apis.html#sparsity">Sparsity</a></li>
<li class="toctree-l4"><a class="reference internal" href="api_reference/akida_apis.html#compatibility">Compatibility</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="api_reference/cnn2snn_apis.html">CNN2SNN</a><ul>
<li class="toctree-l3"><a class="reference internal" href="api_reference/cnn2snn_apis.html#akida-version">Akida version</a></li>
<li class="toctree-l3"><a class="reference internal" href="api_reference/cnn2snn_apis.html#conversion">Conversion</a></li>
<li class="toctree-l3"><a class="reference internal" href="api_reference/cnn2snn_apis.html#legacy-quantization-api">Legacy quantization API</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api_reference/cnn2snn_apis.html#utils">Utils</a></li>
<li class="toctree-l4"><a class="reference internal" href="api_reference/cnn2snn_apis.html#calibration">Calibration</a></li>
<li class="toctree-l4"><a class="reference internal" href="api_reference/cnn2snn_apis.html#transforms">Transforms</a></li>
<li class="toctree-l4"><a class="reference internal" href="api_reference/cnn2snn_apis.html#constraint">Constraint</a></li>
<li class="toctree-l4"><a class="reference internal" href="api_reference/cnn2snn_apis.html#quantization">Quantization</a></li>
<li class="toctree-l4"><a class="reference internal" href="api_reference/cnn2snn_apis.html#quantizers">Quantizers</a></li>
<li class="toctree-l4"><a class="reference internal" href="api_reference/cnn2snn_apis.html#quantized-layers">Quantized layers</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="api_reference/quantizeml_apis.html">QuantizeML</a><ul>
<li class="toctree-l3"><a class="reference internal" href="api_reference/quantizeml_apis.html#layers">Layers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api_reference/quantizeml_apis.html#reshaping">Reshaping</a></li>
<li class="toctree-l4"><a class="reference internal" href="api_reference/quantizeml_apis.html#activations">Activations</a></li>
<li class="toctree-l4"><a class="reference internal" href="api_reference/quantizeml_apis.html#attention">Attention</a></li>
<li class="toctree-l4"><a class="reference internal" href="api_reference/quantizeml_apis.html#normalization">Normalization</a></li>
<li class="toctree-l4"><a class="reference internal" href="api_reference/quantizeml_apis.html#convolution">Convolution</a></li>
<li class="toctree-l4"><a class="reference internal" href="api_reference/quantizeml_apis.html#depthwise-convolution">Depthwise convolution</a></li>
<li class="toctree-l4"><a class="reference internal" href="api_reference/quantizeml_apis.html#separable-convolution">Separable convolution</a></li>
<li class="toctree-l4"><a class="reference internal" href="api_reference/quantizeml_apis.html#dense">Dense</a></li>
<li class="toctree-l4"><a class="reference internal" href="api_reference/quantizeml_apis.html#skip-connection">Skip connection</a></li>
<li class="toctree-l4"><a class="reference internal" href="api_reference/quantizeml_apis.html#pooling">Pooling</a></li>
<li class="toctree-l4"><a class="reference internal" href="api_reference/quantizeml_apis.html#shiftmax">Shiftmax</a></li>
<li class="toctree-l4"><a class="reference internal" href="api_reference/quantizeml_apis.html#transformers">Transformers</a></li>
<li class="toctree-l4"><a class="reference internal" href="api_reference/quantizeml_apis.html#rescaling">Rescaling</a></li>
<li class="toctree-l4"><a class="reference internal" href="api_reference/quantizeml_apis.html#dropout">Dropout</a></li>
<li class="toctree-l4"><a class="reference internal" href="api_reference/quantizeml_apis.html#quantizers">Quantizers</a></li>
<li class="toctree-l4"><a class="reference internal" href="api_reference/quantizeml_apis.html#quantization-parameters">Quantization parameters</a></li>
<li class="toctree-l4"><a class="reference internal" href="api_reference/quantizeml_apis.html#calibration">Calibration</a></li>
<li class="toctree-l4"><a class="reference internal" href="api_reference/quantizeml_apis.html#recording">Recording</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="api_reference/quantizeml_apis.html#models">Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api_reference/quantizeml_apis.html#transforms">Transforms</a></li>
<li class="toctree-l4"><a class="reference internal" href="api_reference/quantizeml_apis.html#quantization">Quantization</a></li>
<li class="toctree-l4"><a class="reference internal" href="api_reference/quantizeml_apis.html#id1">Calibration</a></li>
<li class="toctree-l4"><a class="reference internal" href="api_reference/quantizeml_apis.html#utils">Utils</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="api_reference/quantizeml_apis.html#tensors">Tensors</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api_reference/quantizeml_apis.html#qtensor">QTensor</a></li>
<li class="toctree-l4"><a class="reference internal" href="api_reference/quantizeml_apis.html#fixedpoint">FixedPoint</a></li>
<li class="toctree-l4"><a class="reference internal" href="api_reference/quantizeml_apis.html#qfloat">QFloat</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="api_reference/akida_models_apis.html">Akida models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="api_reference/akida_models_apis.html#layer-blocks">Layer blocks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api_reference/akida_models_apis.html#cnn-blocks">CNN blocks</a></li>
<li class="toctree-l4"><a class="reference internal" href="api_reference/akida_models_apis.html#transformers-blocks">Transformers blocks</a></li>
<li class="toctree-l4"><a class="reference internal" href="api_reference/akida_models_apis.html#transposed-blocks">Transposed blocks</a></li>
<li class="toctree-l4"><a class="reference internal" href="api_reference/akida_models_apis.html#detection-block">Detection block</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="api_reference/akida_models_apis.html#helpers">Helpers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api_reference/akida_models_apis.html#gamma-constraint">Gamma constraint</a></li>
<li class="toctree-l4"><a class="reference internal" href="api_reference/akida_models_apis.html#unfusing-separableconvolutional">Unfusing SeparableConvolutional</a></li>
<li class="toctree-l4"><a class="reference internal" href="api_reference/akida_models_apis.html#extract-samples">Extract samples</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="api_reference/akida_models_apis.html#knowledge-distillation">Knowledge distillation</a></li>
<li class="toctree-l3"><a class="reference internal" href="api_reference/akida_models_apis.html#training">Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="api_reference/akida_models_apis.html#macs">MACS</a></li>
<li class="toctree-l3"><a class="reference internal" href="api_reference/akida_models_apis.html#model-i-o">Model I/O</a></li>
<li class="toctree-l3"><a class="reference internal" href="api_reference/akida_models_apis.html#utils">Utils</a></li>
<li class="toctree-l3"><a class="reference internal" href="api_reference/akida_models_apis.html#model-zoo">Model zoo</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api_reference/akida_models_apis.html#akidanet">AkidaNet</a></li>
<li class="toctree-l4"><a class="reference internal" href="api_reference/akida_models_apis.html#mobilenet">Mobilenet</a></li>
<li class="toctree-l4"><a class="reference internal" href="api_reference/akida_models_apis.html#ds-cnn">DS-CNN</a></li>
<li class="toctree-l4"><a class="reference internal" href="api_reference/akida_models_apis.html#vgg">VGG</a></li>
<li class="toctree-l4"><a class="reference internal" href="api_reference/akida_models_apis.html#yolo">YOLO</a></li>
<li class="toctree-l4"><a class="reference internal" href="api_reference/akida_models_apis.html#pointnet">PointNet++</a></li>
<li class="toctree-l4"><a class="reference internal" href="api_reference/akida_models_apis.html#gxnor">GXNOR</a></li>
<li class="toctree-l4"><a class="reference internal" href="api_reference/akida_models_apis.html#centernet">CenterNet</a></li>
<li class="toctree-l4"><a class="reference internal" href="api_reference/akida_models_apis.html#akidaunet">AkidaUNet</a></li>
<li class="toctree-l4"><a class="reference internal" href="api_reference/akida_models_apis.html#transformers">Transformers</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="examples/index.html">Examples</a><ul>
<li class="toctree-l2"><a class="reference internal" href="examples/index.html#general-examples">General examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples/index.html#cnn2snn-tutorials">CNN2SNN tutorials</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples/index.html#edge-examples">Edge examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="examples/general/index.html">General examples</a><ul>
<li class="toctree-l4"><a class="reference internal" href="examples/general/plot_0_gxnor_mnist.html">GXNOR/MNIST inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="examples/general/plot_1_akidanet_imagenet.html">AkidaNet/ImageNet inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="examples/general/plot_2_ds_cnn_kws.html">DS-CNN/KWS inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="examples/general/plot_3_regression.html">Regression tutorial</a></li>
<li class="toctree-l4"><a class="reference internal" href="examples/general/plot_4_transfer_learning.html">Transfer learning with AkidaNet for PlantVillage</a></li>
<li class="toctree-l4"><a class="reference internal" href="examples/general/plot_5_voc_yolo_detection.html">YOLO/PASCAL-VOC detection tutorial</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="examples/cnn2snn/index.html">CNN2SNN tutorials</a><ul>
<li class="toctree-l4"><a class="reference internal" href="examples/cnn2snn/plot_0_cnn_flow.html">CNN conversion flow tutorial</a></li>
<li class="toctree-l4"><a class="reference internal" href="examples/cnn2snn/plot_1_advanced_cnn2snn.html">Advanced CNN2SNN tutorial</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="examples/edge/index.html">Edge examples</a><ul>
<li class="toctree-l4"><a class="reference internal" href="examples/edge/plot_0_edge_learning_vision.html">Akida vision edge learning</a></li>
<li class="toctree-l4"><a class="reference internal" href="examples/edge/plot_1_edge_learning_kws.html">Akida edge learning for keyword spotting</a></li>
<li class="toctree-l4"><a class="reference internal" href="examples/edge/plot_2_edge_learning_parameters.html">Tips to set Akida learning parameters</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Model zoo performances</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#akida-1-0-models">Akida 1.0 models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#image-icon-ref-image-domain"> Image domain</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#classification">Classification</a></li>
<li class="toctree-l4"><a class="reference internal" href="#object-detection">Object detection</a></li>
<li class="toctree-l4"><a class="reference internal" href="#regression">Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="#face-recognition">Face recognition</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#audio-icon-ref-audio-domain"> Audio domain</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#keyword-spotting">Keyword spotting</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#pointcloud-icon-ref-point-cloud"> Point cloud</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id1">Classification</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#akida-2-0-models">Akida 2.0 models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id2"> Image domain</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id3">Classification</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id6">Object detection</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id7">Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id8">Face recognition</a></li>
<li class="toctree-l4"><a class="reference internal" href="#segmentation">Segmentation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id10"> Audio domain</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id11">Keyword spotting</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id12"> Point cloud</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id13">Classification</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/Brainchip-Inc/akida_examples/releases">Changelog</a></li>
<li class="toctree-l1"><a class="reference external" href="https://support.brainchip.com/portal/home">Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="license.html">License</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #989898" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Akida Examples</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Model zoo performances</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="model-zoo-performances">
<h1>Model zoo performances<a class="headerlink" href="#model-zoo-performances" title="Permalink to this headline"></a></h1>
<p>This page lets you discover all of Akida model zoo machine learning models with
their respective performances.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The download links provided point towards standard Tensorflow Keras models
that must be converted to Akida model using
<a class="reference external" href="api_reference/cnn2snn_apis.html#convert">cnn2snn.convert</a>.</p>
</div>
<section id="akida-1-0-models">
<h2>Akida 1.0 models<a class="headerlink" href="#akida-1-0-models" title="Permalink to this headline"></a></h2>
<p>For 1.0 models, 4bit accuracy are provided and are always obtained through a QAT phase.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The “8/4/4” quantization scheme stands for 8bit weights in the input layer, 4bit weights in
other layers and 4bit activations.</p>
</div>
<section id="image-icon-ref-image-domain">
<h3><a class="reference internal" href="_images/image_icon.png"><img alt="image_icon_ref" src="_images/image_icon.png" style="width: 25.6px; height: 25.6px;" /></a> Image domain<a class="headerlink" href="#image-icon-ref-image-domain" title="Permalink to this headline"></a></h3>
<section id="classification">
<h4>Classification<a class="headerlink" href="#classification" title="Permalink to this headline"></a></h4>
<table class="docutils align-default">
<colgroup>
<col style="width: 13%" />
<col style="width: 9%" />
<col style="width: 15%" />
<col style="width: 7%" />
<col style="width: 10%" />
<col style="width: 12%" />
<col style="width: 10%" />
<col style="width: 8%" />
<col style="width: 4%" />
<col style="width: 12%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Architecture</p></th>
<th class="head"><p>Resolution</p></th>
<th class="head"><p>Dataset</p></th>
<th class="head"><p>#Params</p></th>
<th class="head"><p>Quantization</p></th>
<th class="head"><p>Top-1 accuracy</p></th>
<th class="head"><p>Example</p></th>
<th class="head"><p>Size (KB)</p></th>
<th class="head"><p>NPs</p></th>
<th class="head"><p>Download</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>AkidaNet 0.25</p></td>
<td><p>160</p></td>
<td><p>ImageNet</p></td>
<td><p>480K</p></td>
<td><p>8/4/4</p></td>
<td><p>42.58%</p></td>
<td><p><a class="reference external" href="examples/general/plot_1_akidanet_imagenet.html"><img alt="an_ex" src="_images/link_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p></td>
<td><p>392.3</p></td>
<td><p>23</p></td>
<td><p><a class="reference external" href="https://data.brainchip.com/models/AkidaV1/akidanet/akidanet_imagenet_160_alpha_25_iq8_wq4_aq4.h5"><img alt="an_160_25_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p></td>
</tr>
<tr class="row-odd"><td><p>AkidaNet 0.5</p></td>
<td><p>160</p></td>
<td><p>ImageNet</p></td>
<td><p>1.4M</p></td>
<td><p>8/4/4</p></td>
<td><p>57.80%</p></td>
<td><p><a class="reference external" href="examples/general/plot_1_akidanet_imagenet.html"><img alt="an_ex" src="_images/link_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p></td>
<td><p>1099.4</p></td>
<td><p>30</p></td>
<td><p><a class="reference external" href="https://data.brainchip.com/models/AkidaV1/akidanet/akidanet_imagenet_160_alpha_50_iq8_wq4_aq4.h5"><img alt="an_160_50_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p></td>
</tr>
<tr class="row-even"><td><p>AkidaNet</p></td>
<td><p>160</p></td>
<td><p>ImageNet</p></td>
<td><p>4.4M</p></td>
<td><p>8/4/4</p></td>
<td><p>66.94%</p></td>
<td><p><a class="reference external" href="examples/general/plot_1_akidanet_imagenet.html"><img alt="an_ex" src="_images/link_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p></td>
<td><p>4090.2</p></td>
<td><p>81</p></td>
<td><p><a class="reference external" href="https://data.brainchip.com/models/AkidaV1/akidanet/akidanet_imagenet_160_iq8_wq4_aq4.h5"><img alt="an_160_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p></td>
</tr>
<tr class="row-odd"><td><p>AkidaNet 0.25</p></td>
<td><p>224</p></td>
<td><p>ImageNet</p></td>
<td><p>480K</p></td>
<td><p>8/4/4</p></td>
<td><p>46.71%</p></td>
<td><p><a class="reference external" href="examples/general/plot_1_akidanet_imagenet.html"><img alt="an_ex" src="_images/link_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p></td>
<td><p>398.1</p></td>
<td><p>25</p></td>
<td><p><a class="reference external" href="https://data.brainchip.com/models/AkidaV1/akidanet/akidanet_imagenet_224_alpha_25_iq8_wq4_aq4.h5"><img alt="an_224_25_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p></td>
</tr>
<tr class="row-even"><td><p>AkidaNet 0.5</p></td>
<td><p>224</p></td>
<td><p>ImageNet</p></td>
<td><p>1.4M</p></td>
<td><p>8/4/4</p></td>
<td><p>61.30%</p></td>
<td><p><a class="reference external" href="examples/general/plot_1_akidanet_imagenet.html"><img alt="an_ex" src="_images/link_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p></td>
<td><p>1214.4</p></td>
<td><p>38</p></td>
<td><p><a class="reference external" href="https://data.brainchip.com/models/AkidaV1/akidanet/akidanet_imagenet_224_alpha_50_iq8_wq4_aq4.h5"><img alt="an_224_50_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p></td>
</tr>
<tr class="row-odd"><td><p>AkidaNet</p></td>
<td><p>224</p></td>
<td><p>ImageNet</p></td>
<td><p>4.4M</p></td>
<td><p>8/4/4</p></td>
<td><p>69.65%</p></td>
<td><p><a class="reference external" href="examples/general/plot_1_akidanet_imagenet.html"><img alt="an_ex" src="_images/link_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p></td>
<td><p>6322.6</p></td>
<td><p>129</p></td>
<td><p><a class="reference external" href="https://data.brainchip.com/models/AkidaV1/akidanet/akidanet_imagenet_224_iq8_wq4_aq4.h5"><img alt="an_224_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p></td>
</tr>
<tr class="row-even"><td><p>AkidaNet 0.5
edge</p></td>
<td><p>160</p></td>
<td><p>ImageNet</p></td>
<td><p>4.0M</p></td>
<td><p>8/4/4</p></td>
<td><p>51.66%</p></td>
<td><p><a class="reference external" href="examples/edge/plot_0_edge_learning_vision.html#"><img alt="ane_ex" src="_images/link_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p></td>
<td><p>2017.1</p></td>
<td><p>38</p></td>
<td><p><a class="reference external" href="https://data.brainchip.com/models/AkidaV1/akidanet_edge/akidanet_imagenet_160_alpha_50_edge_iq8_wq4_aq4.h5"><img alt="ane_160_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p></td>
</tr>
<tr class="row-odd"><td><p>AkidaNet 0.5
edge</p></td>
<td><p>224</p></td>
<td><p>ImageNet</p></td>
<td><p>4.0M</p></td>
<td><p>8/4/4</p></td>
<td><p>54.03%</p></td>
<td><p><a class="reference external" href="examples/edge/plot_0_edge_learning_vision.html#"><img alt="ane_ex" src="_images/link_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p></td>
<td><p>2130.1</p></td>
<td><p>46</p></td>
<td><p><a class="reference external" href="https://data.brainchip.com/models/AkidaV1/akidanet_edge/akidanet_imagenet_224_alpha_50_edge_iq8_wq4_aq4.h5"><img alt="ane_224_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p></td>
</tr>
<tr class="row-even"><td><p>AkidaNet 0.5</p></td>
<td><p>224</p></td>
<td><p>PlantVillage</p></td>
<td><p>1.1M</p></td>
<td><p>8/4/4</p></td>
<td><p>97.92%</p></td>
<td><p><a class="reference external" href="examples/general/plot_4_transfer_learning.html"><img alt="an_pv_ex" src="_images/link_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p></td>
<td><p>1018.8</p></td>
<td><p>33</p></td>
<td><p><a class="reference external" href="https://data.brainchip.com/models/AkidaV1/akidanet/akidanet_plantvillage_iq8_wq4_aq4.h5"><img alt="an_pv_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p></td>
</tr>
<tr class="row-odd"><td><p>AkidaNet 0.25</p></td>
<td><p>96</p></td>
<td><p>Visual Wake Words</p></td>
<td><p>229K</p></td>
<td><p>8/4/4</p></td>
<td><p>84.77%</p></td>
<td></td>
<td><p>179.2</p></td>
<td><p>16</p></td>
<td><p><a class="reference external" href="https://data.brainchip.com/models/AkidaV1/akidanet/akidanet_vww_iq8_wq4_aq4.h5"><img alt="vww_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p></td>
</tr>
<tr class="row-even"><td><p>MobileNetV1 0.25</p></td>
<td><p>160</p></td>
<td><p>ImageNet</p></td>
<td><p>467K</p></td>
<td><p>8/4/4</p></td>
<td><p>36.05%</p></td>
<td></td>
<td><p>365.4</p></td>
<td><p>23</p></td>
<td><p><a class="reference external" href="https://data.brainchip.com/models/AkidaV1/mobilenet/mobilenet_imagenet_160_alpha_25_iq8_wq4_aq4.h5"><img alt="mb_160_25_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p></td>
</tr>
<tr class="row-odd"><td><p>MobileNetV1 0.5</p></td>
<td><p>160</p></td>
<td><p>ImageNet</p></td>
<td><p>1.3M</p></td>
<td><p>8/4/4</p></td>
<td><p>54.59%</p></td>
<td></td>
<td><p>1017.1</p></td>
<td><p>30</p></td>
<td><p><a class="reference external" href="https://data.brainchip.com/models/AkidaV1/mobilenet/mobilenet_imagenet_160_alpha_50_iq8_wq4_aq4.h5"><img alt="mb_160_50_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p></td>
</tr>
<tr class="row-even"><td><p>MobileNetV1</p></td>
<td><p>160</p></td>
<td><p>ImageNet</p></td>
<td><p>4.2M</p></td>
<td><p>8/4/4</p></td>
<td><p>65.47%</p></td>
<td></td>
<td><p>3554.5</p></td>
<td><p>78</p></td>
<td><p><a class="reference external" href="https://data.brainchip.com/models/AkidaV1/mobilenet/mobilenet_imagenet_160_iq8_wq4_aq4.h5"><img alt="mb_160_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p></td>
</tr>
<tr class="row-odd"><td><p>MobileNetV1 0.25</p></td>
<td><p>224</p></td>
<td><p>ImageNet</p></td>
<td><p>467K</p></td>
<td><p>8/4/4</p></td>
<td><p>39.73%</p></td>
<td></td>
<td><p>366.9</p></td>
<td><p>25</p></td>
<td><p><a class="reference external" href="https://data.brainchip.com/models/AkidaV1/mobilenet/mobilenet_imagenet_224_alpha_25_iq8_wq4_aq4.h5"><img alt="mb_224_25_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p></td>
</tr>
<tr class="row-even"><td><p>MobileNetV1 0.5</p></td>
<td><p>224</p></td>
<td><p>ImageNet</p></td>
<td><p>1.3M</p></td>
<td><p>8/4/4</p></td>
<td><p>58.50%</p></td>
<td></td>
<td><p>1075.4</p></td>
<td><p>38</p></td>
<td><p><a class="reference external" href="https://data.brainchip.com/models/AkidaV1/mobilenet/mobilenet_imagenet_224_alpha_50_iq8_wq4_aq4.h5"><img alt="mb_224_50_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p></td>
</tr>
<tr class="row-odd"><td><p>MobileNetV1</p></td>
<td><p>224</p></td>
<td><p>ImageNet</p></td>
<td><p>4.2M</p></td>
<td><p>8/4/4</p></td>
<td><p>68.76%</p></td>
<td></td>
<td><p>5251.8</p></td>
<td><p>123</p></td>
<td><p><a class="reference external" href="https://data.brainchip.com/models/AkidaV1/mobilenet/mobilenet_imagenet_224_iq8_wq4_aq4.h5"><img alt="mb_224_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p></td>
</tr>
<tr class="row-even"><td><p>GXNOR</p></td>
<td><p>28</p></td>
<td><p>MNIST</p></td>
<td><p>1.6M</p></td>
<td><p>2/2/1</p></td>
<td><p>98.03%</p></td>
<td><p><a class="reference external" href="examples/general/plot_0_gxnor_mnist.html"><img alt="gx_ex" src="_images/link_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p></td>
<td><p>412.6</p></td>
<td><p>3</p></td>
<td><p><a class="reference external" href="https://data.brainchip.com/models/AkidaV1/gxnor/gxnor_mnist_iq2_wq2_aq1.h5"><img alt="gx_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p></td>
</tr>
</tbody>
</table>
</section>
<section id="object-detection">
<h4>Object detection<a class="headerlink" href="#object-detection" title="Permalink to this headline"></a></h4>
<table class="docutils align-default">
<colgroup>
<col style="width: 11%" />
<col style="width: 10%" />
<col style="width: 21%" />
<col style="width: 7%" />
<col style="width: 11%" />
<col style="width: 6%" />
<col style="width: 10%" />
<col style="width: 9%" />
<col style="width: 4%" />
<col style="width: 10%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Architecture</p></th>
<th class="head"><p>Resolution</p></th>
<th class="head"><p>Dataset</p></th>
<th class="head"><p>#Params</p></th>
<th class="head"><p>Quantization</p></th>
<th class="head"><p>mAP</p></th>
<th class="head"><p>Example</p></th>
<th class="head"><p>Size (KB)</p></th>
<th class="head"><p>NPs</p></th>
<th class="head"><p>Download</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>YOLOv2</p></td>
<td><p>224</p></td>
<td><p>PASCAL-VOC 2007 -
person and car classes</p></td>
<td><p>3.6M</p></td>
<td><p>8/4/4</p></td>
<td><p>41.51%</p></td>
<td><p><a class="reference external" href="examples/general/plot_5_voc_yolo_detection.html"><img alt="yl_voc_ex" src="_images/link_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p></td>
<td><p>3061.0</p></td>
<td><p>71</p></td>
<td><p><a class="reference external" href="https://data.brainchip.com/models/AkidaV1/yolo/yolo_akidanet_voc_iq8_wq4_aq4.h5"><img alt="yl_voc_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p></td>
</tr>
<tr class="row-odd"><td><p>YOLOv2</p></td>
<td><p>224</p></td>
<td><p>WIDER FACE</p></td>
<td><p>3.5M</p></td>
<td><p>8/4/4</p></td>
<td><p>77.63%</p></td>
<td></td>
<td><p>3052.7</p></td>
<td><p>71</p></td>
<td><p><a class="reference external" href="https://data.brainchip.com/models/AkidaV1/yolo/yolo_akidanet_widerface_iq8_wq4_aq4.h5"><img alt="yl_wf_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p></td>
</tr>
</tbody>
</table>
</section>
<section id="regression">
<h4>Regression<a class="headerlink" href="#regression" title="Permalink to this headline"></a></h4>
<table class="docutils align-default">
<colgroup>
<col style="width: 12%" />
<col style="width: 10%" />
<col style="width: 22%" />
<col style="width: 8%" />
<col style="width: 12%" />
<col style="width: 7%" />
<col style="width: 8%" />
<col style="width: 9%" />
<col style="width: 4%" />
<col style="width: 8%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Architecture</p></th>
<th class="head"><p>Resolution</p></th>
<th class="head"><p>Dataset</p></th>
<th class="head"><p>#Params</p></th>
<th class="head"><p>Quantization</p></th>
<th class="head"><p>MAE</p></th>
<th class="head"><p>Example</p></th>
<th class="head"><p>Size (KB)</p></th>
<th class="head"><p>NPs</p></th>
<th class="head"><p>Download</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>VGG-like</p></td>
<td><p>32</p></td>
<td><p>UTKFace (age estimation)</p></td>
<td><p>458K</p></td>
<td><p>8/2/2</p></td>
<td><p>6.1791</p></td>
<td><p><a class="reference external" href="examples/general/plot_3_regression.html"><img alt="reg_ex" src="_images/link_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p></td>
<td><p>139.8</p></td>
<td><p>6</p></td>
<td><p><a class="reference external" href="https://data.brainchip.com/models/AkidaV1/vgg/vgg_utk_face_iq8_wq2_aq2.h5"><img alt="reg_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p></td>
</tr>
</tbody>
</table>
</section>
<section id="face-recognition">
<h4>Face recognition<a class="headerlink" href="#face-recognition" title="Permalink to this headline"></a></h4>
<table class="docutils align-default">
<colgroup>
<col style="width: 13%" />
<col style="width: 11%" />
<col style="width: 20%" />
<col style="width: 8%" />
<col style="width: 13%" />
<col style="width: 9%" />
<col style="width: 10%" />
<col style="width: 5%" />
<col style="width: 10%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Architecture</p></th>
<th class="head"><p>Resolution</p></th>
<th class="head"><p>Dataset</p></th>
<th class="head"><p>#Params</p></th>
<th class="head"><p>Quantization</p></th>
<th class="head"><p>Accuracy</p></th>
<th class="head"><p>Size (KB)</p></th>
<th class="head"><p>NPs</p></th>
<th class="head"><p>Download</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>AkidaNet 0.5</p></td>
<td><p>112×96</p></td>
<td><p>CASIA Webface
face identification</p></td>
<td><p>2.3M</p></td>
<td><p>8/4/4</p></td>
<td><p>70.18%</p></td>
<td><p>1929.8</p></td>
<td><p>21</p></td>
<td><p><a class="reference external" href="https://data.brainchip.com/models/AkidaV1/akidanet/akidanet_faceidentification_iq8_wq4_aq4.h5"><img alt="fid_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p></td>
</tr>
<tr class="row-odd"><td><p>AkidaNet 0.5
edge</p></td>
<td><p>112×96</p></td>
<td><p>CASIA Webface
face identification</p></td>
<td><p>23.6M</p></td>
<td><p>8/4/4</p></td>
<td><p>71.13%</p></td>
<td><p>6979.6</p></td>
<td><p>35</p></td>
<td><p><a class="reference external" href="https://data.brainchip.com/models/AkidaV1/akidanet_edge/akidanet_faceidentification_edge_iq8_wq4_aq4.h5"><img alt="fide_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="audio-icon-ref-audio-domain">
<h3><a class="reference internal" href="_images/headphones_icon.png"><img alt="audio_icon_ref" src="_images/headphones_icon.png" style="width: 25.6px; height: 25.6px;" /></a> Audio domain<a class="headerlink" href="#audio-icon-ref-audio-domain" title="Permalink to this headline"></a></h3>
<section id="keyword-spotting">
<h4>Keyword spotting<a class="headerlink" href="#keyword-spotting" title="Permalink to this headline"></a></h4>
<table class="docutils align-default">
<colgroup>
<col style="width: 13%" />
<col style="width: 21%" />
<col style="width: 8%" />
<col style="width: 13%" />
<col style="width: 14%" />
<col style="width: 9%" />
<col style="width: 10%" />
<col style="width: 4%" />
<col style="width: 9%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Architecture</p></th>
<th class="head"><p>Dataset</p></th>
<th class="head"><p>#Params</p></th>
<th class="head"><p>Quantization</p></th>
<th class="head"><p>Top-1 accuracy</p></th>
<th class="head"><p>Example</p></th>
<th class="head"><p>Size (KB)</p></th>
<th class="head"><p>NPs</p></th>
<th class="head"><p>Download</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>DS-CNN</p></td>
<td><p>Google speech command</p></td>
<td><p>22.7K</p></td>
<td><p>8/4/4</p></td>
<td><p>91.72%</p></td>
<td><p><a class="reference external" href="examples/general/plot_2_ds_cnn_kws.html"><img alt="kws_ex" src="_images/link_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p></td>
<td><p>22.8</p></td>
<td><p>5</p></td>
<td><p><a class="reference external" href="https://data.brainchip.com/models/AkidaV1/ds_cnn/ds_cnn_kws_iq8_wq4_aq4_laq1.h5"><img alt="kws_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="pointcloud-icon-ref-point-cloud">
<h3><a class="reference internal" href="_images/pointcloud_icon.png"><img alt="pointcloud_icon_ref" src="_images/pointcloud_icon.png" style="width: 25.6px; height: 25.6px;" /></a> Point cloud<a class="headerlink" href="#pointcloud-icon-ref-point-cloud" title="Permalink to this headline"></a></h3>
<section id="id1">
<h4>Classification<a class="headerlink" href="#id1" title="Permalink to this headline"></a></h4>
<table class="docutils align-default">
<colgroup>
<col style="width: 12%" />
<col style="width: 18%" />
<col style="width: 13%" />
<col style="width: 8%" />
<col style="width: 12%" />
<col style="width: 12%" />
<col style="width: 10%" />
<col style="width: 4%" />
<col style="width: 10%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Architecture</p></th>
<th class="head"><p>Dataset</p></th>
<th class="head"><p>Input scaling</p></th>
<th class="head"><p>#Params</p></th>
<th class="head"><p>Quantization</p></th>
<th class="head"><p>Accuracy</p></th>
<th class="head"><p>Size (KB)</p></th>
<th class="head"><p>NPs</p></th>
<th class="head"><p>Download</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>PointNet++</p></td>
<td><p>ModelNet40
3D Point Cloud</p></td>
<td><p>(127, 127)</p></td>
<td><p>602K</p></td>
<td><p>8/4/4</p></td>
<td><p>84.76%</p></td>
<td><p>528.5</p></td>
<td><p>17</p></td>
<td><p><a class="reference external" href="https://data.brainchip.com/models/AkidaV1/pointnet_plus/pointnet_plus_modelnet40_iq8_wq4_aq4.h5"><img alt="p++_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p></td>
</tr>
</tbody>
</table>
</section>
</section>
</section>
<section id="akida-2-0-models">
<h2>Akida 2.0 models<a class="headerlink" href="#akida-2-0-models" title="Permalink to this headline"></a></h2>
<p>For 2.0 models, both 8bit PTQ and 4bit QAT numbers are given. When not explicitely stated 8bit PTQ
accuracy is given as is (ie no further tuning/training, only quantization and calibration). The 4bit
QAT is the same as for 1.0.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The digit for quantization scheme stands for both weights and activations bitwidth. Weights in
the first layer are always quantized to 8bit. When given, ‘edge’ means that the model backbone
output (before classification layer) is quantized to 1bit to allow Akida edge learning.</p>
</div>
<section id="id2">
<h3><a class="reference internal" href="_images/image_icon.png"><img alt="image_icon_ref" src="_images/image_icon.png" style="width: 25.6px; height: 25.6px;" /></a> Image domain<a class="headerlink" href="#id2" title="Permalink to this headline"></a></h3>
<section id="id3">
<h4>Classification<a class="headerlink" href="#id3" title="Permalink to this headline"></a></h4>
<section id="cnns">
<h5>CNNs<a class="headerlink" href="#cnns" title="Permalink to this headline"></a></h5>
<table class="docutils align-default">
<colgroup>
<col style="width: 18%" />
<col style="width: 12%" />
<col style="width: 20%" />
<col style="width: 9%" />
<col style="width: 14%" />
<col style="width: 10%" />
<col style="width: 18%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Architecture</p></th>
<th class="head"><p>Resolution</p></th>
<th class="head"><p>Dataset</p></th>
<th class="head"><p>#Params</p></th>
<th class="head"><p>Quantization</p></th>
<th class="head"><p>Accuracy</p></th>
<th class="head"><p>Download</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>AkidaNet 0.25</p></td>
<td><p>160</p></td>
<td><p>ImageNet</p></td>
<td><p>483K</p></td>
<td><p>8</p>
<p>4</p>
</td>
<td><p>48.22%</p>
<p>41.60%</p>
</td>
<td><p><a class="reference external" href="https://data.brainchip.com/models/AkidaV2/akidanet/akidanet_imagenet_160_alpha_0.25_i8_w8_a8.h5"><img alt="an_160_25_8_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p>
<p><a class="reference external" href="https://data.brainchip.com/models/AkidaV2/akidanet/akidanet_imagenet_160_alpha_0.25_i8_w4_a4.h5"><img alt="an_160_25_4_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p>
</td>
</tr>
<tr class="row-odd"><td><p>AkidaNet 0.5</p></td>
<td><p>160</p></td>
<td><p>ImageNet</p></td>
<td><p>1.4M</p></td>
<td><p>8</p>
<p>4</p>
</td>
<td><p>61.70%</p>
<p>57.93%</p>
</td>
<td><p><a class="reference external" href="https://data.brainchip.com/models/AkidaV2/akidanet/akidanet_imagenet_160_alpha_0.5_i8_w8_a8.h5"><img alt="an_160_50_8_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p>
<p><a class="reference external" href="https://data.brainchip.com/models/AkidaV2/akidanet/akidanet_imagenet_160_alpha_0.5_i8_w4_a4.h5"><img alt="an_160_50_4_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p>
</td>
</tr>
<tr class="row-even"><td><p>AkidaNet</p></td>
<td><p>160</p></td>
<td><p>ImageNet</p></td>
<td><p>4.4M</p></td>
<td><p>8</p>
<p>4</p>
</td>
<td><p>69.97%</p>
<p>67.23%</p>
</td>
<td><p><a class="reference external" href="https://data.brainchip.com/models/AkidaV2/akidanet/akidanet_imagenet_160_alpha_1_i8_w8_a8.h5"><img alt="an_160_8_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p>
<p><a class="reference external" href="https://data.brainchip.com/models/AkidaV2/akidanet/akidanet_imagenet_160_alpha_1_i8_w4_a4.h5"><img alt="an_160_4_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p>
</td>
</tr>
<tr class="row-odd"><td><p>AkidaNet 0.25</p></td>
<td><p>224</p></td>
<td><p>ImageNet</p></td>
<td><p>483K</p></td>
<td><p>8</p>
<p>4</p>
</td>
<td><p>51.95%</p>
<p>46.06%</p>
</td>
<td><p><a class="reference external" href="https://data.brainchip.com/models/AkidaV2/akidanet/akidanet_imagenet_224_alpha_0.25_i8_w8_a8.h5"><img alt="an_224_25_8_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p>
<p><a class="reference external" href="https://data.brainchip.com/models/AkidaV2/akidanet/akidanet_imagenet_224_alpha_0.25_i8_w4_a4.h5"><img alt="an_224_25_4_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p>
</td>
</tr>
<tr class="row-even"><td><p>AkidaNet 0.5</p></td>
<td><p>224</p></td>
<td><p>ImageNet</p></td>
<td><p>1.4M</p></td>
<td><p>8</p>
<p>4</p>
</td>
<td><p>64.59%</p>
<p>61.47%</p>
</td>
<td><p><a class="reference external" href="https://data.brainchip.com/models/AkidaV2/akidanet/akidanet_imagenet_224_alpha_0.5_i8_w8_a8.h5"><img alt="an_224_50_8_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p>
<p><a class="reference external" href="https://data.brainchip.com/models/AkidaV2/akidanet/akidanet_imagenet_224_alpha_0.5_i8_w4_a4.h5"><img alt="an_224_50_4_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p>
</td>
</tr>
<tr class="row-odd"><td><p>AkidaNet</p></td>
<td><p>224</p></td>
<td><p>ImageNet</p></td>
<td><p>4.4M</p></td>
<td><p>8</p>
<p>4</p>
</td>
<td><p>72.27%</p>
<p>70.11%</p>
</td>
<td><p><a class="reference external" href="https://data.brainchip.com/models/AkidaV2/akidanet/akidanet_imagenet_224_alpha_1_i8_w8_a8.h5"><img alt="an_224_8_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p>
<p><a class="reference external" href="https://data.brainchip.com/models/AkidaV2/akidanet/akidanet_imagenet_224_alpha_1_i8_w4_a4.h5"><img alt="an_224_4_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p>
</td>
</tr>
<tr class="row-even"><td><p>AkidaNet 0.5</p></td>
<td><p>224</p></td>
<td><p>PlantVillage</p></td>
<td><p>1.2M</p></td>
<td><p>8</p>
<p>4</p>
</td>
<td><p>98.67%</p>
<p>95.14%</p>
</td>
<td><p><a class="reference external" href="https://data.brainchip.com/models/AkidaV2/akidanet/akidanet_plantvillage_i8_w8_a8.h5"><img alt="an_pv8_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p>
<p><a class="reference external" href="https://data.brainchip.com/models/AkidaV2/akidanet/akidanet_plantvillage_i8_w4_a4.h5"><img alt="an_pv4_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p>
</td>
</tr>
<tr class="row-odd"><td><p>AkidaNet 0.25</p></td>
<td><p>96</p></td>
<td><p>Visual Wake Words</p></td>
<td><p>227K</p></td>
<td><p>8</p>
<p>4</p>
</td>
<td><p>86.85%</p>
<p>85.80%</p>
</td>
<td><p><a class="reference external" href="https://data.brainchip.com/models/AkidaV2/akidanet/akidanet_vww_i8_w8_a8.h5"><img alt="vww8_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p>
<p><a class="reference external" href="https://data.brainchip.com/models/AkidaV2/akidanet/akidanet_vww_i8_w4_a4.h5"><img alt="vww4_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p>
</td>
</tr>
<tr class="row-even"><td><p>AkidaNet18</p></td>
<td><p>160</p></td>
<td><p>ImageNet</p></td>
<td><p>2.4M</p></td>
<td><p>8</p></td>
<td><p>64.60%</p></td>
<td><p><a class="reference external" href="https://data.brainchip.com/models/AkidaV2/akidanet18/akidanet18_imagenet_160_i8_w8_a8.h5"><img alt="an18_160_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p></td>
</tr>
<tr class="row-odd"><td><p>AkidaNet18</p></td>
<td><p>224</p></td>
<td><p>ImageNet</p></td>
<td><p>2.4M</p></td>
<td><p>8</p></td>
<td><p>67.05%</p></td>
<td><p><a class="reference external" href="https://data.brainchip.com/models/AkidaV2/akidanet18/akidanet18_imagenet_224_i8_w8_a8.h5"><img alt="an18_224_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p></td>
</tr>
<tr class="row-even"><td><p>MobileNetV1 0.25</p></td>
<td><p>160</p></td>
<td><p>ImageNet</p></td>
<td><p>469K</p></td>
<td><p>8</p>
<p>4</p>
</td>
<td><p>44.67%</p>
<p>37.51%</p>
</td>
<td><p><a class="reference external" href="https://data.brainchip.com/models/AkidaV2/mobilenet/mobilenet_imagenet_160_alpha_0.25_i8_w8_a8.h5"><img alt="mb_160_25_8_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p>
<p><a class="reference external" href="https://data.brainchip.com/models/AkidaV2/mobilenet/mobilenet_imagenet_160_alpha_0.25_i8_w4_a4.h5"><img alt="mb_160_25_4_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p>
</td>
</tr>
<tr class="row-odd"><td><p>MobileNetV1 0.5</p></td>
<td><p>160</p></td>
<td><p>ImageNet</p></td>
<td><p>1.3M</p></td>
<td><p>8</p>
<p>4</p>
</td>
<td><p>60.13%</p>
<p>54.81%</p>
</td>
<td><p><a class="reference external" href="https://data.brainchip.com/models/AkidaV2/mobilenet/mobilenet_imagenet_160_alpha_0.5_i8_w8_a8.h5"><img alt="mb_160_50_8_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p>
<p><a class="reference external" href="https://data.brainchip.com/models/AkidaV2/mobilenet/mobilenet_imagenet_160_alpha_0.5_i8_w4_a4.h5"><img alt="mb_160_50_4_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p>
</td>
</tr>
<tr class="row-even"><td><p>MobileNetV1</p></td>
<td><p>160</p></td>
<td><p>ImageNet</p></td>
<td><p>4.2M</p></td>
<td><p>8</p>
<p>4</p>
</td>
<td><p>71.17%</p>
<p>65.28%</p>
</td>
<td><p><a class="reference external" href="https://data.brainchip.com/models/AkidaV2/mobilenet/mobilenet_imagenet_160_alpha_1_i8_w8_a8.h5"><img alt="mb_160_8_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p>
<p><a class="reference external" href="https://data.brainchip.com/models/AkidaV2/mobilenet/mobilenet_imagenet_160_alpha_1_i8_w4_a4.h5"><img alt="mb_160_4_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p>
</td>
</tr>
<tr class="row-odd"><td><p>MobileNetV1 0.25</p></td>
<td><p>224</p></td>
<td><p>ImageNet</p></td>
<td><p>469K</p></td>
<td><p>8</p>
<p>4</p>
</td>
<td><p>48.13%</p>
<p>42.08%</p>
</td>
<td><p><a class="reference external" href="https://data.brainchip.com/models/AkidaV2/mobilenet/mobilenet_imagenet_224_alpha_0.25_i8_w8_a8.h5"><img alt="mb_224_25_8_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p>
<p><a class="reference external" href="https://data.brainchip.com/models/AkidaV2/mobilenet/mobilenet_imagenet_224_alpha_0.25_i8_w4_a4.h5"><img alt="mb_224_25_4_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p>
</td>
</tr>
<tr class="row-even"><td><p>MobileNetV1 0.5</p></td>
<td><p>224</p></td>
<td><p>ImageNet</p></td>
<td><p>1.3M</p></td>
<td><p>8</p>
<p>4</p>
</td>
<td><p>63.43%</p>
<p>59.20%</p>
</td>
<td><p><a class="reference external" href="https://data.brainchip.com/models/AkidaV2/mobilenet/mobilenet_imagenet_224_alpha_0.5_i8_w8_a8.h5"><img alt="mb_224_50_8_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p>
<p><a class="reference external" href="https://data.brainchip.com/models/AkidaV2/mobilenet/mobilenet_imagenet_224_alpha_0.5_i8_w4_a4.h5"><img alt="mb_224_50_4_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p>
</td>
</tr>
<tr class="row-odd"><td><p>MobileNetV1</p></td>
<td><p>224</p></td>
<td><p>ImageNet</p></td>
<td><p>4.2M</p></td>
<td><p>8</p>
<p>4</p>
</td>
<td><p>68.86%</p>
<p>68.52%</p>
</td>
<td><p><a class="reference external" href="https://data.brainchip.com/models/AkidaV2/mobilenet/mobilenet_imagenet_224_alpha_1_i8_w8_a8.h5"><img alt="mb_224_8_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p>
<p><a class="reference external" href="https://data.brainchip.com/models/AkidaV2/mobilenet/mobilenet_imagenet_224_alpha_1_i8_w4_a4.h5"><img alt="mb_224_4_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p>
</td>
</tr>
<tr class="row-even"><td><p>GXNOR</p></td>
<td><p>28</p></td>
<td><p>MNIST</p></td>
<td><p>1.6M</p></td>
<td><p>2/2/1</p></td>
<td><p>98.98%</p></td>
<td><p><a class="reference external" href="https://data.brainchip.com/models/AkidaV2/gxnor/gxnor_mnist_i2_w2_a1.h5"><img alt="gx2_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p></td>
</tr>
</tbody>
</table>
</section>
<section id="transformers">
<h5>Transformers<a class="headerlink" href="#transformers" title="Permalink to this headline"></a></h5>
<table class="docutils align-default">
<colgroup>
<col style="width: 16%" />
<col style="width: 14%" />
<col style="width: 11%" />
<col style="width: 10%" />
<col style="width: 16%" />
<col style="width: 19%" />
<col style="width: 14%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Architecture</p></th>
<th class="head"><p>Resolution</p></th>
<th class="head"><p>Dataset</p></th>
<th class="head"><p>#Params</p></th>
<th class="head"><p>Quantization</p></th>
<th class="head"><p>Accuracy</p></th>
<th class="head"><p>Download</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>ViT</p></td>
<td><p>224</p></td>
<td><p>ImageNet</p></td>
<td><p>5.8M</p></td>
<td><p>8</p></td>
<td><p>73.71% <a class="footnote-reference brackets" href="#fn-2" id="id4">1</a></p></td>
<td><p><a class="reference external" href="https://data.brainchip.com/models/AkidaV2/vit/bc_vit_ti16_224_i8_w8_a8.h5"><img alt="vit_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p></td>
</tr>
<tr class="row-odd"><td><p>DeiT-dist</p></td>
<td><p>224</p></td>
<td><p>ImageNet</p></td>
<td><p>6.0M</p></td>
<td><p>8</p></td>
<td><p>74.37% <a class="footnote-reference brackets" href="#fn-2" id="id5">1</a></p></td>
<td><p><a class="reference external" href="https://data.brainchip.com/models/AkidaV2/deit/bc_deit_dist_ti16_224_i8_w8_a8.h5"><img alt="deitd_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p></td>
</tr>
</tbody>
</table>
<dl class="footnote brackets">
<dt class="label" id="fn-2"><span class="brackets">1</span><span class="fn-backref">(<a href="#id4">1</a>,<a href="#id5">2</a>)</span></dt>
<dd><p>PTQ accuracy boosted with 1 epoch of QAT.</p>
</dd>
</dl>
</section>
</section>
<section id="id6">
<h4>Object detection<a class="headerlink" href="#id6" title="Permalink to this headline"></a></h4>
<table class="docutils align-default">
<colgroup>
<col style="width: 14%" />
<col style="width: 12%" />
<col style="width: 27%" />
<col style="width: 9%" />
<col style="width: 14%" />
<col style="width: 8%" />
<col style="width: 14%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Architecture</p></th>
<th class="head"><p>Resolution</p></th>
<th class="head"><p>Dataset</p></th>
<th class="head"><p>#Params</p></th>
<th class="head"><p>Quantization</p></th>
<th class="head"><p>mAP</p></th>
<th class="head"><p>Download</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>YOLOv2</p></td>
<td><p>224</p></td>
<td><p>PASCAL-VOC 2007 -
person and car classes</p></td>
<td><p>3.6M</p></td>
<td><p>8</p>
<p>4</p>
</td>
<td><p>48.22%</p>
<p>48.24%</p>
</td>
<td><p><a class="reference external" href="https://data.brainchip.com/models/AkidaV2/yolo/yolo_akidanet_voc_i8_w8_a8.h5"><img alt="yl_voc8_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p>
<p><a class="reference external" href="https://data.brainchip.com/models/AkidaV2/yolo/yolo_akidanet_voc_i8_w4_a4.h5"><img alt="yl_voc4_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p>
</td>
</tr>
<tr class="row-odd"><td><p>CenterNet</p></td>
<td><p>224</p></td>
<td><p>PASCAL-VOC 2007 -
person and car classes</p></td>
<td><p>1.9M</p></td>
<td><p>8</p></td>
<td><p>63.32%</p></td>
<td><p><a class="reference external" href="https://data.brainchip.com/models/AkidaV2/centernet/centernet_akidanet_voc_i8_w8_a8.h5"><img alt="ce_voc_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p></td>
</tr>
<tr class="row-even"><td><p>YOLOv2</p></td>
<td><p>224</p></td>
<td><p>WIDER FACE</p></td>
<td><p>3.6M</p></td>
<td><p>8</p>
<p>4</p>
</td>
<td><p>79.22%</p>
<p>77.71%</p>
</td>
<td><p><a class="reference external" href="https://data.brainchip.com/models/AkidaV2/yolo/yolo_akidanet_widerface_i8_w8_a8.h5"><img alt="yl_wf8_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p>
<p><a class="reference external" href="https://data.brainchip.com/models/AkidaV2/yolo/yolo_akidanet_widerface_i8_w4_a4.h5"><img alt="yl_wf4_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p>
</td>
</tr>
</tbody>
</table>
</section>
<section id="id7">
<h4>Regression<a class="headerlink" href="#id7" title="Permalink to this headline"></a></h4>
<table class="docutils align-default">
<colgroup>
<col style="width: 15%" />
<col style="width: 13%" />
<col style="width: 28%" />
<col style="width: 10%" />
<col style="width: 15%" />
<col style="width: 9%" />
<col style="width: 12%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Architecture</p></th>
<th class="head"><p>Resolution</p></th>
<th class="head"><p>Dataset</p></th>
<th class="head"><p>#Params</p></th>
<th class="head"><p>Quantization</p></th>
<th class="head"><p>MAE</p></th>
<th class="head"><p>Download</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>VGG-like</p></td>
<td><p>32</p></td>
<td><p>UTKFace (age estimation)</p></td>
<td><p>458K</p></td>
<td><p>8</p>
<p>4</p>
</td>
<td><p>6.7672</p>
<p>6.0405</p>
</td>
<td><p><a class="reference external" href="https://data.brainchip.com/models/AkidaV2/vgg/vgg_utk_face_i8_w8_a8.h5"><img alt="reg8_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p>
<p><a class="reference external" href="https://data.brainchip.com/models/AkidaV2/vgg/vgg_utk_face_i8_w4_a4.h5"><img alt="reg4_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p>
</td>
</tr>
</tbody>
</table>
</section>
<section id="id8">
<h4>Face recognition<a class="headerlink" href="#id8" title="Permalink to this headline"></a></h4>
<table class="docutils align-default">
<colgroup>
<col style="width: 15%" />
<col style="width: 13%" />
<col style="width: 24%" />
<col style="width: 10%" />
<col style="width: 15%" />
<col style="width: 11%" />
<col style="width: 12%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Architecture</p></th>
<th class="head"><p>Resolution</p></th>
<th class="head"><p>Dataset</p></th>
<th class="head"><p>#Params</p></th>
<th class="head"><p>Quantization</p></th>
<th class="head"><p>Accuracy</p></th>
<th class="head"><p>Download</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>AkidaNet 0.5</p></td>
<td><p>112×96</p></td>
<td><p>CASIA Webface
face identification</p></td>
<td><p>2.3M</p></td>
<td><p>8</p>
<p>4</p>
</td>
<td><p>72.92%</p>
<p>69.79%</p>
</td>
<td><p><a class="reference external" href="https://data.brainchip.com/models/AkidaV2/akidanet/akidanet_faceidentification_i8_w8_a8.h5"><img alt="fid8_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p>
<p><a class="reference external" href="https://data.brainchip.com/models/AkidaV2/akidanet/akidanet_faceidentification_i8_w4_a4.h5"><img alt="fid4_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p>
</td>
</tr>
</tbody>
</table>
</section>
<section id="segmentation">
<h4>Segmentation<a class="headerlink" href="#segmentation" title="Permalink to this headline"></a></h4>
<table class="docutils align-default">
<colgroup>
<col style="width: 16%" />
<col style="width: 13%" />
<col style="width: 14%" />
<col style="width: 10%" />
<col style="width: 15%" />
<col style="width: 19%" />
<col style="width: 12%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Architecture</p></th>
<th class="head"><p>Resolution</p></th>
<th class="head"><p>Dataset</p></th>
<th class="head"><p>#Params</p></th>
<th class="head"><p>Quantization</p></th>
<th class="head"><p>Binary IOU</p></th>
<th class="head"><p>Download</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>AkidaUNet 0.5</p></td>
<td><p>128</p></td>
<td><p>Portrait128</p></td>
<td><p>1.1M</p></td>
<td><p>8</p></td>
<td><p>89.44% <a class="footnote-reference brackets" href="#fn-3" id="id9">2</a></p></td>
<td><p><a class="reference external" href="https://data.brainchip.com/models/AkidaV2/akida_unet/akida_unet_portrait128_i8_w8_a8.h5"><img alt="unet_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p></td>
</tr>
</tbody>
</table>
<dl class="footnote brackets">
<dt class="label" id="fn-3"><span class="brackets"><a class="fn-backref" href="#id9">2</a></span></dt>
<dd><p>PTQ accuracy boosted with 1 epoch QAT.</p>
</dd>
</dl>
</section>
</section>
<section id="id10">
<h3><a class="reference internal" href="_images/headphones_icon.png"><img alt="audio_icon_ref" src="_images/headphones_icon.png" style="width: 25.6px; height: 25.6px;" /></a> Audio domain<a class="headerlink" href="#id10" title="Permalink to this headline"></a></h3>
<section id="id11">
<h4>Keyword spotting<a class="headerlink" href="#id11" title="Permalink to this headline"></a></h4>
<table class="docutils align-default">
<colgroup>
<col style="width: 16%" />
<col style="width: 26%" />
<col style="width: 10%" />
<col style="width: 16%" />
<col style="width: 18%" />
<col style="width: 14%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Architecture</p></th>
<th class="head"><p>Dataset</p></th>
<th class="head"><p>#Params</p></th>
<th class="head"><p>Quantization</p></th>
<th class="head"><p>Top-1 accuracy</p></th>
<th class="head"><p>Download</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>DS-CNN</p></td>
<td><p>Google speech command</p></td>
<td><p>23.8K</p></td>
<td><p>8</p>
<p>4</p>
<p>4 + edge</p>
</td>
<td><p>93.13%</p>
<p>92.69%</p>
<p>90.53%</p>
</td>
<td><p><a class="reference external" href="https://data.brainchip.com/models/AkidaV2/ds_cnn/ds_cnn_kws_i8_w8_a8.h5"><img alt="kws8_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p>
<p><a class="reference external" href="https://data.brainchip.com/models/AkidaV2/ds_cnn/ds_cnn_kws_i8_w4_a4.h5"><img alt="kws4_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p>
<p><a class="reference external" href="https://data.brainchip.com/models/AkidaV2/ds_cnn/ds_cnn_kws_edge_i8_w4_a4.h5"><img alt="kws4e_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p>
</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="id12">
<h3><a class="reference internal" href="_images/pointcloud_icon.png"><img alt="pointcloud_icon_ref" src="_images/pointcloud_icon.png" style="width: 25.6px; height: 25.6px;" /></a> Point cloud<a class="headerlink" href="#id12" title="Permalink to this headline"></a></h3>
<section id="id13">
<h4>Classification<a class="headerlink" href="#id13" title="Permalink to this headline"></a></h4>
<table class="docutils align-default">
<colgroup>
<col style="width: 14%" />
<col style="width: 20%" />
<col style="width: 15%" />
<col style="width: 9%" />
<col style="width: 14%" />
<col style="width: 17%" />
<col style="width: 11%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Architecture</p></th>
<th class="head"><p>Dataset</p></th>
<th class="head"><p>Input scaling</p></th>
<th class="head"><p>#Params</p></th>
<th class="head"><p>Quantization</p></th>
<th class="head"><p>Accuracy</p></th>
<th class="head"><p>Download</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>PointNet++</p></td>
<td><p>ModelNet40
3D Point Cloud</p></td>
<td><p>(127, 127)</p></td>
<td><p>605K</p></td>
<td><p>8</p>
<p>4</p>
</td>
<td><p>86.14% <a class="footnote-reference brackets" href="#fn-1" id="id14">3</a></p>
<p>85.66%</p>
</td>
<td><p><a class="reference external" href="https://data.brainchip.com/models/AkidaV2/pointnet_plus/pointnet_plus_modelnet40_i8_w8_a8.h5"><img alt="p++8_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p>
<p><a class="reference external" href="https://data.brainchip.com/models/AkidaV2/pointnet_plus/pointnet_plus_modelnet40_i8_w4_a4.h5"><img alt="p++4_dl" src="_images/download_icon.png" style="width: 20.48px; height: 20.48px;" /></a></p>
</td>
</tr>
</tbody>
</table>
<dl class="footnote brackets">
<dt class="label" id="fn-1"><span class="brackets"><a class="fn-backref" href="#id14">3</a></span></dt>
<dd><p>PTQ accuracy boosted with 5 epochs QAT.</p>
</dd>
</dl>
</section>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="examples/edge/plot_2_edge_learning_parameters.html" class="btn btn-neutral float-left" title="Tips to set Akida learning parameters" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="license.html" class="btn btn-neutral float-right" title="License" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, BrainChip Holdings Ltd. All Rights Reserved.</p>
  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>