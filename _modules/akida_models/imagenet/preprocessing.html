<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>akida_models.imagenet.preprocessing &mdash; Akida Examples  documentation</title>
      <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/sg_gallery.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/sg_gallery-binder.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/sg_gallery-dataframe.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/sg_gallery-rendered-html.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/custom.css" type="text/css" />
    <link rel="shortcut icon" href="../../../_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #78b3ff" >
            <a href="../../../index.html">
            <img src="../../../_static/akida.svg" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                MetaTF 2.1.1
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../index.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../installation.html">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../installation.html#requirements">Requirements</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../installation.html#quick-installation">Quick installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../installation.html#running-examples">Running examples</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../user_guide/user_guide.html">User guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../user_guide/getting_started.html">Getting started</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../user_guide/getting_started.html#for-beginners">For beginners</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../user_guide/getting_started.html#for-users-familiar-with-deep-learning">For users familiar with deep-learning</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../user_guide/akida.html">Akida user guide</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../user_guide/akida.html#introduction">Introduction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../user_guide/akida.html#akida-layers">Akida layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../user_guide/akida.html#input-format">Input Format</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../user_guide/akida.html#a-versatile-machine-learning-framework">A versatile machine learning framework</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../user_guide/akida.html#the-sequential-model">The Sequential model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../user_guide/akida.html#specifying-the-model">Specifying the model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../user_guide/akida.html#accessing-layer-parameters-and-weights">Accessing layer parameters and weights</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../user_guide/akida.html#inference">Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../user_guide/akida.html#saving-and-loading">Saving and loading</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../user_guide/akida.html#input-layer-types">Input layer types</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../user_guide/akida.html#data-processing-layer-types">Data-Processing layer types</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../user_guide/akida.html#model-hardware-mapping">Model Hardware Mapping</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../user_guide/akida.html#devices">Devices</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../user_guide/akida.html#model-mapping">Model mapping</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../user_guide/akida.html#advanced-mapping-details-and-hardware-devices-usage">Advanced Mapping Details and Hardware Devices Usage</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../user_guide/akida.html#id1">Using Akida Edge learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../user_guide/akida.html#learning-constraints">Learning constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../user_guide/akida.html#compiling-a-layer">Compiling a layer</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../user_guide/cnn2snn.html">CNN2SNN toolkit</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../user_guide/cnn2snn.html#overview">Overview</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../user_guide/cnn2snn.html#conversion-workflow">Conversion workflow</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../user_guide/cnn2snn.html#typical-training-scenario">Typical training scenario</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../user_guide/cnn2snn.html#design-compatibility-constraints">Design compatibility constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../user_guide/cnn2snn.html#quantization-compatibility-constraints">Quantization compatibility constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../user_guide/cnn2snn.html#command-line-interface">Command-line interface</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../user_guide/cnn2snn.html#layers-considerations">Layers Considerations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../user_guide/cnn2snn.html#supported-layer-types">Supported layer types</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../user_guide/cnn2snn.html#cnn2snn-quantization-aware-layers">CNN2SNN Quantization-aware layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../user_guide/cnn2snn.html#training-only-layers">Training-Only Layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../user_guide/cnn2snn.html#first-layers">First Layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../user_guide/cnn2snn.html#id6">Final Layers</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../user_guide/cnn2snn.html#tips-and-tricks">Tips and Tricks</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../user_guide/akida_models.html">Akida models zoo</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../user_guide/akida_models.html#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../user_guide/akida_models.html#command-line-interface-for-model-creation">Command-line interface for model creation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../user_guide/akida_models.html#command-line-interface-for-model-training">Command-line interface for model training</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../user_guide/akida_models.html#cifar10-training-and-tuning">CIFAR10 training and tuning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../user_guide/akida_models.html#utk-face-training">UTK Face training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../user_guide/akida_models.html#kws-training">KWS training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../user_guide/akida_models.html#yolo-training">YOLO training</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../user_guide/akida_models.html#command-line-interface-for-model-evaluation">Command-line interface for model evaluation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../user_guide/akida_models.html#id1">Layer Blocks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../user_guide/akida_models.html#conv-block"><code class="docutils literal notranslate"><span class="pre">conv_block</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../user_guide/akida_models.html#dense-block"><code class="docutils literal notranslate"><span class="pre">dense_block</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../user_guide/akida_models.html#separable-conv-block"><code class="docutils literal notranslate"><span class="pre">separable_conv_block</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../user_guide/hw_constraints.html">Hardware constraints</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../user_guide/hw_constraints.html#inputconvolutional">InputConvolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../user_guide/hw_constraints.html#convolutional">Convolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../user_guide/hw_constraints.html#separableconvolutional">SeparableConvolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../user_guide/hw_constraints.html#fullyconnected">FullyConnected</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../user_guide/compatibility.html">Akida versions compatibility</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../user_guide/compatibility.html#upgrading-models-with-legacy-quantizers">Upgrading models with legacy quantizers</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_reference/api_reference.html">API reference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../api_reference/akida_apis.html">Akida runtime</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../api_reference/akida_apis.html#model">Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api_reference/akida_apis.html#layer">Layer</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../api_reference/akida_apis.html#id1">Layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../api_reference/akida_apis.html#mapping">Mapping</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../api_reference/akida_apis.html#inputdata">InputData</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api_reference/akida_apis.html#inputconvolutional">InputConvolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api_reference/akida_apis.html#fullyconnected">FullyConnected</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api_reference/akida_apis.html#convolutional">Convolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api_reference/akida_apis.html#separableconvolutional">SeparableConvolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api_reference/akida_apis.html#concat">Concat</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api_reference/akida_apis.html#layer-parameters">Layer parameters</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../api_reference/akida_apis.html#layertype">LayerType</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../api_reference/akida_apis.html#padding">Padding</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../api_reference/akida_apis.html#pooltype">PoolType</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../api_reference/akida_apis.html#learningtype">LearningType</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../api_reference/akida_apis.html#sequence">Sequence</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../api_reference/akida_apis.html#id2">Sequence</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../api_reference/akida_apis.html#backendtype">BackendType</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../api_reference/akida_apis.html#pass">Pass</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../api_reference/akida_apis.html#device">Device</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../api_reference/akida_apis.html#id3">Device</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../api_reference/akida_apis.html#hwversion">HwVersion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../api_reference/akida_apis.html#hwdevice">HWDevice</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../api_reference/akida_apis.html#id4">HWDevice</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../api_reference/akida_apis.html#socdriver">SocDriver</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../api_reference/akida_apis.html#clockmode">ClockMode</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../api_reference/akida_apis.html#powermeter">PowerMeter</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api_reference/akida_apis.html#np">NP</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api_reference/akida_apis.html#tools">Tools</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../api_reference/akida_apis.html#sparsity">Sparsity</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../api_reference/akida_apis.html#compatibility">Compatibility</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../api_reference/cnn2snn_apis.html">CNN2SNN</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../api_reference/cnn2snn_apis.html#tool-functions">Tool functions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../api_reference/cnn2snn_apis.html#quantize">quantize</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../api_reference/cnn2snn_apis.html#quantize-layer">quantize_layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../api_reference/cnn2snn_apis.html#convert">convert</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../api_reference/cnn2snn_apis.html#check-model-compatibility">check_model_compatibility</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../api_reference/cnn2snn_apis.html#load-quantized-model">load_quantized_model</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../api_reference/cnn2snn_apis.html#quantizers">Quantizers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../api_reference/cnn2snn_apis.html#weightquantizer">WeightQuantizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../api_reference/cnn2snn_apis.html#linearweightquantizer">LinearWeightQuantizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../api_reference/cnn2snn_apis.html#stdweightquantizer">StdWeightQuantizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../api_reference/cnn2snn_apis.html#stdperaxisquantizer">StdPerAxisQuantizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../api_reference/cnn2snn_apis.html#maxquantizer">MaxQuantizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../api_reference/cnn2snn_apis.html#maxperaxisquantizer">MaxPerAxisQuantizer</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../api_reference/cnn2snn_apis.html#quantized-layers">Quantized layers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../api_reference/cnn2snn_apis.html#quantizedconv2d">QuantizedConv2D</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../api_reference/cnn2snn_apis.html#quantizeddense">QuantizedDense</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../api_reference/cnn2snn_apis.html#quantizedseparableconv2d">QuantizedSeparableConv2D</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../api_reference/cnn2snn_apis.html#quantizedactivation">QuantizedActivation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../api_reference/cnn2snn_apis.html#activationdiscreterelu">ActivationDiscreteRelu</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../api_reference/cnn2snn_apis.html#quantizedrelu">QuantizedReLU</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../api_reference/akida_models_apis.html">Akida models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../api_reference/akida_models_apis.html#layer-blocks">Layer blocks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../api_reference/akida_models_apis.html#conv-block">conv_block</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../api_reference/akida_models_apis.html#separable-conv-block">separable_conv_block</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../api_reference/akida_models_apis.html#dense-block">dense_block</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../api_reference/akida_models_apis.html#helpers">Helpers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../api_reference/akida_models_apis.html#batchnormalization-gamma-constraint">BatchNormalization gamma constraint</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../api_reference/akida_models_apis.html#knowledge-distillation">Knowledge distillation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api_reference/akida_models_apis.html#pruning">Pruning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../api_reference/akida_models_apis.html#model-zoo">Model zoo</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../api_reference/akida_models_apis.html#akidanet">AkidaNet</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../api_reference/akida_models_apis.html#mobilenet">Mobilenet</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../api_reference/akida_models_apis.html#ds-cnn">DS-CNN</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../api_reference/akida_models_apis.html#vgg">VGG</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../api_reference/akida_models_apis.html#yolo">YOLO</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../api_reference/akida_models_apis.html#convtiny">ConvTiny</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../api_reference/akida_models_apis.html#pointnet">PointNet++</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../api_reference/akida_models_apis.html#gxnor">GXNOR</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../examples/index.html">Examples</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/index.html#general-examples">General examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/general/plot_0_gxnor_mnist.html">GXNOR/MNIST inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/general/plot_0_gxnor_mnist.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/general/plot_0_gxnor_mnist.html#create-a-keras-gxnor-model">2. Create a Keras GXNOR model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/general/plot_0_gxnor_mnist.html#conversion-to-akida">3. Conversion to Akida</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/general/plot_1_ds_cnn_cifar10.html">DS-CNN CIFAR10 inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/general/plot_1_ds_cnn_cifar10.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/general/plot_1_ds_cnn_cifar10.html#create-a-keras-ds-cnn-model">2. Create a Keras DS-CNN model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/general/plot_1_ds_cnn_cifar10.html#quantized-model">3. Quantized model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/general/plot_1_ds_cnn_cifar10.html#pretrained-quantized-model">4. Pretrained quantized model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/general/plot_1_ds_cnn_cifar10.html#conversion-to-akida">5. Conversion to Akida</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/general/plot_2_akidanet_imagenet.html">AkidaNet/ImageNet inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/general/plot_2_akidanet_imagenet.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/general/plot_2_akidanet_imagenet.html#create-a-keras-akidanet-model">2. Create a Keras AkidaNet model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/general/plot_2_akidanet_imagenet.html#quantized-model">3. Quantized model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/general/plot_2_akidanet_imagenet.html#pretrained-quantized-model">4. Pretrained quantized model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/general/plot_2_akidanet_imagenet.html#conversion-to-akida">5. Conversion to Akida</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/general/plot_3_ds_cnn_kws.html">DS-CNN/KWS inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/general/plot_3_ds_cnn_kws.html#load-the-preprocessed-dataset">1. Load the preprocessed dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/general/plot_3_ds_cnn_kws.html#load-a-pre-trained-native-keras-model">2. Load a pre-trained native Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/general/plot_3_ds_cnn_kws.html#load-a-pre-trained-quantized-keras-model-satisfying-akida-nsoc-requirements">3. Load a pre-trained quantized Keras model satisfying Akida NSoC requirements</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/general/plot_3_ds_cnn_kws.html#conversion-to-akida">4. Conversion to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/general/plot_3_ds_cnn_kws.html#confusion-matrix">5. Confusion matrix</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/general/plot_4_regression.html">Regression tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/general/plot_4_regression.html#load-the-dataset">1. Load the dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/general/plot_4_regression.html#load-a-pre-trained-native-keras-model">2. Load a pre-trained native Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/general/plot_4_regression.html#load-a-pre-trained-quantized-keras-model-satisfying-akida-nsoc-requirements">3. Load a pre-trained quantized Keras model satisfying Akida NSoC requirements</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/general/plot_4_regression.html#conversion-to-akida">4. Conversion to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/general/plot_4_regression.html#estimate-age-on-a-single-image">5. Estimate age on a single image</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/general/plot_5_transfer_learning.html">Transfer learning with AkidaNet for cats vs. dogs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/general/plot_5_transfer_learning.html#transfer-learning-process">Transfer learning process</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/general/plot_5_transfer_learning.html#load-and-preprocess-data">1. Load and preprocess data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/general/plot_5_transfer_learning.html#modify-a-pre-trained-base-keras-model">2. Modify a pre-trained base Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/general/plot_5_transfer_learning.html#train-the-transferred-model-for-the-new-task">3. Train the transferred model for the new task</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/general/plot_5_transfer_learning.html#quantize-the-top-layer">4. Quantize the top layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/general/plot_5_transfer_learning.html#convert-to-akida">5. Convert to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/general/plot_5_transfer_learning.html#plot-confusion-matrix">6. Plot confusion matrix</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/general/plot_6_voc_yolo_detection.html">YOLO/PASCAL-VOC detection tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/general/plot_6_voc_yolo_detection.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/general/plot_6_voc_yolo_detection.html#preprocessing-tools">2. Preprocessing tools</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/general/plot_6_voc_yolo_detection.html#model-architecture">3. Model architecture</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/general/plot_6_voc_yolo_detection.html#training">4. Training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/general/plot_6_voc_yolo_detection.html#performance">5. Performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/general/plot_6_voc_yolo_detection.html#conversion-to-akida">6. Conversion to Akida</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/index.html#cnn2snn-tutorials">CNN2SNN tutorials</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/cnn2snn/plot_0_cnn_flow.html">CNN conversion flow tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/cnn2snn/plot_0_cnn_flow.html#load-and-reshape-mnist-dataset">1. Load and reshape MNIST dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/cnn2snn/plot_0_cnn_flow.html#model-definition">2. Model definition</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/cnn2snn/plot_0_cnn_flow.html#model-training">3. Model training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/cnn2snn/plot_0_cnn_flow.html#model-quantization">4. Model quantization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/cnn2snn/plot_0_cnn_flow.html#model-fine-tuning-quantization-aware-training">5. Model fine tuning (quantization-aware training)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/cnn2snn/plot_0_cnn_flow.html#model-conversion">6. Model conversion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/cnn2snn/plot_1_advanced_cnn2snn.html">Advanced CNN2SNN tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/cnn2snn/plot_1_advanced_cnn2snn.html#design-a-cnn2snn-quantized-model">1. Design a CNN2SNN quantized model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/cnn2snn/plot_1_advanced_cnn2snn.html#weight-quantizer-details">2. Weight Quantizer Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/cnn2snn/plot_1_advanced_cnn2snn.html#quantized-activation-layer-details">3. Quantized Activation Layer Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/cnn2snn/plot_1_advanced_cnn2snn.html#how-to-deal-with-too-high-scale-factors">4. How to deal with too high scale factors</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/index.html#edge-examples">Edge examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/edge/plot_0_edge_learning_vision.html">Akida vision edge learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/edge/plot_0_edge_learning_vision.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/edge/plot_0_edge_learning_vision.html#prepare-akida-model-for-learning">2. Prepare Akida model for learning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/edge/plot_0_edge_learning_vision.html#edge-learning-with-akida">3. Edge learning with Akida</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/edge/plot_1_edge_learning_kws.html">Akida edge learning for keyword spotting</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/edge/plot_1_edge_learning_kws.html#edge-learning-process">1. Edge learning process</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/edge/plot_1_edge_learning_kws.html#dataset-preparation">2. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/edge/plot_1_edge_learning_kws.html#prepare-akida-model-for-learning">3. Prepare Akida model for learning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/edge/plot_1_edge_learning_kws.html#learn-with-akida-using-the-training-set">4. Learn with Akida using the training set</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/edge/plot_1_edge_learning_kws.html#edge-learning">5. Edge learning</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/edge/plot_2_edge_learning_parameters.html">Tips to set Akida learning parameters</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/edge/plot_2_edge_learning_parameters.html#akida-learning-parameters">1. Akida learning parameters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/edge/plot_2_edge_learning_parameters.html#create-akida-model">2. Create Akida model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/edge/plot_2_edge_learning_parameters.html#estimate-the-required-number-of-weights-of-the-trainable-layer">3. Estimate the required number of weights of the trainable layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../examples/edge/plot_2_edge_learning_parameters.html#estimate-the-number-of-neurons-per-class">4. Estimate the number of neurons per class</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../zoo_performances.html">Model zoo performances</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../zoo_performances.html#image-icon-ref-image-domain"> Image domain</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../zoo_performances.html#classification">Classification</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../zoo_performances.html#object-detection">Object detection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../zoo_performances.html#regression">Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../zoo_performances.html#face-recognition">Face recognition</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../zoo_performances.html#audio-icon-ref-audio-domain"> Audio domain</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../zoo_performances.html#keyword-spotting">Keyword spotting</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../zoo_performances.html#time-icon-ref-time-domain"> Time domain</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../zoo_performances.html#fault-detection">Fault detection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../zoo_performances.html#id1">Classification</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../zoo_performances.html#pointcloud-icon-ref-point-cloud"> Point cloud</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../zoo_performances.html#id2">Classification</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/Brainchip-Inc/akida_examples/releases">Changelog</a></li>
<li class="toctree-l1"><a class="reference external" href="https://support.brainchip.com/portal/home">Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../license.html">License</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #78b3ff" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">Akida Examples</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../index.html">Module code</a> &raquo;</li>
      <li>akida_models.imagenet.preprocessing</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for akida_models.imagenet.preprocessing</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2016 The TensorFlow Authors. All Rights Reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ==============================================================================</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Provides utilities to preprocess images.</span>

<span class="sd">Training images are sampled using the provided bounding boxes, and subsequently</span>
<span class="sd">cropped to the sampled bounding box. Images are additionally flipped randomly,</span>
<span class="sd">then resized to the target output size (without aspect-ratio preservation).</span>

<span class="sd">Images used during evaluation are resized (with aspect-ratio preservation) and</span>
<span class="sd">centrally cropped.</span>

<span class="sd">All images undergo mean color subtraction.</span>

<span class="sd">Note that these steps are colloquially referred to as &quot;ResNet preprocessing,&quot;</span>
<span class="sd">and they differ from &quot;VGG preprocessing,&quot; which does not use bounding boxes</span>
<span class="sd">and instead does an aspect-preserving resize followed by random crop during</span>
<span class="sd">training. (These both differ from &quot;Inception preprocessing,&quot; which introduces</span>
<span class="sd">color distortion steps.)</span>

<span class="sd">Brainchip changes:</span>
<span class="sd">        - Original file: https://github.com/tensorflow/models/blob/master/official/vision/image_classification/imagenet_preprocessing.py</span>
<span class="sd">        - Update preprocess_image() return value for Akida compatibility</span>
<span class="sd">        - Change cycle_length &#39;parallelization&#39; parameter</span>
<span class="sd">        - Add _distort_color, _apply_with_random_selector, and</span>
<span class="sd">          _random_distort_color methods</span>
<span class="sd">          from https://github.com/tensorflow/models/blob/master/research/slim/preprocessing/inception_preprocessing.py</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">absl</span> <span class="kn">import</span> <span class="n">logging</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">control_flow_ops</span>  <span class="c1"># pylint: disable=no-name-in-module</span>
<span class="kn">from</span> <span class="nn">.imagenet_labels2names</span> <span class="kn">import</span> <span class="n">imagenet_labels</span>

<span class="n">NUM_CHANNELS</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">NUM_CLASSES</span> <span class="o">=</span> <span class="mi">1001</span>

<span class="n">NUM_IMAGES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;train&#39;</span><span class="p">:</span> <span class="mi">1281167</span><span class="p">,</span>
    <span class="s1">&#39;validation&#39;</span><span class="p">:</span> <span class="mi">50000</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">_NUM_TRAIN_FILES</span> <span class="o">=</span> <span class="mi">1024</span>
<span class="n">_SHUFFLE_BUFFER</span> <span class="o">=</span> <span class="mi">10000</span>

<span class="n">_R_MEAN</span> <span class="o">=</span> <span class="mf">123.68</span>
<span class="n">_G_MEAN</span> <span class="o">=</span> <span class="mf">116.78</span>
<span class="n">_B_MEAN</span> <span class="o">=</span> <span class="mf">103.94</span>
<span class="n">CHANNEL_MEANS</span> <span class="o">=</span> <span class="p">[</span><span class="n">_R_MEAN</span><span class="p">,</span> <span class="n">_G_MEAN</span><span class="p">,</span> <span class="n">_B_MEAN</span><span class="p">]</span>


<div class="viewcode-block" id="process_record_dataset"><a class="viewcode-back" href="../../../api_reference/akida_models_apis.html#akida_models.imagenet.preprocessing.process_record_dataset">[docs]</a><span class="k">def</span> <span class="nf">process_record_dataset</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span>
                           <span class="n">is_training</span><span class="p">,</span>
                           <span class="n">batch_size</span><span class="p">,</span>
                           <span class="n">im_size</span><span class="p">,</span>
                           <span class="n">shuffle_buffer</span><span class="p">,</span>
                           <span class="n">parse_record_fn</span><span class="p">,</span>
                           <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                           <span class="n">datasets_num_private_threads</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                           <span class="n">drop_remainder</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                           <span class="n">tf_data_experimental_slack</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Given a Dataset with raw records, return an iterator over the records.</span>

<span class="sd">  Args:</span>
<span class="sd">    dataset: A Dataset representing raw records</span>
<span class="sd">    is_training: A boolean denoting whether the input is for training.</span>
<span class="sd">    batch_size: The number of samples per batch.</span>
<span class="sd">    shuffle_buffer: The buffer size to use when shuffling records. A larger</span>
<span class="sd">      value results in better randomness, but smaller values reduce startup</span>
<span class="sd">      time and use less memory.</span>
<span class="sd">    parse_record_fn: A function that takes a raw record and returns the</span>
<span class="sd">      corresponding (image, label) pair.</span>
<span class="sd">    num_epochs: The number of epochs to repeat the dataset.</span>
<span class="sd">    dtype: Data type to use for images/features.</span>
<span class="sd">    datasets_num_private_threads: Number of threads for a private</span>
<span class="sd">      threadpool created for all datasets computation.</span>
<span class="sd">    drop_remainder: A boolean indicates whether to drop the remainder of the</span>
<span class="sd">      batches. If True, the batch dimension will be static.</span>
<span class="sd">    tf_data_experimental_slack: Whether to enable tf.data&#39;s</span>
<span class="sd">      `experimental_slack` option.</span>

<span class="sd">  Returns:</span>
<span class="sd">    Dataset of (image, label) pairs ready for iteration.</span>
<span class="sd">  &quot;&quot;&quot;</span>
    <span class="c1"># Defines a specific size thread pool for tf.data operations.</span>
    <span class="k">if</span> <span class="n">datasets_num_private_threads</span><span class="p">:</span>
        <span class="n">options</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Options</span><span class="p">()</span>
        <span class="n">options</span><span class="o">.</span><span class="n">threading</span><span class="o">.</span><span class="n">private_threadpool_size</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">datasets_num_private_threads</span><span class="p">)</span>
        <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">with_options</span><span class="p">(</span><span class="n">options</span><span class="p">)</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;datasets_num_private_threads: </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span>
                     <span class="n">datasets_num_private_threads</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">is_training</span><span class="p">:</span>
        <span class="c1"># Shuffles records before repeating to respect epoch boundaries.</span>
        <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">buffer_size</span><span class="o">=</span><span class="n">shuffle_buffer</span><span class="p">)</span>
        <span class="c1"># Repeats the dataset for the number of epochs to train.</span>
        <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">repeat</span><span class="p">()</span>

    <span class="c1"># Parses the raw records into images and labels.</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">value</span><span class="p">:</span> <span class="n">parse_record_fn</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">im_size</span><span class="p">,</span> <span class="n">is_training</span><span class="p">,</span> <span class="n">dtype</span><span class="p">),</span>
        <span class="n">num_parallel_calls</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">AUTOTUNE</span><span class="p">)</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">drop_remainder</span><span class="o">=</span><span class="n">drop_remainder</span><span class="p">)</span>

    <span class="c1"># Operations between the final prefetch and the get_next call to the</span>
    <span class="c1"># iterator will happen synchronously during run time. We prefetch here again</span>
    <span class="c1"># to background all of the above processing work and keep it out of the</span>
    <span class="c1"># critical training path. Setting buffer_size to</span>
    <span class="c1"># tf.data.experimental.AUTOTUNE allows DistributionStrategies to adjust how</span>
    <span class="c1"># many batches to fetch based on how many devices are present.</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">prefetch</span><span class="p">(</span><span class="n">buffer_size</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">AUTOTUNE</span><span class="p">)</span>

    <span class="n">options</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Options</span><span class="p">()</span>
    <span class="n">options</span><span class="o">.</span><span class="n">experimental_slack</span> <span class="o">=</span> <span class="n">tf_data_experimental_slack</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">with_options</span><span class="p">(</span><span class="n">options</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">dataset</span></div>


<div class="viewcode-block" id="get_filenames"><a class="viewcode-back" href="../../../api_reference/akida_models_apis.html#akida_models.imagenet.preprocessing.get_filenames">[docs]</a><span class="k">def</span> <span class="nf">get_filenames</span><span class="p">(</span><span class="n">is_training</span><span class="p">,</span> <span class="n">data_dir</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Return filenames for dataset.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">is_training</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">[</span>
            <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="s1">&#39;train-</span><span class="si">%05d</span><span class="s1">-of-01024&#39;</span> <span class="o">%</span> <span class="n">i</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">_NUM_TRAIN_FILES</span><span class="p">)</span>
        <span class="p">]</span>
    <span class="k">return</span> <span class="p">[</span>
        <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="s1">&#39;validation-</span><span class="si">%05d</span><span class="s1">-of-00128&#39;</span> <span class="o">%</span> <span class="n">i</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span>
    <span class="p">]</span></div>


<span class="k">def</span> <span class="nf">_parse_example_proto</span><span class="p">(</span><span class="n">example_serialized</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Parses an Example proto containing a training example of an image.</span>

<span class="sd">  The output of the build_image_data.py image preprocessing script is a dataset</span>
<span class="sd">  containing serialized Example protocol buffers. Each Example proto contains</span>
<span class="sd">  the following fields (values are included as examples):</span>

<span class="sd">    image/height: 462</span>
<span class="sd">    image/width: 581</span>
<span class="sd">    image/colorspace: &#39;RGB&#39;</span>
<span class="sd">    image/channels: 3</span>
<span class="sd">    image/class/label: 615</span>
<span class="sd">    image/class/synset: &#39;n03623198&#39;</span>
<span class="sd">    image/class/text: &#39;knee pad&#39;</span>
<span class="sd">    image/object/bbox/xmin: 0.1</span>
<span class="sd">    image/object/bbox/xmax: 0.9</span>
<span class="sd">    image/object/bbox/ymin: 0.2</span>
<span class="sd">    image/object/bbox/ymax: 0.6</span>
<span class="sd">    image/object/bbox/label: 615</span>
<span class="sd">    image/format: &#39;JPEG&#39;</span>
<span class="sd">    image/filename: &#39;ILSVRC2012_val_00041207.JPEG&#39;</span>
<span class="sd">    image/encoded: &lt;JPEG encoded string&gt;</span>

<span class="sd">  Args:</span>
<span class="sd">    example_serialized: scalar Tensor tf.string containing a serialized</span>
<span class="sd">      Example protocol buffer.</span>

<span class="sd">  Returns:</span>
<span class="sd">    image_buffer: Tensor tf.string containing the contents of a JPEG file.</span>
<span class="sd">    label: Tensor tf.int32 containing the label.</span>
<span class="sd">    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]</span>
<span class="sd">      where each coordinate is [0, 1) and the coordinates are arranged as</span>
<span class="sd">      [ymin, xmin, ymax, xmax].</span>
<span class="sd">  &quot;&quot;&quot;</span>
    <span class="c1"># Dense features in Example proto.</span>
    <span class="n">feature_map</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;image/encoded&#39;</span><span class="p">:</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">FixedLenFeature</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">string</span><span class="p">,</span> <span class="n">default_value</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">),</span>
        <span class="s1">&#39;image/class/label&#39;</span><span class="p">:</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">FixedLenFeature</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">default_value</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
        <span class="s1">&#39;image/class/text&#39;</span><span class="p">:</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">FixedLenFeature</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">string</span><span class="p">,</span> <span class="n">default_value</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">),</span>
    <span class="p">}</span>
    <span class="n">sparse_float32</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">VarLenFeature</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="c1"># Sparse features in Example proto.</span>
    <span class="n">feature_map</span><span class="o">.</span><span class="n">update</span><span class="p">({</span>
        <span class="n">k</span><span class="p">:</span> <span class="n">sparse_float32</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="p">[</span>
            <span class="s1">&#39;image/object/bbox/xmin&#39;</span><span class="p">,</span> <span class="s1">&#39;image/object/bbox/ymin&#39;</span><span class="p">,</span>
            <span class="s1">&#39;image/object/bbox/xmax&#39;</span><span class="p">,</span> <span class="s1">&#39;image/object/bbox/ymax&#39;</span>
        <span class="p">]</span>
    <span class="p">})</span>

    <span class="n">features</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">parse_single_example</span><span class="p">(</span><span class="n">serialized</span><span class="o">=</span><span class="n">example_serialized</span><span class="p">,</span>
                                          <span class="n">features</span><span class="o">=</span><span class="n">feature_map</span><span class="p">)</span>
    <span class="n">label</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">features</span><span class="p">[</span><span class="s1">&#39;image/class/label&#39;</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

    <span class="n">xmin</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">features</span><span class="p">[</span><span class="s1">&#39;image/object/bbox/xmin&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">ymin</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">features</span><span class="p">[</span><span class="s1">&#39;image/object/bbox/ymin&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">xmax</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">features</span><span class="p">[</span><span class="s1">&#39;image/object/bbox/xmax&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">ymax</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">features</span><span class="p">[</span><span class="s1">&#39;image/object/bbox/ymax&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Note that we impose an ordering of (y, x) just to make life difficult.</span>
    <span class="n">bbox</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">ymin</span><span class="p">,</span> <span class="n">xmin</span><span class="p">,</span> <span class="n">ymax</span><span class="p">,</span> <span class="n">xmax</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Force the variable number of bounding boxes into the shape</span>
    <span class="c1"># [1, num_boxes, coords].</span>
    <span class="n">bbox</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">bbox</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">bbox</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="n">bbox</span><span class="p">,</span> <span class="n">perm</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">features</span><span class="p">[</span><span class="s1">&#39;image/encoded&#39;</span><span class="p">],</span> <span class="n">label</span><span class="p">,</span> <span class="n">bbox</span>


<div class="viewcode-block" id="parse_record"><a class="viewcode-back" href="../../../api_reference/akida_models_apis.html#akida_models.imagenet.preprocessing.parse_record">[docs]</a><span class="k">def</span> <span class="nf">parse_record</span><span class="p">(</span><span class="n">raw_record</span><span class="p">,</span> <span class="n">im_size</span><span class="p">,</span> <span class="n">is_training</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Parses a record containing a training example of an image.</span>

<span class="sd">  The input record is parsed into a label and image, and the image is passed</span>
<span class="sd">  through preprocessing steps (cropping, flipping, and so on).</span>

<span class="sd">  Args:</span>
<span class="sd">    raw_record: scalar Tensor tf.string containing a serialized</span>
<span class="sd">      Example protocol buffer.</span>
<span class="sd">    is_training: A boolean denoting whether the input is for training.</span>
<span class="sd">    dtype: data type to use for images/features.</span>

<span class="sd">  Returns:</span>
<span class="sd">    Tuple with processed image tensor and one-hot-encoded label tensor.</span>
<span class="sd">  &quot;&quot;&quot;</span>
    <span class="n">image_buffer</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">bbox</span> <span class="o">=</span> <span class="n">_parse_example_proto</span><span class="p">(</span><span class="n">raw_record</span><span class="p">)</span>

    <span class="n">image</span> <span class="o">=</span> <span class="n">preprocess_image</span><span class="p">(</span><span class="n">image_buffer</span><span class="o">=</span><span class="n">image_buffer</span><span class="p">,</span>
                             <span class="n">bbox</span><span class="o">=</span><span class="n">bbox</span><span class="p">,</span>
                             <span class="n">output_height</span><span class="o">=</span><span class="n">im_size</span><span class="p">,</span>
                             <span class="n">output_width</span><span class="o">=</span><span class="n">im_size</span><span class="p">,</span>
                             <span class="n">num_channels</span><span class="o">=</span><span class="n">NUM_CHANNELS</span><span class="p">,</span>
                             <span class="n">is_training</span><span class="o">=</span><span class="n">is_training</span><span class="p">)</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>

    <span class="c1"># Subtract one so that labels are in [0, 1000), and cast to float32 for</span>
    <span class="c1"># Keras model.</span>
    <span class="n">label</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">image</span><span class="p">,</span> <span class="n">label</span></div>


<span class="k">def</span> <span class="nf">get_val_dataset</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">im_size</span><span class="p">,</span> <span class="n">preprocessing_fn</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Input function which provides validation batches</span>

<span class="sd">  Args:</span>
<span class="sd">    data_dir: The directory containing the input data.</span>
<span class="sd">    batch_size: The number of samples per batch.</span>
<span class="sd">    im_size: The size of output images</span>
<span class="sd">    preprocessing_fn: Function to use for preprocessing images.</span>
<span class="sd">    dtype: Data type to use for images/features</span>

<span class="sd">  Returns:</span>
<span class="sd">    A dataset that can be used for iteration.</span>
<span class="sd">  &quot;&quot;&quot;</span>
    <span class="c1"># Read validation files</span>
    <span class="n">filenames</span> <span class="o">=</span> <span class="n">get_filenames</span><span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="n">data_dir</span><span class="p">)</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">(</span><span class="n">filenames</span><span class="p">)</span>
    <span class="c1"># Convert to individual records.</span>
    <span class="c1"># cycle_length = 10 means that up to 10 files will be read and deserialized</span>
    <span class="c1"># in parallel. You may want to increase this number if you have a large</span>
    <span class="c1"># number of CPU cores.</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">interleave</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">TFRecordDataset</span><span class="p">,</span>
        <span class="n">cycle_length</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">AUTOTUNE</span><span class="p">,</span>
        <span class="n">num_parallel_calls</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">AUTOTUNE</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">process_record_dataset</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
                                  <span class="n">is_training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                  <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                  <span class="n">im_size</span><span class="o">=</span><span class="n">im_size</span><span class="p">,</span>
                                  <span class="n">shuffle_buffer</span><span class="o">=</span><span class="n">_SHUFFLE_BUFFER</span><span class="p">,</span>
                                  <span class="n">parse_record_fn</span><span class="o">=</span><span class="n">preprocessing_fn</span><span class="p">,</span>
                                  <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
                                  <span class="n">datasets_num_private_threads</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                  <span class="n">drop_remainder</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                  <span class="n">tf_data_experimental_slack</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>


<div class="viewcode-block" id="input_fn"><a class="viewcode-back" href="../../../api_reference/akida_models_apis.html#akida_models.imagenet.preprocessing.input_fn">[docs]</a><span class="k">def</span> <span class="nf">input_fn</span><span class="p">(</span><span class="n">is_training</span><span class="p">,</span>
             <span class="n">data_dir</span><span class="p">,</span>
             <span class="n">batch_size</span><span class="p">,</span>
             <span class="n">im_size</span><span class="p">,</span>
             <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
             <span class="n">datasets_num_private_threads</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">parse_record_fn</span><span class="o">=</span><span class="n">parse_record</span><span class="p">,</span>
             <span class="n">input_context</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">drop_remainder</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
             <span class="n">tf_data_experimental_slack</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
             <span class="n">training_dataset_cache</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Input function which provides batches for train or eval.</span>

<span class="sd">  Args:</span>
<span class="sd">    is_training: A boolean denoting whether the input is for training.</span>
<span class="sd">    data_dir: The directory containing the input data.</span>
<span class="sd">    batch_size: The number of samples per batch.</span>
<span class="sd">    num_epochs: The number of epochs to repeat the dataset.</span>
<span class="sd">    dtype: Data type to use for images/features</span>
<span class="sd">    datasets_num_private_threads: Number of private threads for tf.data.</span>
<span class="sd">    parse_record_fn: Function to use for parsing the records.</span>
<span class="sd">    input_context: A `tf.distribute.InputContext` object passed in by</span>
<span class="sd">      `tf.distribute.Strategy`.</span>
<span class="sd">    drop_remainder: A boolean indicates whether to drop the remainder of the</span>
<span class="sd">      batches. If True, the batch dimension will be static.</span>
<span class="sd">    tf_data_experimental_slack: Whether to enable tf.data&#39;s</span>
<span class="sd">      `experimental_slack` option.</span>
<span class="sd">    training_dataset_cache: Whether to cache the training dataset on workers.</span>
<span class="sd">       Typically used to improve training performance when training data is in</span>
<span class="sd">       remote storage and can fit into worker memory.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A dataset that can be used for iteration.</span>
<span class="sd">  &quot;&quot;&quot;</span>
    <span class="n">filenames</span> <span class="o">=</span> <span class="n">get_filenames</span><span class="p">(</span><span class="n">is_training</span><span class="p">,</span> <span class="n">data_dir</span><span class="p">)</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">(</span><span class="n">filenames</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">input_context</span><span class="p">:</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="s1">&#39;Sharding the dataset: input_pipeline_id=</span><span class="si">%d</span><span class="s1"> num_input_pipelines=</span><span class="si">%d</span><span class="s1">&#39;</span><span class="p">,</span>
            <span class="n">input_context</span><span class="o">.</span><span class="n">input_pipeline_id</span><span class="p">,</span> <span class="n">input_context</span><span class="o">.</span><span class="n">num_input_pipelines</span><span class="p">)</span>
        <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">input_context</span><span class="o">.</span><span class="n">num_input_pipelines</span><span class="p">,</span>
                                <span class="n">input_context</span><span class="o">.</span><span class="n">input_pipeline_id</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">is_training</span><span class="p">:</span>
        <span class="c1"># Shuffle the input files</span>
        <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">buffer_size</span><span class="o">=</span><span class="n">_NUM_TRAIN_FILES</span><span class="p">)</span>

    <span class="c1"># Convert to individual records.</span>
    <span class="c1"># cycle_length = 10 means that up to 10 files will be read and deserialized</span>
    <span class="c1"># in parallel. You may want to increase this number if you have a large</span>
    <span class="c1"># number of CPU cores.</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">interleave</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">TFRecordDataset</span><span class="p">,</span>
        <span class="n">cycle_length</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">AUTOTUNE</span><span class="p">,</span>
        <span class="n">num_parallel_calls</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">AUTOTUNE</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">is_training</span> <span class="ow">and</span> <span class="n">training_dataset_cache</span><span class="p">:</span>
        <span class="c1"># Improve training performance when training data is in remote storage</span>
        <span class="c1"># and can fit into worker memory.</span>
        <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">cache</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">process_record_dataset</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
        <span class="n">is_training</span><span class="o">=</span><span class="n">is_training</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">im_size</span><span class="o">=</span><span class="n">im_size</span><span class="p">,</span>
        <span class="n">shuffle_buffer</span><span class="o">=</span><span class="n">_SHUFFLE_BUFFER</span><span class="p">,</span>
        <span class="n">parse_record_fn</span><span class="o">=</span><span class="n">parse_record_fn</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">datasets_num_private_threads</span><span class="o">=</span><span class="n">datasets_num_private_threads</span><span class="p">,</span>
        <span class="n">drop_remainder</span><span class="o">=</span><span class="n">drop_remainder</span><span class="p">,</span>
        <span class="n">tf_data_experimental_slack</span><span class="o">=</span><span class="n">tf_data_experimental_slack</span><span class="p">,</span>
    <span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_decode_crop_and_flip</span><span class="p">(</span><span class="n">image_buffer</span><span class="p">,</span> <span class="n">bbox</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">,</span> <span class="n">decode</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Crops the given image to a random part of the image, and randomly flips.</span>

<span class="sd">  We use the fused decode_and_crop op, which performs better than the two ops</span>
<span class="sd">  used separately in series, but note that this requires that the image be</span>
<span class="sd">  passed in as an un-decoded string Tensor.</span>

<span class="sd">  Args:</span>
<span class="sd">    image_buffer: scalar string Tensor representing the raw JPEG image buffer.</span>
<span class="sd">    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]</span>
<span class="sd">      where each coordinate is [0, 1) and the coordinates are arranged as</span>
<span class="sd">      [ymin, xmin, ymax, xmax].</span>
<span class="sd">    num_channels: Integer depth of the image buffer for decoding.</span>
<span class="sd">    decode: whether to decode the image or not</span>

<span class="sd">  Returns:</span>
<span class="sd">    3-D tensor with cropped image.</span>

<span class="sd">  &quot;&quot;&quot;</span>
    <span class="c1"># A large fraction of image datasets contain a human-annotated bounding box</span>
    <span class="c1"># delineating the region of the image containing the object of interest.  We</span>
    <span class="c1"># choose to create a new bounding box for the object which is a randomly</span>
    <span class="c1"># distorted version of the human-annotated bounding box that obeys an</span>
    <span class="c1"># allowed range of aspect ratios, sizes and overlap with the human-annotated</span>
    <span class="c1"># bounding box. If no box is supplied, then we assume the bounding box is</span>
    <span class="c1"># the entire image.</span>

    <span class="k">if</span> <span class="n">decode</span><span class="p">:</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">image</span><span class="o">.</span><span class="n">extract_jpeg_shape</span><span class="p">(</span><span class="n">image_buffer</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">image_buffer</span><span class="p">)</span>

    <span class="n">sample_distorted_bounding_box</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">image</span><span class="o">.</span><span class="n">sample_distorted_bounding_box</span><span class="p">(</span>
        <span class="n">shape</span><span class="p">,</span>
        <span class="n">bounding_boxes</span><span class="o">=</span><span class="n">bbox</span><span class="p">,</span>
        <span class="n">min_object_covered</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
        <span class="n">aspect_ratio_range</span><span class="o">=</span><span class="p">[</span><span class="mf">0.75</span><span class="p">,</span> <span class="mf">1.33</span><span class="p">],</span>
        <span class="n">area_range</span><span class="o">=</span><span class="p">[</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span>
        <span class="n">max_attempts</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
        <span class="n">use_image_if_no_bounding_boxes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">bbox_begin</span><span class="p">,</span> <span class="n">bbox_size</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">sample_distorted_bounding_box</span>

    <span class="k">if</span> <span class="n">decode</span><span class="p">:</span>
        <span class="c1"># Reassemble the bounding box in the format the crop op requires.</span>
        <span class="n">offset_y</span><span class="p">,</span> <span class="n">offset_x</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">unstack</span><span class="p">(</span><span class="n">bbox_begin</span><span class="p">)</span>
        <span class="n">target_height</span><span class="p">,</span> <span class="n">target_width</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">unstack</span><span class="p">(</span><span class="n">bbox_size</span><span class="p">)</span>
        <span class="n">crop_window</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span>
            <span class="p">[</span><span class="n">offset_y</span><span class="p">,</span> <span class="n">offset_x</span><span class="p">,</span> <span class="n">target_height</span><span class="p">,</span> <span class="n">target_width</span><span class="p">])</span>

        <span class="c1"># Use the fused decode and crop op here, which is faster than each in</span>
        <span class="c1"># series.</span>
        <span class="n">cropped</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">image</span><span class="o">.</span><span class="n">decode_and_crop_jpeg</span><span class="p">(</span><span class="n">image_buffer</span><span class="p">,</span>
                                                <span class="n">crop_window</span><span class="p">,</span>
                                                <span class="n">channels</span><span class="o">=</span><span class="n">num_channels</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">cropped</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span><span class="n">image_buffer</span><span class="p">,</span> <span class="n">bbox_begin</span><span class="p">,</span> <span class="n">bbox_size</span><span class="p">)</span>

    <span class="c1"># Flip to add a little more random distortion in.</span>
    <span class="n">cropped</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">image</span><span class="o">.</span><span class="n">random_flip_left_right</span><span class="p">(</span><span class="n">cropped</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cropped</span>


<span class="k">def</span> <span class="nf">_central_crop</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">crop_height</span><span class="p">,</span> <span class="n">crop_width</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Performs central crops of the given image list.</span>

<span class="sd">  Args:</span>
<span class="sd">    image: a 3-D image tensor</span>
<span class="sd">    crop_height: the height of the image following the crop.</span>
<span class="sd">    crop_width: the width of the image following the crop.</span>

<span class="sd">  Returns:</span>
<span class="sd">    3-D tensor with cropped image.</span>
<span class="sd">  &quot;&quot;&quot;</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">image</span><span class="p">)</span>
    <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="n">amount_to_be_cropped_h</span> <span class="o">=</span> <span class="p">(</span><span class="n">height</span> <span class="o">-</span> <span class="n">crop_height</span><span class="p">)</span>
    <span class="n">crop_top</span> <span class="o">=</span> <span class="n">amount_to_be_cropped_h</span> <span class="o">//</span> <span class="mi">2</span>
    <span class="n">amount_to_be_cropped_w</span> <span class="o">=</span> <span class="p">(</span><span class="n">width</span> <span class="o">-</span> <span class="n">crop_width</span><span class="p">)</span>
    <span class="n">crop_left</span> <span class="o">=</span> <span class="n">amount_to_be_cropped_w</span> <span class="o">//</span> <span class="mi">2</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="p">[</span><span class="n">crop_top</span><span class="p">,</span> <span class="n">crop_left</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                    <span class="p">[</span><span class="n">crop_height</span><span class="p">,</span> <span class="n">crop_width</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>


<span class="k">def</span> <span class="nf">_mean_image_subtraction</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">means</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Subtracts the given means from each image channel.</span>

<span class="sd">  For example:</span>
<span class="sd">    means = [123.68, 116.779, 103.939]</span>
<span class="sd">    image = _mean_image_subtraction(image, means)</span>

<span class="sd">  Note that the rank of `image` must be known.</span>

<span class="sd">  Args:</span>
<span class="sd">    image: a tensor of size [height, width, C].</span>
<span class="sd">    means: a C-vector of values to subtract from each channel.</span>
<span class="sd">    num_channels: number of color channels in the image that will be distorted.</span>

<span class="sd">  Returns:</span>
<span class="sd">    the centered image.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: If the rank of `image` is unknown, if `image` has a rank other</span>
<span class="sd">      than three or if the number of channels in `image` doesn&#39;t match the</span>
<span class="sd">      number of values in `means`.</span>
<span class="sd">  &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">image</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">ndims</span> <span class="o">!=</span> <span class="mi">3</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Input must be of size [height, width, C&gt;0]&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">means</span><span class="p">)</span> <span class="o">!=</span> <span class="n">num_channels</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;len(means) must match the number of channels&#39;</span><span class="p">)</span>

    <span class="c1"># We have a 1-D tensor of means; convert to 3-D.</span>
    <span class="c1"># Note(b/130245863): we explicitly call `broadcast` instead of simply</span>
    <span class="c1"># expanding dimensions for better performance.</span>
    <span class="n">means</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">means</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">image</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">image</span> <span class="o">-</span> <span class="n">means</span>


<span class="k">def</span> <span class="nf">_smallest_size_at_least</span><span class="p">(</span><span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">resize_min</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes new shape with the smallest side equal to `smallest_side`.</span>

<span class="sd">  Computes new shape with the smallest side equal to `smallest_side` while</span>
<span class="sd">  preserving the original aspect ratio.</span>

<span class="sd">  Args:</span>
<span class="sd">    height: an int32 scalar tensor indicating the current height.</span>
<span class="sd">    width: an int32 scalar tensor indicating the current width.</span>
<span class="sd">    resize_min: A python integer or scalar `Tensor` indicating the size of</span>
<span class="sd">      the smallest side after resize.</span>

<span class="sd">  Returns:</span>
<span class="sd">    new_height: an int32 scalar tensor indicating the new height.</span>
<span class="sd">    new_width: an int32 scalar tensor indicating the new width.</span>
<span class="sd">  &quot;&quot;&quot;</span>
    <span class="n">resize_min</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">resize_min</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="c1"># Convert to floats to make subsequent calculations go smoothly.</span>
    <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">height</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">width</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="n">smaller_dim</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>
    <span class="n">scale_ratio</span> <span class="o">=</span> <span class="n">resize_min</span> <span class="o">/</span> <span class="n">smaller_dim</span>

    <span class="c1"># Convert back to ints to make heights and widths that TF ops will accept.</span>
    <span class="n">new_height</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">height</span> <span class="o">*</span> <span class="n">scale_ratio</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">new_width</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">width</span> <span class="o">*</span> <span class="n">scale_ratio</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">new_height</span><span class="p">,</span> <span class="n">new_width</span>


<span class="k">def</span> <span class="nf">_aspect_preserving_resize</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">resize_min</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Resize images preserving the original aspect ratio.</span>

<span class="sd">  Args:</span>
<span class="sd">    image: A 3-D image `Tensor`.</span>
<span class="sd">    resize_min: A python integer or scalar `Tensor` indicating the size of</span>
<span class="sd">      the smallest side after resize.</span>

<span class="sd">  Returns:</span>
<span class="sd">    resized_image: A 3-D tensor containing the resized image.</span>
<span class="sd">  &quot;&quot;&quot;</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">image</span><span class="p">)</span>
    <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="n">new_height</span><span class="p">,</span> <span class="n">new_width</span> <span class="o">=</span> <span class="n">_smallest_size_at_least</span><span class="p">(</span><span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">resize_min</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">_resize_image</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">new_height</span><span class="p">,</span> <span class="n">new_width</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_resize_image</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Simple wrapper around tf.resize_images.</span>

<span class="sd">  This is primarily to make sure we use the same `ResizeMethod` and other</span>
<span class="sd">  details each time.</span>

<span class="sd">  Args:</span>
<span class="sd">    image: A 3-D image `Tensor`.</span>
<span class="sd">    height: The target height for the resized image.</span>
<span class="sd">    width: The target width for the resized image.</span>

<span class="sd">  Returns:</span>
<span class="sd">    resized_image: A 3-D tensor containing the resized image. The first two</span>
<span class="sd">      dimensions have the shape [height, width].</span>
<span class="sd">  &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">image</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="p">[</span><span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">],</span>
                                     <span class="n">method</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">image</span><span class="o">.</span><span class="n">ResizeMethod</span><span class="o">.</span><span class="n">BILINEAR</span><span class="p">,</span>
                                     <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>


<span class="c1"># added this function from original inception preprocessing -- KDC</span>
<span class="k">def</span> <span class="nf">_random_distort_color</span><span class="p">(</span><span class="n">image</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;randomly distort the color of a Tensor image.</span>

<span class="sd">      Each color distortion is non-commutative and thus ordering of the color</span>
<span class="sd">      ops matters. Ideally we would randomly permute the ordering of the color</span>
<span class="sd">      ops. Rather then adding that level of complication, we select a distinct</span>
<span class="sd">      ordering of color ops for each preprocessing thread.</span>

<span class="sd">      Args:</span>
<span class="sd">        image: 3-D Tensor containing single image in [0, 255].</span>
<span class="sd">        color_ordering: Python int, a type of distortion (valid values: 0-3).</span>
<span class="sd">      Returns:</span>
<span class="sd">        3-D Tensor color-distorted image on range [0, 1]</span>
<span class="sd">      Raises:</span>
<span class="sd">        ValueError: if color_ordering not in [0, 3]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">num_distort_cases</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="c1"># scale image from 0 to 255 to 0 to 1 to use distort color</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">scalar_mul</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="mf">255.0</span><span class="p">,</span> <span class="n">image</span><span class="p">)</span>
    <span class="n">distorted_image</span> <span class="o">=</span> <span class="n">_apply_with_random_selector</span><span class="p">(</span><span class="n">image</span><span class="p">,</span>
                                                  <span class="n">_distort_color</span><span class="p">,</span>
                                                  <span class="n">num_cases</span><span class="o">=</span><span class="n">num_distort_cases</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">distorted_image</span>


<span class="c1"># added this function from original inception preprocessing -- KDC</span>
<span class="k">def</span> <span class="nf">_apply_with_random_selector</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">num_cases</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes func(x, sel), with sel sampled from [0...num_cases-1].</span>

<span class="sd">      Args:</span>
<span class="sd">        x: input Tensor.</span>
<span class="sd">        func: Python function to apply.</span>
<span class="sd">        num_cases: Python int32, number of cases to sample sel from.</span>

<span class="sd">      Returns:</span>
<span class="sd">        The result of func(x, sel), where func receives the value of the</span>
<span class="sd">        selector as a python integer, but sel is sampled dynamically.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># pylint must be disabled here as it incorrectly identifies tf API as</span>
    <span class="c1"># explained in https://github.com/tensorflow/tensorflow/issues/43038.</span>
    <span class="c1"># pylint: disable=unexpected-keyword-arg</span>
    <span class="n">sel</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">([],</span> <span class="n">maxval</span><span class="o">=</span><span class="n">num_cases</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="c1"># Pass the real x only to one of the func calls.</span>
    <span class="k">return</span> <span class="n">control_flow_ops</span><span class="o">.</span><span class="n">merge</span><span class="p">([</span>
        <span class="n">func</span><span class="p">(</span><span class="n">control_flow_ops</span><span class="o">.</span><span class="n">switch</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">sel</span><span class="p">,</span> <span class="n">case</span><span class="p">))[</span><span class="mi">1</span><span class="p">],</span> <span class="n">case</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">case</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_cases</span><span class="p">)</span>
    <span class="p">])[</span><span class="mi">0</span><span class="p">]</span>


<span class="c1"># added this function from original inception preprocessing -- KDC</span>
<span class="k">def</span> <span class="nf">_distort_color</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">color_ordering</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Distort the color of a Tensor image.</span>

<span class="sd">      Each color distortion is non-commutative and thus ordering of the color</span>
<span class="sd">      ops matters. Ideally we would randomly permute the ordering of the color</span>
<span class="sd">      ops. Rather then adding that level of complication, we select a distinct</span>
<span class="sd">      ordering of color ops for each preprocessing thread.</span>

<span class="sd">      Args:</span>
<span class="sd">        image: 3-D Tensor containing single image in [0, 1].</span>
<span class="sd">        color_ordering: Python int, a type of distortion (valid values: 0-3).</span>
<span class="sd">      Returns:</span>
<span class="sd">        3-D Tensor color-distorted image on range [0, 1]</span>
<span class="sd">      Raises:</span>
<span class="sd">        ValueError: if color_ordering not in [0, 3]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">color_ordering</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">image</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">image</span><span class="o">.</span><span class="n">random_brightness</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">max_delta</span><span class="o">=</span><span class="mf">32.</span> <span class="o">/</span> <span class="mf">255.</span><span class="p">)</span>
        <span class="n">image</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">image</span><span class="o">.</span><span class="n">random_saturation</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">image</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">image</span><span class="o">.</span><span class="n">random_saturation</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>
        <span class="n">image</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">image</span><span class="o">.</span><span class="n">random_brightness</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">max_delta</span><span class="o">=</span><span class="mf">32.</span> <span class="o">/</span> <span class="mf">255.</span><span class="p">)</span>

    <span class="c1"># The random_* ops do not necessarily clamp.</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>


<div class="viewcode-block" id="resize_and_crop"><a class="viewcode-back" href="../../../api_reference/akida_models_apis.html#akida_models.imagenet.preprocessing.resize_and_crop">[docs]</a><span class="k">def</span> <span class="nf">resize_and_crop</span><span class="p">(</span><span class="n">image_buffer</span><span class="p">,</span> <span class="n">output_height</span><span class="p">,</span> <span class="n">output_width</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Resize and crop the given image.</span>

<span class="sd">  Args:</span>
<span class="sd">    image_buffer: scalar string Tensor representing the raw JPEG image buffer.</span>
<span class="sd">    output_height: The height of the image after preprocessing.</span>
<span class="sd">    output_width: The width of the image after preprocessing.</span>
<span class="sd">    num_channels: Integer depth of the image buffer for decoding.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A resized and cropped image as a numpy array in uint8.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">preprocess_image</span><span class="p">(</span><span class="n">image_buffer</span><span class="o">=</span><span class="n">image_buffer</span><span class="p">,</span>
                             <span class="n">bbox</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                             <span class="n">output_height</span><span class="o">=</span><span class="n">output_height</span><span class="p">,</span>
                             <span class="n">output_width</span><span class="o">=</span><span class="n">output_width</span><span class="p">,</span>
                             <span class="n">num_channels</span><span class="o">=</span><span class="n">num_channels</span><span class="p">,</span>
                             <span class="n">is_training</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="n">image</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">image</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span></div>


<div class="viewcode-block" id="preprocess_image"><a class="viewcode-back" href="../../../api_reference/akida_models_apis.html#akida_models.imagenet.preprocessing.preprocess_image">[docs]</a><span class="k">def</span> <span class="nf">preprocess_image</span><span class="p">(</span><span class="n">image_buffer</span><span class="p">,</span>
                     <span class="n">bbox</span><span class="p">,</span>
                     <span class="n">output_height</span><span class="p">,</span>
                     <span class="n">output_width</span><span class="p">,</span>
                     <span class="n">num_channels</span><span class="p">,</span>
                     <span class="n">is_training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                     <span class="n">decode</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Preprocesses the given image.</span>

<span class="sd">  Preprocessing includes decoding, cropping, and resizing for both training</span>
<span class="sd">  and eval images. Training preprocessing, however, introduces some random</span>
<span class="sd">  distortion of the image to improve accuracy.</span>

<span class="sd">  Args:</span>
<span class="sd">    image_buffer: scalar string Tensor representing the raw JPEG image buffer.</span>
<span class="sd">    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]</span>
<span class="sd">      where each coordinate is [0, 1) and the coordinates are arranged as</span>
<span class="sd">      [ymin, xmin, ymax, xmax].</span>
<span class="sd">    output_height: The height of the image after preprocessing.</span>
<span class="sd">    output_width: The width of the image after preprocessing.</span>
<span class="sd">    num_channels: Integer depth of the image buffer for decoding.</span>
<span class="sd">    is_training: `True` if we&#39;re preprocessing the image for training and</span>
<span class="sd">      `False` otherwise.</span>
<span class="sd">    decode: whether to decode the image or not</span>

<span class="sd">  Returns:</span>
<span class="sd">    A preprocessed image.</span>
<span class="sd">  &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">is_training</span><span class="p">:</span>
        <span class="c1"># For training, we want to randomize some of the distortions.</span>
        <span class="n">image</span> <span class="o">=</span> <span class="n">_decode_crop_and_flip</span><span class="p">(</span><span class="n">image_buffer</span><span class="p">,</span> <span class="n">bbox</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">,</span> <span class="n">decode</span><span class="p">)</span>
        <span class="n">image</span> <span class="o">=</span> <span class="n">_resize_image</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">output_height</span><span class="p">,</span> <span class="n">output_width</span><span class="p">)</span>
        <span class="c1"># this could improve things but we skip it for now -- KDC</span>
        <span class="c1">#image = _random_distort_color(image)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># For validation, we want to decode, resize, then just crop the middle.</span>
        <span class="k">if</span> <span class="n">decode</span><span class="p">:</span>
            <span class="n">image</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">image</span><span class="o">.</span><span class="n">decode_jpeg</span><span class="p">(</span><span class="n">image_buffer</span><span class="p">,</span> <span class="n">channels</span><span class="o">=</span><span class="n">num_channels</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">image</span> <span class="o">=</span> <span class="n">image_buffer</span>
        <span class="n">resize_min</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">output_height</span><span class="p">,</span> <span class="n">output_width</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.143</span><span class="p">))</span>
        <span class="n">image</span> <span class="o">=</span> <span class="n">_aspect_preserving_resize</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">resize_min</span><span class="p">)</span>
        <span class="n">image</span> <span class="o">=</span> <span class="n">_central_crop</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">output_height</span><span class="p">,</span> <span class="n">output_width</span><span class="p">)</span>

    <span class="n">image</span><span class="o">.</span><span class="n">set_shape</span><span class="p">([</span><span class="n">output_height</span><span class="p">,</span> <span class="n">output_width</span><span class="p">,</span> <span class="n">num_channels</span><span class="p">])</span>

    <span class="c1"># Convert image to float in case the resize didn&#39;t</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span></div>


<div class="viewcode-block" id="index_to_label"><a class="viewcode-back" href="../../../api_reference/akida_models_apis.html#akida_models.imagenet.preprocessing.index_to_label">[docs]</a><span class="k">def</span> <span class="nf">index_to_label</span><span class="p">(</span><span class="n">index</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Function to get an ImageNet label from an index.</span>

<span class="sd">  Args:</span>
<span class="sd">    index: between 0 and 999</span>


<span class="sd">  Returns:</span>
<span class="sd">    a string of coma separated labels</span>
<span class="sd">  &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">imagenet_labels</span><span class="p">[</span><span class="n">index</span><span class="p">]</span></div>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, BrainChip Holdings Ltd. All Rights Reserved.</p>
  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>