<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Akida edge learning for keyword spotting &mdash; Akida Examples  documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/sg_gallery.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/sg_gallery-binder.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/sg_gallery-dataframe.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/sg_gallery-rendered-html.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Tips to set Akida learning parameters" href="plot_2_edge_learning_parameters.html" />
    <link rel="prev" title="Akida vision edge learning" href="plot_0_edge_learning_vision.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #78b3ff" >
            <a href="../../index.html">
            <img src="../../_static/akida.svg" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                MetaTF 2.0.5
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../index.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../installation.html">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../installation.html#requirements">Requirements</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../installation.html#quick-installation">Quick installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../installation.html#running-examples">Running examples</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../user_guide/user_guide.html">User guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../user_guide/getting_started.html">Getting started</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/getting_started.html#for-beginners">For beginners</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/getting_started.html#for-users-familiar-with-deep-learning">For users familiar with deep-learning</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../user_guide/aee.html">Akida user guide</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/aee.html#introduction">Introduction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/aee.html#akida-layers">Akida layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/aee.html#input-format">Input Format</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/aee.html#a-versatile-machine-learning-framework">A versatile machine learning framework</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/aee.html#the-sequential-model">The Sequential model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/aee.html#specifying-the-model">Specifying the model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/aee.html#accessing-layer-parameters-and-weights">Accessing layer parameters and weights</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/aee.html#inference">Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/aee.html#saving-and-loading">Saving and loading</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/aee.html#input-layer-types">Input layer types</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/aee.html#data-processing-layer-types">Data-Processing layer types</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/aee.html#model-hardware-mapping">Model Hardware Mapping</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/aee.html#devices">Devices</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/aee.html#model-mapping">Model mapping</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/aee.html#advanced-mapping-details-and-hardware-devices-usage">Advanced Mapping Details and Hardware Devices Usage</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/aee.html#id1">Using Akida Edge learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/aee.html#learning-constraints">Learning constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/aee.html#compiling-a-layer">Compiling a layer</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../user_guide/cnn2snn.html">CNN2SNN toolkit</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/cnn2snn.html#overview">Overview</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#conversion-workflow">Conversion workflow</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#typical-training-scenario">Typical training scenario</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#design-compatibility-constraints">Design compatibility constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#quantization-compatibility-constraints">Quantization compatibility constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#command-line-interface">Command-line interface</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/cnn2snn.html#layers-considerations">Layers Considerations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#supported-layer-types">Supported layer types</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#cnn2snn-quantization-aware-layers">CNN2SNN Quantization-aware layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#training-only-layers">Training-Only Layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#first-layers">First Layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#id6">Final Layers</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/cnn2snn.html#tips-and-tricks">Tips and Tricks</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../user_guide/akida_models.html">Akida models zoo</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/akida_models.html#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/akida_models.html#command-line-interface-for-model-creation">Command-line interface for model creation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/akida_models.html#command-line-interface-for-model-training">Command-line interface for model training</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida_models.html#cifar10-training-and-tuning">CIFAR10 training and tuning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida_models.html#utk-face-training">UTK Face training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida_models.html#kws-training">KWS training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida_models.html#yolo-training">YOLO training</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/akida_models.html#command-line-interface-for-model-evaluation">Command-line interface for model evaluation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/akida_models.html#id1">Layer Blocks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida_models.html#conv-block"><code class="docutils literal notranslate"><span class="pre">conv_block</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida_models.html#dense-block"><code class="docutils literal notranslate"><span class="pre">dense_block</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida_models.html#separable-conv-block"><code class="docutils literal notranslate"><span class="pre">separable_conv_block</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../user_guide/hw_constraints.html">Hardware constraints</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/hw_constraints.html#akida-nsoc-pre-production">Akida NSoC (Pre-production)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/hw_constraints.html#inputconvolutional">InputConvolutional</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/hw_constraints.html#convolutional">Convolutional</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/hw_constraints.html#separableconvolutional">SeparableConvolutional</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/hw_constraints.html#fullyconnected">FullyConnected</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/hw_constraints.html#akida-nsoc-production">Akida NSoC (Production)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/hw_constraints.html#id1">InputConvolutional</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/hw_constraints.html#id2">Convolutional</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/hw_constraints.html#id3">SeparableConvolutional</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/hw_constraints.html#id4">FullyConnected</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../user_guide/compatibility.html">Akida versions compatibility</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/compatibility.html#upgrading-models-with-legacy-quantizers">Upgrading models with legacy quantizers</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../api_reference/api_reference.html">API reference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../api_reference/aee_apis.html">Akida Execution Engine</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#model">Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#layer">Layer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#sparsity">Sparsity</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#inputdata">InputData</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#inputconvolutional">InputConvolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#fullyconnected">FullyConnected</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#convolutional">Convolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#separableconvolutional">SeparableConvolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#concat">Concat</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#backendtype">BackendType</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#padding">Padding</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#pooltype">PoolType</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#learningtype">LearningType</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#hwversion">HwVersion</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#compatibility">Compatibility</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#device">Device</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#hwdevice">HWDevice</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#socdriver">SocDriver</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#sequence">Sequence</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#np">NP</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#soc">soc</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#powermeter">PowerMeter</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html">CNN2SNN</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#tool-functions">Tool functions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#quantize">quantize</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#quantize-layer">quantize_layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#convert">convert</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#check-model-compatibility">check_model_compatibility</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#load-quantized-model">load_quantized_model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#load-partial-weights">load_partial_weights</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#quantizers">Quantizers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#weightquantizer">WeightQuantizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#linearweightquantizer">LinearWeightQuantizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#stdweightquantizer">StdWeightQuantizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#trainablestdweightquantizer">TrainableStdWeightQuantizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#maxquantizer">MaxQuantizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#maxperaxisquantizer">MaxPerAxisQuantizer</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#quantized-layers">Quantized layers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#quantizedconv2d">QuantizedConv2D</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#quantizeddepthwiseconv2d">QuantizedDepthwiseConv2D</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#quantizeddense">QuantizedDense</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#quantizedseparableconv2d">QuantizedSeparableConv2D</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#quantizedactivation">QuantizedActivation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#activationdiscreterelu">ActivationDiscreteRelu</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#quantizedrelu">QuantizedReLU</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../api_reference/akida_models_apis.html">Akida models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_models_apis.html#layer-blocks">Layer blocks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#conv-block">conv_block</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#separable-conv-block">separable_conv_block</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#dense-block">dense_block</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_models_apis.html#helpers">Helpers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#batchnormalization-gamma-constraint">BatchNormalization gamma constraint</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_models_apis.html#knowledge-distillation">Knowledge distillation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_models_apis.html#pruning">Pruning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_models_apis.html#model-zoo">Model zoo</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#mobilenet">Mobilenet</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#ds-cnn">DS-CNN</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#vgg">VGG</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#yolo">YOLO</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#convtiny">ConvTiny</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#pointnet">PointNet++</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#gxnor">GXNOR</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Examples</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../index.html#general-examples">General examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../general/plot_0_gxnor_mnist.html">GXNOR/MNIST inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_0_gxnor_mnist.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_0_gxnor_mnist.html#create-a-keras-gxnor-model">2. Create a Keras GXNOR model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_0_gxnor_mnist.html#conversion-to-akida">3. Conversion to Akida</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../general/plot_1_ds_cnn_cifar10.html">DS-CNN CIFAR10 inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_1_ds_cnn_cifar10.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_1_ds_cnn_cifar10.html#create-a-keras-ds-cnn-model">2. Create a Keras DS-CNN model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_1_ds_cnn_cifar10.html#quantized-model">3. Quantized model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_1_ds_cnn_cifar10.html#pretrained-quantized-model">4. Pretrained quantized model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_1_ds_cnn_cifar10.html#conversion-to-akida">5. Conversion to Akida</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../general/plot_2_mobilenet_imagenet.html">MobileNet/ImageNet inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_2_mobilenet_imagenet.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_2_mobilenet_imagenet.html#create-a-keras-mobilenet-model">2. Create a Keras MobileNet model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_2_mobilenet_imagenet.html#quantized-model">3. Quantized model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_2_mobilenet_imagenet.html#pretrained-quantized-model">4. Pretrained quantized model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_2_mobilenet_imagenet.html#conversion-to-akida">5. Conversion to Akida</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../general/plot_3_ds_cnn_kws.html">DS-CNN/KWS inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_3_ds_cnn_kws.html#load-the-preprocessed-dataset">1. Load the preprocessed dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_3_ds_cnn_kws.html#load-a-pre-trained-native-keras-model">2. Load a pre-trained native Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_3_ds_cnn_kws.html#load-a-pre-trained-quantized-keras-model-satisfying-akida-nsoc-requirements">3. Load a pre-trained quantized Keras model satisfying Akida NSoC requirements</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_3_ds_cnn_kws.html#conversion-to-akida">4. Conversion to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_3_ds_cnn_kws.html#confusion-matrix">5. Confusion matrix</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../general/plot_4_regression.html">Regression tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_4_regression.html#load-the-dataset">1. Load the dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_4_regression.html#load-a-pre-trained-native-keras-model">2. Load a pre-trained native Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_4_regression.html#load-a-pre-trained-quantized-keras-model-satisfying-akida-nsoc-requirements">3. Load a pre-trained quantized Keras model satisfying Akida NSoC requirements</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_4_regression.html#conversion-to-akida">4. Conversion to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_4_regression.html#estimate-age-on-a-single-image">5. Estimate age on a single image</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../general/plot_5_transfer_learning.html">Transfer learning with MobileNet for cats vs. dogs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_5_transfer_learning.html#transfer-learning-process">Transfer learning process</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_5_transfer_learning.html#load-and-preprocess-data">1. Load and preprocess data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_5_transfer_learning.html#modify-a-pre-trained-base-keras-model">2. Modify a pre-trained base Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_5_transfer_learning.html#train-the-transferred-model-for-the-new-task">3. Train the transferred model for the new task</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_5_transfer_learning.html#quantize-the-top-layer">4 Quantize the top layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_5_transfer_learning.html#convert-to-akida">5. Convert to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_5_transfer_learning.html#plot-confusion-matrix">6. Plot confusion matrix</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../general/plot_6_voc_yolo_detection.html">YOLO/PASCAL-VOC detection tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_6_voc_yolo_detection.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_6_voc_yolo_detection.html#preprocessing-tools">2. Preprocessing tools</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_6_voc_yolo_detection.html#model-architecture">3. Model architecture</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_6_voc_yolo_detection.html#training">4. Training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_6_voc_yolo_detection.html#performance">5. Performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_6_voc_yolo_detection.html#conversion-to-akida">6. Conversion to Akida</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#cnn2snn-tutorials">CNN2SNN tutorials</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../cnn2snn/plot_0_cnn_flow.html">CNN conversion flow tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../cnn2snn/plot_0_cnn_flow.html#load-and-reshape-mnist-dataset">1. Load and reshape MNIST dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cnn2snn/plot_0_cnn_flow.html#model-definition">2. Model definition</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cnn2snn/plot_0_cnn_flow.html#model-training">3. Model training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cnn2snn/plot_0_cnn_flow.html#model-quantization">4. Model quantization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cnn2snn/plot_0_cnn_flow.html#model-fine-tuning-quantization-aware-training">5. Model fine tuning (quantization-aware training)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cnn2snn/plot_0_cnn_flow.html#model-conversion">6. Model conversion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../cnn2snn/plot_1_advanced_cnn2snn.html">Advanced CNN2SNN tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../cnn2snn/plot_1_advanced_cnn2snn.html#design-a-cnn2snn-quantized-model">1. Design a CNN2SNN quantized model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cnn2snn/plot_1_advanced_cnn2snn.html#weight-quantizer-details">2. Weight Quantizer Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cnn2snn/plot_1_advanced_cnn2snn.html#quantized-activation-layer-details">3. Quantized Activation Layer Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cnn2snn/plot_1_advanced_cnn2snn.html#how-to-deal-with-too-high-scale-factors">4. How to deal with too high scale factors</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 current"><a class="reference internal" href="../index.html#edge-examples">Edge examples</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="plot_0_edge_learning_vision.html">Akida vision edge learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="plot_0_edge_learning_vision.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_0_edge_learning_vision.html#prepare-akida-model-for-learning">2. Prepare Akida model for learning</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_0_edge_learning_vision.html#edge-learning-with-akida">3. Edge learning with Akida</a></li>
</ul>
</li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Akida edge learning for keyword spotting</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#edge-learning-process">1. Edge learning process</a></li>
<li class="toctree-l4"><a class="reference internal" href="#dataset-preparation">2. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#prepare-akida-model-for-learning">3. Prepare Akida model for learning</a></li>
<li class="toctree-l4"><a class="reference internal" href="#learn-with-akida-using-the-training-set">4. Learn with Akida using the training set</a></li>
<li class="toctree-l4"><a class="reference internal" href="#edge-learning">5. Edge learning</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="plot_2_edge_learning_parameters.html">Tips to set Akida learning parameters</a><ul>
<li class="toctree-l4"><a class="reference internal" href="plot_2_edge_learning_parameters.html#akida-learning-parameters">1. Akida learning parameters</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_2_edge_learning_parameters.html#create-akida-model">2. Create Akida model</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_2_edge_learning_parameters.html#estimate-the-required-number-of-weights-of-the-trainable-layer">3. Estimate the required number of weights of the trainable layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_2_edge_learning_parameters.html#estimate-the-number-of-neurons-per-class">4. Estimate the number of neurons per class</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../zoo_performances.html">Model zoo performances</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../zoo_performances.html#image-icon-ref-image-domain"> Image domain</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../zoo_performances.html#classification">Classification</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zoo_performances.html#object-detection">Object detection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zoo_performances.html#regression">Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zoo_performances.html#face-recognition">Face recognition</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../zoo_performances.html#audio-icon-ref-audio-domain"> Audio domain</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../zoo_performances.html#keyword-spotting">Keyword spotting</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../zoo_performances.html#time-icon-ref-time-domain"> Time domain</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../zoo_performances.html#fault-detection">Fault detection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zoo_performances.html#id1">Classification</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../zoo_performances.html#pointcloud-icon-ref-point-cloud"> Point cloud</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../zoo_performances.html#id2">Classification</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/Brainchip-Inc/akida_examples/releases">Changelog</a></li>
<li class="toctree-l1"><a class="reference external" href="https://support.brainchip.com/portal/home">Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../license.html">License</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #78b3ff" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Akida Examples</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../index.html">Akida examples</a> &raquo;</li>
      <li>Akida edge learning for keyword spotting</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p>Click <a class="reference internal" href="#sphx-glr-download-examples-edge-plot-1-edge-learning-kws-py"><span class="std std-ref">here</span></a>
to download the full example code</p>
</div>
<section class="sphx-glr-example-title" id="akida-edge-learning-for-keyword-spotting">
<span id="sphx-glr-examples-edge-plot-1-edge-learning-kws-py"></span><h1>Akida edge learning for keyword spotting<a class="headerlink" href="#akida-edge-learning-for-keyword-spotting" title="Permalink to this headline"></a></h1>
<p>This tutorial demonstrates the Akida NSoC <strong>edge learning</strong> capabilities using
its built-in learning algorithm.</p>
<p>It focuses on a keyword spotting (KWS) example, where an existing Akida network
is re-trained to be able to classify new audio keywords.</p>
<p>Just a few samples (few-shot learning) of the new words are sufficient to
augment the Akida model with extra classes, while preserving high accuracy.</p>
<section id="edge-learning-process">
<h2>1. Edge learning process<a class="headerlink" href="#edge-learning-process" title="Permalink to this headline"></a></h2>
<p>By “edge learning”, we mean the process of network learning in an edge device.
Aside from technical requirements imposed by the device (low power, latency,
etc.), the task itself will often present particular challenges:</p>
<ol class="arabic simple">
<li><p>The application cannot know which, or indeed, how many classes it will
be trained on ultimately, so it must be possible to <strong>add new classes</strong>
to the classifier online, i.e. requires <strong>continual learning</strong>.</p></li>
<li><p>Often, there will be no large labelled dataset for new classes, which
must instead be learned from just a few samples, i.e. requires <strong>few-shot
learning</strong>.</p></li>
</ol>
<p>The Akida NSoC has a built-in learning algorithm designed for training the
last layer of a model and well suited for edge learning.
The specific use case in this tutorial mimics the process of a mobile
phone user who wants to add new speech commands, i.e. new keywords, to a
pre-trained voice recognition system with a few preset keywords.
To achieve this using the Akida NSoC, learning occurs in 3 stages:</p>
<ol class="arabic simple">
<li><p>The Akida model preparation: an Akida model must meet specific conditions
to be compatible for <a class="reference external" href="../../user_guide/aee.html#id5">Akida learning</a>.</p></li>
<li><p>The “offline” Akida learning: the last layer of the Akida model is trained
from scratch with a large dataset. In this KWS case, the model is trained
with 32 keywords from the Google “Speech Commands dataset”.</p></li>
<li><p>The “online” (edge learning) stage: new keywords are learned with few
samples, adding to the pre-trained words from stage 2.</p></li>
</ol>
<section id="akida-model-preparation">
<h3>1.1 Akida model preparation<a class="headerlink" href="#akida-model-preparation" title="Permalink to this headline"></a></h3>
<p>The Akida NSoC embeds a native learning algorithm allowing training of the
last layer of an Akida model. The overall model can then be seen as the
combination of two parts:</p>
<ul class="simple">
<li><p>a feature extractor (or spike generator) corresponding to all but the last
layer of a standard (back-propagation trained) neural network. This part of
the model cannot be trained on the Akida NSoC, and would typically be
prepared in advance, e.g. using the CNN2SNN conversion tool. Also, to be
compatible with Akida learning, the feature extractor must return binary
spikes (1-bit activations).</p></li>
<li><p>the classification layer (i.e. the last layer). It must have 1-bit weights
and usually has several output neurons per class. This layer will be the
only one trained using the built-in learning algorithm.</p></li>
</ul>
<p>Note that, unlike a standard CNN network where each class is represented by
a single output neuron, an Akida native training requires several neurons
for each class. The number of neurons per class can be seen as the number
of centroids to represent a class; there is an analogy with k-means clustering
applied to one-class samples, k being the number of neurons. The choice of the
number of neurons is a trade-off: too many neurons per class may be
computationally inefficient; in contrast too few neurons per class may have
difficulty representing within-class heterogeneity. Like k-means
clustering, the choice of k depends on the cluster representation of the data.</p>
<p>Like any training process, hyper-parameters must be set appropriately.
The only mandatory parameter is the number of weights (i.e. number of
connections for each neuron) which must be correlated to the number of spikes
at the end of the feature extractor. Other parameters, such as
<code class="docutils literal notranslate"><span class="pre">min_plasticity</span></code> or <code class="docutils literal notranslate"><span class="pre">learning_competition</span></code>, are optional and mainly used
for model fine-tuning: one can set them to default for a first try.</p>
</section>
<section id="offline-akida-learning">
<h3>1.2 “Offline” Akida learning<a class="headerlink" href="#offline-akida-learning" title="Permalink to this headline"></a></h3>
<p>The model is ready for training. Remember that the feature extractor has
already been trained in stage 1. Here, only the last Akida layer is
trainable. Training is still “offline” though, corresponding to the
preparation of the network with the “preset” command keywords. The last layer
is trained from scratch: its binary weights are randomly initialized.</p>
<p>A large dataset is passed through the Akida network and the on-chip learning
algorithm updates the weights of the classification layer accordingly.
In this KWS example, we take a dataset containing 32 words + a “silence”
class (33 classes) for a total of about 94,000 samples.</p>
<p>Note that the dataset on which the feature extractor was trained does not need
to be the same as the one used for “offline” training of the classification
layer. What is important is that the features extracted are as good as
possible for the expected inputs. Since the “edge” classes are, by
definition, not known in advance, in practice that typically means making
your feature extractor as general as possible.</p>
</section>
<section id="online-edge-learning">
<h3>1.3 “Online” edge learning<a class="headerlink" href="#online-edge-learning" title="Permalink to this headline"></a></h3>
<p>“Online” edge learning consists in adding and learning new classes to a
former pre-trained model. This stage is meant to be performed on a chip with
few examples for each new class.</p>
<p>In practice, edge learning with Akida is similar to “offline” learning,
except that:</p>
<ul class="simple">
<li><p>the network has already been trained on a set of classes which need to be
kept, and so the novel classes are in addition to those.</p></li>
<li><p>only few samples are available for training.</p></li>
</ul>
<p>In this KWS example, 3 new keywords are learned using 4 samples per word from
a single user. Applying data augmentation on these samples adds variability
to improve generalization. After edge learning, the model is able to classify
the 3 new classes with similar accuracy to the 33 existing classes (and
performance on the existing classes is unaffected).</p>
</section>
</section>
<section id="dataset-preparation">
<h2>2. Dataset preparation<a class="headerlink" href="#dataset-preparation" title="Permalink to this headline"></a></h2>
<p>The data comes from the Google “Speech Commands” dataset containing audio
files for 35 keywords. The number of utterances for each word varies from
1,500 to 4,000.
32 words are used to train the Akida model and 3 new words are added for
edge learning.</p>
<p>Two datasets are loaded:</p>
<ul class="simple">
<li><p>The first dataset contains all samples of the 32 following keywords
extended with the “silence” samples (see the
<a class="reference external" href="https://arxiv.org/pdf/1804.03209.pdf">original paper</a> for details on
the dataset). In total, 94,252 samples are used. These are split into a
training set (90%) and a validation set (10%), used to train the model
“offline” (stage 2).</p></li>
<li><p>The second dataset contains samples of the 3 new keywords from a single
speaker: ‘backward’, ‘follow’ and ‘forward’. Since the aim of edge learning
is to train with few samples, only 4 utterances will be used for
training and the rest for testing (ideally, one would test with many more
samples, but the number of repetitions per individual speaker in the
database makes this impossible). Data augmentation is applied with time
shift and additional background noise, generating 40 training samples per
utterances, therefore 4 x 40 = 160 training samples per new word.</p></li>
</ul>
<p>The audio files are pre-processed: the mel-frequency cepstral coefficients
(MFCC) are computed as features to represent each audio sample. The obtained
features for one sample are stored in an array of shape (49, 10, 1). This
array of features is chosen as input in the Akida network.</p>
<p>For the sake of simplicity, the pre-processing step is not detailed here;
this tutorial directly fetches the pre-processed audio data for both datasets.
The pre-processed utility methods to generate these MFCC data are available in
the <code class="docutils literal notranslate"><span class="pre">akida_models</span></code> package.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pickle</span>

<span class="kn">from</span> <span class="nn">tensorflow.keras.utils</span> <span class="kn">import</span> <span class="n">get_file</span>

<span class="c1"># Fetch pre-processed data for 32 keywords</span>
<span class="n">fname</span> <span class="o">=</span> <span class="n">get_file</span><span class="p">(</span>
    <span class="n">fname</span><span class="o">=</span><span class="s1">&#39;kws_preprocessed_all_words_except_backward_follow_forward.pkl&#39;</span><span class="p">,</span>
    <span class="n">origin</span><span class="o">=</span>
    <span class="s2">&quot;http://data.brainchip.com/dataset-mirror/kws/kws_preprocessed_all_words_except_backward_follow_forward.pkl&quot;</span><span class="p">,</span>
    <span class="n">cache_subdir</span><span class="o">=</span><span class="s1">&#39;datasets/kws&#39;</span><span class="p">)</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">fname</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="p">[</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">x_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">word_to_index</span><span class="p">,</span>
     <span class="n">data_transform</span><span class="p">]</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

<span class="c1"># Fetch pre-processed data for the 3 new keywords</span>
<span class="n">fname2</span> <span class="o">=</span> <span class="n">get_file</span><span class="p">(</span>
    <span class="n">fname</span><span class="o">=</span><span class="s1">&#39;kws_preprocessed_edge_backward_follow_forward.pkl&#39;</span><span class="p">,</span>
    <span class="n">origin</span><span class="o">=</span>
    <span class="s2">&quot;http://data.brainchip.com/dataset-mirror/kws/kws_preprocessed_edge_backward_follow_forward.pkl&quot;</span><span class="p">,</span>
    <span class="n">cache_subdir</span><span class="o">=</span><span class="s1">&#39;datasets/kws&#39;</span><span class="p">)</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">fname2</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="p">[</span>
        <span class="n">x_train_new</span><span class="p">,</span> <span class="n">y_train_new</span><span class="p">,</span> <span class="n">x_val_new</span><span class="p">,</span> <span class="n">y_val_new</span><span class="p">,</span> <span class="n">files_train</span><span class="p">,</span> <span class="n">files_val</span><span class="p">,</span>
        <span class="n">word_to_index_new</span><span class="p">,</span> <span class="n">dt2</span>
    <span class="p">]</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Wanted words and labels:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">word_to_index</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;New words:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">word_to_index_new</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Downloading data from http://data.brainchip.com/dataset-mirror/kws/kws_preprocessed_edge_backward_follow_forward.pkl

 16384/251079 [&gt;.............................] - ETA: 0s
139264/251079 [===============&gt;..............] - ETA: 0s
253952/251079 [==============================] - 0s 0us/step

262144/251079 [===============================] - 0s 0us/step
Wanted words and labels:
 {&#39;six&#39;: 23, &#39;three&#39;: 25, &#39;seven&#39;: 21, &#39;bed&#39;: 1, &#39;eight&#39;: 6, &#39;yes&#39;: 31, &#39;cat&#39;: 3, &#39;on&#39;: 18, &#39;one&#39;: 19, &#39;stop&#39;: 24, &#39;two&#39;: 27, &#39;house&#39;: 11, &#39;five&#39;: 7, &#39;down&#39;: 5, &#39;four&#39;: 8, &#39;go&#39;: 9, &#39;up&#39;: 28, &#39;learn&#39;: 12, &#39;no&#39;: 16, &#39;bird&#39;: 2, &#39;zero&#39;: 32, &#39;nine&#39;: 15, &#39;visual&#39;: 29, &#39;wow&#39;: 30, &#39;sheila&#39;: 22, &#39;marvin&#39;: 14, &#39;off&#39;: 17, &#39;right&#39;: 20, &#39;left&#39;: 13, &#39;happy&#39;: 10, &#39;dog&#39;: 4, &#39;tree&#39;: 26, &#39;_silence_&#39;: 0}
New words:
 {&#39;backward&#39;: 0, &#39;follow&#39;: 1, &#39;forward&#39;: 2}
</pre></div>
</div>
</section>
<section id="prepare-akida-model-for-learning">
<h2>3. Prepare Akida model for learning<a class="headerlink" href="#prepare-akida-model-for-learning" title="Permalink to this headline"></a></h2>
<p>As explained above, to be compatible with Akida:</p>
<ul class="simple">
<li><p>the feature extractor must return <strong>binary spikes</strong>.</p></li>
<li><p>the classification layer must have <strong>binary weights</strong>.</p></li>
</ul>
<p>For this example, we load a pre-trained model from which we keep the feature
extractor, returning binary spikes. This model was previously trained and
quantized with Keras and the CNN2SNN tool. The first dataset with 33 classes
(32 keywords + “silence”) was used for training.</p>
<p>However, the last layer of this pre-trained model is not compatible for Akida
learning since it doesn’t have binary weights. We then remove this last layer
and add a new classification layer with 33 classes and
15 neurons per class. One can try with different values of neurons per
class, e.g. from 1 to 500 neurons per class, and see the effects on
performance and time cost.</p>
<p>Moreover, as for any training algorithm, the learning hyper-parameters have to
be correctly set. For the Akida learning algorithm, one important parameter
is the <strong>number of weights</strong>: because of the way the Akida learning algorithm
works, the number of spikes at the end of the feature extractor provides a
good starting point for this hyper-parameter. Here, we estimate this number
of output spikes using 10% of the training set, which is enough to have a
reasonable estimation.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">akida_models</span> <span class="kn">import</span> <span class="n">ds_cnn_kws_pretrained</span>

<span class="c1"># Instantiate a quantized model with pretrained quantized weights</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ds_cnn_kws_pretrained</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Model: &quot;ds_cnn_kws&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         [(None, 49, 10, 1)]       0
_________________________________________________________________
rescaling (Rescaling)        (None, 49, 10, 1)         0
_________________________________________________________________
conv_0 (QuantizedConv2D)     (None, 25, 5, 64)         1664
_________________________________________________________________
conv_0_relu (ActivationDiscr (None, 25, 5, 64)         0
_________________________________________________________________
separable_1 (QuantizedSepara (None, 25, 5, 64)         4736
_________________________________________________________________
separable_1_relu (Activation (None, 25, 5, 64)         0
_________________________________________________________________
separable_2 (QuantizedSepara (None, 25, 5, 64)         4736
_________________________________________________________________
separable_2_relu (Activation (None, 25, 5, 64)         0
_________________________________________________________________
separable_3 (QuantizedSepara (None, 25, 5, 64)         4736
_________________________________________________________________
separable_3_relu (Activation (None, 25, 5, 64)         0
_________________________________________________________________
separable_4 (QuantizedSepara (None, 25, 5, 64)         4736
_________________________________________________________________
separable_4_global_avg (Glob (None, 64)                0
_________________________________________________________________
separable_4_relu (Activation (None, 64)                0
_________________________________________________________________
reshape_1 (Reshape)          (None, 1, 1, 64)          0
_________________________________________________________________
flatten (Flatten)            (None, 64)                0
_________________________________________________________________
dense_5 (QuantizedDense)     (None, 33)                2145
_________________________________________________________________
act_softmax (Activation)     (None, 33)                0
=================================================================
Total params: 22,753
Trainable params: 22,753
Non-trainable params: 0
_________________________________________________________________
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">ceil</span>

<span class="kn">from</span> <span class="nn">cnn2snn</span> <span class="kn">import</span> <span class="n">convert</span>

<span class="c1">#  Convert to an Akida model</span>
<span class="n">model_ak</span> <span class="o">=</span> <span class="n">convert</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">model_ak</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>                Model Summary
______________________________________________
Input shape  Output shape  Sequences  Layers
==============================================
[49, 10, 1]  [1, 1, 33]    1          6
______________________________________________

             SW/conv_0-dense_5 (Software)
_______________________________________________________
Layer (type)             Output shape  Kernel shape
=======================================================
conv_0 (InputConv.)      [5, 25, 64]   (5, 5, 1, 64)
_______________________________________________________
separable_1 (Sep.Conv.)  [5, 25, 64]   (3, 3, 64, 1)
_______________________________________________________
                                       (1, 1, 64, 64)
_______________________________________________________
separable_2 (Sep.Conv.)  [5, 25, 64]   (3, 3, 64, 1)
_______________________________________________________
                                       (1, 1, 64, 64)
_______________________________________________________
separable_3 (Sep.Conv.)  [5, 25, 64]   (3, 3, 64, 1)
_______________________________________________________
                                       (1, 1, 64, 64)
_______________________________________________________
separable_4 (Sep.Conv.)  [1, 1, 64]    (3, 3, 64, 1)
_______________________________________________________
                                       (1, 1, 64, 64)
_______________________________________________________
dense_5 (Fully.)         [1, 1, 33]    (1, 1, 64, 33)
_______________________________________________________
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Measure Akida accuracy on validation set</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">preds_ak</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">y_val</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">num_batches_val</span> <span class="o">=</span> <span class="n">ceil</span><span class="p">(</span><span class="n">x_val</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">batch_size</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_batches_val</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="nb">slice</span><span class="p">(</span><span class="n">i</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">,</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">)</span>
    <span class="n">preds_ak</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="n">model_ak</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_val</span><span class="p">[</span><span class="n">s</span><span class="p">])</span>

<span class="n">acc_val_ak</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">preds_ak</span> <span class="o">==</span> <span class="n">y_val</span><span class="p">)</span> <span class="o">/</span> <span class="n">y_val</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Akida CNN2SNN validation set accuracy: </span><span class="si">{</span><span class="mi">100</span> <span class="o">*</span> <span class="n">acc_val_ak</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> %&quot;</span><span class="p">)</span>

<span class="c1"># For non-regression purpose</span>
<span class="k">assert</span> <span class="n">acc_val_ak</span> <span class="o">&gt;</span> <span class="mf">0.88</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Akida CNN2SNN validation set accuracy: 91.34 %
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">akida</span> <span class="kn">import</span> <span class="n">FullyConnected</span>

<span class="c1"># Replace the last layer by a classification layer with binary weights</span>
<span class="c1"># Here, we choose to set 15 neurons per class.</span>
<span class="n">num_classes</span> <span class="o">=</span> <span class="mi">33</span>
<span class="n">num_neurons_per_class</span> <span class="o">=</span> <span class="mi">15</span>

<span class="n">model_ak</span><span class="o">.</span><span class="n">pop_layer</span><span class="p">()</span>
<span class="n">layer_fc</span> <span class="o">=</span> <span class="n">FullyConnected</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;akida_edge_layer&#39;</span><span class="p">,</span>
                          <span class="n">units</span><span class="o">=</span><span class="n">num_classes</span> <span class="o">*</span> <span class="n">num_neurons_per_class</span><span class="p">,</span>
                          <span class="n">activation</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">model_ak</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layer_fc</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">akida</span> <span class="kn">import</span> <span class="n">evaluate_sparsity</span>

<span class="c1"># Compute sparsity information for the model using 10% of the training data</span>
<span class="c1"># which is enough for a good estimate</span>
<span class="n">num_samples</span> <span class="o">=</span> <span class="n">ceil</span><span class="p">(</span><span class="mf">0.1</span> <span class="o">*</span> <span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">sparsities</span> <span class="o">=</span> <span class="n">evaluate_sparsity</span><span class="p">(</span><span class="n">model_ak</span><span class="p">,</span> <span class="n">x_train</span><span class="p">[:</span><span class="n">num_samples</span><span class="p">])</span>

<span class="c1"># Retrieve the number of output spikes from the feature extractor output</span>
<span class="n">output_density</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">sparsities</span><span class="p">[</span><span class="n">model_ak</span><span class="o">.</span><span class="n">get_layer</span><span class="p">(</span><span class="s1">&#39;separable_4&#39;</span><span class="p">)]</span>
<span class="n">avg_spikes</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_layer</span><span class="p">(</span><span class="s1">&#39;separable_4&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">output_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">output_density</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Average number of spikes: </span><span class="si">{</span><span class="n">avg_spikes</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Fix the number of weights to 1.2 times the average number of output spikes</span>
<span class="n">num_weights</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">1.2</span> <span class="o">*</span> <span class="n">avg_spikes</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The number of weights is then set to:&quot;</span><span class="p">,</span> <span class="n">num_weights</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Average number of spikes: 23.85412688308933
The number of weights is then set to: 28
</pre></div>
</div>
</section>
<section id="learn-with-akida-using-the-training-set">
<h2>4. Learn with Akida using the training set<a class="headerlink" href="#learn-with-akida-using-the-training-set" title="Permalink to this headline"></a></h2>
<p>This stage shows how to train the Akida model using the built-in learning
algorithm in an “offline” stage, i.e. training the classification layer
from scratch using a large training set.
The dataset containing the 33 classes (32 keywords + “silence”) is used.</p>
<p>Now that the Akida model is ready for training, the hyper-parameters
must be set using the <a class="reference external" href="../../api_reference/aee_apis.html#akida.Model.compile">compile</a>
method of the last layer. Compiling a layer means that this layer is
configured for training and ready to be trained. For more information about
the learning hyper-parameters, check the <a class="reference external" href="../../user_guide/aee.html#id5">user guide</a>.
Note that we set the <cite>learning_competition</cite> to 0.1, which gives a little
competition between neurons to prevent learning similar features.</p>
<p>Once the last layer is compiled, the
<a class="reference external" href="../../api_reference/aee_apis.html#akida.Model.fit">fit</a> method is used to
pass the dataset for training. This call is similar to the <cite>fit</cite> method in
tf.keras.</p>
<p>After training, the model is assessed on the validation set using the
<a class="reference external" href="../../api_reference/aee_apis.html#akida.Model.predict">predict</a> method. It
returns the estimated labels for the validation samples.
The model is then saved to a <code class="docutils literal notranslate"><span class="pre">.fbz</span></code> file.</p>
<p>Note that in this specific case, the same dataset was used to train the
feature extractor using the CNN2SNN tool in an early stage, and to train this
classification layer using the native learning algorithm. However, the edge
learning in the next stage passes completely new data in the network.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compile Akida model with learning parameters</span>
<span class="n">model_ak</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">num_weights</span><span class="o">=</span><span class="n">num_weights</span><span class="p">,</span>
                 <span class="n">num_classes</span><span class="o">=</span><span class="n">num_classes</span><span class="p">,</span>
                 <span class="n">learning_competition</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">model_ak</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>                Model Summary
______________________________________________
Input shape  Output shape  Sequences  Layers
==============================================
[49, 10, 1]  [1, 1, 495]   1          6
______________________________________________

          SW/conv_0-akida_edge_layer (Software)
__________________________________________________________
Layer (type)               Output shape  Kernel shape
==========================================================
conv_0 (InputConv.)        [5, 25, 64]   (5, 5, 1, 64)
__________________________________________________________
separable_1 (Sep.Conv.)    [5, 25, 64]   (3, 3, 64, 1)
__________________________________________________________
                                         (1, 1, 64, 64)
__________________________________________________________
separable_2 (Sep.Conv.)    [5, 25, 64]   (3, 3, 64, 1)
__________________________________________________________
                                         (1, 1, 64, 64)
__________________________________________________________
separable_3 (Sep.Conv.)    [5, 25, 64]   (3, 3, 64, 1)
__________________________________________________________
                                         (1, 1, 64, 64)
__________________________________________________________
separable_4 (Sep.Conv.)    [1, 1, 64]    (3, 3, 64, 1)
__________________________________________________________
                                         (1, 1, 64, 64)
__________________________________________________________
akida_edge_layer (Fully.)  [1, 1, 495]   (1, 1, 64, 495)
__________________________________________________________

              Learning Summary
____________________________________________
Learning Layer    # Input Conn.  # Weights
============================================
akida_edge_layer  64             28
____________________________________________
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">time</span>

<span class="c1"># Train the last layer using Akida `fit` method</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Akida learning with </span><span class="si">{</span><span class="n">num_classes</span><span class="si">}</span><span class="s2"> classes... </span><span class="se">\</span>
<span class="s2">        (this step can take a few minutes)&quot;</span><span class="p">)</span>
<span class="n">num_batches</span> <span class="o">=</span> <span class="n">ceil</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">batch_size</span><span class="p">)</span>
<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_batches</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="nb">slice</span><span class="p">(</span><span class="n">i</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">,</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">)</span>
    <span class="n">model_ak</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">[</span><span class="n">s</span><span class="p">],</span> <span class="n">y_train</span><span class="p">[</span><span class="n">s</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
<span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Elapsed time for Akida training: </span><span class="si">{</span><span class="n">end</span><span class="o">-</span><span class="n">start</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> s&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Akida learning with 33 classes...         (this step can take a few minutes)
Elapsed time for Akida training: 33.18 s
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Measure Akida accuracy on validation set</span>
<span class="n">preds_val_ak</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">y_val</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_batches_val</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="nb">slice</span><span class="p">(</span><span class="n">i</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">,</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">)</span>
    <span class="n">preds_val_ak</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="n">model_ak</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_val</span><span class="p">[</span><span class="n">s</span><span class="p">],</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">num_classes</span><span class="p">)</span>

<span class="n">acc_val_ak</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">preds_val_ak</span> <span class="o">==</span> <span class="n">y_val</span><span class="p">)</span> <span class="o">/</span> <span class="n">y_val</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Akida validation set accuracy: </span><span class="si">{</span><span class="mi">100</span> <span class="o">*</span> <span class="n">acc_val_ak</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> %&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Akida validation set accuracy: 86.62 %
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>

<span class="kn">from</span> <span class="nn">tempfile</span> <span class="kn">import</span> <span class="n">TemporaryDirectory</span>

<span class="c1"># Save Akida model</span>
<span class="n">temp_dir</span> <span class="o">=</span> <span class="n">TemporaryDirectory</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;edge_learning_kws&#39;</span><span class="p">)</span>
<span class="n">model_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">temp_dir</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="s1">&#39;ds_cnn_edge_kws.fbz&#39;</span><span class="p">)</span>
<span class="n">model_ak</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model_file</span><span class="p">)</span>
<span class="k">del</span> <span class="n">model_ak</span>
</pre></div>
</div>
</section>
<section id="edge-learning">
<h2>5. Edge learning<a class="headerlink" href="#edge-learning" title="Permalink to this headline"></a></h2>
<p>After the “offline” training stage, we emulate the use case where the
pre-trained Akida model is loaded on an Akida chip, ready to learn new
classes. Our previously saved Akida model has 33 output classes with learned
weights.
We now add 3 classes to the existing model using the
<a class="reference external" href="../../api_reference/aee_apis.html#akida.Model.add_classes">add_classes</a> method
and learn the 3 new keywords without changing the already learned weights.</p>
<p>There is no need to compile the final layer again; the new neurons were
initialized along with the old ones, based on the learning hyper-parameters
given in the <a class="reference external" href="../../api_reference/aee_apis.html#akida.Model.compile">compile</a>
call. The edge learning then uses the same scheme as for the “offline” Akida
learning - only the number of samples used is much more restricted.</p>
<p>Here, each new class is trained using 160 samples, stored in the second
dataset: 4 utterances per word from a single speaker, augmented 40 times each.
The validation set for new words [‘backward’, ‘follow’, ‘forward’] contains
respectively 6, 7 and 6 utterances.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Validation set of new words (</span><span class="si">{</span><span class="n">y_val_new</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> samples):&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">word_to_index_new</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot; - </span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s2"> (label </span><span class="si">{</span><span class="n">label</span><span class="si">}</span><span class="s2">): </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y_val_new</span> <span class="o">==</span> <span class="n">label</span><span class="p">)</span><span class="si">}</span><span class="s2"> samples&quot;</span><span class="p">)</span>

<span class="c1"># Update new labels following the numbering of the old keywords, i.e, new word</span>
<span class="c1"># with label &#39;0&#39; becomes label &#39;34&#39;, new word label &#39;1&#39; becomes &#39;35&#39;, etc.</span>
<span class="n">y_train_new</span> <span class="o">+=</span> <span class="n">num_classes</span>
<span class="n">y_val_new</span> <span class="o">+=</span> <span class="n">num_classes</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Validation set of new words (19 samples):
 - backward (label 0): 6 samples
 - follow (label 1): 7 samples
 - forward (label 2): 6 samples
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">akida</span> <span class="kn">import</span> <span class="n">Model</span>

<span class="c1"># Load the pre-trained model (no need to compile it again)</span>
<span class="n">model_edge</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">model_file</span><span class="p">)</span>
<span class="n">model_edge</span><span class="o">.</span><span class="n">add_classes</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>

<span class="c1"># Train the Akida model with new keywords; only few samples are used.</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Edge learning with 3 new classes ...&quot;</span><span class="p">)</span>
<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
<span class="n">model_edge</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train_new</span><span class="p">,</span> <span class="n">y_train_new</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
<span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Elapsed time for Akida edge learning: </span><span class="si">{</span><span class="n">end</span><span class="o">-</span><span class="n">start</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> s&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Edge learning with 3 new classes ...
Elapsed time for Akida edge learning: 0.17 s
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Predict on the new validation set</span>
<span class="n">preds_ak_new</span> <span class="o">=</span> <span class="n">model_edge</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_val_new</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">num_classes</span> <span class="o">+</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">good_preds_val_new_ak</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">preds_ak_new</span> <span class="o">==</span> <span class="n">y_val_new</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Akida validation set accuracy on 3 new keywords: </span><span class="se">\</span>
<span class="s2">        </span><span class="si">{</span><span class="n">good_preds_val_new_ak</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">y_val_new</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Predict on the old validation set. Edge learning of the 3 new keywords barely</span>
<span class="c1"># affects the accuracy of the old classes.</span>
<span class="n">preds_ak_old</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">y_val</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_batches_val</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="nb">slice</span><span class="p">(</span><span class="n">i</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">,</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">)</span>
    <span class="n">preds_ak_old</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="n">model_edge</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_val</span><span class="p">[</span><span class="n">s</span><span class="p">],</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">num_classes</span> <span class="o">+</span> <span class="mi">3</span><span class="p">)</span>

<span class="n">acc_val_old_ak</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">preds_ak_old</span> <span class="o">==</span> <span class="n">y_val</span><span class="p">)</span> <span class="o">/</span> <span class="n">y_val</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Akida validation set accuracy on 33 old classes: </span><span class="se">\</span>
<span class="s2">        </span><span class="si">{</span><span class="mi">100</span> <span class="o">*</span> <span class="n">acc_val_old_ak</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> %&quot;</span><span class="p">)</span>

<span class="c1"># For non-regression purpose</span>
<span class="k">assert</span> <span class="n">acc_val_old_ak</span> <span class="o">&gt;</span> <span class="mf">0.85</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Akida validation set accuracy on 3 new keywords:         19/19
Akida validation set accuracy on 33 old classes:         86.37 %
</pre></div>
</div>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> ( 0 minutes  51.086 seconds)</p>
<div class="sphx-glr-footer class sphx-glr-footer-example docutils container" id="sphx-glr-download-examples-edge-plot-1-edge-learning-kws-py">
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/2b41d7b19be0b67175397b3549fe25da/plot_1_edge_learning_kws.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">plot_1_edge_learning_kws.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/200bf86d654623ddfbf6d0f9d2e65919/plot_1_edge_learning_kws.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">plot_1_edge_learning_kws.ipynb</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="plot_0_edge_learning_vision.html" class="btn btn-neutral float-left" title="Akida vision edge learning" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="plot_2_edge_learning_parameters.html" class="btn btn-neutral float-right" title="Tips to set Akida learning parameters" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, BrainChip Holdings Ltd. All Rights Reserved.</p>
  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>