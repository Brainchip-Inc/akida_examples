<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Build Vision Transformers for Akida &mdash; Akida Examples  documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/sg_gallery.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/sg_gallery-binder.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/sg_gallery-dataframe.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/sg_gallery-rendered-html.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Advanced QuantizeML tutorial" href="../quantization/plot_0_advanced_quantizeml.html" />
    <link rel="prev" title="Segmentation tutorial" href="plot_6_segmentation.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #989898" >

          
          
          <a href="../../index.html">
            
              <img src="../../_static/MetaTF_logo.png" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                Akida, 2nd Generation
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../index.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../installation.html">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../installation.html#supported-configurations">Supported configurations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../installation.html#quick-installation">Quick installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../installation.html#running-examples">Running examples</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../user_guide/user_guide.html">User guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../user_guide/akida.html">Akida user guide</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/akida.html#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/akida.html#programming-interface">Programming interface</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida.html#the-akida-model">The Akida Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida.html#akida-layers">Akida layers</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/akida.html#model-hardware-mapping">Model Hardware Mapping</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida.html#devices">Devices</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida.html#model-mapping">Model mapping</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida.html#advanced-mapping-details-and-hardware-devices-usage">Advanced Mapping Details and Hardware Devices Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida.html#performance-measurement">Performance measurement</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/akida.html#using-akida-edge-learning">Using Akida Edge learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida.html#learning-constraints">Learning constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida.html#compiling-a-layer">Compiling a layer</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../user_guide/quantizeml.html">QuantizeML toolkit</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/quantizeml.html#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/quantizeml.html#the-fixedpoint-representation">The FixedPoint representation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/quantizeml.html#quantization-flow">Quantization flow</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/quantizeml.html#compatibility-constraints">Compatibility constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/quantizeml.html#model-loading">Model loading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/quantizeml.html#command-line-interface">Command line interface</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/quantizeml.html#quantize-cli">quantize CLI</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/quantizeml.html#config-cli">config CLI</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/quantizeml.html#check-cli">check CLI</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/quantizeml.html#insert-rescaling-cli">insert_rescaling CLI</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/quantizeml.html#supported-layer-types">Supported layer types</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../user_guide/cnn2snn.html">CNN2SNN toolkit</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/cnn2snn.html#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/cnn2snn.html#conversion-flow">Conversion flow</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#conversion-compatibility">Conversion compatibility</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#command-line-interface">Command-line interface</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/cnn2snn.html#handling-akida-1-0-and-akida-2-0-specificities">Handling Akida 1.0 and Akida 2.0 specificities</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/cnn2snn.html#legacy-quantization-api">Legacy quantization API</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#typical-quantization-scenario">Typical quantization scenario</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#design-compatibility-constraints">Design compatibility constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#id3">Command-line interface</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#layers-considerations">Layers Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#tips-and-tricks">Tips and Tricks</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../user_guide/akida_models.html">Akida models zoo</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/akida_models.html#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/akida_models.html#command-line-interface-for-model-creation">Command-line interface for model creation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/akida_models.html#command-line-interface-for-model-training">Command-line interface for model training</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida_models.html#kws-training">KWS training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida_models.html#akidanet-training">AkidaNet training</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/akida_models.html#command-line-interface-for-model-evaluation">Command-line interface for model evaluation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/akida_models.html#command-line-interface-to-evaluate-model-macs">Command-line interface to evaluate model MACS</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/akida_models.html#id1">Layer Blocks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/akida_models.html#handling-akida-1-0-and-akida-2-0-specificities">Handling Akida 1.0 and Akida 2.0 specificities</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../user_guide/hw_constraints.html">Hardware constraints</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/hw_constraints.html#inputconvolutional">InputConvolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/hw_constraints.html#convolutional">Convolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/hw_constraints.html#separableconvolutional">SeparableConvolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/hw_constraints.html#fullyconnected">FullyConnected</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../api_reference/api_reference.html">API reference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../api_reference/akida_apis.html">Akida runtime</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_apis.html#model">Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_apis.html#layer">Layer</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_apis.html#id1">Layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_apis.html#mapping">Mapping</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_apis.html#akida-v1-layers">Akida V1 layers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_apis.html#akida-v2-layers">Akida V2 layers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_apis.html#layer-parameters">Layer parameters</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_apis.html#layertype">LayerType</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_apis.html#padding">Padding</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_apis.html#pooltype">PoolType</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_apis.html#optimizers">Optimizers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_apis.html#sequence">Sequence</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_apis.html#id2">Sequence</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_apis.html#backendtype">BackendType</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_apis.html#pass">Pass</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_apis.html#device">Device</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_apis.html#id3">Device</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_apis.html#hwversion">HwVersion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_apis.html#hwdevice">HWDevice</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_apis.html#id4">HWDevice</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_apis.html#socdriver">SocDriver</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_apis.html#clockmode">ClockMode</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_apis.html#powermeter">PowerMeter</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_apis.html#np">NP</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_apis.html#tools">Tools</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_apis.html#sparsity">Sparsity</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_apis.html#compatibility">Compatibility</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html">CNN2SNN</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#akida-version">Akida version</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#conversion">Conversion</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#legacy-quantization-api">Legacy quantization API</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#utils">Utils</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#calibration">Calibration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#transforms">Transforms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#constraint">Constraint</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#quantization">Quantization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#quantizers">Quantizers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#quantized-layers">Quantized layers</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../api_reference/quantizeml_apis.html">QuantizeML</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#layers">Layers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#reshaping">Reshaping</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#activations">Activations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#attention">Attention</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#normalization">Normalization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#convolution">Convolution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#depthwise-convolution">Depthwise convolution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#separable-convolution">Separable convolution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#dense">Dense</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#skip-connection">Skip connection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#pooling">Pooling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#shiftmax">Shiftmax</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#transformers">Transformers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#rescaling">Rescaling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#dropout">Dropout</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#quantizers">Quantizers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#quantization-parameters">Quantization parameters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#calibration">Calibration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#recording">Recording</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#models">Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#transforms">Transforms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#quantization">Quantization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#id1">Calibration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#utils">Utils</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#tensors">Tensors</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#qtensor">QTensor</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#fixedpoint">FixedPoint</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#qfloat">QFloat</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../api_reference/akida_models_apis.html">Akida models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_models_apis.html#layer-blocks">Layer blocks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#cnn-blocks">CNN blocks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#transformers-blocks">Transformers blocks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#transposed-blocks">Transposed blocks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#detection-block">Detection block</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_models_apis.html#helpers">Helpers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#gamma-constraint">Gamma constraint</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#unfusing-separableconvolutional">Unfusing SeparableConvolutional</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#extract-samples">Extract samples</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_models_apis.html#knowledge-distillation">Knowledge distillation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_models_apis.html#training">Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_models_apis.html#macs">MACS</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_models_apis.html#model-i-o">Model I/O</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_models_apis.html#utils">Utils</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_models_apis.html#model-zoo">Model zoo</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#akidanet">AkidaNet</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#mobilenet">Mobilenet</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#ds-cnn">DS-CNN</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#vgg">VGG</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#yolo">YOLO</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#pointnet">PointNet++</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#gxnor">GXNOR</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#centernet">CenterNet</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#akidaunet">AkidaUNet</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#transformers">Transformers</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Examples</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../index.html#general-examples">General examples</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="plot_0_global_workflow.html">Global Akida workflow</a><ul>
<li class="toctree-l4"><a class="reference internal" href="plot_0_global_workflow.html#create-and-train">1. Create and train</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_0_global_workflow.html#quantize">2. Quantize</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_0_global_workflow.html#convert">3. Convert</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_0_global_workflow.html#gxnor-mnist">4. GXNOR/MNIST</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="plot_1_akidanet_imagenet.html">AkidaNet/ImageNet inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="plot_1_akidanet_imagenet.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_1_akidanet_imagenet.html#pretrained-quantized-model">2. Pretrained quantized model</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_1_akidanet_imagenet.html#conversion-to-akida">3. Conversion to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_1_akidanet_imagenet.html#hardware-mapping-and-performance">4. Hardware mapping and performance</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="plot_2_ds_cnn_kws.html">DS-CNN/KWS inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="plot_2_ds_cnn_kws.html#load-the-preprocessed-dataset">1. Load the preprocessed dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_2_ds_cnn_kws.html#load-a-pre-trained-native-keras-model">2. Load a pre-trained native Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_2_ds_cnn_kws.html#load-a-pre-trained-quantized-keras-model">3. Load a pre-trained quantized Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_2_ds_cnn_kws.html#conversion-to-akida">4. Conversion to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_2_ds_cnn_kws.html#confusion-matrix">5. Confusion matrix</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="plot_3_regression.html">Age estimation (regression) example</a><ul>
<li class="toctree-l4"><a class="reference internal" href="plot_3_regression.html#load-the-utkface-dataset">1. Load the UTKFace Dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_3_regression.html#load-a-pre-trained-native-keras-model">2. Load a pre-trained native Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_3_regression.html#load-a-pre-trained-quantized-keras-model">3. Load a pre-trained quantized Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_3_regression.html#conversion-to-akida">4. Conversion to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_3_regression.html#estimate-age-on-a-single-image">5. Estimate age on a single image</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="plot_4_transfer_learning.html">Transfer learning with AkidaNet for PlantVillage</a><ul>
<li class="toctree-l4"><a class="reference internal" href="plot_4_transfer_learning.html#transfer-learning-process">Transfer learning process</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_4_transfer_learning.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_4_transfer_learning.html#get-a-trained-akidanet-base-model">2. Get a trained AkidaNet base model</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_4_transfer_learning.html#add-a-classification-head-to-the-model">3. Add a classification head to the model</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_4_transfer_learning.html#freeze-the-base-model">4. Freeze the base model</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_4_transfer_learning.html#train-for-a-few-epochs">5. Train for a few epochs</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_4_transfer_learning.html#quantize-the-model">6. Quantize the model</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_4_transfer_learning.html#compute-accuracy">7. Compute accuracy</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="plot_5_voc_yolo_detection.html">YOLO/PASCAL-VOC detection tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="plot_5_voc_yolo_detection.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_5_voc_yolo_detection.html#preprocessing-tools">2. Preprocessing tools</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_5_voc_yolo_detection.html#model-architecture">3. Model architecture</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_5_voc_yolo_detection.html#training">4. Training</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_5_voc_yolo_detection.html#performance">5. Performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_5_voc_yolo_detection.html#conversion-to-akida">6. Conversion to Akida</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="plot_6_segmentation.html">Segmentation tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="plot_6_segmentation.html#load-the-dataset">1. Load the dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_6_segmentation.html#load-a-pre-trained-native-keras-model">2. Load a pre-trained native Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_6_segmentation.html#load-a-pre-trained-quantized-keras-model">3. Load a pre-trained quantized Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_6_segmentation.html#conversion-to-akida">4. Conversion to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_6_segmentation.html#segment-a-single-image">5. Segment a single image</a></li>
</ul>
</li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Build Vision Transformers for Akida</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#model-selection">1. Model selection</a></li>
<li class="toctree-l4"><a class="reference internal" href="#model-optimization-for-akida-hardware">2. Model optimization for Akida hardware</a></li>
<li class="toctree-l4"><a class="reference internal" href="#model-training">3. Model Training</a></li>
<li class="toctree-l4"><a class="reference internal" href="#model-quantization">4. Model quantization</a></li>
<li class="toctree-l4"><a class="reference internal" href="#conversion-to-akida">5. Conversion to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="#displaying-results-attention-maps">6. Displaying results Attention Maps</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#quantization">Quantization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../quantization/plot_0_advanced_quantizeml.html">Advanced QuantizeML tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../quantization/plot_0_advanced_quantizeml.html#defining-a-quantization-scheme">1. Defining a quantization scheme</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantization/plot_0_advanced_quantizeml.html#calibration">2. Calibration</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../quantization/plot_1_upgrading_to_2.0.html">Upgrading to Akida 2.0</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../quantization/plot_1_upgrading_to_2.0.html#workflow-differences">1. Workflow differences</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantization/plot_1_upgrading_to_2.0.html#models-architecture-differences">2. Models architecture differences</a></li>
<li class="toctree-l4"><a class="reference internal" href="../quantization/plot_1_upgrading_to_2.0.html#using-akidaversion">3. Using <code class="docutils literal notranslate"><span class="pre">AkidaVersion</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#edge-examples">Edge examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../edge/plot_0_edge_learning_vision.html">Akida vision edge learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../edge/plot_0_edge_learning_vision.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edge/plot_0_edge_learning_vision.html#prepare-akida-model-for-learning">2. Prepare Akida model for learning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edge/plot_0_edge_learning_vision.html#edge-learning-with-akida">3. Edge learning with Akida</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../edge/plot_1_edge_learning_kws.html">Akida edge learning for keyword spotting</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../edge/plot_1_edge_learning_kws.html#edge-learning-process">1. Edge learning process</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edge/plot_1_edge_learning_kws.html#dataset-preparation">2. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edge/plot_1_edge_learning_kws.html#prepare-akida-model-for-learning">3. Prepare Akida model for learning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edge/plot_1_edge_learning_kws.html#learn-with-akida-using-the-training-set">4. Learn with Akida using the training set</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edge/plot_1_edge_learning_kws.html#edge-learning">5. Edge learning</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../edge/plot_2_edge_learning_parameters.html">Tips to set Akida learning parameters</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../edge/plot_2_edge_learning_parameters.html#akida-learning-parameters">1. Akida learning parameters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edge/plot_2_edge_learning_parameters.html#create-akida-model">2. Create Akida model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edge/plot_2_edge_learning_parameters.html#estimate-the-required-number-of-weights-of-the-trainable-layer">3. Estimate the required number of weights of the trainable layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edge/plot_2_edge_learning_parameters.html#estimate-the-number-of-neurons-per-class">4. Estimate the number of neurons per class</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#deprecated-cnn2snn-tutorials">[Deprecated] CNN2SNN tutorials</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../cnn2snn/plot_1_advanced_cnn2snn.html">Advanced CNN2SNN tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../cnn2snn/plot_1_advanced_cnn2snn.html#design-a-cnn2snn-quantized-model">1. Design a CNN2SNN quantized model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cnn2snn/plot_1_advanced_cnn2snn.html#weight-quantizer-details">2. Weight Quantizer Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cnn2snn/plot_1_advanced_cnn2snn.html#understanding-quantized-activation">3. Understanding quantized activation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cnn2snn/plot_1_advanced_cnn2snn.html#how-to-deal-with-too-high-scale-factors">4. How to deal with too high scale factors</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../model_zoo_performance.html">Model zoo performance</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../model_zoo_performance.html#akida-1-0-models">Akida 1.0 models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../model_zoo_performance.html#image-icon-ref-image-domain"> Image domain</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../model_zoo_performance.html#classification">Classification</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../model_zoo_performance.html#object-detection">Object detection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../model_zoo_performance.html#regression">Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../model_zoo_performance.html#face-recognition">Face recognition</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../model_zoo_performance.html#audio-icon-ref-audio-domain"> Audio domain</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../model_zoo_performance.html#keyword-spotting">Keyword spotting</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../model_zoo_performance.html#pointcloud-icon-ref-point-cloud"> Point cloud</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../model_zoo_performance.html#id1">Classification</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../model_zoo_performance.html#akida-2-0-models">Akida 2.0 models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../model_zoo_performance.html#id2"> Image domain</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../model_zoo_performance.html#id3">Classification</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../model_zoo_performance.html#id6">Object detection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../model_zoo_performance.html#id7">Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../model_zoo_performance.html#id8">Face recognition</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../model_zoo_performance.html#segmentation">Segmentation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../model_zoo_performance.html#id10"> Audio domain</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../model_zoo_performance.html#id11">Keyword spotting</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../model_zoo_performance.html#id12"> Point cloud</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../model_zoo_performance.html#id13">Classification</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../changelog.html">Changelog</a></li>
<li class="toctree-l1"><a class="reference external" href="https://support.brainchip.com/portal/home">Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../license.html">License</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #989898" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Akida Examples</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">Akida examples</a></li>
      <li class="breadcrumb-item active">Build Vision Transformers for Akida</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#sphx-glr-download-examples-general-plot-7-vision-transformer-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code</p>
</div>
<section class="sphx-glr-example-title" id="build-vision-transformers-for-akida">
<span id="sphx-glr-examples-general-plot-7-vision-transformer-py"></span><h1>Build Vision Transformers for Akida<a class="headerlink" href="#build-vision-transformers-for-akida" title="Permalink to this headline"></a></h1>
<p>The Vision Transformer, or ViT, is a model for image classification that employs a Transformer-like
architecture over patches of the image. An image is split into fixed-size patches, each of them are
then linearly embedded, position embeddings are added, and the resulting sequence of vectors are
fed to a standard Transformer encoder. Please refer to <a class="reference external" href="https://arxiv.org/abs/2010.11929">https://arxiv.org/abs/2010.11929</a> for further
details.</p>
<p>Akida 2.0 now supports patch and position embeddings, and the encoder block in hardware. This
tutorial explains how to build an optimized ViT using Akida models python API for Akida 2.0 hardware.</p>
<section id="model-selection">
<h2>1. Model selection<a class="headerlink" href="#model-selection" title="Permalink to this headline"></a></h2>
<p>There are many variants of ViT. The choice of the model is typically influenced by the tradeoff
among architecture size, accuracy, inference speed, and training capabilities.</p>
<p>The following table shows few variants of commonly used ViT:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 23%" />
<col style="width: 31%" />
<col style="width: 15%" />
<col style="width: 31%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Architecture</p></th>
<th class="head"><p>Original accuracy</p></th>
<th class="head"><p>#Params</p></th>
<th class="head"><p>Architecture</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>ViT Base</p></td>
<td><p>79.90%</p></td>
<td><p>86M</p></td>
<td><p>12 heads,
12 blocks,
hidden size 768</p></td>
</tr>
<tr class="row-odd"><td><p>ViT Tiny</p></td>
<td><p>75.48%</p></td>
<td><p>5.8M</p></td>
<td><p>3 heads,
12 blocks,
hidden size 192</p></td>
</tr>
<tr class="row-even"><td><p>DeiT-dist
Tiny</p></td>
<td><p>74.17%</p></td>
<td><p>5.8M</p></td>
<td><p>3 heads,
12 blocks,
hidden size 192</p></td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The Vision Transformers support has been introduced in Akida 2.0.</p>
</div>
<p>The Akida model zoo provides tiny  ViT architectures that are optimized to run on Akida
hardware:</p>
<blockquote>
<div><ul class="simple">
<li><p><a class="reference external" href="../../api_reference/akida_models_apis.html#akida_models.bc_vit_ti16">ViT (tiny)</a>,</p></li>
<li><p><a class="reference external" href="../../api_reference/akida_models_apis.html#akida_models.bc_deit_ti16">DeiT-dist (tiny)</a>.</p></li>
</ul>
</div></blockquote>
<p>Both architectures have been modified so that their layers can be quantized to integer only
operations.</p>
</section>
<section id="model-optimization-for-akida-hardware">
<h2>2. Model optimization for Akida hardware<a class="headerlink" href="#model-optimization-for-akida-hardware" title="Permalink to this headline"></a></h2>
<p>ViT has many encoder blocks that perform self-attention to process visual data. Each encoder
block consists of many different layers. To optimally run ViT at the edge using Akida requires
transforming this encoder block in the following way:</p>
<blockquote>
<div><ul class="simple">
<li><p>replace <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/LayerNormalization">LayerNormalization</a> with
<a class="reference external" href="../../api_reference/quantizeml_apis.html#quantizeml.layers.LayerMadNormalization">LayerMadNormalization</a>,</p></li>
<li><p>replace the last <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/LayerNormalization">LayerNormalization</a> previous
to the classification head with a <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization">BatchNormalization</a>,</p></li>
<li><p>replace <a class="reference external" href="https://www.tensorflow.org/addons/api_docs/python/tfa/layers/GELU">GeLU</a>
with <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/ReLU">ReLU8</a> activations,</p></li>
<li><p>replace <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/activations/softmax">Softmax</a> operation in
<a class="reference external" href="../../api_reference/quantizeml_apis.html#quantizeml.layers.Attention">Attention</a> with a
<a class="reference external" href="../../api_reference/quantizeml_apis.html#quantizeml.layers.shiftmax">shiftmax</a> operation.</p></li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Sections below show different ways to train a ViT for Akida which uses the above
transformations.</p>
</div>
</section>
<section id="model-training">
<h2>3. Model Training<a class="headerlink" href="#model-training" title="Permalink to this headline"></a></h2>
<p>Akida accelerates ViT model that has the transformation mentioned in Section 2. Training a ViT
that optimally runs on Akida can be made possible in the following two ways:</p>
<section id="option-1-training-a-vit-original-model-first-and-then-transforming-each-layer-incrementally">
<h3>3.1 Option 1: Training a ViT (original) model first and then transforming each layer incrementally<a class="headerlink" href="#option-1-training-a-vit-original-model-first-and-then-transforming-each-layer-incrementally" title="Permalink to this headline"></a></h3>
<p>First, train a ViT (original) model on a custom dataset until satisfactory accuracy. It is then
possible to transform this model into an Akida optimized one as per Section 2. The layers mentioned
in Section 2 are functionally equivalent to each of the layers present in the original model.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To overcome the accuracy drop from the original when transforming the model as per Section 2,
it is recommended to replace the original layers one at a time and to fine-tune at every
step.</p>
</div>
<p>The example below shows the transformation of ViT (tiny) into an optimized model that can run on
the Akida hardware.</p>
<p>The <a class="reference external" href="https://pypi.org/project/akida-models">akida_models</a> python package provides a Command Line
Interface (CLI) to transform <a class="reference external" href="../../_modules/akida_models/transformers/model_vit.html#vit_ti16">vit_ti16</a>
and <a class="reference external" href="../../_modules/akida_models/transformers/model_deit.html#deit_ti16">deit_ti16</a> model architectures
and fine-tune them respectively.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>akida_models<span class="w"> </span>create<span class="w"> </span>vit_ti16<span class="w"> </span>-h
usage:<span class="w"> </span>akida_models<span class="w"> </span>create<span class="w"> </span>vit_ti16<span class="w"> </span><span class="o">[</span>-h<span class="o">]</span><span class="w"> </span><span class="o">[</span>-c<span class="w"> </span>CLASSES<span class="o">]</span><span class="w"> </span><span class="o">[</span>-bw<span class="w"> </span>BASE_WEIGHTS<span class="o">]</span><span class="w"> </span><span class="o">[</span>--norm<span class="w"> </span><span class="o">{</span>LN,GN1,BN,LMN<span class="o">}]</span>
<span class="w">                                    </span><span class="o">[</span>--last_norm<span class="w"> </span><span class="o">{</span>LN,BN<span class="o">}]</span><span class="w"> </span><span class="o">[</span>--softmax<span class="w"> </span><span class="o">{</span>softmax,softmax2<span class="o">}]</span>
<span class="w">                                    </span><span class="o">[</span>--act<span class="w"> </span><span class="o">{</span>GeLU,ReLU8,swish<span class="o">}]</span><span class="w"> </span><span class="o">[</span>-i<span class="w"> </span><span class="o">{</span><span class="m">224</span>,384<span class="o">}]</span>

optional<span class="w"> </span>arguments:
<span class="w">  </span>-h,<span class="w"> </span>--help<span class="w">            </span>show<span class="w"> </span>this<span class="w"> </span><span class="nb">help</span><span class="w"> </span>message<span class="w"> </span>and<span class="w"> </span><span class="nb">exit</span>
<span class="w">  </span>-c<span class="w"> </span>CLASSES,<span class="w"> </span>--classes<span class="w"> </span>CLASSES
<span class="w">                        </span>The<span class="w"> </span>number<span class="w"> </span>of<span class="w"> </span>classes,<span class="w"> </span>by<span class="w"> </span>default<span class="w"> </span><span class="m">1000</span>.
<span class="w">  </span>-bw<span class="w"> </span>BASE_WEIGHTS,<span class="w"> </span>--base_weights<span class="w"> </span>BASE_WEIGHTS
<span class="w">                        </span>Optional<span class="w"> </span>keras<span class="w"> </span>weights<span class="w"> </span>to<span class="w"> </span>load<span class="w"> </span><span class="k">in</span><span class="w"> </span>the<span class="w"> </span>model,<span class="w"> </span>by<span class="w"> </span>default<span class="w"> </span>None.
<span class="w">  </span>--norm<span class="w"> </span><span class="o">{</span>LN,GN1,BN,LMN<span class="o">}</span>
<span class="w">                        </span>Replace<span class="w"> </span>normalization<span class="w"> </span><span class="k">in</span><span class="w"> </span>model<span class="w"> </span>with<span class="w"> </span>a<span class="w"> </span>custom<span class="w"> </span><span class="k">function</span>,<span class="w"> </span>by<span class="w"> </span>default<span class="w"> </span>LN
<span class="w">  </span>--last_norm<span class="w"> </span><span class="o">{</span>LN,BN<span class="o">}</span><span class="w">   </span>Replace<span class="w"> </span>last<span class="w"> </span>normalization<span class="w"> </span><span class="k">in</span><span class="w"> </span>model<span class="w"> </span>with<span class="w"> </span>a<span class="w"> </span>custom<span class="w"> </span><span class="k">function</span>,<span class="w"> </span>by<span class="w"> </span>default<span class="w"> </span>LN
<span class="w">  </span>--softmax<span class="w"> </span><span class="o">{</span>softmax,softmax2<span class="o">}</span>
<span class="w">                        </span>Replace<span class="w"> </span>softmax<span class="w"> </span>operation<span class="w"> </span><span class="k">in</span><span class="w"> </span>model<span class="w"> </span>with<span class="w"> </span>custom<span class="w"> </span><span class="k">function</span>,<span class="w"> </span>by<span class="w"> </span>default<span class="w"> </span>softmax
<span class="w">  </span>--act<span class="w"> </span><span class="o">{</span>GeLU,ReLU8,swish<span class="o">}</span>
<span class="w">                        </span>Replace<span class="w"> </span>activation<span class="w"> </span><span class="k">function</span><span class="w"> </span><span class="k">in</span><span class="w"> </span>model<span class="w"> </span>with<span class="w"> </span>custom<span class="w"> </span><span class="k">function</span>,<span class="w"> </span>by<span class="w"> </span>default<span class="w"> </span>GeLU
<span class="w">  </span>-i<span class="w"> </span><span class="o">{</span><span class="m">224</span>,384<span class="o">}</span>,<span class="w"> </span>--image_size<span class="w"> </span><span class="o">{</span><span class="m">224</span>,384<span class="o">}</span>
<span class="w">                        </span>The<span class="w"> </span>square<span class="w"> </span>input<span class="w"> </span>image<span class="w"> </span>size
</pre></div>
</div>
<p>The following shows the transformation of a vit_ti16 model architecture which was trained on ImageNet. The
same methods can be applied for other datasets.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># download the pre-trained weights</span>
wget<span class="w"> </span>https://data.brainchip.com/models/AkidaV2/vit/vit_ti16_224.h5

<span class="c1"># transformation 1: replace layer normalization with mad norm layer and last layer normalization with batch normalization</span>
akida_models<span class="w"> </span>create<span class="w"> </span>-s<span class="w"> </span>vit_ti16_lmnbn.h5<span class="w"> </span>vit_ti16<span class="w"> </span>-bw<span class="w"> </span>vit_ti16_224.h5<span class="w"> </span>--norm<span class="w"> </span>LMN<span class="w"> </span>--last_norm<span class="w"> </span>BN
<span class="c1"># fine-tuning</span>
imagenet_train<span class="w"> </span>tune<span class="w"> </span>-m<span class="w"> </span>vit_ti16_lmnbn.h5<span class="w"> </span>-e<span class="w"> </span><span class="m">15</span><span class="w"> </span>--optim<span class="w"> </span>Adam<span class="w"> </span>--lr_policy<span class="w"> </span>cosine_decay<span class="w"> </span><span class="se">\</span>
<span class="w">                    </span>-lr<span class="w"> </span>6e-5<span class="w"> </span>-s<span class="w"> </span>vit_ti16_lmnbn_tuned.h5

<span class="c1"># transformation 2: replace GeLU layer with ReLU</span>
akida_models<span class="w"> </span>create<span class="w"> </span>-s<span class="w"> </span>vit_ti16_relu.h5<span class="w"> </span>vit_ti16<span class="w"> </span>-bw<span class="w"> </span>vit_ti16_lmnbn_tuned.h5<span class="w"> </span>--norm<span class="w"> </span>LMN<span class="w"> </span>--last_norm<span class="w"> </span>BN<span class="w"> </span>--act<span class="w"> </span>ReLU8
<span class="c1"># fine-tuning</span>
imagenet_train<span class="w"> </span>tune<span class="w"> </span>-m<span class="w"> </span>vit_ti16_relu.h5<span class="w"> </span>-e<span class="w"> </span><span class="m">15</span><span class="w"> </span>--optim<span class="w"> </span>Adam<span class="w"> </span>--lr_policy<span class="w"> </span>cosine_decay<span class="w"> </span><span class="se">\</span>
<span class="w">                    </span>-lr<span class="w"> </span>6e-5<span class="w"> </span>-s<span class="w"> </span>vit_ti16_relu_tuned.h5

<span class="c1"># transformation 3: replace softmax with shiftmax layer</span>
akida_models<span class="w"> </span>create<span class="w"> </span>-s<span class="w"> </span>vit_ti16_shiftmax.h5<span class="w"> </span>vit_ti16<span class="w"> </span>-bw<span class="w"> </span>vit_ti16_relu_tuned.h5<span class="w"> </span>--norm<span class="w"> </span>LMN<span class="w"> </span>--last_norm<span class="w"> </span>BN<span class="w"> </span>--act<span class="w"> </span>ReLU8<span class="w"> </span>--softmax<span class="w"> </span>softmax2
<span class="c1"># fine-tuning</span>
imagenet_train<span class="w"> </span>tune<span class="w"> </span>-m<span class="w"> </span>vit_ti16_shiftmax.h5<span class="w"> </span>-e<span class="w"> </span><span class="m">15</span><span class="w"> </span>--optim<span class="w"> </span>Adam<span class="w"> </span>--lr_policy<span class="w"> </span>cosine_decay<span class="w"> </span><span class="se">\</span>
<span class="w">                    </span>-lr<span class="w"> </span>6e-5<span class="w"> </span>-s<span class="w"> </span>vit_ti16_transformed.h5
</pre></div>
</div>
<p>The above transformation generates a ViT model that is optimized to run efficiently on Akida hardware.
Similar steps can also be applied to deit_ti16. The table below highlights the accuracy of the original
and transformed models.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 25%" />
<col style="width: 35%" />
<col style="width: 40%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Architecture</p></th>
<th class="head"><p>Original accuracy</p></th>
<th class="head"><p>Transformed accuracy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>ViT</p></td>
<td><p>75.48%</p></td>
<td><p>74.25%</p></td>
</tr>
<tr class="row-odd"><td><p>DeiT-dist</p></td>
<td><p>74.17%</p></td>
<td><p>75.03%</p></td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The models obtained above have floating point weights and are ready to be quantized.
See Section 4.</p>
</div>
</section>
<section id="option-2-transfer-learning-using-pre-trained-transformed-model">
<h3>3.2 Option 2: Transfer Learning using Pre-trained transformed model<a class="headerlink" href="#option-2-transfer-learning-using-pre-trained-transformed-model" title="Permalink to this headline"></a></h3>
<p>The <a class="reference external" href="../../api_reference/akida_models_apis.html">Akida models python package</a> has  <a class="reference external" href="../../api_reference/akida_models_apis.html#layer-blocks">APIs for ViTs</a> which provides pre-trained models for
<a class="reference external" href="../../_modules/akida_models/transformers/model_vit.html#vit_ti16">vit_ti16</a> and <a class="reference external" href="../../_modules/akida_models/transformers/model_deit.html#deit_ti16">deit_ti16</a>. These models can be used
for Transfer Learning on a custom dataset. Since the above models are already transformed, no
further transformation is required.</p>
<p>Visit our <a class="reference external" href="plot_4_transfer_learning.html">Transfer Learning Example</a> to learn more about Transfer
Learning using the <a class="reference external" href="../../api_reference/akida_models_apis.html">Akida models python package</a>. The
following code snippet downloads a pre-trained model that can be used for Transfer Learning.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># The following is the API download the vit_t16 model trained on ImageNet dataset</span>
<span class="kn">from</span> <span class="nn">akida_models.model_io</span> <span class="kn">import</span> <span class="n">load_model</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.utils</span> <span class="kn">import</span> <span class="n">get_file</span>

<span class="c1"># Retrieve the float model with pretrained weights and load it</span>
<span class="n">model_file</span> <span class="o">=</span> <span class="n">get_file</span><span class="p">(</span>
    <span class="s2">&quot;bc_vit_ti16_224.h5&quot;</span><span class="p">,</span>
    <span class="s2">&quot;https://data.brainchip.com/models/AkidaV2/vit/bc_vit_ti16_224.h5&quot;</span><span class="p">,</span>
    <span class="n">cache_subdir</span><span class="o">=</span><span class="s1">&#39;models/akidanet_imagenet&#39;</span><span class="p">)</span>
<span class="n">model_keras</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="n">model_file</span><span class="p">)</span>
<span class="n">model_keras</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Downloading data from https://data.brainchip.com/models/AkidaV2/vit/bc_vit_ti16_224.h5

    8192/23695632 [..............................] - ETA: 0s
  204800/23695632 [..............................] - ETA: 6s
  663552/23695632 [..............................] - ETA: 3s
 1032192/23695632 [&gt;.............................] - ETA: 3s
 1409024/23695632 [&gt;.............................] - ETA: 3s
 1761280/23695632 [=&gt;............................] - ETA: 3s
 2179072/23695632 [=&gt;............................] - ETA: 3s
 2572288/23695632 [==&gt;...........................] - ETA: 2s
 2957312/23695632 [==&gt;...........................] - ETA: 2s
 3366912/23695632 [===&gt;..........................] - ETA: 2s
 3743744/23695632 [===&gt;..........................] - ETA: 2s
 4153344/23695632 [====&gt;.........................] - ETA: 2s
 4579328/23695632 [====&gt;.........................] - ETA: 2s
 4988928/23695632 [=====&gt;........................] - ETA: 2s
 5414912/23695632 [=====&gt;........................] - ETA: 2s
 5857280/23695632 [======&gt;.......................] - ETA: 2s
 6266880/23695632 [======&gt;.......................] - ETA: 2s
 6725632/23695632 [=======&gt;......................] - ETA: 2s
 7151616/23695632 [========&gt;.....................] - ETA: 2s
 7610368/23695632 [========&gt;.....................] - ETA: 2s
 8069120/23695632 [=========&gt;....................] - ETA: 1s
 8511488/23695632 [=========&gt;....................] - ETA: 1s
 8970240/23695632 [==========&gt;...................] - ETA: 1s
 9412608/23695632 [==========&gt;...................] - ETA: 1s
 9895936/23695632 [===========&gt;..................] - ETA: 1s
10346496/23695632 [============&gt;.................] - ETA: 1s
10805248/23695632 [============&gt;.................] - ETA: 1s
11313152/23695632 [=============&gt;................] - ETA: 1s
11771904/23695632 [=============&gt;................] - ETA: 1s
12263424/23695632 [==============&gt;...............] - ETA: 1s
12738560/23695632 [===============&gt;..............] - ETA: 1s
13262848/23695632 [===============&gt;..............] - ETA: 1s
13770752/23695632 [================&gt;.............] - ETA: 1s
14262272/23695632 [=================&gt;............] - ETA: 1s
14737408/23695632 [=================&gt;............] - ETA: 1s
15245312/23695632 [==================&gt;...........] - ETA: 0s
15769600/23695632 [==================&gt;...........] - ETA: 0s
16293888/23695632 [===================&gt;..........] - ETA: 0s
16834560/23695632 [====================&gt;.........] - ETA: 0s
17375232/23695632 [====================&gt;.........] - ETA: 0s
17883136/23695632 [=====================&gt;........] - ETA: 0s
18423808/23695632 [======================&gt;.......] - ETA: 0s
18980864/23695632 [=======================&gt;......] - ETA: 0s
19537920/23695632 [=======================&gt;......] - ETA: 0s
20037632/23695632 [========================&gt;.....] - ETA: 0s
20602880/23695632 [=========================&gt;....] - ETA: 0s
21159936/23695632 [=========================&gt;....] - ETA: 0s
21733376/23695632 [==========================&gt;...] - ETA: 0s
22306816/23695632 [===========================&gt;..] - ETA: 0s
22896640/23695632 [===========================&gt;..] - ETA: 0s
23486464/23695632 [============================&gt;.] - ETA: 0s
23695632/23695632 [==============================] - 3s 0us/step
/usr/local/lib/python3.8/dist-packages/keras/initializers/initializers.py:120: UserWarning: The initializer TruncatedNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.
  warnings.warn(
Model: &quot;vit-tiny&quot;
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to
==================================================================================================
 input (InputLayer)             [(None, 224, 224, 3  0           []
                                )]

 Rescale (Rescaling)            (None, 224, 224, 3)  0           [&#39;input[0][0]&#39;]

 Embedding (Conv2D)             (None, 14, 14, 192)  147648      [&#39;Rescale[0][0]&#39;]

 reshape (Reshape)              (None, 196, 192)     0           [&#39;Embedding[0][0]&#39;]

 ClassToken (ClassToken)        (None, 197, 192)     192         [&#39;reshape[0][0]&#39;]

 Transformer/PosEmbed (AddPosit  (None, 197, 192)    37824       [&#39;ClassToken[0][0]&#39;]
 ionEmbs)

 Transformer/EncoderBlock_0/Lay  (None, 197, 192)    384         [&#39;Transformer/PosEmbed[0][0]&#39;]
 erNorm_0 (LayerMadNormalizatio
 n)

 Transformer/EncoderBlock_0/Mul  (None, 197, 192)    37056       [&#39;Transformer/EncoderBlock_0/Laye
 tiHeadDotProductAttention_1/qu                                  rNorm_0[0][0]&#39;]
 ery (Dense)

 Transformer/EncoderBlock_0/Mul  (None, 197, 192)    37056       [&#39;Transformer/EncoderBlock_0/Laye
 tiHeadDotProductAttention_1/ke                                  rNorm_0[0][0]&#39;]
 y (Dense)

 Transformer/EncoderBlock_0/Mul  (None, 197, 192)    37056       [&#39;Transformer/EncoderBlock_0/Laye
 tiHeadDotProductAttention_1/va                                  rNorm_0[0][0]&#39;]
 lue (Dense)

 Transformer/EncoderBlock_0/Mul  ((None, 197, 192),  0           [&#39;Transformer/EncoderBlock_0/Mult
 tiHeadDotProductAttention_1/at   (None, 3, 197, 197             iHeadDotProductAttention_1/query[
 tention (Attention)            ))                               0][0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_0/Mult
                                                                 iHeadDotProductAttention_1/key[0]
                                                                 [0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_0/Mult
                                                                 iHeadDotProductAttention_1/value[
                                                                 0][0]&#39;]

 Transformer/EncoderBlock_0/Mul  (None, 197, 192)    37056       [&#39;Transformer/EncoderBlock_0/Mult
 tiHeadDotProductAttention_1/ou                                  iHeadDotProductAttention_1/attent
 t (Dense)                                                       ion[0][0]&#39;]

 dropout (Dropout)              (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_0/Mult
                                                                 iHeadDotProductAttention_1/out[0]
                                                                 [0]&#39;]

 Transformer/EncoderBlock_0/add  (None, 197, 192)    0           [&#39;dropout[0][0]&#39;,
 _1 (Add)                                                         &#39;Transformer/PosEmbed[0][0]&#39;]

 Transformer/EncoderBlock_0/Lay  (None, 197, 192)    384         [&#39;Transformer/EncoderBlock_0/add_
 erNorm_2 (LayerMadNormalizatio                                  1[0][0]&#39;]
 n)

 Transformer/EncoderBlock_0/Mlp  (None, 197, 768)    148224      [&#39;Transformer/EncoderBlock_0/Laye
 Block/Dense_0 (Dense)                                           rNorm_2[0][0]&#39;]

 Transformer/EncoderBlock_0/Mlp  (None, 197, 768)    0           [&#39;Transformer/EncoderBlock_0/MlpB
 Block/activation (ReLU)                                         lock/Dense_0[0][0]&#39;]

 dropout_1 (Dropout)            (None, 197, 768)     0           [&#39;Transformer/EncoderBlock_0/MlpB
                                                                 lock/activation[0][0]&#39;]

 Transformer/EncoderBlock_0/Mlp  (None, 197, 192)    147648      [&#39;dropout_1[0][0]&#39;]
 Block/Dense_1 (Dense)

 dropout_2 (Dropout)            (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_0/MlpB
                                                                 lock/Dense_1[0][0]&#39;]

 Transformer/EncoderBlock_0/add  (None, 197, 192)    0           [&#39;Transformer/EncoderBlock_0/add_
 _2 (Add)                                                        1[0][0]&#39;,
                                                                  &#39;dropout_2[0][0]&#39;]

 Transformer/EncoderBlock_1/Lay  (None, 197, 192)    384         [&#39;Transformer/EncoderBlock_0/add_
 erNorm_0 (LayerMadNormalizatio                                  2[0][0]&#39;]
 n)

 Transformer/EncoderBlock_1/Mul  (None, 197, 192)    37056       [&#39;Transformer/EncoderBlock_1/Laye
 tiHeadDotProductAttention_1/qu                                  rNorm_0[0][0]&#39;]
 ery (Dense)

 Transformer/EncoderBlock_1/Mul  (None, 197, 192)    37056       [&#39;Transformer/EncoderBlock_1/Laye
 tiHeadDotProductAttention_1/ke                                  rNorm_0[0][0]&#39;]
 y (Dense)

 Transformer/EncoderBlock_1/Mul  (None, 197, 192)    37056       [&#39;Transformer/EncoderBlock_1/Laye
 tiHeadDotProductAttention_1/va                                  rNorm_0[0][0]&#39;]
 lue (Dense)

 Transformer/EncoderBlock_1/Mul  ((None, 197, 192),  0           [&#39;Transformer/EncoderBlock_1/Mult
 tiHeadDotProductAttention_1/at   (None, 3, 197, 197             iHeadDotProductAttention_1/query[
 tention (Attention)            ))                               0][0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_1/Mult
                                                                 iHeadDotProductAttention_1/key[0]
                                                                 [0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_1/Mult
                                                                 iHeadDotProductAttention_1/value[
                                                                 0][0]&#39;]

 Transformer/EncoderBlock_1/Mul  (None, 197, 192)    37056       [&#39;Transformer/EncoderBlock_1/Mult
 tiHeadDotProductAttention_1/ou                                  iHeadDotProductAttention_1/attent
 t (Dense)                                                       ion[0][0]&#39;]

 dropout_3 (Dropout)            (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_1/Mult
                                                                 iHeadDotProductAttention_1/out[0]
                                                                 [0]&#39;]

 Transformer/EncoderBlock_1/add  (None, 197, 192)    0           [&#39;dropout_3[0][0]&#39;,
 _1 (Add)                                                         &#39;Transformer/EncoderBlock_0/add_
                                                                 2[0][0]&#39;]

 Transformer/EncoderBlock_1/Lay  (None, 197, 192)    384         [&#39;Transformer/EncoderBlock_1/add_
 erNorm_2 (LayerMadNormalizatio                                  1[0][0]&#39;]
 n)

 Transformer/EncoderBlock_1/Mlp  (None, 197, 768)    148224      [&#39;Transformer/EncoderBlock_1/Laye
 Block/Dense_0 (Dense)                                           rNorm_2[0][0]&#39;]

 Transformer/EncoderBlock_1/Mlp  (None, 197, 768)    0           [&#39;Transformer/EncoderBlock_1/MlpB
 Block/activation (ReLU)                                         lock/Dense_0[0][0]&#39;]

 dropout_4 (Dropout)            (None, 197, 768)     0           [&#39;Transformer/EncoderBlock_1/MlpB
                                                                 lock/activation[0][0]&#39;]

 Transformer/EncoderBlock_1/Mlp  (None, 197, 192)    147648      [&#39;dropout_4[0][0]&#39;]
 Block/Dense_1 (Dense)

 dropout_5 (Dropout)            (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_1/MlpB
                                                                 lock/Dense_1[0][0]&#39;]

 Transformer/EncoderBlock_1/add  (None, 197, 192)    0           [&#39;Transformer/EncoderBlock_1/add_
 _2 (Add)                                                        1[0][0]&#39;,
                                                                  &#39;dropout_5[0][0]&#39;]

 Transformer/EncoderBlock_2/Lay  (None, 197, 192)    384         [&#39;Transformer/EncoderBlock_1/add_
 erNorm_0 (LayerMadNormalizatio                                  2[0][0]&#39;]
 n)

 Transformer/EncoderBlock_2/Mul  (None, 197, 192)    37056       [&#39;Transformer/EncoderBlock_2/Laye
 tiHeadDotProductAttention_1/qu                                  rNorm_0[0][0]&#39;]
 ery (Dense)

 Transformer/EncoderBlock_2/Mul  (None, 197, 192)    37056       [&#39;Transformer/EncoderBlock_2/Laye
 tiHeadDotProductAttention_1/ke                                  rNorm_0[0][0]&#39;]
 y (Dense)

 Transformer/EncoderBlock_2/Mul  (None, 197, 192)    37056       [&#39;Transformer/EncoderBlock_2/Laye
 tiHeadDotProductAttention_1/va                                  rNorm_0[0][0]&#39;]
 lue (Dense)

 Transformer/EncoderBlock_2/Mul  ((None, 197, 192),  0           [&#39;Transformer/EncoderBlock_2/Mult
 tiHeadDotProductAttention_1/at   (None, 3, 197, 197             iHeadDotProductAttention_1/query[
 tention (Attention)            ))                               0][0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_2/Mult
                                                                 iHeadDotProductAttention_1/key[0]
                                                                 [0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_2/Mult
                                                                 iHeadDotProductAttention_1/value[
                                                                 0][0]&#39;]

 Transformer/EncoderBlock_2/Mul  (None, 197, 192)    37056       [&#39;Transformer/EncoderBlock_2/Mult
 tiHeadDotProductAttention_1/ou                                  iHeadDotProductAttention_1/attent
 t (Dense)                                                       ion[0][0]&#39;]

 dropout_6 (Dropout)            (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_2/Mult
                                                                 iHeadDotProductAttention_1/out[0]
                                                                 [0]&#39;]

 Transformer/EncoderBlock_2/add  (None, 197, 192)    0           [&#39;dropout_6[0][0]&#39;,
 _1 (Add)                                                         &#39;Transformer/EncoderBlock_1/add_
                                                                 2[0][0]&#39;]

 Transformer/EncoderBlock_2/Lay  (None, 197, 192)    384         [&#39;Transformer/EncoderBlock_2/add_
 erNorm_2 (LayerMadNormalizatio                                  1[0][0]&#39;]
 n)

 Transformer/EncoderBlock_2/Mlp  (None, 197, 768)    148224      [&#39;Transformer/EncoderBlock_2/Laye
 Block/Dense_0 (Dense)                                           rNorm_2[0][0]&#39;]

 Transformer/EncoderBlock_2/Mlp  (None, 197, 768)    0           [&#39;Transformer/EncoderBlock_2/MlpB
 Block/activation (ReLU)                                         lock/Dense_0[0][0]&#39;]

 dropout_7 (Dropout)            (None, 197, 768)     0           [&#39;Transformer/EncoderBlock_2/MlpB
                                                                 lock/activation[0][0]&#39;]

 Transformer/EncoderBlock_2/Mlp  (None, 197, 192)    147648      [&#39;dropout_7[0][0]&#39;]
 Block/Dense_1 (Dense)

 dropout_8 (Dropout)            (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_2/MlpB
                                                                 lock/Dense_1[0][0]&#39;]

 Transformer/EncoderBlock_2/add  (None, 197, 192)    0           [&#39;Transformer/EncoderBlock_2/add_
 _2 (Add)                                                        1[0][0]&#39;,
                                                                  &#39;dropout_8[0][0]&#39;]

 Transformer/EncoderBlock_3/Lay  (None, 197, 192)    384         [&#39;Transformer/EncoderBlock_2/add_
 erNorm_0 (LayerMadNormalizatio                                  2[0][0]&#39;]
 n)

 Transformer/EncoderBlock_3/Mul  (None, 197, 192)    37056       [&#39;Transformer/EncoderBlock_3/Laye
 tiHeadDotProductAttention_1/qu                                  rNorm_0[0][0]&#39;]
 ery (Dense)

 Transformer/EncoderBlock_3/Mul  (None, 197, 192)    37056       [&#39;Transformer/EncoderBlock_3/Laye
 tiHeadDotProductAttention_1/ke                                  rNorm_0[0][0]&#39;]
 y (Dense)

 Transformer/EncoderBlock_3/Mul  (None, 197, 192)    37056       [&#39;Transformer/EncoderBlock_3/Laye
 tiHeadDotProductAttention_1/va                                  rNorm_0[0][0]&#39;]
 lue (Dense)

 Transformer/EncoderBlock_3/Mul  ((None, 197, 192),  0           [&#39;Transformer/EncoderBlock_3/Mult
 tiHeadDotProductAttention_1/at   (None, 3, 197, 197             iHeadDotProductAttention_1/query[
 tention (Attention)            ))                               0][0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_3/Mult
                                                                 iHeadDotProductAttention_1/key[0]
                                                                 [0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_3/Mult
                                                                 iHeadDotProductAttention_1/value[
                                                                 0][0]&#39;]

 Transformer/EncoderBlock_3/Mul  (None, 197, 192)    37056       [&#39;Transformer/EncoderBlock_3/Mult
 tiHeadDotProductAttention_1/ou                                  iHeadDotProductAttention_1/attent
 t (Dense)                                                       ion[0][0]&#39;]

 dropout_9 (Dropout)            (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_3/Mult
                                                                 iHeadDotProductAttention_1/out[0]
                                                                 [0]&#39;]

 Transformer/EncoderBlock_3/add  (None, 197, 192)    0           [&#39;dropout_9[0][0]&#39;,
 _1 (Add)                                                         &#39;Transformer/EncoderBlock_2/add_
                                                                 2[0][0]&#39;]

 Transformer/EncoderBlock_3/Lay  (None, 197, 192)    384         [&#39;Transformer/EncoderBlock_3/add_
 erNorm_2 (LayerMadNormalizatio                                  1[0][0]&#39;]
 n)

 Transformer/EncoderBlock_3/Mlp  (None, 197, 768)    148224      [&#39;Transformer/EncoderBlock_3/Laye
 Block/Dense_0 (Dense)                                           rNorm_2[0][0]&#39;]

 Transformer/EncoderBlock_3/Mlp  (None, 197, 768)    0           [&#39;Transformer/EncoderBlock_3/MlpB
 Block/activation (ReLU)                                         lock/Dense_0[0][0]&#39;]

 dropout_10 (Dropout)           (None, 197, 768)     0           [&#39;Transformer/EncoderBlock_3/MlpB
                                                                 lock/activation[0][0]&#39;]

 Transformer/EncoderBlock_3/Mlp  (None, 197, 192)    147648      [&#39;dropout_10[0][0]&#39;]
 Block/Dense_1 (Dense)

 dropout_11 (Dropout)           (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_3/MlpB
                                                                 lock/Dense_1[0][0]&#39;]

 Transformer/EncoderBlock_3/add  (None, 197, 192)    0           [&#39;Transformer/EncoderBlock_3/add_
 _2 (Add)                                                        1[0][0]&#39;,
                                                                  &#39;dropout_11[0][0]&#39;]

 Transformer/EncoderBlock_4/Lay  (None, 197, 192)    384         [&#39;Transformer/EncoderBlock_3/add_
 erNorm_0 (LayerMadNormalizatio                                  2[0][0]&#39;]
 n)

 Transformer/EncoderBlock_4/Mul  (None, 197, 192)    37056       [&#39;Transformer/EncoderBlock_4/Laye
 tiHeadDotProductAttention_1/qu                                  rNorm_0[0][0]&#39;]
 ery (Dense)

 Transformer/EncoderBlock_4/Mul  (None, 197, 192)    37056       [&#39;Transformer/EncoderBlock_4/Laye
 tiHeadDotProductAttention_1/ke                                  rNorm_0[0][0]&#39;]
 y (Dense)

 Transformer/EncoderBlock_4/Mul  (None, 197, 192)    37056       [&#39;Transformer/EncoderBlock_4/Laye
 tiHeadDotProductAttention_1/va                                  rNorm_0[0][0]&#39;]
 lue (Dense)

 Transformer/EncoderBlock_4/Mul  ((None, 197, 192),  0           [&#39;Transformer/EncoderBlock_4/Mult
 tiHeadDotProductAttention_1/at   (None, 3, 197, 197             iHeadDotProductAttention_1/query[
 tention (Attention)            ))                               0][0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_4/Mult
                                                                 iHeadDotProductAttention_1/key[0]
                                                                 [0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_4/Mult
                                                                 iHeadDotProductAttention_1/value[
                                                                 0][0]&#39;]

 Transformer/EncoderBlock_4/Mul  (None, 197, 192)    37056       [&#39;Transformer/EncoderBlock_4/Mult
 tiHeadDotProductAttention_1/ou                                  iHeadDotProductAttention_1/attent
 t (Dense)                                                       ion[0][0]&#39;]

 dropout_12 (Dropout)           (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_4/Mult
                                                                 iHeadDotProductAttention_1/out[0]
                                                                 [0]&#39;]

 Transformer/EncoderBlock_4/add  (None, 197, 192)    0           [&#39;dropout_12[0][0]&#39;,
 _1 (Add)                                                         &#39;Transformer/EncoderBlock_3/add_
                                                                 2[0][0]&#39;]

 Transformer/EncoderBlock_4/Lay  (None, 197, 192)    384         [&#39;Transformer/EncoderBlock_4/add_
 erNorm_2 (LayerMadNormalizatio                                  1[0][0]&#39;]
 n)

 Transformer/EncoderBlock_4/Mlp  (None, 197, 768)    148224      [&#39;Transformer/EncoderBlock_4/Laye
 Block/Dense_0 (Dense)                                           rNorm_2[0][0]&#39;]

 Transformer/EncoderBlock_4/Mlp  (None, 197, 768)    0           [&#39;Transformer/EncoderBlock_4/MlpB
 Block/activation (ReLU)                                         lock/Dense_0[0][0]&#39;]

 dropout_13 (Dropout)           (None, 197, 768)     0           [&#39;Transformer/EncoderBlock_4/MlpB
                                                                 lock/activation[0][0]&#39;]

 Transformer/EncoderBlock_4/Mlp  (None, 197, 192)    147648      [&#39;dropout_13[0][0]&#39;]
 Block/Dense_1 (Dense)

 dropout_14 (Dropout)           (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_4/MlpB
                                                                 lock/Dense_1[0][0]&#39;]

 Transformer/EncoderBlock_4/add  (None, 197, 192)    0           [&#39;Transformer/EncoderBlock_4/add_
 _2 (Add)                                                        1[0][0]&#39;,
                                                                  &#39;dropout_14[0][0]&#39;]

 Transformer/EncoderBlock_5/Lay  (None, 197, 192)    384         [&#39;Transformer/EncoderBlock_4/add_
 erNorm_0 (LayerMadNormalizatio                                  2[0][0]&#39;]
 n)

 Transformer/EncoderBlock_5/Mul  (None, 197, 192)    37056       [&#39;Transformer/EncoderBlock_5/Laye
 tiHeadDotProductAttention_1/qu                                  rNorm_0[0][0]&#39;]
 ery (Dense)

 Transformer/EncoderBlock_5/Mul  (None, 197, 192)    37056       [&#39;Transformer/EncoderBlock_5/Laye
 tiHeadDotProductAttention_1/ke                                  rNorm_0[0][0]&#39;]
 y (Dense)

 Transformer/EncoderBlock_5/Mul  (None, 197, 192)    37056       [&#39;Transformer/EncoderBlock_5/Laye
 tiHeadDotProductAttention_1/va                                  rNorm_0[0][0]&#39;]
 lue (Dense)

 Transformer/EncoderBlock_5/Mul  ((None, 197, 192),  0           [&#39;Transformer/EncoderBlock_5/Mult
 tiHeadDotProductAttention_1/at   (None, 3, 197, 197             iHeadDotProductAttention_1/query[
 tention (Attention)            ))                               0][0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_5/Mult
                                                                 iHeadDotProductAttention_1/key[0]
                                                                 [0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_5/Mult
                                                                 iHeadDotProductAttention_1/value[
                                                                 0][0]&#39;]

 Transformer/EncoderBlock_5/Mul  (None, 197, 192)    37056       [&#39;Transformer/EncoderBlock_5/Mult
 tiHeadDotProductAttention_1/ou                                  iHeadDotProductAttention_1/attent
 t (Dense)                                                       ion[0][0]&#39;]

 dropout_15 (Dropout)           (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_5/Mult
                                                                 iHeadDotProductAttention_1/out[0]
                                                                 [0]&#39;]

 Transformer/EncoderBlock_5/add  (None, 197, 192)    0           [&#39;dropout_15[0][0]&#39;,
 _1 (Add)                                                         &#39;Transformer/EncoderBlock_4/add_
                                                                 2[0][0]&#39;]

 Transformer/EncoderBlock_5/Lay  (None, 197, 192)    384         [&#39;Transformer/EncoderBlock_5/add_
 erNorm_2 (LayerMadNormalizatio                                  1[0][0]&#39;]
 n)

 Transformer/EncoderBlock_5/Mlp  (None, 197, 768)    148224      [&#39;Transformer/EncoderBlock_5/Laye
 Block/Dense_0 (Dense)                                           rNorm_2[0][0]&#39;]

 Transformer/EncoderBlock_5/Mlp  (None, 197, 768)    0           [&#39;Transformer/EncoderBlock_5/MlpB
 Block/activation (ReLU)                                         lock/Dense_0[0][0]&#39;]

 dropout_16 (Dropout)           (None, 197, 768)     0           [&#39;Transformer/EncoderBlock_5/MlpB
                                                                 lock/activation[0][0]&#39;]

 Transformer/EncoderBlock_5/Mlp  (None, 197, 192)    147648      [&#39;dropout_16[0][0]&#39;]
 Block/Dense_1 (Dense)

 dropout_17 (Dropout)           (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_5/MlpB
                                                                 lock/Dense_1[0][0]&#39;]

 Transformer/EncoderBlock_5/add  (None, 197, 192)    0           [&#39;Transformer/EncoderBlock_5/add_
 _2 (Add)                                                        1[0][0]&#39;,
                                                                  &#39;dropout_17[0][0]&#39;]

 Transformer/EncoderBlock_6/Lay  (None, 197, 192)    384         [&#39;Transformer/EncoderBlock_5/add_
 erNorm_0 (LayerMadNormalizatio                                  2[0][0]&#39;]
 n)

 Transformer/EncoderBlock_6/Mul  (None, 197, 192)    37056       [&#39;Transformer/EncoderBlock_6/Laye
 tiHeadDotProductAttention_1/qu                                  rNorm_0[0][0]&#39;]
 ery (Dense)

 Transformer/EncoderBlock_6/Mul  (None, 197, 192)    37056       [&#39;Transformer/EncoderBlock_6/Laye
 tiHeadDotProductAttention_1/ke                                  rNorm_0[0][0]&#39;]
 y (Dense)

 Transformer/EncoderBlock_6/Mul  (None, 197, 192)    37056       [&#39;Transformer/EncoderBlock_6/Laye
 tiHeadDotProductAttention_1/va                                  rNorm_0[0][0]&#39;]
 lue (Dense)

 Transformer/EncoderBlock_6/Mul  ((None, 197, 192),  0           [&#39;Transformer/EncoderBlock_6/Mult
 tiHeadDotProductAttention_1/at   (None, 3, 197, 197             iHeadDotProductAttention_1/query[
 tention (Attention)            ))                               0][0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_6/Mult
                                                                 iHeadDotProductAttention_1/key[0]
                                                                 [0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_6/Mult
                                                                 iHeadDotProductAttention_1/value[
                                                                 0][0]&#39;]

 Transformer/EncoderBlock_6/Mul  (None, 197, 192)    37056       [&#39;Transformer/EncoderBlock_6/Mult
 tiHeadDotProductAttention_1/ou                                  iHeadDotProductAttention_1/attent
 t (Dense)                                                       ion[0][0]&#39;]

 dropout_18 (Dropout)           (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_6/Mult
                                                                 iHeadDotProductAttention_1/out[0]
                                                                 [0]&#39;]

 Transformer/EncoderBlock_6/add  (None, 197, 192)    0           [&#39;dropout_18[0][0]&#39;,
 _1 (Add)                                                         &#39;Transformer/EncoderBlock_5/add_
                                                                 2[0][0]&#39;]

 Transformer/EncoderBlock_6/Lay  (None, 197, 192)    384         [&#39;Transformer/EncoderBlock_6/add_
 erNorm_2 (LayerMadNormalizatio                                  1[0][0]&#39;]
 n)

 Transformer/EncoderBlock_6/Mlp  (None, 197, 768)    148224      [&#39;Transformer/EncoderBlock_6/Laye
 Block/Dense_0 (Dense)                                           rNorm_2[0][0]&#39;]

 Transformer/EncoderBlock_6/Mlp  (None, 197, 768)    0           [&#39;Transformer/EncoderBlock_6/MlpB
 Block/activation (ReLU)                                         lock/Dense_0[0][0]&#39;]

 dropout_19 (Dropout)           (None, 197, 768)     0           [&#39;Transformer/EncoderBlock_6/MlpB
                                                                 lock/activation[0][0]&#39;]

 Transformer/EncoderBlock_6/Mlp  (None, 197, 192)    147648      [&#39;dropout_19[0][0]&#39;]
 Block/Dense_1 (Dense)

 dropout_20 (Dropout)           (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_6/MlpB
                                                                 lock/Dense_1[0][0]&#39;]

 Transformer/EncoderBlock_6/add  (None, 197, 192)    0           [&#39;Transformer/EncoderBlock_6/add_
 _2 (Add)                                                        1[0][0]&#39;,
                                                                  &#39;dropout_20[0][0]&#39;]

 Transformer/EncoderBlock_7/Lay  (None, 197, 192)    384         [&#39;Transformer/EncoderBlock_6/add_
 erNorm_0 (LayerMadNormalizatio                                  2[0][0]&#39;]
 n)

 Transformer/EncoderBlock_7/Mul  (None, 197, 192)    37056       [&#39;Transformer/EncoderBlock_7/Laye
 tiHeadDotProductAttention_1/qu                                  rNorm_0[0][0]&#39;]
 ery (Dense)

 Transformer/EncoderBlock_7/Mul  (None, 197, 192)    37056       [&#39;Transformer/EncoderBlock_7/Laye
 tiHeadDotProductAttention_1/ke                                  rNorm_0[0][0]&#39;]
 y (Dense)

 Transformer/EncoderBlock_7/Mul  (None, 197, 192)    37056       [&#39;Transformer/EncoderBlock_7/Laye
 tiHeadDotProductAttention_1/va                                  rNorm_0[0][0]&#39;]
 lue (Dense)

 Transformer/EncoderBlock_7/Mul  ((None, 197, 192),  0           [&#39;Transformer/EncoderBlock_7/Mult
 tiHeadDotProductAttention_1/at   (None, 3, 197, 197             iHeadDotProductAttention_1/query[
 tention (Attention)            ))                               0][0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_7/Mult
                                                                 iHeadDotProductAttention_1/key[0]
                                                                 [0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_7/Mult
                                                                 iHeadDotProductAttention_1/value[
                                                                 0][0]&#39;]

 Transformer/EncoderBlock_7/Mul  (None, 197, 192)    37056       [&#39;Transformer/EncoderBlock_7/Mult
 tiHeadDotProductAttention_1/ou                                  iHeadDotProductAttention_1/attent
 t (Dense)                                                       ion[0][0]&#39;]

 dropout_21 (Dropout)           (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_7/Mult
                                                                 iHeadDotProductAttention_1/out[0]
                                                                 [0]&#39;]

 Transformer/EncoderBlock_7/add  (None, 197, 192)    0           [&#39;dropout_21[0][0]&#39;,
 _1 (Add)                                                         &#39;Transformer/EncoderBlock_6/add_
                                                                 2[0][0]&#39;]

 Transformer/EncoderBlock_7/Lay  (None, 197, 192)    384         [&#39;Transformer/EncoderBlock_7/add_
 erNorm_2 (LayerMadNormalizatio                                  1[0][0]&#39;]
 n)

 Transformer/EncoderBlock_7/Mlp  (None, 197, 768)    148224      [&#39;Transformer/EncoderBlock_7/Laye
 Block/Dense_0 (Dense)                                           rNorm_2[0][0]&#39;]

 Transformer/EncoderBlock_7/Mlp  (None, 197, 768)    0           [&#39;Transformer/EncoderBlock_7/MlpB
 Block/activation (ReLU)                                         lock/Dense_0[0][0]&#39;]

 dropout_22 (Dropout)           (None, 197, 768)     0           [&#39;Transformer/EncoderBlock_7/MlpB
                                                                 lock/activation[0][0]&#39;]

 Transformer/EncoderBlock_7/Mlp  (None, 197, 192)    147648      [&#39;dropout_22[0][0]&#39;]
 Block/Dense_1 (Dense)

 dropout_23 (Dropout)           (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_7/MlpB
                                                                 lock/Dense_1[0][0]&#39;]

 Transformer/EncoderBlock_7/add  (None, 197, 192)    0           [&#39;Transformer/EncoderBlock_7/add_
 _2 (Add)                                                        1[0][0]&#39;,
                                                                  &#39;dropout_23[0][0]&#39;]

 Transformer/EncoderBlock_8/Lay  (None, 197, 192)    384         [&#39;Transformer/EncoderBlock_7/add_
 erNorm_0 (LayerMadNormalizatio                                  2[0][0]&#39;]
 n)

 Transformer/EncoderBlock_8/Mul  (None, 197, 192)    37056       [&#39;Transformer/EncoderBlock_8/Laye
 tiHeadDotProductAttention_1/qu                                  rNorm_0[0][0]&#39;]
 ery (Dense)

 Transformer/EncoderBlock_8/Mul  (None, 197, 192)    37056       [&#39;Transformer/EncoderBlock_8/Laye
 tiHeadDotProductAttention_1/ke                                  rNorm_0[0][0]&#39;]
 y (Dense)

 Transformer/EncoderBlock_8/Mul  (None, 197, 192)    37056       [&#39;Transformer/EncoderBlock_8/Laye
 tiHeadDotProductAttention_1/va                                  rNorm_0[0][0]&#39;]
 lue (Dense)

 Transformer/EncoderBlock_8/Mul  ((None, 197, 192),  0           [&#39;Transformer/EncoderBlock_8/Mult
 tiHeadDotProductAttention_1/at   (None, 3, 197, 197             iHeadDotProductAttention_1/query[
 tention (Attention)            ))                               0][0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_8/Mult
                                                                 iHeadDotProductAttention_1/key[0]
                                                                 [0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_8/Mult
                                                                 iHeadDotProductAttention_1/value[
                                                                 0][0]&#39;]

 Transformer/EncoderBlock_8/Mul  (None, 197, 192)    37056       [&#39;Transformer/EncoderBlock_8/Mult
 tiHeadDotProductAttention_1/ou                                  iHeadDotProductAttention_1/attent
 t (Dense)                                                       ion[0][0]&#39;]

 dropout_24 (Dropout)           (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_8/Mult
                                                                 iHeadDotProductAttention_1/out[0]
                                                                 [0]&#39;]

 Transformer/EncoderBlock_8/add  (None, 197, 192)    0           [&#39;dropout_24[0][0]&#39;,
 _1 (Add)                                                         &#39;Transformer/EncoderBlock_7/add_
                                                                 2[0][0]&#39;]

 Transformer/EncoderBlock_8/Lay  (None, 197, 192)    384         [&#39;Transformer/EncoderBlock_8/add_
 erNorm_2 (LayerMadNormalizatio                                  1[0][0]&#39;]
 n)

 Transformer/EncoderBlock_8/Mlp  (None, 197, 768)    148224      [&#39;Transformer/EncoderBlock_8/Laye
 Block/Dense_0 (Dense)                                           rNorm_2[0][0]&#39;]

 Transformer/EncoderBlock_8/Mlp  (None, 197, 768)    0           [&#39;Transformer/EncoderBlock_8/MlpB
 Block/activation (ReLU)                                         lock/Dense_0[0][0]&#39;]

 dropout_25 (Dropout)           (None, 197, 768)     0           [&#39;Transformer/EncoderBlock_8/MlpB
                                                                 lock/activation[0][0]&#39;]

 Transformer/EncoderBlock_8/Mlp  (None, 197, 192)    147648      [&#39;dropout_25[0][0]&#39;]
 Block/Dense_1 (Dense)

 dropout_26 (Dropout)           (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_8/MlpB
                                                                 lock/Dense_1[0][0]&#39;]

 Transformer/EncoderBlock_8/add  (None, 197, 192)    0           [&#39;Transformer/EncoderBlock_8/add_
 _2 (Add)                                                        1[0][0]&#39;,
                                                                  &#39;dropout_26[0][0]&#39;]

 Transformer/EncoderBlock_9/Lay  (None, 197, 192)    384         [&#39;Transformer/EncoderBlock_8/add_
 erNorm_0 (LayerMadNormalizatio                                  2[0][0]&#39;]
 n)

 Transformer/EncoderBlock_9/Mul  (None, 197, 192)    37056       [&#39;Transformer/EncoderBlock_9/Laye
 tiHeadDotProductAttention_1/qu                                  rNorm_0[0][0]&#39;]
 ery (Dense)

 Transformer/EncoderBlock_9/Mul  (None, 197, 192)    37056       [&#39;Transformer/EncoderBlock_9/Laye
 tiHeadDotProductAttention_1/ke                                  rNorm_0[0][0]&#39;]
 y (Dense)

 Transformer/EncoderBlock_9/Mul  (None, 197, 192)    37056       [&#39;Transformer/EncoderBlock_9/Laye
 tiHeadDotProductAttention_1/va                                  rNorm_0[0][0]&#39;]
 lue (Dense)

 Transformer/EncoderBlock_9/Mul  ((None, 197, 192),  0           [&#39;Transformer/EncoderBlock_9/Mult
 tiHeadDotProductAttention_1/at   (None, 3, 197, 197             iHeadDotProductAttention_1/query[
 tention (Attention)            ))                               0][0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_9/Mult
                                                                 iHeadDotProductAttention_1/key[0]
                                                                 [0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_9/Mult
                                                                 iHeadDotProductAttention_1/value[
                                                                 0][0]&#39;]

 Transformer/EncoderBlock_9/Mul  (None, 197, 192)    37056       [&#39;Transformer/EncoderBlock_9/Mult
 tiHeadDotProductAttention_1/ou                                  iHeadDotProductAttention_1/attent
 t (Dense)                                                       ion[0][0]&#39;]

 dropout_27 (Dropout)           (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_9/Mult
                                                                 iHeadDotProductAttention_1/out[0]
                                                                 [0]&#39;]

 Transformer/EncoderBlock_9/add  (None, 197, 192)    0           [&#39;dropout_27[0][0]&#39;,
 _1 (Add)                                                         &#39;Transformer/EncoderBlock_8/add_
                                                                 2[0][0]&#39;]

 Transformer/EncoderBlock_9/Lay  (None, 197, 192)    384         [&#39;Transformer/EncoderBlock_9/add_
 erNorm_2 (LayerMadNormalizatio                                  1[0][0]&#39;]
 n)

 Transformer/EncoderBlock_9/Mlp  (None, 197, 768)    148224      [&#39;Transformer/EncoderBlock_9/Laye
 Block/Dense_0 (Dense)                                           rNorm_2[0][0]&#39;]

 Transformer/EncoderBlock_9/Mlp  (None, 197, 768)    0           [&#39;Transformer/EncoderBlock_9/MlpB
 Block/activation (ReLU)                                         lock/Dense_0[0][0]&#39;]

 dropout_28 (Dropout)           (None, 197, 768)     0           [&#39;Transformer/EncoderBlock_9/MlpB
                                                                 lock/activation[0][0]&#39;]

 Transformer/EncoderBlock_9/Mlp  (None, 197, 192)    147648      [&#39;dropout_28[0][0]&#39;]
 Block/Dense_1 (Dense)

 dropout_29 (Dropout)           (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_9/MlpB
                                                                 lock/Dense_1[0][0]&#39;]

 Transformer/EncoderBlock_9/add  (None, 197, 192)    0           [&#39;Transformer/EncoderBlock_9/add_
 _2 (Add)                                                        1[0][0]&#39;,
                                                                  &#39;dropout_29[0][0]&#39;]

 Transformer/EncoderBlock_10/La  (None, 197, 192)    384         [&#39;Transformer/EncoderBlock_9/add_
 yerNorm_0 (LayerMadNormalizati                                  2[0][0]&#39;]
 on)

 Transformer/EncoderBlock_10/Mu  (None, 197, 192)    37056       [&#39;Transformer/EncoderBlock_10/Lay
 ltiHeadDotProductAttention_1/q                                  erNorm_0[0][0]&#39;]
 uery (Dense)

 Transformer/EncoderBlock_10/Mu  (None, 197, 192)    37056       [&#39;Transformer/EncoderBlock_10/Lay
 ltiHeadDotProductAttention_1/k                                  erNorm_0[0][0]&#39;]
 ey (Dense)

 Transformer/EncoderBlock_10/Mu  (None, 197, 192)    37056       [&#39;Transformer/EncoderBlock_10/Lay
 ltiHeadDotProductAttention_1/v                                  erNorm_0[0][0]&#39;]
 alue (Dense)

 Transformer/EncoderBlock_10/Mu  ((None, 197, 192),  0           [&#39;Transformer/EncoderBlock_10/Mul
 ltiHeadDotProductAttention_1/a   (None, 3, 197, 197             tiHeadDotProductAttention_1/query
 ttention (Attention)           ))                               [0][0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_10/Mul
                                                                 tiHeadDotProductAttention_1/key[0
                                                                 ][0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_10/Mul
                                                                 tiHeadDotProductAttention_1/value
                                                                 [0][0]&#39;]

 Transformer/EncoderBlock_10/Mu  (None, 197, 192)    37056       [&#39;Transformer/EncoderBlock_10/Mul
 ltiHeadDotProductAttention_1/o                                  tiHeadDotProductAttention_1/atten
 ut (Dense)                                                      tion[0][0]&#39;]

 dropout_30 (Dropout)           (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_10/Mul
                                                                 tiHeadDotProductAttention_1/out[0
                                                                 ][0]&#39;]

 Transformer/EncoderBlock_10/ad  (None, 197, 192)    0           [&#39;dropout_30[0][0]&#39;,
 d_1 (Add)                                                        &#39;Transformer/EncoderBlock_9/add_
                                                                 2[0][0]&#39;]

 Transformer/EncoderBlock_10/La  (None, 197, 192)    384         [&#39;Transformer/EncoderBlock_10/add
 yerNorm_2 (LayerMadNormalizati                                  _1[0][0]&#39;]
 on)

 Transformer/EncoderBlock_10/Ml  (None, 197, 768)    148224      [&#39;Transformer/EncoderBlock_10/Lay
 pBlock/Dense_0 (Dense)                                          erNorm_2[0][0]&#39;]

 Transformer/EncoderBlock_10/Ml  (None, 197, 768)    0           [&#39;Transformer/EncoderBlock_10/Mlp
 pBlock/activation (ReLU)                                        Block/Dense_0[0][0]&#39;]

 dropout_31 (Dropout)           (None, 197, 768)     0           [&#39;Transformer/EncoderBlock_10/Mlp
                                                                 Block/activation[0][0]&#39;]

 Transformer/EncoderBlock_10/Ml  (None, 197, 192)    147648      [&#39;dropout_31[0][0]&#39;]
 pBlock/Dense_1 (Dense)

 dropout_32 (Dropout)           (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_10/Mlp
                                                                 Block/Dense_1[0][0]&#39;]

 Transformer/EncoderBlock_10/ad  (None, 197, 192)    0           [&#39;Transformer/EncoderBlock_10/add
 d_2 (Add)                                                       _1[0][0]&#39;,
                                                                  &#39;dropout_32[0][0]&#39;]

 Transformer/EncoderBlock_11/La  (None, 197, 192)    384         [&#39;Transformer/EncoderBlock_10/add
 yerNorm_0 (LayerMadNormalizati                                  _2[0][0]&#39;]
 on)

 Transformer/EncoderBlock_11/Mu  (None, 197, 192)    37056       [&#39;Transformer/EncoderBlock_11/Lay
 ltiHeadDotProductAttention_1/q                                  erNorm_0[0][0]&#39;]
 uery (Dense)

 Transformer/EncoderBlock_11/Mu  (None, 197, 192)    37056       [&#39;Transformer/EncoderBlock_11/Lay
 ltiHeadDotProductAttention_1/k                                  erNorm_0[0][0]&#39;]
 ey (Dense)

 Transformer/EncoderBlock_11/Mu  (None, 197, 192)    37056       [&#39;Transformer/EncoderBlock_11/Lay
 ltiHeadDotProductAttention_1/v                                  erNorm_0[0][0]&#39;]
 alue (Dense)

 Transformer/EncoderBlock_11/Mu  ((None, 197, 192),  0           [&#39;Transformer/EncoderBlock_11/Mul
 ltiHeadDotProductAttention_1/a   (None, 3, 197, 197             tiHeadDotProductAttention_1/query
 ttention (Attention)           ))                               [0][0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_11/Mul
                                                                 tiHeadDotProductAttention_1/key[0
                                                                 ][0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_11/Mul
                                                                 tiHeadDotProductAttention_1/value
                                                                 [0][0]&#39;]

 Transformer/EncoderBlock_11/Mu  (None, 197, 192)    37056       [&#39;Transformer/EncoderBlock_11/Mul
 ltiHeadDotProductAttention_1/o                                  tiHeadDotProductAttention_1/atten
 ut (Dense)                                                      tion[0][0]&#39;]

 dropout_33 (Dropout)           (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_11/Mul
                                                                 tiHeadDotProductAttention_1/out[0
                                                                 ][0]&#39;]

 Transformer/EncoderBlock_11/ad  (None, 197, 192)    0           [&#39;dropout_33[0][0]&#39;,
 d_1 (Add)                                                        &#39;Transformer/EncoderBlock_10/add
                                                                 _2[0][0]&#39;]

 Transformer/EncoderBlock_11/La  (None, 197, 192)    384         [&#39;Transformer/EncoderBlock_11/add
 yerNorm_2 (LayerMadNormalizati                                  _1[0][0]&#39;]
 on)

 Transformer/EncoderBlock_11/Ml  (None, 197, 768)    148224      [&#39;Transformer/EncoderBlock_11/Lay
 pBlock/Dense_0 (Dense)                                          erNorm_2[0][0]&#39;]

 Transformer/EncoderBlock_11/Ml  (None, 197, 768)    0           [&#39;Transformer/EncoderBlock_11/Mlp
 pBlock/activation (ReLU)                                        Block/Dense_0[0][0]&#39;]

 dropout_34 (Dropout)           (None, 197, 768)     0           [&#39;Transformer/EncoderBlock_11/Mlp
                                                                 Block/activation[0][0]&#39;]

 Transformer/EncoderBlock_11/Ml  (None, 197, 192)    147648      [&#39;dropout_34[0][0]&#39;]
 pBlock/Dense_1 (Dense)

 dropout_35 (Dropout)           (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_11/Mlp
                                                                 Block/Dense_1[0][0]&#39;]

 Transformer/EncoderBlock_11/ad  (None, 197, 192)    0           [&#39;Transformer/EncoderBlock_11/add
 d_2 (Add)                                                       _1[0][0]&#39;,
                                                                  &#39;dropout_35[0][0]&#39;]

 Transformer/EncoderNorm (Batch  (None, 197, 192)    768         [&#39;Transformer/EncoderBlock_11/add
 Normalization)                                                  _2[0][0]&#39;]

 ExtractToken (ExtractToken)    (None, 192)          0           [&#39;Transformer/EncoderNorm[0][0]&#39;]

 Head (Dense)                   (None, 1000)         193000      [&#39;ExtractToken[0][0]&#39;]

==================================================================================================
Total params: 5,717,800
Trainable params: 5,717,416
Non-trainable params: 384
__________________________________________________________________________________________________
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The models in Section 3 have floating point weights. Once the desired accuracy is obtained,
these models should go through quantization before converting to Akida.</p>
</div>
</section>
</section>
<section id="model-quantization">
<h2>4. Model quantization<a class="headerlink" href="#model-quantization" title="Permalink to this headline"></a></h2>
<p>Akida 2.0 hardware adds efficient processing of 8-bit weights and activations for Vision Transformer
models. This requires models in Section 3 to be quantized to 8-bit integer numbers. This means both
weights and activation outputs become 8-bit integer numbers. This results in a smaller  model with
minimal to no drop in accuracy and achieves improvements in latency and power when running on Akida
hardware.</p>
<p>Quantization of ViT models can be done using <a class="reference external" href="../../user_guide/quantizeml.html">QuantizeML python package</a>
using either Post Training Quantization (PTQ) or Quantization Aware Training (QAT) methods. The following
section shows quantization an example, quantization of <a class="reference external" href="../../_modules/akida_models/transformers/model_vit.html#vit_ti16">vit_ti16</a> trained on ImageNet dataset.</p>
<section id="post-training-quantization">
<h3>4.1 Post-Training Quantization<a class="headerlink" href="#post-training-quantization" title="Permalink to this headline"></a></h3>
<p>Using <a class="reference external" href="../../user_guide/quantizeml.html">QuantizeML python package</a>, ViT model can be quantized to
8-bit integer numbers (both weights and activation outputs). PTQ requires calibration (ideally using
reference data) which helps to determine optimal quantization ranges. To learn more about PTQ, refer
to <a class="reference external" href="../quantization/plot_0_advanced_quantizeml.html">Advanced QuantizeML tutorial</a>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Obtain calibration samples</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">akida_models</span> <span class="kn">import</span> <span class="n">fetch_file</span>

<span class="n">samples</span> <span class="o">=</span> <span class="n">fetch_file</span><span class="p">(</span><span class="s2">&quot;https://data.brainchip.com/dataset-mirror/samples/imagenet/imagenet_batch1024_224.npz&quot;</span><span class="p">,</span>
                     <span class="n">fname</span><span class="o">=</span><span class="s2">&quot;imagenet_batch1024_224.npz&quot;</span><span class="p">)</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">samples</span><span class="p">[</span><span class="n">item</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">samples</span><span class="o">.</span><span class="n">files</span><span class="p">])</span>

<span class="c1"># Using QuantizeML to perform quantization</span>
<span class="kn">from</span> <span class="nn">quantizeml.models</span> <span class="kn">import</span> <span class="n">quantize</span>
<span class="kn">from</span> <span class="nn">quantizeml.layers</span> <span class="kn">import</span> <span class="n">QuantizationParams</span>

<span class="c1"># Define the quantization parameters.</span>
<span class="n">qparams</span> <span class="o">=</span> <span class="n">QuantizationParams</span><span class="p">(</span><span class="n">weight_bits</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">activation_bits</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>

<span class="c1"># Quantize the model defined in Section 3.2</span>
<span class="n">model_quantized</span> <span class="o">=</span> <span class="n">quantize</span><span class="p">(</span><span class="n">model_keras</span><span class="p">,</span>
                           <span class="n">qparams</span><span class="o">=</span><span class="n">qparams</span><span class="p">,</span>
                           <span class="n">samples</span><span class="o">=</span><span class="n">samples</span><span class="p">,</span>
                           <span class="n">num_samples</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">model_quantized</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Downloading data from https://data.brainchip.com/dataset-mirror/samples/imagenet/imagenet_batch1024_224.npz.

        0/616562950 [..............................] - ETA: 0s
    98304/616562950 [..............................] - ETA: 6:21
   385024/616562950 [..............................] - ETA: 3:11
   933888/616562950 [..............................] - ETA: 1:53
  1302528/616562950 [..............................] - ETA: 1:45
  1695744/616562950 [..............................] - ETA: 1:38
  2072576/616562950 [..............................] - ETA: 1:35
  2441216/616562950 [..............................] - ETA: 1:33
  2850816/616562950 [..............................] - ETA: 1:31
  3252224/616562950 [..............................] - ETA: 1:29
  3653632/616562950 [..............................] - ETA: 1:28
  4038656/616562950 [..............................] - ETA: 1:27
  4456448/616562950 [..............................] - ETA: 1:26
  4890624/616562950 [..............................] - ETA: 1:25
  5283840/616562950 [..............................] - ETA: 1:24
  5701632/616562950 [..............................] - ETA: 1:23
  6144000/616562950 [..............................] - ETA: 1:22
  6569984/616562950 [..............................] - ETA: 1:22
  7012352/616562950 [..............................] - ETA: 1:21
  7454720/616562950 [..............................] - ETA: 1:20
  7897088/616562950 [..............................] - ETA: 1:20
  8339456/616562950 [..............................] - ETA: 1:19
  8798208/616562950 [..............................] - ETA: 1:18
  9240576/616562950 [..............................] - ETA: 1:18
  9699328/616562950 [..............................] - ETA: 1:17
 10174464/616562950 [..............................] - ETA: 1:17
 10633216/616562950 [..............................] - ETA: 1:16
 11091968/616562950 [..............................] - ETA: 1:16
 11567104/616562950 [..............................] - ETA: 1:15
 12058624/616562950 [..............................] - ETA: 1:15
 12533760/616562950 [..............................] - ETA: 1:14
 13017088/616562950 [..............................] - ETA: 1:14
 13516800/616562950 [..............................] - ETA: 1:13
 14008320/616562950 [..............................] - ETA: 1:13
 14499840/616562950 [..............................] - ETA: 1:12
 14991360/616562950 [..............................] - ETA: 1:12
 15499264/616562950 [..............................] - ETA: 1:11
 16007168/616562950 [..............................] - ETA: 1:11
 16531456/616562950 [..............................] - ETA: 1:11
 17039360/616562950 [..............................] - ETA: 1:10
 17563648/616562950 [..............................] - ETA: 1:10
 18071552/616562950 [..............................] - ETA: 1:09
 18595840/616562950 [..............................] - ETA: 1:09
 19136512/616562950 [..............................] - ETA: 1:09
 19660800/616562950 [..............................] - ETA: 1:08
 20201472/616562950 [..............................] - ETA: 1:08
 20742144/616562950 [&gt;.............................] - ETA: 1:07
 21282816/616562950 [&gt;.............................] - ETA: 1:07
 21823488/616562950 [&gt;.............................] - ETA: 1:07
 22380544/616562950 [&gt;.............................] - ETA: 1:06
 22921216/616562950 [&gt;.............................] - ETA: 1:06
 23470080/616562950 [&gt;.............................] - ETA: 1:06
 24018944/616562950 [&gt;.............................] - ETA: 1:05
 24592384/616562950 [&gt;.............................] - ETA: 1:05
 25165824/616562950 [&gt;.............................] - ETA: 1:05
 25739264/616562950 [&gt;.............................] - ETA: 1:04
 26312704/616562950 [&gt;.............................] - ETA: 1:04
 26886144/616562950 [&gt;.............................] - ETA: 1:04
 27475968/616562950 [&gt;.............................] - ETA: 1:03
 28049408/616562950 [&gt;.............................] - ETA: 1:03
 28639232/616562950 [&gt;.............................] - ETA: 1:03
 29229056/616562950 [&gt;.............................] - ETA: 1:02
 29818880/616562950 [&gt;.............................] - ETA: 1:02
 30408704/616562950 [&gt;.............................] - ETA: 1:02
 30998528/616562950 [&gt;.............................] - ETA: 1:01
 31588352/616562950 [&gt;.............................] - ETA: 1:01
 32178176/616562950 [&gt;.............................] - ETA: 1:01
 32735232/616562950 [&gt;.............................] - ETA: 1:01
 33333248/616562950 [&gt;.............................] - ETA: 1:00
 33914880/616562950 [&gt;.............................] - ETA: 1:00
 34504704/616562950 [&gt;.............................] - ETA: 1:00
 35094528/616562950 [&gt;.............................] - ETA: 1:00
 35684352/616562950 [&gt;.............................] - ETA: 59s 
 36274176/616562950 [&gt;.............................] - ETA: 59s
 36864000/616562950 [&gt;.............................] - ETA: 59s
 37453824/616562950 [&gt;.............................] - ETA: 59s
 38043648/616562950 [&gt;.............................] - ETA: 59s
 38633472/616562950 [&gt;.............................] - ETA: 58s
 39223296/616562950 [&gt;.............................] - ETA: 58s
 39813120/616562950 [&gt;.............................] - ETA: 58s
 40402944/616562950 [&gt;.............................] - ETA: 58s
 40992768/616562950 [&gt;.............................] - ETA: 58s
 41582592/616562950 [=&gt;............................] - ETA: 57s
 42172416/616562950 [=&gt;............................] - ETA: 57s
 42778624/616562950 [=&gt;............................] - ETA: 57s
 43368448/616562950 [=&gt;............................] - ETA: 57s
 43958272/616562950 [=&gt;............................] - ETA: 57s
 44548096/616562950 [=&gt;............................] - ETA: 56s
 45137920/616562950 [=&gt;............................] - ETA: 56s
 45727744/616562950 [=&gt;............................] - ETA: 56s
 46317568/616562950 [=&gt;............................] - ETA: 56s
 46907392/616562950 [=&gt;............................] - ETA: 56s
 47497216/616562950 [=&gt;............................] - ETA: 56s
 48087040/616562950 [=&gt;............................] - ETA: 56s
 48676864/616562950 [=&gt;............................] - ETA: 55s
 49266688/616562950 [=&gt;............................] - ETA: 55s
 49709056/616562950 [=&gt;............................] - ETA: 55s
 49840128/616562950 [=&gt;............................] - ETA: 56s
 50364416/616562950 [=&gt;............................] - ETA: 56s
 50692096/616562950 [=&gt;............................] - ETA: 56s
 51085312/616562950 [=&gt;............................] - ETA: 56s
 51462144/616562950 [=&gt;............................] - ETA: 56s
 51838976/616562950 [=&gt;............................] - ETA: 56s
 52232192/616562950 [=&gt;............................] - ETA: 56s
 52592640/616562950 [=&gt;............................] - ETA: 56s
 53002240/616562950 [=&gt;............................] - ETA: 56s
 53411840/616562950 [=&gt;............................] - ETA: 57s
 53821440/616562950 [=&gt;............................] - ETA: 57s
 54231040/616562950 [=&gt;............................] - ETA: 57s
 54624256/616562950 [=&gt;............................] - ETA: 57s
 55050240/616562950 [=&gt;............................] - ETA: 57s
 55476224/616562950 [=&gt;............................] - ETA: 57s
 55902208/616562950 [=&gt;............................] - ETA: 57s
 56328192/616562950 [=&gt;............................] - ETA: 57s
 56737792/616562950 [=&gt;............................] - ETA: 57s
 57180160/616562950 [=&gt;............................] - ETA: 57s
 57622528/616562950 [=&gt;............................] - ETA: 57s
 58048512/616562950 [=&gt;............................] - ETA: 57s
 58507264/616562950 [=&gt;............................] - ETA: 57s
 58949632/616562950 [=&gt;............................] - ETA: 57s
 59392000/616562950 [=&gt;............................] - ETA: 57s
 59850752/616562950 [=&gt;............................] - ETA: 57s
 60309504/616562950 [=&gt;............................] - ETA: 57s
 60768256/616562950 [=&gt;............................] - ETA: 57s
 61243392/616562950 [=&gt;............................] - ETA: 57s
 61718528/616562950 [==&gt;...........................] - ETA: 57s
 62193664/616562950 [==&gt;...........................] - ETA: 57s
 62685184/616562950 [==&gt;...........................] - ETA: 57s
 63160320/616562950 [==&gt;...........................] - ETA: 57s
 63651840/616562950 [==&gt;...........................] - ETA: 57s
 64143360/616562950 [==&gt;...........................] - ETA: 57s
 64634880/616562950 [==&gt;...........................] - ETA: 57s
 65142784/616562950 [==&gt;...........................] - ETA: 57s
 65650688/616562950 [==&gt;...........................] - ETA: 56s
 66142208/616562950 [==&gt;...........................] - ETA: 56s
 66650112/616562950 [==&gt;...........................] - ETA: 56s
 67158016/616562950 [==&gt;...........................] - ETA: 56s
 67665920/616562950 [==&gt;...........................] - ETA: 56s
 68190208/616562950 [==&gt;...........................] - ETA: 56s
 68714496/616562950 [==&gt;...........................] - ETA: 56s
 69238784/616562950 [==&gt;...........................] - ETA: 56s
 69746688/616562950 [==&gt;...........................] - ETA: 56s
 70287360/616562950 [==&gt;...........................] - ETA: 56s
 70828032/616562950 [==&gt;...........................] - ETA: 56s
 71368704/616562950 [==&gt;...........................] - ETA: 56s
 71892992/616562950 [==&gt;...........................] - ETA: 56s
 72433664/616562950 [==&gt;...........................] - ETA: 55s
 72974336/616562950 [==&gt;...........................] - ETA: 55s
 73515008/616562950 [==&gt;...........................] - ETA: 55s
 74072064/616562950 [==&gt;...........................] - ETA: 55s
 74629120/616562950 [==&gt;...........................] - ETA: 55s
 75186176/616562950 [==&gt;...........................] - ETA: 55s
 75759616/616562950 [==&gt;...........................] - ETA: 55s
 76333056/616562950 [==&gt;...........................] - ETA: 55s
 76906496/616562950 [==&gt;...........................] - ETA: 55s
 77479936/616562950 [==&gt;...........................] - ETA: 55s
 78069760/616562950 [==&gt;...........................] - ETA: 54s
 78643200/616562950 [==&gt;...........................] - ETA: 54s
 79233024/616562950 [==&gt;...........................] - ETA: 54s
 79790080/616562950 [==&gt;...........................] - ETA: 54s
 80347136/616562950 [==&gt;...........................] - ETA: 54s
 80904192/616562950 [==&gt;...........................] - ETA: 54s
 81461248/616562950 [==&gt;...........................] - ETA: 54s
 82018304/616562950 [==&gt;...........................] - ETA: 54s
 82575360/616562950 [===&gt;..........................] - ETA: 54s
 83116032/616562950 [===&gt;..........................] - ETA: 54s
 83656704/616562950 [===&gt;..........................] - ETA: 53s
 84213760/616562950 [===&gt;..........................] - ETA: 53s
 84770816/616562950 [===&gt;..........................] - ETA: 53s
 85327872/616562950 [===&gt;..........................] - ETA: 53s
 85901312/616562950 [===&gt;..........................] - ETA: 53s
 86491136/616562950 [===&gt;..........................] - ETA: 53s
 87097344/616562950 [===&gt;..........................] - ETA: 53s
 87687168/616562950 [===&gt;..........................] - ETA: 53s
 88276992/616562950 [===&gt;..........................] - ETA: 53s
 88875008/616562950 [===&gt;..........................] - ETA: 52s
 89456640/616562950 [===&gt;..........................] - ETA: 52s
 90030080/616562950 [===&gt;..........................] - ETA: 52s
 90488832/616562950 [===&gt;..........................] - ETA: 52s
 90947584/616562950 [===&gt;..........................] - ETA: 52s
 91406336/616562950 [===&gt;..........................] - ETA: 52s
 91897856/616562950 [===&gt;..........................] - ETA: 52s
 92389376/616562950 [===&gt;..........................] - ETA: 52s
 92880896/616562950 [===&gt;..........................] - ETA: 52s
 93356032/616562950 [===&gt;..........................] - ETA: 52s
 93847552/616562950 [===&gt;..........................] - ETA: 52s
 94355456/616562950 [===&gt;..........................] - ETA: 52s
 94863360/616562950 [===&gt;..........................] - ETA: 52s
 95371264/616562950 [===&gt;..........................] - ETA: 52s
 95911936/616562950 [===&gt;..........................] - ETA: 52s
 96436224/616562950 [===&gt;..........................] - ETA: 52s
 96976896/616562950 [===&gt;..........................] - ETA: 52s
 97517568/616562950 [===&gt;..........................] - ETA: 52s
 98025472/616562950 [===&gt;..........................] - ETA: 52s
 98566144/616562950 [===&gt;..........................] - ETA: 51s
 99123200/616562950 [===&gt;..........................] - ETA: 51s
 99663872/616562950 [===&gt;..........................] - ETA: 51s
100220928/616562950 [===&gt;..........................] - ETA: 51s
100777984/616562950 [===&gt;..........................] - ETA: 51s
101335040/616562950 [===&gt;..........................] - ETA: 51s
101892096/616562950 [===&gt;..........................] - ETA: 51s
102449152/616562950 [===&gt;..........................] - ETA: 51s
103006208/616562950 [====&gt;.........................] - ETA: 51s
103579648/616562950 [====&gt;.........................] - ETA: 51s
104120320/616562950 [====&gt;.........................] - ETA: 51s
104644608/616562950 [====&gt;.........................] - ETA: 51s
105136128/616562950 [====&gt;.........................] - ETA: 51s
105660416/616562950 [====&gt;.........................] - ETA: 51s
106201088/616562950 [====&gt;.........................] - ETA: 50s
106692608/616562950 [====&gt;.........................] - ETA: 50s
106889216/616562950 [====&gt;.........................] - ETA: 51s
107560960/616562950 [====&gt;.........................] - ETA: 50s
107954176/616562950 [====&gt;.........................] - ETA: 50s
108347392/616562950 [====&gt;.........................] - ETA: 50s
108707840/616562950 [====&gt;.........................] - ETA: 50s
108822528/616562950 [====&gt;.........................] - ETA: 51s
109412352/616562950 [====&gt;.........................] - ETA: 51s
109625344/616562950 [====&gt;.........................] - ETA: 51s
109854720/616562950 [====&gt;.........................] - ETA: 51s
110084096/616562950 [====&gt;.........................] - ETA: 51s
110313472/616562950 [====&gt;.........................] - ETA: 51s
110575616/616562950 [====&gt;.........................] - ETA: 51s
110821376/616562950 [====&gt;.........................] - ETA: 51s
111083520/616562950 [====&gt;.........................] - ETA: 51s
111296512/616562950 [====&gt;.........................] - ETA: 52s
111558656/616562950 [====&gt;.........................] - ETA: 52s
111837184/616562950 [====&gt;.........................] - ETA: 52s
112115712/616562950 [====&gt;.........................] - ETA: 52s
112410624/616562950 [====&gt;.........................] - ETA: 52s
112672768/616562950 [====&gt;.........................] - ETA: 52s
112951296/616562950 [====&gt;.........................] - ETA: 52s
113262592/616562950 [====&gt;.........................] - ETA: 52s
113557504/616562950 [====&gt;.........................] - ETA: 52s
113852416/616562950 [====&gt;.........................] - ETA: 52s
114147328/616562950 [====&gt;.........................] - ETA: 52s
114458624/616562950 [====&gt;.........................] - ETA: 52s
114737152/616562950 [====&gt;.........................] - ETA: 52s
115040256/616562950 [====&gt;.........................] - ETA: 52s
115359744/616562950 [====&gt;.........................] - ETA: 53s
115654656/616562950 [====&gt;.........................] - ETA: 53s
115982336/616562950 [====&gt;.........................] - ETA: 53s
116310016/616562950 [====&gt;.........................] - ETA: 53s
116621312/616562950 [====&gt;.........................] - ETA: 53s
116981760/616562950 [====&gt;.........................] - ETA: 53s
117309440/616562950 [====&gt;.........................] - ETA: 53s
117653504/616562950 [====&gt;.........................] - ETA: 53s
118013952/616562950 [====&gt;.........................] - ETA: 53s
118341632/616562950 [====&gt;.........................] - ETA: 53s
118702080/616562950 [====&gt;.........................] - ETA: 53s
119062528/616562950 [====&gt;.........................] - ETA: 53s
119439360/616562950 [====&gt;.........................] - ETA: 53s
119808000/616562950 [====&gt;.........................] - ETA: 53s
120143872/616562950 [====&gt;.........................] - ETA: 53s
120520704/616562950 [====&gt;.........................] - ETA: 53s
120897536/616562950 [====&gt;.........................] - ETA: 53s
121274368/616562950 [====&gt;.........................] - ETA: 53s
121667584/616562950 [====&gt;.........................] - ETA: 53s
122044416/616562950 [====&gt;.........................] - ETA: 53s
122437632/616562950 [====&gt;.........................] - ETA: 53s
122847232/616562950 [====&gt;.........................] - ETA: 53s
123232256/616562950 [====&gt;.........................] - ETA: 53s
123633664/616562950 [=====&gt;........................] - ETA: 53s
124059648/616562950 [=====&gt;........................] - ETA: 53s
124469248/616562950 [=====&gt;........................] - ETA: 53s
124878848/616562950 [=====&gt;........................] - ETA: 53s
125288448/616562950 [=====&gt;........................] - ETA: 53s
125648896/616562950 [=====&gt;........................] - ETA: 53s
126058496/616562950 [=====&gt;........................] - ETA: 53s
126353408/616562950 [=====&gt;........................] - ETA: 53s
126533632/616562950 [=====&gt;........................] - ETA: 53s
127025152/616562950 [=====&gt;........................] - ETA: 53s
127336448/616562950 [=====&gt;........................] - ETA: 53s
127680512/616562950 [=====&gt;........................] - ETA: 53s
128008192/616562950 [=====&gt;........................] - ETA: 53s
128335872/616562950 [=====&gt;........................] - ETA: 53s
128663552/616562950 [=====&gt;........................] - ETA: 53s
128999424/616562950 [=====&gt;........................] - ETA: 53s
129335296/616562950 [=====&gt;........................] - ETA: 53s
129679360/616562950 [=====&gt;........................] - ETA: 53s
130039808/616562950 [=====&gt;........................] - ETA: 53s
130383872/616562950 [=====&gt;........................] - ETA: 53s
130744320/616562950 [=====&gt;........................] - ETA: 53s
131104768/616562950 [=====&gt;........................] - ETA: 53s
131465216/616562950 [=====&gt;........................] - ETA: 53s
131809280/616562950 [=====&gt;........................] - ETA: 53s
132186112/616562950 [=====&gt;........................] - ETA: 53s
132562944/616562950 [=====&gt;........................] - ETA: 53s
132923392/616562950 [=====&gt;........................] - ETA: 53s
133300224/616562950 [=====&gt;........................] - ETA: 53s
133709824/616562950 [=====&gt;........................] - ETA: 53s
134103040/616562950 [=====&gt;........................] - ETA: 53s
134496256/616562950 [=====&gt;........................] - ETA: 53s
134905856/616562950 [=====&gt;........................] - ETA: 53s
135299072/616562950 [=====&gt;........................] - ETA: 53s
135692288/616562950 [=====&gt;........................] - ETA: 53s
136118272/616562950 [=====&gt;........................] - ETA: 53s
136527872/616562950 [=====&gt;........................] - ETA: 53s
136937472/616562950 [=====&gt;........................] - ETA: 53s
137379840/616562950 [=====&gt;........................] - ETA: 53s
137805824/616562950 [=====&gt;........................] - ETA: 53s
138231808/616562950 [=====&gt;........................] - ETA: 53s
138674176/616562950 [=====&gt;........................] - ETA: 53s
139100160/616562950 [=====&gt;........................] - ETA: 53s
139526144/616562950 [=====&gt;........................] - ETA: 53s
139984896/616562950 [=====&gt;........................] - ETA: 53s
140443648/616562950 [=====&gt;........................] - ETA: 53s
140886016/616562950 [=====&gt;........................] - ETA: 53s
141344768/616562950 [=====&gt;........................] - ETA: 52s
141803520/616562950 [=====&gt;........................] - ETA: 52s
142262272/616562950 [=====&gt;........................] - ETA: 52s
142737408/616562950 [=====&gt;........................] - ETA: 52s
143212544/616562950 [=====&gt;........................] - ETA: 52s
143687680/616562950 [=====&gt;........................] - ETA: 52s
144179200/616562950 [======&gt;.......................] - ETA: 52s
144670720/616562950 [======&gt;.......................] - ETA: 52s
145145856/616562950 [======&gt;.......................] - ETA: 52s
145637376/616562950 [======&gt;.......................] - ETA: 52s
146145280/616562950 [======&gt;.......................] - ETA: 52s
146636800/616562950 [======&gt;.......................] - ETA: 52s
147128320/616562950 [======&gt;.......................] - ETA: 52s
147636224/616562950 [======&gt;.......................] - ETA: 52s
148144128/616562950 [======&gt;.......................] - ETA: 52s
148635648/616562950 [======&gt;.......................] - ETA: 52s
149159936/616562950 [======&gt;.......................] - ETA: 51s
149684224/616562950 [======&gt;.......................] - ETA: 51s
150208512/616562950 [======&gt;.......................] - ETA: 51s
150749184/616562950 [======&gt;.......................] - ETA: 51s
151273472/616562950 [======&gt;.......................] - ETA: 51s
151805952/616562950 [======&gt;.......................] - ETA: 51s
152338432/616562950 [======&gt;.......................] - ETA: 51s
152895488/616562950 [======&gt;.......................] - ETA: 51s
153436160/616562950 [======&gt;.......................] - ETA: 51s
153993216/616562950 [======&gt;.......................] - ETA: 51s
154550272/616562950 [======&gt;.......................] - ETA: 51s
155107328/616562950 [======&gt;.......................] - ETA: 50s
155664384/616562950 [======&gt;.......................] - ETA: 50s
156221440/616562950 [======&gt;.......................] - ETA: 50s
156786688/616562950 [======&gt;.......................] - ETA: 50s
157351936/616562950 [======&gt;.......................] - ETA: 50s
157925376/616562950 [======&gt;.......................] - ETA: 50s
158498816/616562950 [======&gt;.......................] - ETA: 50s
159072256/616562950 [======&gt;.......................] - ETA: 50s
159645696/616562950 [======&gt;.......................] - ETA: 50s
160219136/616562950 [======&gt;.......................] - ETA: 50s
160808960/616562950 [======&gt;.......................] - ETA: 49s
161398784/616562950 [======&gt;.......................] - ETA: 49s
161988608/616562950 [======&gt;.......................] - ETA: 49s
162578432/616562950 [======&gt;.......................] - ETA: 49s
163184640/616562950 [======&gt;.......................] - ETA: 49s
163774464/616562950 [======&gt;.......................] - ETA: 49s
164364288/616562950 [======&gt;.......................] - ETA: 49s
164970496/616562950 [=======&gt;......................] - ETA: 49s
165560320/616562950 [=======&gt;......................] - ETA: 49s
166133760/616562950 [=======&gt;......................] - ETA: 49s
166690816/616562950 [=======&gt;......................] - ETA: 48s
167247872/616562950 [=======&gt;......................] - ETA: 48s
167804928/616562950 [=======&gt;......................] - ETA: 48s
168361984/616562950 [=======&gt;......................] - ETA: 48s
168919040/616562950 [=======&gt;......................] - ETA: 48s
169476096/616562950 [=======&gt;......................] - ETA: 48s
170016768/616562950 [=======&gt;......................] - ETA: 48s
170573824/616562950 [=======&gt;......................] - ETA: 48s
171130880/616562950 [=======&gt;......................] - ETA: 48s
171687936/616562950 [=======&gt;......................] - ETA: 48s
172244992/616562950 [=======&gt;......................] - ETA: 48s
172834816/616562950 [=======&gt;......................] - ETA: 48s
173424640/616562950 [=======&gt;......................] - ETA: 47s
174014464/616562950 [=======&gt;......................] - ETA: 47s
174604288/616562950 [=======&gt;......................] - ETA: 47s
175194112/616562950 [=======&gt;......................] - ETA: 47s
175783936/616562950 [=======&gt;......................] - ETA: 47s
176373760/616562950 [=======&gt;......................] - ETA: 47s
176979968/616562950 [=======&gt;......................] - ETA: 47s
177569792/616562950 [=======&gt;......................] - ETA: 47s
178159616/616562950 [=======&gt;......................] - ETA: 47s
178520064/616562950 [=======&gt;......................] - ETA: 47s
179011584/616562950 [=======&gt;......................] - ETA: 47s
179625984/616562950 [=======&gt;......................] - ETA: 47s
179961856/616562950 [=======&gt;......................] - ETA: 47s
180322304/616562950 [=======&gt;......................] - ETA: 46s
180699136/616562950 [=======&gt;......................] - ETA: 46s
181059584/616562950 [=======&gt;......................] - ETA: 46s
181436416/616562950 [=======&gt;......................] - ETA: 46s
181829632/616562950 [=======&gt;......................] - ETA: 46s
182206464/616562950 [=======&gt;......................] - ETA: 46s
182599680/616562950 [=======&gt;......................] - ETA: 46s
182992896/616562950 [=======&gt;......................] - ETA: 46s
183377920/616562950 [=======&gt;......................] - ETA: 46s
183762944/616562950 [=======&gt;......................] - ETA: 46s
184156160/616562950 [=======&gt;......................] - ETA: 46s
184565760/616562950 [=======&gt;......................] - ETA: 46s
184975360/616562950 [========&gt;.....................] - ETA: 46s
185384960/616562950 [========&gt;.....................] - ETA: 46s
185810944/616562950 [========&gt;.....................] - ETA: 46s
186236928/616562950 [========&gt;.....................] - ETA: 46s
186662912/616562950 [========&gt;.....................] - ETA: 46s
187088896/616562950 [========&gt;.....................] - ETA: 46s
187531264/616562950 [========&gt;.....................] - ETA: 46s
187973632/616562950 [========&gt;.....................] - ETA: 46s
188416000/616562950 [========&gt;.....................] - ETA: 46s
188841984/616562950 [========&gt;.....................] - ETA: 46s
188989440/616562950 [========&gt;.....................] - ETA: 46s
189382656/616562950 [========&gt;.....................] - ETA: 46s
189677568/616562950 [========&gt;.....................] - ETA: 46s
189972480/616562950 [========&gt;.....................] - ETA: 46s
190283776/616562950 [========&gt;.....................] - ETA: 46s
190611456/616562950 [========&gt;.....................] - ETA: 46s
190922752/616562950 [========&gt;.....................] - ETA: 46s
191283200/616562950 [========&gt;.....................] - ETA: 46s
191627264/616562950 [========&gt;.....................] - ETA: 46s
191971328/616562950 [========&gt;.....................] - ETA: 46s
192348160/616562950 [========&gt;.....................] - ETA: 46s
192708608/616562950 [========&gt;.....................] - ETA: 46s
193085440/616562950 [========&gt;.....................] - ETA: 46s
193445888/616562950 [========&gt;.....................] - ETA: 46s
193806336/616562950 [========&gt;.....................] - ETA: 46s
194199552/616562950 [========&gt;.....................] - ETA: 46s
194560000/616562950 [========&gt;.....................] - ETA: 46s
194936832/616562950 [========&gt;.....................] - ETA: 46s
195330048/616562950 [========&gt;.....................] - ETA: 46s
195723264/616562950 [========&gt;.....................] - ETA: 46s
196116480/616562950 [========&gt;.....................] - ETA: 46s
196509696/616562950 [========&gt;.....................] - ETA: 46s
196919296/616562950 [========&gt;.....................] - ETA: 46s
197345280/616562950 [========&gt;.....................] - ETA: 46s
197738496/616562950 [========&gt;.....................] - ETA: 46s
198148096/616562950 [========&gt;.....................] - ETA: 46s
198557696/616562950 [========&gt;.....................] - ETA: 46s
198967296/616562950 [========&gt;.....................] - ETA: 46s
199409664/616562950 [========&gt;.....................] - ETA: 45s
199819264/616562950 [========&gt;.....................] - ETA: 45s
200261632/616562950 [========&gt;.....................] - ETA: 45s
200687616/616562950 [========&gt;.....................] - ETA: 45s
201129984/616562950 [========&gt;.....................] - ETA: 45s
201572352/616562950 [========&gt;.....................] - ETA: 45s
202014720/616562950 [========&gt;.....................] - ETA: 45s
202457088/616562950 [========&gt;.....................] - ETA: 45s
202915840/616562950 [========&gt;.....................] - ETA: 45s
203390976/616562950 [========&gt;.....................] - ETA: 45s
203849728/616562950 [========&gt;.....................] - ETA: 45s
204316672/616562950 [========&gt;.....................] - ETA: 45s
204783616/616562950 [========&gt;.....................] - ETA: 45s
205258752/616562950 [========&gt;.....................] - ETA: 45s
205717504/616562950 [=========&gt;....................] - ETA: 45s
206209024/616562950 [=========&gt;....................] - ETA: 45s
206700544/616562950 [=========&gt;....................] - ETA: 45s
207175680/616562950 [=========&gt;....................] - ETA: 45s
207667200/616562950 [=========&gt;....................] - ETA: 45s
208175104/616562950 [=========&gt;....................] - ETA: 45s
208683008/616562950 [=========&gt;....................] - ETA: 44s
209190912/616562950 [=========&gt;....................] - ETA: 44s
209698816/616562950 [=========&gt;....................] - ETA: 44s
210223104/616562950 [=========&gt;....................] - ETA: 44s
210731008/616562950 [=========&gt;....................] - ETA: 44s
211255296/616562950 [=========&gt;....................] - ETA: 44s
211779584/616562950 [=========&gt;....................] - ETA: 44s
212303872/616562950 [=========&gt;....................] - ETA: 44s
212811776/616562950 [=========&gt;....................] - ETA: 44s
213352448/616562950 [=========&gt;....................] - ETA: 44s
213893120/616562950 [=========&gt;....................] - ETA: 44s
214433792/616562950 [=========&gt;....................] - ETA: 44s
214958080/616562950 [=========&gt;....................] - ETA: 44s
215515136/616562950 [=========&gt;....................] - ETA: 44s
216072192/616562950 [=========&gt;....................] - ETA: 43s
216596480/616562950 [=========&gt;....................] - ETA: 43s
217153536/616562950 [=========&gt;....................] - ETA: 43s
217710592/616562950 [=========&gt;....................] - ETA: 43s
218284032/616562950 [=========&gt;....................] - ETA: 43s
218857472/616562950 [=========&gt;....................] - ETA: 43s
219430912/616562950 [=========&gt;....................] - ETA: 43s
220020736/616562950 [=========&gt;....................] - ETA: 43s
220594176/616562950 [=========&gt;....................] - ETA: 43s
221184000/616562950 [=========&gt;....................] - ETA: 43s
221773824/616562950 [=========&gt;....................] - ETA: 43s
222363648/616562950 [=========&gt;....................] - ETA: 43s
222953472/616562950 [=========&gt;....................] - ETA: 42s
223559680/616562950 [=========&gt;....................] - ETA: 42s
224133120/616562950 [=========&gt;....................] - ETA: 42s
224722944/616562950 [=========&gt;....................] - ETA: 42s
225312768/616562950 [=========&gt;....................] - ETA: 42s
225902592/616562950 [=========&gt;....................] - ETA: 42s
226492416/616562950 [==========&gt;...................] - ETA: 42s
227082240/616562950 [==========&gt;...................] - ETA: 42s
227672064/616562950 [==========&gt;...................] - ETA: 42s
228261888/616562950 [==========&gt;...................] - ETA: 42s
228851712/616562950 [==========&gt;...................] - ETA: 42s
229441536/616562950 [==========&gt;...................] - ETA: 41s
230031360/616562950 [==========&gt;...................] - ETA: 41s
230621184/616562950 [==========&gt;...................] - ETA: 41s
231211008/616562950 [==========&gt;...................] - ETA: 41s
231800832/616562950 [==========&gt;...................] - ETA: 41s
232390656/616562950 [==========&gt;...................] - ETA: 41s
232980480/616562950 [==========&gt;...................] - ETA: 41s
233570304/616562950 [==========&gt;...................] - ETA: 41s
234176512/616562950 [==========&gt;...................] - ETA: 41s
234766336/616562950 [==========&gt;...................] - ETA: 41s
235356160/616562950 [==========&gt;...................] - ETA: 41s
235962368/616562950 [==========&gt;...................] - ETA: 41s
236552192/616562950 [==========&gt;...................] - ETA: 40s
237142016/616562950 [==========&gt;...................] - ETA: 40s
237731840/616562950 [==========&gt;...................] - ETA: 40s
238321664/616562950 [==========&gt;...................] - ETA: 40s
238911488/616562950 [==========&gt;...................] - ETA: 40s
239501312/616562950 [==========&gt;...................] - ETA: 40s
240091136/616562950 [==========&gt;...................] - ETA: 40s
240697344/616562950 [==========&gt;...................] - ETA: 40s
241287168/616562950 [==========&gt;...................] - ETA: 40s
241876992/616562950 [==========&gt;...................] - ETA: 40s
242434048/616562950 [==========&gt;...................] - ETA: 40s
242925568/616562950 [==========&gt;...................] - ETA: 40s
243449856/616562950 [==========&gt;...................] - ETA: 40s
243843072/616562950 [==========&gt;...................] - ETA: 40s
244203520/616562950 [==========&gt;...................] - ETA: 39s
244563968/616562950 [==========&gt;...................] - ETA: 39s
244957184/616562950 [==========&gt;...................] - ETA: 39s
245317632/616562950 [==========&gt;...................] - ETA: 39s
245694464/616562950 [==========&gt;...................] - ETA: 39s
246104064/616562950 [==========&gt;...................] - ETA: 39s
246480896/616562950 [==========&gt;...................] - ETA: 39s
246890496/616562950 [===========&gt;..................] - ETA: 39s
247300096/616562950 [===========&gt;..................] - ETA: 39s
247693312/616562950 [===========&gt;..................] - ETA: 39s
248119296/616562950 [===========&gt;..................] - ETA: 39s
248528896/616562950 [===========&gt;..................] - ETA: 39s
248938496/616562950 [===========&gt;..................] - ETA: 39s
249364480/616562950 [===========&gt;..................] - ETA: 39s
249806848/616562950 [===========&gt;..................] - ETA: 39s
250232832/616562950 [===========&gt;..................] - ETA: 39s
250675200/616562950 [===========&gt;..................] - ETA: 39s
251101184/616562950 [===========&gt;..................] - ETA: 39s
251543552/616562950 [===========&gt;..................] - ETA: 39s
252002304/616562950 [===========&gt;..................] - ETA: 39s
252428288/616562950 [===========&gt;..................] - ETA: 39s
252887040/616562950 [===========&gt;..................] - ETA: 39s
253362176/616562950 [===========&gt;..................] - ETA: 39s
253820928/616562950 [===========&gt;..................] - ETA: 39s
254296064/616562950 [===========&gt;..................] - ETA: 39s
254771200/616562950 [===========&gt;..................] - ETA: 39s
255229952/616562950 [===========&gt;..................] - ETA: 39s
255721472/616562950 [===========&gt;..................] - ETA: 38s
256212992/616562950 [===========&gt;..................] - ETA: 38s
256688128/616562950 [===========&gt;..................] - ETA: 38s
257179648/616562950 [===========&gt;..................] - ETA: 38s
257671168/616562950 [===========&gt;..................] - ETA: 38s
258162688/616562950 [===========&gt;..................] - ETA: 38s
258670592/616562950 [===========&gt;..................] - ETA: 38s
259178496/616562950 [===========&gt;..................] - ETA: 38s
259686400/616562950 [===========&gt;..................] - ETA: 38s
260194304/616562950 [===========&gt;..................] - ETA: 38s
260702208/616562950 [===========&gt;..................] - ETA: 38s
261226496/616562950 [===========&gt;..................] - ETA: 38s
261750784/616562950 [===========&gt;..................] - ETA: 38s
262275072/616562950 [===========&gt;..................] - ETA: 38s
262799360/616562950 [===========&gt;..................] - ETA: 38s
263323648/616562950 [===========&gt;..................] - ETA: 38s
263847936/616562950 [===========&gt;..................] - ETA: 38s
264388608/616562950 [===========&gt;..................] - ETA: 37s
264929280/616562950 [===========&gt;..................] - ETA: 37s
265469952/616562950 [===========&gt;..................] - ETA: 37s
266027008/616562950 [===========&gt;..................] - ETA: 37s
266584064/616562950 [===========&gt;..................] - ETA: 37s
267141120/616562950 [===========&gt;..................] - ETA: 37s
267698176/616562950 [============&gt;.................] - ETA: 37s
268271616/616562950 [============&gt;.................] - ETA: 37s
268845056/616562950 [============&gt;.................] - ETA: 37s
269418496/616562950 [============&gt;.................] - ETA: 37s
269991936/616562950 [============&gt;.................] - ETA: 37s
270565376/616562950 [============&gt;.................] - ETA: 37s
271155200/616562950 [============&gt;.................] - ETA: 37s
271745024/616562950 [============&gt;.................] - ETA: 36s
272334848/616562950 [============&gt;.................] - ETA: 36s
272826368/616562950 [============&gt;.................] - ETA: 36s
273252352/616562950 [============&gt;.................] - ETA: 36s
273694720/616562950 [============&gt;.................] - ETA: 36s
274137088/616562950 [============&gt;.................] - ETA: 36s
274579456/616562950 [============&gt;.................] - ETA: 36s
275038208/616562950 [============&gt;.................] - ETA: 36s
275496960/616562950 [============&gt;.................] - ETA: 36s
275955712/616562950 [============&gt;.................] - ETA: 36s
276430848/616562950 [============&gt;.................] - ETA: 36s
276905984/616562950 [============&gt;.................] - ETA: 36s
277364736/616562950 [============&gt;.................] - ETA: 36s
277839872/616562950 [============&gt;.................] - ETA: 36s
278331392/616562950 [============&gt;.................] - ETA: 36s
278822912/616562950 [============&gt;.................] - ETA: 36s
279330816/616562950 [============&gt;.................] - ETA: 36s
279822336/616562950 [============&gt;.................] - ETA: 36s
280313856/616562950 [============&gt;.................] - ETA: 36s
280821760/616562950 [============&gt;.................] - ETA: 36s
281329664/616562950 [============&gt;.................] - ETA: 35s
281837568/616562950 [============&gt;.................] - ETA: 35s
282345472/616562950 [============&gt;.................] - ETA: 35s
282869760/616562950 [============&gt;.................] - ETA: 35s
283377664/616562950 [============&gt;.................] - ETA: 35s
283901952/616562950 [============&gt;.................] - ETA: 35s
284426240/616562950 [============&gt;.................] - ETA: 35s
284950528/616562950 [============&gt;.................] - ETA: 35s
285474816/616562950 [============&gt;.................] - ETA: 35s
286015488/616562950 [============&gt;.................] - ETA: 35s
286539776/616562950 [============&gt;.................] - ETA: 35s
287080448/616562950 [============&gt;.................] - ETA: 35s
287621120/616562950 [============&gt;.................] - ETA: 35s
288161792/616562950 [=============&gt;................] - ETA: 35s
288702464/616562950 [=============&gt;................] - ETA: 35s
289259520/616562950 [=============&gt;................] - ETA: 34s
289816576/616562950 [=============&gt;................] - ETA: 34s
290373632/616562950 [=============&gt;................] - ETA: 34s
290930688/616562950 [=============&gt;................] - ETA: 34s
291504128/616562950 [=============&gt;................] - ETA: 34s
292061184/616562950 [=============&gt;................] - ETA: 34s
292634624/616562950 [=============&gt;................] - ETA: 34s
293208064/616562950 [=============&gt;................] - ETA: 34s
293797888/616562950 [=============&gt;................] - ETA: 34s
294371328/616562950 [=============&gt;................] - ETA: 34s
294961152/616562950 [=============&gt;................] - ETA: 34s
295550976/616562950 [=============&gt;................] - ETA: 34s
296124416/616562950 [=============&gt;................] - ETA: 34s
296714240/616562950 [=============&gt;................] - ETA: 34s
297320448/616562950 [=============&gt;................] - ETA: 33s
297910272/616562950 [=============&gt;................] - ETA: 33s
298500096/616562950 [=============&gt;................] - ETA: 33s
299089920/616562950 [=============&gt;................] - ETA: 33s
299679744/616562950 [=============&gt;................] - ETA: 33s
300269568/616562950 [=============&gt;................] - ETA: 33s
300859392/616562950 [=============&gt;................] - ETA: 33s
301449216/616562950 [=============&gt;................] - ETA: 33s
302006272/616562950 [=============&gt;................] - ETA: 33s
302596096/616562950 [=============&gt;................] - ETA: 33s
303185920/616562950 [=============&gt;................] - ETA: 33s
303775744/616562950 [=============&gt;................] - ETA: 33s
304365568/616562950 [=============&gt;................] - ETA: 33s
304955392/616562950 [=============&gt;................] - ETA: 33s
305545216/616562950 [=============&gt;................] - ETA: 32s
306135040/616562950 [=============&gt;................] - ETA: 32s
306724864/616562950 [=============&gt;................] - ETA: 32s
307314688/616562950 [=============&gt;................] - ETA: 32s
307904512/616562950 [=============&gt;................] - ETA: 32s
308494336/616562950 [==============&gt;...............] - ETA: 32s
309084160/616562950 [==============&gt;...............] - ETA: 32s
309673984/616562950 [==============&gt;...............] - ETA: 32s
310263808/616562950 [==============&gt;...............] - ETA: 32s
310853632/616562950 [==============&gt;...............] - ETA: 32s
311443456/616562950 [==============&gt;...............] - ETA: 32s
312033280/616562950 [==============&gt;...............] - ETA: 32s
312623104/616562950 [==============&gt;...............] - ETA: 32s
313229312/616562950 [==============&gt;...............] - ETA: 31s
313819136/616562950 [==============&gt;...............] - ETA: 31s
314408960/616562950 [==============&gt;...............] - ETA: 31s
314998784/616562950 [==============&gt;...............] - ETA: 31s
315588608/616562950 [==============&gt;...............] - ETA: 31s
316178432/616562950 [==============&gt;...............] - ETA: 31s
316768256/616562950 [==============&gt;...............] - ETA: 31s
317112320/616562950 [==============&gt;...............] - ETA: 31s
317718528/616562950 [==============&gt;...............] - ETA: 31s
318324736/616562950 [==============&gt;...............] - ETA: 31s
318701568/616562950 [==============&gt;...............] - ETA: 31s
319078400/616562950 [==============&gt;...............] - ETA: 31s
319455232/616562950 [==============&gt;...............] - ETA: 31s
319848448/616562950 [==============&gt;...............] - ETA: 31s
320225280/616562950 [==============&gt;...............] - ETA: 31s
320585728/616562950 [==============&gt;...............] - ETA: 31s
320995328/616562950 [==============&gt;...............] - ETA: 31s
321396736/616562950 [==============&gt;...............] - ETA: 31s
321765376/616562950 [==============&gt;...............] - ETA: 31s
322191360/616562950 [==============&gt;...............] - ETA: 31s
322600960/616562950 [==============&gt;...............] - ETA: 31s
323018752/616562950 [==============&gt;...............] - ETA: 30s
323436544/616562950 [==============&gt;...............] - ETA: 30s
323846144/616562950 [==============&gt;...............] - ETA: 30s
324288512/616562950 [==============&gt;...............] - ETA: 30s
324730880/616562950 [==============&gt;...............] - ETA: 30s
325156864/616562950 [==============&gt;...............] - ETA: 30s
325599232/616562950 [==============&gt;...............] - ETA: 30s
326041600/616562950 [==============&gt;...............] - ETA: 30s
326483968/616562950 [==============&gt;...............] - ETA: 30s
326942720/616562950 [==============&gt;...............] - ETA: 30s
327401472/616562950 [==============&gt;...............] - ETA: 30s
327843840/616562950 [==============&gt;...............] - ETA: 30s
328318976/616562950 [==============&gt;...............] - ETA: 30s
328761344/616562950 [==============&gt;...............] - ETA: 30s
329236480/616562950 [===============&gt;..............] - ETA: 30s
329711616/616562950 [===============&gt;..............] - ETA: 30s
330186752/616562950 [===============&gt;..............] - ETA: 30s
330678272/616562950 [===============&gt;..............] - ETA: 30s
331153408/616562950 [===============&gt;..............] - ETA: 30s
331628544/616562950 [===============&gt;..............] - ETA: 30s
332120064/616562950 [===============&gt;..............] - ETA: 30s
332627968/616562950 [===============&gt;..............] - ETA: 30s
333119488/616562950 [===============&gt;..............] - ETA: 29s
333611008/616562950 [===============&gt;..............] - ETA: 29s
334118912/616562950 [===============&gt;..............] - ETA: 29s
334626816/616562950 [===============&gt;..............] - ETA: 29s
335134720/616562950 [===============&gt;..............] - ETA: 29s
335659008/616562950 [===============&gt;..............] - ETA: 29s
336183296/616562950 [===============&gt;..............] - ETA: 29s
336723968/616562950 [===============&gt;..............] - ETA: 29s
337248256/616562950 [===============&gt;..............] - ETA: 29s
337788928/616562950 [===============&gt;..............] - ETA: 29s
338329600/616562950 [===============&gt;..............] - ETA: 29s
338870272/616562950 [===============&gt;..............] - ETA: 29s
339410944/616562950 [===============&gt;..............] - ETA: 29s
339951616/616562950 [===============&gt;..............] - ETA: 29s
340492288/616562950 [===============&gt;..............] - ETA: 29s
341049344/616562950 [===============&gt;..............] - ETA: 29s
341622784/616562950 [===============&gt;..............] - ETA: 28s
342196224/616562950 [===============&gt;..............] - ETA: 28s
342769664/616562950 [===============&gt;..............] - ETA: 28s
343326720/616562950 [===============&gt;..............] - ETA: 28s
343900160/616562950 [===============&gt;..............] - ETA: 28s
344473600/616562950 [===============&gt;..............] - ETA: 28s
345047040/616562950 [===============&gt;..............] - ETA: 28s
345636864/616562950 [===============&gt;..............] - ETA: 28s
346210304/616562950 [===============&gt;..............] - ETA: 28s
346800128/616562950 [===============&gt;..............] - ETA: 28s
347389952/616562950 [===============&gt;..............] - ETA: 28s
347979776/616562950 [===============&gt;..............] - ETA: 28s
348553216/616562950 [===============&gt;..............] - ETA: 28s
349126656/616562950 [===============&gt;..............] - ETA: 28s
349716480/616562950 [================&gt;.............] - ETA: 28s
350306304/616562950 [================&gt;.............] - ETA: 27s
350650368/616562950 [================&gt;.............] - ETA: 27s
351371264/616562950 [================&gt;.............] - ETA: 27s
351813632/616562950 [================&gt;.............] - ETA: 27s
352272384/616562950 [================&gt;.............] - ETA: 27s
352747520/616562950 [================&gt;.............] - ETA: 27s
353206272/616562950 [================&gt;.............] - ETA: 27s
353648640/616562950 [================&gt;.............] - ETA: 27s
354123776/616562950 [================&gt;.............] - ETA: 27s
354615296/616562950 [================&gt;.............] - ETA: 27s
355090432/616562950 [================&gt;.............] - ETA: 27s
355565568/616562950 [================&gt;.............] - ETA: 27s
356040704/616562950 [================&gt;.............] - ETA: 27s
356532224/616562950 [================&gt;.............] - ETA: 27s
357023744/616562950 [================&gt;.............] - ETA: 27s
357515264/616562950 [================&gt;.............] - ETA: 27s
358023168/616562950 [================&gt;.............] - ETA: 27s
358531072/616562950 [================&gt;.............] - ETA: 27s
359038976/616562950 [================&gt;.............] - ETA: 27s
359563264/616562950 [================&gt;.............] - ETA: 26s
360087552/616562950 [================&gt;.............] - ETA: 26s
360611840/616562950 [================&gt;.............] - ETA: 26s
361152512/616562950 [================&gt;.............] - ETA: 26s
361693184/616562950 [================&gt;.............] - ETA: 26s
362233856/616562950 [================&gt;.............] - ETA: 26s
362758144/616562950 [================&gt;.............] - ETA: 26s
363298816/616562950 [================&gt;.............] - ETA: 26s
363839488/616562950 [================&gt;.............] - ETA: 26s
364380160/616562950 [================&gt;.............] - ETA: 26s
364920832/616562950 [================&gt;.............] - ETA: 26s
365477888/616562950 [================&gt;.............] - ETA: 26s
366034944/616562950 [================&gt;.............] - ETA: 26s
366575616/616562950 [================&gt;.............] - ETA: 26s
367132672/616562950 [================&gt;.............] - ETA: 26s
367689728/616562950 [================&gt;.............] - ETA: 26s
368246784/616562950 [================&gt;.............] - ETA: 26s
368803840/616562950 [================&gt;.............] - ETA: 25s
369377280/616562950 [================&gt;.............] - ETA: 25s
369950720/616562950 [=================&gt;............] - ETA: 25s
370524160/616562950 [=================&gt;............] - ETA: 25s
371097600/616562950 [=================&gt;............] - ETA: 25s
371687424/616562950 [=================&gt;............] - ETA: 25s
372244480/616562950 [=================&gt;............] - ETA: 25s
372834304/616562950 [=================&gt;............] - ETA: 25s
373424128/616562950 [=================&gt;............] - ETA: 25s
374013952/616562950 [=================&gt;............] - ETA: 25s
374603776/616562950 [=================&gt;............] - ETA: 25s
375193600/616562950 [=================&gt;............] - ETA: 25s
375783424/616562950 [=================&gt;............] - ETA: 25s
376373248/616562950 [=================&gt;............] - ETA: 25s
376963072/616562950 [=================&gt;............] - ETA: 25s
377544704/616562950 [=================&gt;............] - ETA: 24s
378109952/616562950 [=================&gt;............] - ETA: 24s
378699776/616562950 [=================&gt;............] - ETA: 24s
379289600/616562950 [=================&gt;............] - ETA: 24s
379879424/616562950 [=================&gt;............] - ETA: 24s
380452864/616562950 [=================&gt;............] - ETA: 24s
381042688/616562950 [=================&gt;............] - ETA: 24s
381632512/616562950 [=================&gt;............] - ETA: 24s
382222336/616562950 [=================&gt;............] - ETA: 24s
382812160/616562950 [=================&gt;............] - ETA: 24s
383401984/616562950 [=================&gt;............] - ETA: 24s
383975424/616562950 [=================&gt;............] - ETA: 24s
384565248/616562950 [=================&gt;............] - ETA: 24s
385138688/616562950 [=================&gt;............] - ETA: 24s
385712128/616562950 [=================&gt;............] - ETA: 23s
386301952/616562950 [=================&gt;............] - ETA: 23s
386891776/616562950 [=================&gt;............] - ETA: 23s
387481600/616562950 [=================&gt;............] - ETA: 23s
388071424/616562950 [=================&gt;............] - ETA: 23s
388661248/616562950 [=================&gt;............] - ETA: 23s
389251072/616562950 [=================&gt;............] - ETA: 23s
389578752/616562950 [=================&gt;............] - ETA: 23s
390316032/616562950 [=================&gt;............] - ETA: 23s
390823936/616562950 [==================&gt;...........] - ETA: 23s
391315456/616562950 [==================&gt;...........] - ETA: 23s
391839744/616562950 [==================&gt;...........] - ETA: 23s
392364032/616562950 [==================&gt;...........] - ETA: 23s
392888320/616562950 [==================&gt;...........] - ETA: 23s
393412608/616562950 [==================&gt;...........] - ETA: 23s
393953280/616562950 [==================&gt;...........] - ETA: 23s
394493952/616562950 [==================&gt;...........] - ETA: 23s
395034624/616562950 [==================&gt;...........] - ETA: 22s
395575296/616562950 [==================&gt;...........] - ETA: 22s
396132352/616562950 [==================&gt;...........] - ETA: 22s
396673024/616562950 [==================&gt;...........] - ETA: 22s
397230080/616562950 [==================&gt;...........] - ETA: 22s
397787136/616562950 [==================&gt;...........] - ETA: 22s
398360576/616562950 [==================&gt;...........] - ETA: 22s
398934016/616562950 [==================&gt;...........] - ETA: 22s
399507456/616562950 [==================&gt;...........] - ETA: 22s
400097280/616562950 [==================&gt;...........] - ETA: 22s
400687104/616562950 [==================&gt;...........] - ETA: 22s
401276928/616562950 [==================&gt;...........] - ETA: 22s
401866752/616562950 [==================&gt;...........] - ETA: 22s
402456576/616562950 [==================&gt;...........] - ETA: 22s
403046400/616562950 [==================&gt;...........] - ETA: 22s
403636224/616562950 [==================&gt;...........] - ETA: 22s
404209664/616562950 [==================&gt;...........] - ETA: 21s
404799488/616562950 [==================&gt;...........] - ETA: 21s
405389312/616562950 [==================&gt;...........] - ETA: 21s
405979136/616562950 [==================&gt;...........] - ETA: 21s
406568960/616562950 [==================&gt;...........] - ETA: 21s
407158784/616562950 [==================&gt;...........] - ETA: 21s
407748608/616562950 [==================&gt;...........] - ETA: 21s
408338432/616562950 [==================&gt;...........] - ETA: 21s
408928256/616562950 [==================&gt;...........] - ETA: 21s
409518080/616562950 [==================&gt;...........] - ETA: 21s
410107904/616562950 [==================&gt;...........] - ETA: 21s
410697728/616562950 [==================&gt;...........] - ETA: 21s
411271168/616562950 [===================&gt;..........] - ETA: 21s
411860992/616562950 [===================&gt;..........] - ETA: 21s
412467200/616562950 [===================&gt;..........] - ETA: 21s
413057024/616562950 [===================&gt;..........] - ETA: 20s
413663232/616562950 [===================&gt;..........] - ETA: 20s
414253056/616562950 [===================&gt;..........] - ETA: 20s
414842880/616562950 [===================&gt;..........] - ETA: 20s
415416320/616562950 [===================&gt;..........] - ETA: 20s
416006144/616562950 [===================&gt;..........] - ETA: 20s
416595968/616562950 [===================&gt;..........] - ETA: 20s
417185792/616562950 [===================&gt;..........] - ETA: 20s
417775616/616562950 [===================&gt;..........] - ETA: 20s
418365440/616562950 [===================&gt;..........] - ETA: 20s
418955264/616562950 [===================&gt;..........] - ETA: 20s
419545088/616562950 [===================&gt;..........] - ETA: 20s
420134912/616562950 [===================&gt;..........] - ETA: 20s
420741120/616562950 [===================&gt;..........] - ETA: 20s
421330944/616562950 [===================&gt;..........] - ETA: 20s
421920768/616562950 [===================&gt;..........] - ETA: 19s
422510592/616562950 [===================&gt;..........] - ETA: 19s
423100416/616562950 [===================&gt;..........] - ETA: 19s
423690240/616562950 [===================&gt;..........] - ETA: 19s
424296448/616562950 [===================&gt;..........] - ETA: 19s
424591360/616562950 [===================&gt;..........] - ETA: 19s
425410560/616562950 [===================&gt;..........] - ETA: 19s
425902080/616562950 [===================&gt;..........] - ETA: 19s
426426368/616562950 [===================&gt;..........] - ETA: 19s
426688512/616562950 [===================&gt;..........] - ETA: 19s
427196416/616562950 [===================&gt;..........] - ETA: 19s
427573248/616562950 [===================&gt;..........] - ETA: 19s
427974656/616562950 [===================&gt;..........] - ETA: 19s
428310528/616562950 [===================&gt;..........] - ETA: 19s
428687360/616562950 [===================&gt;..........] - ETA: 19s
429096960/616562950 [===================&gt;..........] - ETA: 19s
429457408/616562950 [===================&gt;..........] - ETA: 19s
429850624/616562950 [===================&gt;..........] - ETA: 19s
430260224/616562950 [===================&gt;..........] - ETA: 19s
430637056/616562950 [===================&gt;..........] - ETA: 19s
431063040/616562950 [===================&gt;..........] - ETA: 19s
431472640/616562950 [===================&gt;..........] - ETA: 19s
431882240/616562950 [====================&gt;.........] - ETA: 18s
432324608/616562950 [====================&gt;.........] - ETA: 18s
432734208/616562950 [====================&gt;.........] - ETA: 18s
433160192/616562950 [====================&gt;.........] - ETA: 18s
433569792/616562950 [====================&gt;.........] - ETA: 18s
434012160/616562950 [====================&gt;.........] - ETA: 18s
434454528/616562950 [====================&gt;.........] - ETA: 18s
434896896/616562950 [====================&gt;.........] - ETA: 18s
435355648/616562950 [====================&gt;.........] - ETA: 18s
435814400/616562950 [====================&gt;.........] - ETA: 18s
436256768/616562950 [====================&gt;.........] - ETA: 18s
436731904/616562950 [====================&gt;.........] - ETA: 18s
437207040/616562950 [====================&gt;.........] - ETA: 18s
437673984/616562950 [====================&gt;.........] - ETA: 18s
438124544/616562950 [====================&gt;.........] - ETA: 18s
438616064/616562950 [====================&gt;.........] - ETA: 18s
439091200/616562950 [====================&gt;.........] - ETA: 18s
439566336/616562950 [====================&gt;.........] - ETA: 18s
440057856/616562950 [====================&gt;.........] - ETA: 18s
440549376/616562950 [====================&gt;.........] - ETA: 18s
441040896/616562950 [====================&gt;.........] - ETA: 18s
441548800/616562950 [====================&gt;.........] - ETA: 18s
442040320/616562950 [====================&gt;.........] - ETA: 17s
442548224/616562950 [====================&gt;.........] - ETA: 17s
443072512/616562950 [====================&gt;.........] - ETA: 17s
443580416/616562950 [====================&gt;.........] - ETA: 17s
444104704/616562950 [====================&gt;.........] - ETA: 17s
444628992/616562950 [====================&gt;.........] - ETA: 17s
445153280/616562950 [====================&gt;.........] - ETA: 17s
445677568/616562950 [====================&gt;.........] - ETA: 17s
446218240/616562950 [====================&gt;.........] - ETA: 17s
446758912/616562950 [====================&gt;.........] - ETA: 17s
447315968/616562950 [====================&gt;.........] - ETA: 17s
447856640/616562950 [====================&gt;.........] - ETA: 17s
448397312/616562950 [====================&gt;.........] - ETA: 17s
448954368/616562950 [====================&gt;.........] - ETA: 17s
449511424/616562950 [====================&gt;.........] - ETA: 17s
450068480/616562950 [====================&gt;.........] - ETA: 17s
450625536/616562950 [====================&gt;.........] - ETA: 17s
451198976/616562950 [====================&gt;.........] - ETA: 17s
451772416/616562950 [====================&gt;.........] - ETA: 16s
452345856/616562950 [=====================&gt;........] - ETA: 16s
452919296/616562950 [=====================&gt;........] - ETA: 16s
453492736/616562950 [=====================&gt;........] - ETA: 16s
453754880/616562950 [=====================&gt;........] - ETA: 16s
454410240/616562950 [=====================&gt;........] - ETA: 16s
454852608/616562950 [=====================&gt;........] - ETA: 16s
455262208/616562950 [=====================&gt;........] - ETA: 16s
455688192/616562950 [=====================&gt;........] - ETA: 16s
456146944/616562950 [=====================&gt;........] - ETA: 16s
456572928/616562950 [=====================&gt;........] - ETA: 16s
457031680/616562950 [=====================&gt;........] - ETA: 16s
457490432/616562950 [=====================&gt;........] - ETA: 16s
457949184/616562950 [=====================&gt;........] - ETA: 16s
458424320/616562950 [=====================&gt;........] - ETA: 16s
458883072/616562950 [=====================&gt;........] - ETA: 16s
459358208/616562950 [=====================&gt;........] - ETA: 16s
459833344/616562950 [=====================&gt;........] - ETA: 16s
460308480/616562950 [=====================&gt;........] - ETA: 16s
460791808/616562950 [=====================&gt;........] - ETA: 16s
461275136/616562950 [=====================&gt;........] - ETA: 15s
461750272/616562950 [=====================&gt;........] - ETA: 15s
462258176/616562950 [=====================&gt;........] - ETA: 15s
462766080/616562950 [=====================&gt;........] - ETA: 15s
463241216/616562950 [=====================&gt;........] - ETA: 15s
463732736/616562950 [=====================&gt;........] - ETA: 15s
464240640/616562950 [=====================&gt;........] - ETA: 15s
464732160/616562950 [=====================&gt;........] - ETA: 15s
465240064/616562950 [=====================&gt;........] - ETA: 15s
465747968/616562950 [=====================&gt;........] - ETA: 15s
466272256/616562950 [=====================&gt;........] - ETA: 15s
466796544/616562950 [=====================&gt;........] - ETA: 15s
467304448/616562950 [=====================&gt;........] - ETA: 15s
467828736/616562950 [=====================&gt;........] - ETA: 15s
468353024/616562950 [=====================&gt;........] - ETA: 15s
468877312/616562950 [=====================&gt;........] - ETA: 15s
469417984/616562950 [=====================&gt;........] - ETA: 15s
469942272/616562950 [=====================&gt;........] - ETA: 15s
470482944/616562950 [=====================&gt;........] - ETA: 15s
471023616/616562950 [=====================&gt;........] - ETA: 14s
471580672/616562950 [=====================&gt;........] - ETA: 14s
472137728/616562950 [=====================&gt;........] - ETA: 14s
472694784/616562950 [=====================&gt;........] - ETA: 14s
473251840/616562950 [======================&gt;.......] - ETA: 14s
473825280/616562950 [======================&gt;.......] - ETA: 14s
474365952/616562950 [======================&gt;.......] - ETA: 14s
474923008/616562950 [======================&gt;.......] - ETA: 14s
475496448/616562950 [======================&gt;.......] - ETA: 14s
476053504/616562950 [======================&gt;.......] - ETA: 14s
476610560/616562950 [======================&gt;.......] - ETA: 14s
477167616/616562950 [======================&gt;.......] - ETA: 14s
477708288/616562950 [======================&gt;.......] - ETA: 14s
478298112/616562950 [======================&gt;.......] - ETA: 14s
478887936/616562950 [======================&gt;.......] - ETA: 14s
479494144/616562950 [======================&gt;.......] - ETA: 14s
480083968/616562950 [======================&gt;.......] - ETA: 14s
480673792/616562950 [======================&gt;.......] - ETA: 13s
481263616/616562950 [======================&gt;.......] - ETA: 13s
481837056/616562950 [======================&gt;.......] - ETA: 13s
482410496/616562950 [======================&gt;.......] - ETA: 13s
482967552/616562950 [======================&gt;.......] - ETA: 13s
483557376/616562950 [======================&gt;.......] - ETA: 13s
484114432/616562950 [======================&gt;.......] - ETA: 13s
484655104/616562950 [======================&gt;.......] - ETA: 13s
485212160/616562950 [======================&gt;.......] - ETA: 13s
485785600/616562950 [======================&gt;.......] - ETA: 13s
486342656/616562950 [======================&gt;.......] - ETA: 13s
486899712/616562950 [======================&gt;.......] - ETA: 13s
487456768/616562950 [======================&gt;.......] - ETA: 13s
488046592/616562950 [======================&gt;.......] - ETA: 13s
488636416/616562950 [======================&gt;.......] - ETA: 13s
489226240/616562950 [======================&gt;.......] - ETA: 13s
489816064/616562950 [======================&gt;.......] - ETA: 12s
490078208/616562950 [======================&gt;.......] - ETA: 12s
490635264/616562950 [======================&gt;.......] - ETA: 12s
491241472/616562950 [======================&gt;.......] - ETA: 12s
491585536/616562950 [======================&gt;.......] - ETA: 12s
491945984/616562950 [======================&gt;.......] - ETA: 12s
492290048/616562950 [======================&gt;.......] - ETA: 12s
492617728/616562950 [======================&gt;.......] - ETA: 12s
492994560/616562950 [======================&gt;.......] - ETA: 12s
493371392/616562950 [=======================&gt;......] - ETA: 12s
493731840/616562950 [=======================&gt;......] - ETA: 12s
494108672/616562950 [=======================&gt;......] - ETA: 12s
494469120/616562950 [=======================&gt;......] - ETA: 12s
494837760/616562950 [=======================&gt;......] - ETA: 12s
495222784/616562950 [=======================&gt;......] - ETA: 12s
495599616/616562950 [=======================&gt;......] - ETA: 12s
495992832/616562950 [=======================&gt;......] - ETA: 12s
496386048/616562950 [=======================&gt;......] - ETA: 12s
496779264/616562950 [=======================&gt;......] - ETA: 12s
497172480/616562950 [=======================&gt;......] - ETA: 12s
497598464/616562950 [=======================&gt;......] - ETA: 12s
497991680/616562950 [=======================&gt;......] - ETA: 12s
498417664/616562950 [=======================&gt;......] - ETA: 12s
498843648/616562950 [=======================&gt;......] - ETA: 12s
499269632/616562950 [=======================&gt;......] - ETA: 12s
499695616/616562950 [=======================&gt;......] - ETA: 12s
500121600/616562950 [=======================&gt;......] - ETA: 11s
500547584/616562950 [=======================&gt;......] - ETA: 11s
500973568/616562950 [=======================&gt;......] - ETA: 11s
501415936/616562950 [=======================&gt;......] - ETA: 11s
501858304/616562950 [=======================&gt;......] - ETA: 11s
502284288/616562950 [=======================&gt;......] - ETA: 11s
502726656/616562950 [=======================&gt;......] - ETA: 11s
503185408/616562950 [=======================&gt;......] - ETA: 11s
503660544/616562950 [=======================&gt;......] - ETA: 11s
504119296/616562950 [=======================&gt;......] - ETA: 11s
504594432/616562950 [=======================&gt;......] - ETA: 11s
505069568/616562950 [=======================&gt;......] - ETA: 11s
505561088/616562950 [=======================&gt;......] - ETA: 11s
506036224/616562950 [=======================&gt;......] - ETA: 11s
506527744/616562950 [=======================&gt;......] - ETA: 11s
507019264/616562950 [=======================&gt;......] - ETA: 11s
507527168/616562950 [=======================&gt;......] - ETA: 11s
508018688/616562950 [=======================&gt;......] - ETA: 11s
508526592/616562950 [=======================&gt;......] - ETA: 11s
509034496/616562950 [=======================&gt;......] - ETA: 11s
509542400/616562950 [=======================&gt;......] - ETA: 11s
510050304/616562950 [=======================&gt;......] - ETA: 10s
510558208/616562950 [=======================&gt;......] - ETA: 10s
511082496/616562950 [=======================&gt;......] - ETA: 10s
511606784/616562950 [=======================&gt;......] - ETA: 10s
512131072/616562950 [=======================&gt;......] - ETA: 10s
512671744/616562950 [=======================&gt;......] - ETA: 10s
513212416/616562950 [=======================&gt;......] - ETA: 10s
513753088/616562950 [=======================&gt;......] - ETA: 10s
514277376/616562950 [========================&gt;.....] - ETA: 10s
514834432/616562950 [========================&gt;.....] - ETA: 10s
515391488/616562950 [========================&gt;.....] - ETA: 10s
515948544/616562950 [========================&gt;.....] - ETA: 10s
516505600/616562950 [========================&gt;.....] - ETA: 10s
517079040/616562950 [========================&gt;.....] - ETA: 10s
517636096/616562950 [========================&gt;.....] - ETA: 10s
518209536/616562950 [========================&gt;.....] - ETA: 10s
518782976/616562950 [========================&gt;.....] - ETA: 10s
519356416/616562950 [========================&gt;.....] - ETA: 9s 
519946240/616562950 [========================&gt;.....] - ETA: 9s
520519680/616562950 [========================&gt;.....] - ETA: 9s
521109504/616562950 [========================&gt;.....] - ETA: 9s
521699328/616562950 [========================&gt;.....] - ETA: 9s
522289152/616562950 [========================&gt;.....] - ETA: 9s
522878976/616562950 [========================&gt;.....] - ETA: 9s
523468800/616562950 [========================&gt;.....] - ETA: 9s
524058624/616562950 [========================&gt;.....] - ETA: 9s
524648448/616562950 [========================&gt;.....] - ETA: 9s
525238272/616562950 [========================&gt;.....] - ETA: 9s
525828096/616562950 [========================&gt;.....] - ETA: 9s
526417920/616562950 [========================&gt;.....] - ETA: 9s
527007744/616562950 [========================&gt;.....] - ETA: 9s
527597568/616562950 [========================&gt;.....] - ETA: 9s
528203776/616562950 [========================&gt;.....] - ETA: 9s
528793600/616562950 [========================&gt;.....] - ETA: 8s
529334272/616562950 [========================&gt;.....] - ETA: 8s
529825792/616562950 [========================&gt;.....] - ETA: 8s
530366464/616562950 [========================&gt;.....] - ETA: 8s
530702336/616562950 [========================&gt;.....] - ETA: 8s
531038208/616562950 [========================&gt;.....] - ETA: 8s
531365888/616562950 [========================&gt;.....] - ETA: 8s
531726336/616562950 [========================&gt;.....] - ETA: 8s
532054016/616562950 [========================&gt;.....] - ETA: 8s
532398080/616562950 [========================&gt;.....] - ETA: 8s
532774912/616562950 [========================&gt;.....] - ETA: 8s
533118976/616562950 [========================&gt;.....] - ETA: 8s
533495808/616562950 [========================&gt;.....] - ETA: 8s
533848064/616562950 [========================&gt;.....] - ETA: 8s
534200320/616562950 [========================&gt;.....] - ETA: 8s
534560768/616562950 [=========================&gt;....] - ETA: 8s
534953984/616562950 [=========================&gt;....] - ETA: 8s
535298048/616562950 [=========================&gt;....] - ETA: 8s
535691264/616562950 [=========================&gt;....] - ETA: 8s
536100864/616562950 [=========================&gt;....] - ETA: 8s
536461312/616562950 [=========================&gt;....] - ETA: 8s
536870912/616562950 [=========================&gt;....] - ETA: 8s
537247744/616562950 [=========================&gt;....] - ETA: 8s
537657344/616562950 [=========================&gt;....] - ETA: 8s
538075136/616562950 [=========================&gt;....] - ETA: 8s
538468352/616562950 [=========================&gt;....] - ETA: 8s
538886144/616562950 [=========================&gt;....] - ETA: 8s
539312128/616562950 [=========================&gt;....] - ETA: 7s
539738112/616562950 [=========================&gt;....] - ETA: 7s
540147712/616562950 [=========================&gt;....] - ETA: 7s
540606464/616562950 [=========================&gt;....] - ETA: 7s
541048832/616562950 [=========================&gt;....] - ETA: 7s
541483008/616562950 [=========================&gt;....] - ETA: 7s
541917184/616562950 [=========================&gt;....] - ETA: 7s
542375936/616562950 [=========================&gt;....] - ETA: 7s
542818304/616562950 [=========================&gt;....] - ETA: 7s
543293440/616562950 [=========================&gt;....] - ETA: 7s
543768576/616562950 [=========================&gt;....] - ETA: 7s
544194560/616562950 [=========================&gt;....] - ETA: 7s
544669696/616562950 [=========================&gt;....] - ETA: 7s
545144832/616562950 [=========================&gt;....] - ETA: 7s
545619968/616562950 [=========================&gt;....] - ETA: 7s
546095104/616562950 [=========================&gt;....] - ETA: 7s
546603008/616562950 [=========================&gt;....] - ETA: 7s
547094528/616562950 [=========================&gt;....] - ETA: 7s
547586048/616562950 [=========================&gt;....] - ETA: 7s
548077568/616562950 [=========================&gt;....] - ETA: 7s
548569088/616562950 [=========================&gt;....] - ETA: 7s
549076992/616562950 [=========================&gt;....] - ETA: 6s
549584896/616562950 [=========================&gt;....] - ETA: 6s
550092800/616562950 [=========================&gt;....] - ETA: 6s
550617088/616562950 [=========================&gt;....] - ETA: 6s
551141376/616562950 [=========================&gt;....] - ETA: 6s
551649280/616562950 [=========================&gt;....] - ETA: 6s
552173568/616562950 [=========================&gt;....] - ETA: 6s
552697856/616562950 [=========================&gt;....] - ETA: 6s
553238528/616562950 [=========================&gt;....] - ETA: 6s
553779200/616562950 [=========================&gt;....] - ETA: 6s
554303488/616562950 [=========================&gt;....] - ETA: 6s
554844160/616562950 [=========================&gt;....] - ETA: 6s
555401216/616562950 [==========================&gt;...] - ETA: 6s
555941888/616562950 [==========================&gt;...] - ETA: 6s
556498944/616562950 [==========================&gt;...] - ETA: 6s
557056000/616562950 [==========================&gt;...] - ETA: 6s
557629440/616562950 [==========================&gt;...] - ETA: 6s
558186496/616562950 [==========================&gt;...] - ETA: 6s
558759936/616562950 [==========================&gt;...] - ETA: 5s
559333376/616562950 [==========================&gt;...] - ETA: 5s
559906816/616562950 [==========================&gt;...] - ETA: 5s
560480256/616562950 [==========================&gt;...] - ETA: 5s
561070080/616562950 [==========================&gt;...] - ETA: 5s
561659904/616562950 [==========================&gt;...] - ETA: 5s
562249728/616562950 [==========================&gt;...] - ETA: 5s
562839552/616562950 [==========================&gt;...] - ETA: 5s
563429376/616562950 [==========================&gt;...] - ETA: 5s
564019200/616562950 [==========================&gt;...] - ETA: 5s
564592640/616562950 [==========================&gt;...] - ETA: 5s
565149696/616562950 [==========================&gt;...] - ETA: 5s
565706752/616562950 [==========================&gt;...] - ETA: 5s
566263808/616562950 [==========================&gt;...] - ETA: 5s
566820864/616562950 [==========================&gt;...] - ETA: 5s
567361536/616562950 [==========================&gt;...] - ETA: 5s
567902208/616562950 [==========================&gt;...] - ETA: 5s
568459264/616562950 [==========================&gt;...] - ETA: 4s
569016320/616562950 [==========================&gt;...] - ETA: 4s
569573376/616562950 [==========================&gt;...] - ETA: 4s
570146816/616562950 [==========================&gt;...] - ETA: 4s
570703872/616562950 [==========================&gt;...] - ETA: 4s
571293696/616562950 [==========================&gt;...] - ETA: 4s
571883520/616562950 [==========================&gt;...] - ETA: 4s
572473344/616562950 [==========================&gt;...] - ETA: 4s
573063168/616562950 [==========================&gt;...] - ETA: 4s
573652992/616562950 [==========================&gt;...] - ETA: 4s
573964288/616562950 [==========================&gt;...] - ETA: 4s
574488576/616562950 [==========================&gt;...] - ETA: 4s
575070208/616562950 [==========================&gt;...] - ETA: 4s
575422464/616562950 [==========================&gt;...] - ETA: 4s
575758336/616562950 [===========================&gt;..] - ETA: 4s
576110592/616562950 [===========================&gt;..] - ETA: 4s
576471040/616562950 [===========================&gt;..] - ETA: 4s
576831488/616562950 [===========================&gt;..] - ETA: 4s
577191936/616562950 [===========================&gt;..] - ETA: 4s
577552384/616562950 [===========================&gt;..] - ETA: 4s
577929216/616562950 [===========================&gt;..] - ETA: 3s
578306048/616562950 [===========================&gt;..] - ETA: 3s
578682880/616562950 [===========================&gt;..] - ETA: 3s
579076096/616562950 [===========================&gt;..] - ETA: 3s
579469312/616562950 [===========================&gt;..] - ETA: 3s
579862528/616562950 [===========================&gt;..] - ETA: 3s
580272128/616562950 [===========================&gt;..] - ETA: 3s
580648960/616562950 [===========================&gt;..] - ETA: 3s
581058560/616562950 [===========================&gt;..] - ETA: 3s
581468160/616562950 [===========================&gt;..] - ETA: 3s
581861376/616562950 [===========================&gt;..] - ETA: 3s
582287360/616562950 [===========================&gt;..] - ETA: 3s
582713344/616562950 [===========================&gt;..] - ETA: 3s
583122944/616562950 [===========================&gt;..] - ETA: 3s
583565312/616562950 [===========================&gt;..] - ETA: 3s
583991296/616562950 [===========================&gt;..] - ETA: 3s
584417280/616562950 [===========================&gt;..] - ETA: 3s
584859648/616562950 [===========================&gt;..] - ETA: 3s
585285632/616562950 [===========================&gt;..] - ETA: 3s
585728000/616562950 [===========================&gt;..] - ETA: 3s
586186752/616562950 [===========================&gt;..] - ETA: 3s
586645504/616562950 [===========================&gt;..] - ETA: 3s
587104256/616562950 [===========================&gt;..] - ETA: 3s
587563008/616562950 [===========================&gt;..] - ETA: 2s
588038144/616562950 [===========================&gt;..] - ETA: 2s
588513280/616562950 [===========================&gt;..] - ETA: 2s
588972032/616562950 [===========================&gt;..] - ETA: 2s
589447168/616562950 [===========================&gt;..] - ETA: 2s
589938688/616562950 [===========================&gt;..] - ETA: 2s
590430208/616562950 [===========================&gt;..] - ETA: 2s
590921728/616562950 [===========================&gt;..] - ETA: 2s
591429632/616562950 [===========================&gt;..] - ETA: 2s
591921152/616562950 [===========================&gt;..] - ETA: 2s
592412672/616562950 [===========================&gt;..] - ETA: 2s
592920576/616562950 [===========================&gt;..] - ETA: 2s
593428480/616562950 [===========================&gt;..] - ETA: 2s
593920000/616562950 [===========================&gt;..] - ETA: 2s
594444288/616562950 [===========================&gt;..] - ETA: 2s
594935808/616562950 [===========================&gt;..] - ETA: 2s
595460096/616562950 [===========================&gt;..] - ETA: 2s
595984384/616562950 [===========================&gt;..] - ETA: 2s
596525056/616562950 [============================&gt;.] - ETA: 2s
597065728/616562950 [============================&gt;.] - ETA: 2s
597590016/616562950 [============================&gt;.] - ETA: 1s
598130688/616562950 [============================&gt;.] - ETA: 1s
598671360/616562950 [============================&gt;.] - ETA: 1s
599228416/616562950 [============================&gt;.] - ETA: 1s
599769088/616562950 [============================&gt;.] - ETA: 1s
600326144/616562950 [============================&gt;.] - ETA: 1s
600883200/616562950 [============================&gt;.] - ETA: 1s
601440256/616562950 [============================&gt;.] - ETA: 1s
601997312/616562950 [============================&gt;.] - ETA: 1s
602554368/616562950 [============================&gt;.] - ETA: 1s
603111424/616562950 [============================&gt;.] - ETA: 1s
603684864/616562950 [============================&gt;.] - ETA: 1s
604258304/616562950 [============================&gt;.] - ETA: 1s
604831744/616562950 [============================&gt;.] - ETA: 1s
605405184/616562950 [============================&gt;.] - ETA: 1s
605962240/616562950 [============================&gt;.] - ETA: 1s
606535680/616562950 [============================&gt;.] - ETA: 1s
607125504/616562950 [============================&gt;.] - ETA: 0s
607698944/616562950 [============================&gt;.] - ETA: 0s
608305152/616562950 [============================&gt;.] - ETA: 0s
608894976/616562950 [============================&gt;.] - ETA: 0s
609501184/616562950 [============================&gt;.] - ETA: 0s
610091008/616562950 [============================&gt;.] - ETA: 0s
610697216/616562950 [============================&gt;.] - ETA: 0s
611287040/616562950 [============================&gt;.] - ETA: 0s
611876864/616562950 [============================&gt;.] - ETA: 0s
612483072/616562950 [============================&gt;.] - ETA: 0s
613089280/616562950 [============================&gt;.] - ETA: 0s
613679104/616562950 [============================&gt;.] - ETA: 0s
614268928/616562950 [============================&gt;.] - ETA: 0s
614858752/616562950 [============================&gt;.] - ETA: 0s
615448576/616562950 [============================&gt;.] - ETA: 0s
616038400/616562950 [============================&gt;.] - ETA: 0s
616562950/616562950 [==============================] - 63s 0us/step

 1/11 [=&gt;............................] - ETA: 48s
 2/11 [====&gt;.........................] - ETA: 1s 
 3/11 [=======&gt;......................] - ETA: 1s
 4/11 [=========&gt;....................] - ETA: 1s
 5/11 [============&gt;.................] - ETA: 1s
 6/11 [===============&gt;..............] - ETA: 0s
 7/11 [==================&gt;...........] - ETA: 0s
 8/11 [====================&gt;.........] - ETA: 0s
 9/11 [=======================&gt;......] - ETA: 0s
10/11 [==========================&gt;...] - ETA: 0s
11/11 [==============================] - ETA: 0s
11/11 [==============================] - 7s 170ms/step

 1/11 [=&gt;............................] - ETA: 2s
 2/11 [====&gt;.........................] - ETA: 1s
 3/11 [=======&gt;......................] - ETA: 1s
 4/11 [=========&gt;....................] - ETA: 1s
 5/11 [============&gt;.................] - ETA: 1s
 6/11 [===============&gt;..............] - ETA: 0s
 7/11 [==================&gt;...........] - ETA: 0s
 8/11 [====================&gt;.........] - ETA: 0s
 9/11 [=======================&gt;......] - ETA: 0s
10/11 [==========================&gt;...] - ETA: 0s
11/11 [==============================] - ETA: 0s
11/11 [==============================] - 2s 174ms/step
Model: &quot;vit-tiny&quot;
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to
==================================================================================================
 input (InputLayer)             [(None, 224, 224, 3  0           []
                                )]

 Rescale (QuantizedRescaling)   (None, 224, 224, 3)  0           [&#39;input[0][0]&#39;]

 Embedding (QuantizedConv2D)    (None, 14, 14, 192)  147648      [&#39;Rescale[0][0]&#39;]

 reshape (QuantizedReshape)     (None, 196, 192)     0           [&#39;Embedding[0][0]&#39;]

 ClassToken (QuantizedClassToke  (None, 197, 192)    192         [&#39;reshape[0][0]&#39;]
 n)

 Transformer/PosEmbed (Quantize  (None, 197, 192)    38208       [&#39;ClassToken[0][0]&#39;]
 dAddPositionEmbs)

 Transformer/EncoderBlock_0/Lay  (None, 197, 192)    768         [&#39;Transformer/PosEmbed[0][0]&#39;]
 erNorm_0 (QuantizedLayerNormal
 ization)

 Transformer/EncoderBlock_0/Mul  (None, 197, 192)    37058       [&#39;Transformer/EncoderBlock_0/Laye
 tiHeadDotProductAttention_1/qu                                  rNorm_0[0][0]&#39;]
 ery (QuantizedDense)

 Transformer/EncoderBlock_0/Mul  (None, 197, 192)    37058       [&#39;Transformer/EncoderBlock_0/Laye
 tiHeadDotProductAttention_1/ke                                  rNorm_0[0][0]&#39;]
 y (QuantizedDense)

 Transformer/EncoderBlock_0/Mul  (None, 197, 192)    37440       [&#39;Transformer/EncoderBlock_0/Laye
 tiHeadDotProductAttention_1/va                                  rNorm_0[0][0]&#39;]
 lue (QuantizedDense)

 Transformer/EncoderBlock_0/Mul  ((None, 197, 192),  384         [&#39;Transformer/EncoderBlock_0/Mult
 tiHeadDotProductAttention_1/at   (None, 3, 197, 197             iHeadDotProductAttention_1/query[
 tention (QuantizedAttention)   ))                               0][0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_0/Mult
                                                                 iHeadDotProductAttention_1/key[0]
                                                                 [0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_0/Mult
                                                                 iHeadDotProductAttention_1/value[
                                                                 0][0]&#39;]

 Transformer/EncoderBlock_0/Mul  (None, 197, 192)    37440       [&#39;Transformer/EncoderBlock_0/Mult
 tiHeadDotProductAttention_1/ou                                  iHeadDotProductAttention_1/attent
 t (QuantizedDense)                                              ion[0][0]&#39;]

 dropout (QuantizedDropout)     (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_0/Mult
                                                                 iHeadDotProductAttention_1/out[0]
                                                                 [0]&#39;]

 Transformer/EncoderBlock_0/add  (None, 197, 192)    384         [&#39;dropout[0][0]&#39;,
 _1 (QuantizedAdd)                                                &#39;Transformer/PosEmbed[0][0]&#39;]

 Transformer/EncoderBlock_0/Lay  (None, 197, 192)    768         [&#39;Transformer/EncoderBlock_0/add_
 erNorm_2 (QuantizedLayerNormal                                  1[0][0]&#39;]
 ization)

 Transformer/EncoderBlock_0/Mlp  (None, 197, 768)    148224      [&#39;Transformer/EncoderBlock_0/Laye
 Block/Dense_0 (QuantizedDense)                                  rNorm_2[0][0]&#39;]

 Transformer/EncoderBlock_0/Mlp  (None, 197, 768)    1536        [&#39;Transformer/EncoderBlock_0/MlpB
 Block/activation (QuantizedReL                                  lock/Dense_0[0][0]&#39;]
 U)

 dropout_1 (QuantizedDropout)   (None, 197, 768)     0           [&#39;Transformer/EncoderBlock_0/MlpB
                                                                 lock/activation[0][0]&#39;]

 Transformer/EncoderBlock_0/Mlp  (None, 197, 192)    148032      [&#39;dropout_1[0][0]&#39;]
 Block/Dense_1 (QuantizedDense)

 dropout_2 (QuantizedDropout)   (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_0/MlpB
                                                                 lock/Dense_1[0][0]&#39;]

 Transformer/EncoderBlock_0/add  (None, 197, 192)    384         [&#39;Transformer/EncoderBlock_0/add_
 _2 (QuantizedAdd)                                               1[0][0]&#39;,
                                                                  &#39;dropout_2[0][0]&#39;]

 Transformer/EncoderBlock_1/Lay  (None, 197, 192)    768         [&#39;Transformer/EncoderBlock_0/add_
 erNorm_0 (QuantizedLayerNormal                                  2[0][0]&#39;]
 ization)

 Transformer/EncoderBlock_1/Mul  (None, 197, 192)    37058       [&#39;Transformer/EncoderBlock_1/Laye
 tiHeadDotProductAttention_1/qu                                  rNorm_0[0][0]&#39;]
 ery (QuantizedDense)

 Transformer/EncoderBlock_1/Mul  (None, 197, 192)    37058       [&#39;Transformer/EncoderBlock_1/Laye
 tiHeadDotProductAttention_1/ke                                  rNorm_0[0][0]&#39;]
 y (QuantizedDense)

 Transformer/EncoderBlock_1/Mul  (None, 197, 192)    37440       [&#39;Transformer/EncoderBlock_1/Laye
 tiHeadDotProductAttention_1/va                                  rNorm_0[0][0]&#39;]
 lue (QuantizedDense)

 Transformer/EncoderBlock_1/Mul  ((None, 197, 192),  384         [&#39;Transformer/EncoderBlock_1/Mult
 tiHeadDotProductAttention_1/at   (None, 3, 197, 197             iHeadDotProductAttention_1/query[
 tention (QuantizedAttention)   ))                               0][0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_1/Mult
                                                                 iHeadDotProductAttention_1/key[0]
                                                                 [0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_1/Mult
                                                                 iHeadDotProductAttention_1/value[
                                                                 0][0]&#39;]

 Transformer/EncoderBlock_1/Mul  (None, 197, 192)    37440       [&#39;Transformer/EncoderBlock_1/Mult
 tiHeadDotProductAttention_1/ou                                  iHeadDotProductAttention_1/attent
 t (QuantizedDense)                                              ion[0][0]&#39;]

 dropout_3 (QuantizedDropout)   (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_1/Mult
                                                                 iHeadDotProductAttention_1/out[0]
                                                                 [0]&#39;]

 Transformer/EncoderBlock_1/add  (None, 197, 192)    384         [&#39;dropout_3[0][0]&#39;,
 _1 (QuantizedAdd)                                                &#39;Transformer/EncoderBlock_0/add_
                                                                 2[0][0]&#39;]

 Transformer/EncoderBlock_1/Lay  (None, 197, 192)    768         [&#39;Transformer/EncoderBlock_1/add_
 erNorm_2 (QuantizedLayerNormal                                  1[0][0]&#39;]
 ization)

 Transformer/EncoderBlock_1/Mlp  (None, 197, 768)    148224      [&#39;Transformer/EncoderBlock_1/Laye
 Block/Dense_0 (QuantizedDense)                                  rNorm_2[0][0]&#39;]

 Transformer/EncoderBlock_1/Mlp  (None, 197, 768)    1536        [&#39;Transformer/EncoderBlock_1/MlpB
 Block/activation (QuantizedReL                                  lock/Dense_0[0][0]&#39;]
 U)

 dropout_4 (QuantizedDropout)   (None, 197, 768)     0           [&#39;Transformer/EncoderBlock_1/MlpB
                                                                 lock/activation[0][0]&#39;]

 Transformer/EncoderBlock_1/Mlp  (None, 197, 192)    148032      [&#39;dropout_4[0][0]&#39;]
 Block/Dense_1 (QuantizedDense)

 dropout_5 (QuantizedDropout)   (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_1/MlpB
                                                                 lock/Dense_1[0][0]&#39;]

 Transformer/EncoderBlock_1/add  (None, 197, 192)    384         [&#39;Transformer/EncoderBlock_1/add_
 _2 (QuantizedAdd)                                               1[0][0]&#39;,
                                                                  &#39;dropout_5[0][0]&#39;]

 Transformer/EncoderBlock_2/Lay  (None, 197, 192)    768         [&#39;Transformer/EncoderBlock_1/add_
 erNorm_0 (QuantizedLayerNormal                                  2[0][0]&#39;]
 ization)

 Transformer/EncoderBlock_2/Mul  (None, 197, 192)    37058       [&#39;Transformer/EncoderBlock_2/Laye
 tiHeadDotProductAttention_1/qu                                  rNorm_0[0][0]&#39;]
 ery (QuantizedDense)

 Transformer/EncoderBlock_2/Mul  (None, 197, 192)    37058       [&#39;Transformer/EncoderBlock_2/Laye
 tiHeadDotProductAttention_1/ke                                  rNorm_0[0][0]&#39;]
 y (QuantizedDense)

 Transformer/EncoderBlock_2/Mul  (None, 197, 192)    37440       [&#39;Transformer/EncoderBlock_2/Laye
 tiHeadDotProductAttention_1/va                                  rNorm_0[0][0]&#39;]
 lue (QuantizedDense)

 Transformer/EncoderBlock_2/Mul  ((None, 197, 192),  384         [&#39;Transformer/EncoderBlock_2/Mult
 tiHeadDotProductAttention_1/at   (None, 3, 197, 197             iHeadDotProductAttention_1/query[
 tention (QuantizedAttention)   ))                               0][0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_2/Mult
                                                                 iHeadDotProductAttention_1/key[0]
                                                                 [0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_2/Mult
                                                                 iHeadDotProductAttention_1/value[
                                                                 0][0]&#39;]

 Transformer/EncoderBlock_2/Mul  (None, 197, 192)    37440       [&#39;Transformer/EncoderBlock_2/Mult
 tiHeadDotProductAttention_1/ou                                  iHeadDotProductAttention_1/attent
 t (QuantizedDense)                                              ion[0][0]&#39;]

 dropout_6 (QuantizedDropout)   (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_2/Mult
                                                                 iHeadDotProductAttention_1/out[0]
                                                                 [0]&#39;]

 Transformer/EncoderBlock_2/add  (None, 197, 192)    384         [&#39;dropout_6[0][0]&#39;,
 _1 (QuantizedAdd)                                                &#39;Transformer/EncoderBlock_1/add_
                                                                 2[0][0]&#39;]

 Transformer/EncoderBlock_2/Lay  (None, 197, 192)    768         [&#39;Transformer/EncoderBlock_2/add_
 erNorm_2 (QuantizedLayerNormal                                  1[0][0]&#39;]
 ization)

 Transformer/EncoderBlock_2/Mlp  (None, 197, 768)    148224      [&#39;Transformer/EncoderBlock_2/Laye
 Block/Dense_0 (QuantizedDense)                                  rNorm_2[0][0]&#39;]

 Transformer/EncoderBlock_2/Mlp  (None, 197, 768)    1536        [&#39;Transformer/EncoderBlock_2/MlpB
 Block/activation (QuantizedReL                                  lock/Dense_0[0][0]&#39;]
 U)

 dropout_7 (QuantizedDropout)   (None, 197, 768)     0           [&#39;Transformer/EncoderBlock_2/MlpB
                                                                 lock/activation[0][0]&#39;]

 Transformer/EncoderBlock_2/Mlp  (None, 197, 192)    148032      [&#39;dropout_7[0][0]&#39;]
 Block/Dense_1 (QuantizedDense)

 dropout_8 (QuantizedDropout)   (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_2/MlpB
                                                                 lock/Dense_1[0][0]&#39;]

 Transformer/EncoderBlock_2/add  (None, 197, 192)    384         [&#39;Transformer/EncoderBlock_2/add_
 _2 (QuantizedAdd)                                               1[0][0]&#39;,
                                                                  &#39;dropout_8[0][0]&#39;]

 Transformer/EncoderBlock_3/Lay  (None, 197, 192)    768         [&#39;Transformer/EncoderBlock_2/add_
 erNorm_0 (QuantizedLayerNormal                                  2[0][0]&#39;]
 ization)

 Transformer/EncoderBlock_3/Mul  (None, 197, 192)    37058       [&#39;Transformer/EncoderBlock_3/Laye
 tiHeadDotProductAttention_1/qu                                  rNorm_0[0][0]&#39;]
 ery (QuantizedDense)

 Transformer/EncoderBlock_3/Mul  (None, 197, 192)    37058       [&#39;Transformer/EncoderBlock_3/Laye
 tiHeadDotProductAttention_1/ke                                  rNorm_0[0][0]&#39;]
 y (QuantizedDense)

 Transformer/EncoderBlock_3/Mul  (None, 197, 192)    37440       [&#39;Transformer/EncoderBlock_3/Laye
 tiHeadDotProductAttention_1/va                                  rNorm_0[0][0]&#39;]
 lue (QuantizedDense)

 Transformer/EncoderBlock_3/Mul  ((None, 197, 192),  384         [&#39;Transformer/EncoderBlock_3/Mult
 tiHeadDotProductAttention_1/at   (None, 3, 197, 197             iHeadDotProductAttention_1/query[
 tention (QuantizedAttention)   ))                               0][0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_3/Mult
                                                                 iHeadDotProductAttention_1/key[0]
                                                                 [0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_3/Mult
                                                                 iHeadDotProductAttention_1/value[
                                                                 0][0]&#39;]

 Transformer/EncoderBlock_3/Mul  (None, 197, 192)    37440       [&#39;Transformer/EncoderBlock_3/Mult
 tiHeadDotProductAttention_1/ou                                  iHeadDotProductAttention_1/attent
 t (QuantizedDense)                                              ion[0][0]&#39;]

 dropout_9 (QuantizedDropout)   (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_3/Mult
                                                                 iHeadDotProductAttention_1/out[0]
                                                                 [0]&#39;]

 Transformer/EncoderBlock_3/add  (None, 197, 192)    384         [&#39;dropout_9[0][0]&#39;,
 _1 (QuantizedAdd)                                                &#39;Transformer/EncoderBlock_2/add_
                                                                 2[0][0]&#39;]

 Transformer/EncoderBlock_3/Lay  (None, 197, 192)    768         [&#39;Transformer/EncoderBlock_3/add_
 erNorm_2 (QuantizedLayerNormal                                  1[0][0]&#39;]
 ization)

 Transformer/EncoderBlock_3/Mlp  (None, 197, 768)    148224      [&#39;Transformer/EncoderBlock_3/Laye
 Block/Dense_0 (QuantizedDense)                                  rNorm_2[0][0]&#39;]

 Transformer/EncoderBlock_3/Mlp  (None, 197, 768)    1536        [&#39;Transformer/EncoderBlock_3/MlpB
 Block/activation (QuantizedReL                                  lock/Dense_0[0][0]&#39;]
 U)

 dropout_10 (QuantizedDropout)  (None, 197, 768)     0           [&#39;Transformer/EncoderBlock_3/MlpB
                                                                 lock/activation[0][0]&#39;]

 Transformer/EncoderBlock_3/Mlp  (None, 197, 192)    148032      [&#39;dropout_10[0][0]&#39;]
 Block/Dense_1 (QuantizedDense)

 dropout_11 (QuantizedDropout)  (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_3/MlpB
                                                                 lock/Dense_1[0][0]&#39;]

 Transformer/EncoderBlock_3/add  (None, 197, 192)    384         [&#39;Transformer/EncoderBlock_3/add_
 _2 (QuantizedAdd)                                               1[0][0]&#39;,
                                                                  &#39;dropout_11[0][0]&#39;]

 Transformer/EncoderBlock_4/Lay  (None, 197, 192)    768         [&#39;Transformer/EncoderBlock_3/add_
 erNorm_0 (QuantizedLayerNormal                                  2[0][0]&#39;]
 ization)

 Transformer/EncoderBlock_4/Mul  (None, 197, 192)    37058       [&#39;Transformer/EncoderBlock_4/Laye
 tiHeadDotProductAttention_1/qu                                  rNorm_0[0][0]&#39;]
 ery (QuantizedDense)

 Transformer/EncoderBlock_4/Mul  (None, 197, 192)    37058       [&#39;Transformer/EncoderBlock_4/Laye
 tiHeadDotProductAttention_1/ke                                  rNorm_0[0][0]&#39;]
 y (QuantizedDense)

 Transformer/EncoderBlock_4/Mul  (None, 197, 192)    37440       [&#39;Transformer/EncoderBlock_4/Laye
 tiHeadDotProductAttention_1/va                                  rNorm_0[0][0]&#39;]
 lue (QuantizedDense)

 Transformer/EncoderBlock_4/Mul  ((None, 197, 192),  384         [&#39;Transformer/EncoderBlock_4/Mult
 tiHeadDotProductAttention_1/at   (None, 3, 197, 197             iHeadDotProductAttention_1/query[
 tention (QuantizedAttention)   ))                               0][0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_4/Mult
                                                                 iHeadDotProductAttention_1/key[0]
                                                                 [0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_4/Mult
                                                                 iHeadDotProductAttention_1/value[
                                                                 0][0]&#39;]

 Transformer/EncoderBlock_4/Mul  (None, 197, 192)    37440       [&#39;Transformer/EncoderBlock_4/Mult
 tiHeadDotProductAttention_1/ou                                  iHeadDotProductAttention_1/attent
 t (QuantizedDense)                                              ion[0][0]&#39;]

 dropout_12 (QuantizedDropout)  (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_4/Mult
                                                                 iHeadDotProductAttention_1/out[0]
                                                                 [0]&#39;]

 Transformer/EncoderBlock_4/add  (None, 197, 192)    384         [&#39;dropout_12[0][0]&#39;,
 _1 (QuantizedAdd)                                                &#39;Transformer/EncoderBlock_3/add_
                                                                 2[0][0]&#39;]

 Transformer/EncoderBlock_4/Lay  (None, 197, 192)    768         [&#39;Transformer/EncoderBlock_4/add_
 erNorm_2 (QuantizedLayerNormal                                  1[0][0]&#39;]
 ization)

 Transformer/EncoderBlock_4/Mlp  (None, 197, 768)    148224      [&#39;Transformer/EncoderBlock_4/Laye
 Block/Dense_0 (QuantizedDense)                                  rNorm_2[0][0]&#39;]

 Transformer/EncoderBlock_4/Mlp  (None, 197, 768)    1536        [&#39;Transformer/EncoderBlock_4/MlpB
 Block/activation (QuantizedReL                                  lock/Dense_0[0][0]&#39;]
 U)

 dropout_13 (QuantizedDropout)  (None, 197, 768)     0           [&#39;Transformer/EncoderBlock_4/MlpB
                                                                 lock/activation[0][0]&#39;]

 Transformer/EncoderBlock_4/Mlp  (None, 197, 192)    148032      [&#39;dropout_13[0][0]&#39;]
 Block/Dense_1 (QuantizedDense)

 dropout_14 (QuantizedDropout)  (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_4/MlpB
                                                                 lock/Dense_1[0][0]&#39;]

 Transformer/EncoderBlock_4/add  (None, 197, 192)    384         [&#39;Transformer/EncoderBlock_4/add_
 _2 (QuantizedAdd)                                               1[0][0]&#39;,
                                                                  &#39;dropout_14[0][0]&#39;]

 Transformer/EncoderBlock_5/Lay  (None, 197, 192)    768         [&#39;Transformer/EncoderBlock_4/add_
 erNorm_0 (QuantizedLayerNormal                                  2[0][0]&#39;]
 ization)

 Transformer/EncoderBlock_5/Mul  (None, 197, 192)    37058       [&#39;Transformer/EncoderBlock_5/Laye
 tiHeadDotProductAttention_1/qu                                  rNorm_0[0][0]&#39;]
 ery (QuantizedDense)

 Transformer/EncoderBlock_5/Mul  (None, 197, 192)    37058       [&#39;Transformer/EncoderBlock_5/Laye
 tiHeadDotProductAttention_1/ke                                  rNorm_0[0][0]&#39;]
 y (QuantizedDense)

 Transformer/EncoderBlock_5/Mul  (None, 197, 192)    37440       [&#39;Transformer/EncoderBlock_5/Laye
 tiHeadDotProductAttention_1/va                                  rNorm_0[0][0]&#39;]
 lue (QuantizedDense)

 Transformer/EncoderBlock_5/Mul  ((None, 197, 192),  384         [&#39;Transformer/EncoderBlock_5/Mult
 tiHeadDotProductAttention_1/at   (None, 3, 197, 197             iHeadDotProductAttention_1/query[
 tention (QuantizedAttention)   ))                               0][0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_5/Mult
                                                                 iHeadDotProductAttention_1/key[0]
                                                                 [0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_5/Mult
                                                                 iHeadDotProductAttention_1/value[
                                                                 0][0]&#39;]

 Transformer/EncoderBlock_5/Mul  (None, 197, 192)    37440       [&#39;Transformer/EncoderBlock_5/Mult
 tiHeadDotProductAttention_1/ou                                  iHeadDotProductAttention_1/attent
 t (QuantizedDense)                                              ion[0][0]&#39;]

 dropout_15 (QuantizedDropout)  (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_5/Mult
                                                                 iHeadDotProductAttention_1/out[0]
                                                                 [0]&#39;]

 Transformer/EncoderBlock_5/add  (None, 197, 192)    384         [&#39;dropout_15[0][0]&#39;,
 _1 (QuantizedAdd)                                                &#39;Transformer/EncoderBlock_4/add_
                                                                 2[0][0]&#39;]

 Transformer/EncoderBlock_5/Lay  (None, 197, 192)    768         [&#39;Transformer/EncoderBlock_5/add_
 erNorm_2 (QuantizedLayerNormal                                  1[0][0]&#39;]
 ization)

 Transformer/EncoderBlock_5/Mlp  (None, 197, 768)    148224      [&#39;Transformer/EncoderBlock_5/Laye
 Block/Dense_0 (QuantizedDense)                                  rNorm_2[0][0]&#39;]

 Transformer/EncoderBlock_5/Mlp  (None, 197, 768)    1536        [&#39;Transformer/EncoderBlock_5/MlpB
 Block/activation (QuantizedReL                                  lock/Dense_0[0][0]&#39;]
 U)

 dropout_16 (QuantizedDropout)  (None, 197, 768)     0           [&#39;Transformer/EncoderBlock_5/MlpB
                                                                 lock/activation[0][0]&#39;]

 Transformer/EncoderBlock_5/Mlp  (None, 197, 192)    148032      [&#39;dropout_16[0][0]&#39;]
 Block/Dense_1 (QuantizedDense)

 dropout_17 (QuantizedDropout)  (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_5/MlpB
                                                                 lock/Dense_1[0][0]&#39;]

 Transformer/EncoderBlock_5/add  (None, 197, 192)    384         [&#39;Transformer/EncoderBlock_5/add_
 _2 (QuantizedAdd)                                               1[0][0]&#39;,
                                                                  &#39;dropout_17[0][0]&#39;]

 Transformer/EncoderBlock_6/Lay  (None, 197, 192)    768         [&#39;Transformer/EncoderBlock_5/add_
 erNorm_0 (QuantizedLayerNormal                                  2[0][0]&#39;]
 ization)

 Transformer/EncoderBlock_6/Mul  (None, 197, 192)    37058       [&#39;Transformer/EncoderBlock_6/Laye
 tiHeadDotProductAttention_1/qu                                  rNorm_0[0][0]&#39;]
 ery (QuantizedDense)

 Transformer/EncoderBlock_6/Mul  (None, 197, 192)    37058       [&#39;Transformer/EncoderBlock_6/Laye
 tiHeadDotProductAttention_1/ke                                  rNorm_0[0][0]&#39;]
 y (QuantizedDense)

 Transformer/EncoderBlock_6/Mul  (None, 197, 192)    37440       [&#39;Transformer/EncoderBlock_6/Laye
 tiHeadDotProductAttention_1/va                                  rNorm_0[0][0]&#39;]
 lue (QuantizedDense)

 Transformer/EncoderBlock_6/Mul  ((None, 197, 192),  384         [&#39;Transformer/EncoderBlock_6/Mult
 tiHeadDotProductAttention_1/at   (None, 3, 197, 197             iHeadDotProductAttention_1/query[
 tention (QuantizedAttention)   ))                               0][0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_6/Mult
                                                                 iHeadDotProductAttention_1/key[0]
                                                                 [0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_6/Mult
                                                                 iHeadDotProductAttention_1/value[
                                                                 0][0]&#39;]

 Transformer/EncoderBlock_6/Mul  (None, 197, 192)    37440       [&#39;Transformer/EncoderBlock_6/Mult
 tiHeadDotProductAttention_1/ou                                  iHeadDotProductAttention_1/attent
 t (QuantizedDense)                                              ion[0][0]&#39;]

 dropout_18 (QuantizedDropout)  (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_6/Mult
                                                                 iHeadDotProductAttention_1/out[0]
                                                                 [0]&#39;]

 Transformer/EncoderBlock_6/add  (None, 197, 192)    384         [&#39;dropout_18[0][0]&#39;,
 _1 (QuantizedAdd)                                                &#39;Transformer/EncoderBlock_5/add_
                                                                 2[0][0]&#39;]

 Transformer/EncoderBlock_6/Lay  (None, 197, 192)    768         [&#39;Transformer/EncoderBlock_6/add_
 erNorm_2 (QuantizedLayerNormal                                  1[0][0]&#39;]
 ization)

 Transformer/EncoderBlock_6/Mlp  (None, 197, 768)    148224      [&#39;Transformer/EncoderBlock_6/Laye
 Block/Dense_0 (QuantizedDense)                                  rNorm_2[0][0]&#39;]

 Transformer/EncoderBlock_6/Mlp  (None, 197, 768)    1536        [&#39;Transformer/EncoderBlock_6/MlpB
 Block/activation (QuantizedReL                                  lock/Dense_0[0][0]&#39;]
 U)

 dropout_19 (QuantizedDropout)  (None, 197, 768)     0           [&#39;Transformer/EncoderBlock_6/MlpB
                                                                 lock/activation[0][0]&#39;]

 Transformer/EncoderBlock_6/Mlp  (None, 197, 192)    148032      [&#39;dropout_19[0][0]&#39;]
 Block/Dense_1 (QuantizedDense)

 dropout_20 (QuantizedDropout)  (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_6/MlpB
                                                                 lock/Dense_1[0][0]&#39;]

 Transformer/EncoderBlock_6/add  (None, 197, 192)    384         [&#39;Transformer/EncoderBlock_6/add_
 _2 (QuantizedAdd)                                               1[0][0]&#39;,
                                                                  &#39;dropout_20[0][0]&#39;]

 Transformer/EncoderBlock_7/Lay  (None, 197, 192)    768         [&#39;Transformer/EncoderBlock_6/add_
 erNorm_0 (QuantizedLayerNormal                                  2[0][0]&#39;]
 ization)

 Transformer/EncoderBlock_7/Mul  (None, 197, 192)    37058       [&#39;Transformer/EncoderBlock_7/Laye
 tiHeadDotProductAttention_1/qu                                  rNorm_0[0][0]&#39;]
 ery (QuantizedDense)

 Transformer/EncoderBlock_7/Mul  (None, 197, 192)    37058       [&#39;Transformer/EncoderBlock_7/Laye
 tiHeadDotProductAttention_1/ke                                  rNorm_0[0][0]&#39;]
 y (QuantizedDense)

 Transformer/EncoderBlock_7/Mul  (None, 197, 192)    37440       [&#39;Transformer/EncoderBlock_7/Laye
 tiHeadDotProductAttention_1/va                                  rNorm_0[0][0]&#39;]
 lue (QuantizedDense)

 Transformer/EncoderBlock_7/Mul  ((None, 197, 192),  384         [&#39;Transformer/EncoderBlock_7/Mult
 tiHeadDotProductAttention_1/at   (None, 3, 197, 197             iHeadDotProductAttention_1/query[
 tention (QuantizedAttention)   ))                               0][0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_7/Mult
                                                                 iHeadDotProductAttention_1/key[0]
                                                                 [0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_7/Mult
                                                                 iHeadDotProductAttention_1/value[
                                                                 0][0]&#39;]

 Transformer/EncoderBlock_7/Mul  (None, 197, 192)    37440       [&#39;Transformer/EncoderBlock_7/Mult
 tiHeadDotProductAttention_1/ou                                  iHeadDotProductAttention_1/attent
 t (QuantizedDense)                                              ion[0][0]&#39;]

 dropout_21 (QuantizedDropout)  (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_7/Mult
                                                                 iHeadDotProductAttention_1/out[0]
                                                                 [0]&#39;]

 Transformer/EncoderBlock_7/add  (None, 197, 192)    384         [&#39;dropout_21[0][0]&#39;,
 _1 (QuantizedAdd)                                                &#39;Transformer/EncoderBlock_6/add_
                                                                 2[0][0]&#39;]

 Transformer/EncoderBlock_7/Lay  (None, 197, 192)    768         [&#39;Transformer/EncoderBlock_7/add_
 erNorm_2 (QuantizedLayerNormal                                  1[0][0]&#39;]
 ization)

 Transformer/EncoderBlock_7/Mlp  (None, 197, 768)    148224      [&#39;Transformer/EncoderBlock_7/Laye
 Block/Dense_0 (QuantizedDense)                                  rNorm_2[0][0]&#39;]

 Transformer/EncoderBlock_7/Mlp  (None, 197, 768)    1536        [&#39;Transformer/EncoderBlock_7/MlpB
 Block/activation (QuantizedReL                                  lock/Dense_0[0][0]&#39;]
 U)

 dropout_22 (QuantizedDropout)  (None, 197, 768)     0           [&#39;Transformer/EncoderBlock_7/MlpB
                                                                 lock/activation[0][0]&#39;]

 Transformer/EncoderBlock_7/Mlp  (None, 197, 192)    148032      [&#39;dropout_22[0][0]&#39;]
 Block/Dense_1 (QuantizedDense)

 dropout_23 (QuantizedDropout)  (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_7/MlpB
                                                                 lock/Dense_1[0][0]&#39;]

 Transformer/EncoderBlock_7/add  (None, 197, 192)    384         [&#39;Transformer/EncoderBlock_7/add_
 _2 (QuantizedAdd)                                               1[0][0]&#39;,
                                                                  &#39;dropout_23[0][0]&#39;]

 Transformer/EncoderBlock_8/Lay  (None, 197, 192)    768         [&#39;Transformer/EncoderBlock_7/add_
 erNorm_0 (QuantizedLayerNormal                                  2[0][0]&#39;]
 ization)

 Transformer/EncoderBlock_8/Mul  (None, 197, 192)    37058       [&#39;Transformer/EncoderBlock_8/Laye
 tiHeadDotProductAttention_1/qu                                  rNorm_0[0][0]&#39;]
 ery (QuantizedDense)

 Transformer/EncoderBlock_8/Mul  (None, 197, 192)    37058       [&#39;Transformer/EncoderBlock_8/Laye
 tiHeadDotProductAttention_1/ke                                  rNorm_0[0][0]&#39;]
 y (QuantizedDense)

 Transformer/EncoderBlock_8/Mul  (None, 197, 192)    37440       [&#39;Transformer/EncoderBlock_8/Laye
 tiHeadDotProductAttention_1/va                                  rNorm_0[0][0]&#39;]
 lue (QuantizedDense)

 Transformer/EncoderBlock_8/Mul  ((None, 197, 192),  384         [&#39;Transformer/EncoderBlock_8/Mult
 tiHeadDotProductAttention_1/at   (None, 3, 197, 197             iHeadDotProductAttention_1/query[
 tention (QuantizedAttention)   ))                               0][0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_8/Mult
                                                                 iHeadDotProductAttention_1/key[0]
                                                                 [0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_8/Mult
                                                                 iHeadDotProductAttention_1/value[
                                                                 0][0]&#39;]

 Transformer/EncoderBlock_8/Mul  (None, 197, 192)    37440       [&#39;Transformer/EncoderBlock_8/Mult
 tiHeadDotProductAttention_1/ou                                  iHeadDotProductAttention_1/attent
 t (QuantizedDense)                                              ion[0][0]&#39;]

 dropout_24 (QuantizedDropout)  (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_8/Mult
                                                                 iHeadDotProductAttention_1/out[0]
                                                                 [0]&#39;]

 Transformer/EncoderBlock_8/add  (None, 197, 192)    384         [&#39;dropout_24[0][0]&#39;,
 _1 (QuantizedAdd)                                                &#39;Transformer/EncoderBlock_7/add_
                                                                 2[0][0]&#39;]

 Transformer/EncoderBlock_8/Lay  (None, 197, 192)    768         [&#39;Transformer/EncoderBlock_8/add_
 erNorm_2 (QuantizedLayerNormal                                  1[0][0]&#39;]
 ization)

 Transformer/EncoderBlock_8/Mlp  (None, 197, 768)    148224      [&#39;Transformer/EncoderBlock_8/Laye
 Block/Dense_0 (QuantizedDense)                                  rNorm_2[0][0]&#39;]

 Transformer/EncoderBlock_8/Mlp  (None, 197, 768)    1536        [&#39;Transformer/EncoderBlock_8/MlpB
 Block/activation (QuantizedReL                                  lock/Dense_0[0][0]&#39;]
 U)

 dropout_25 (QuantizedDropout)  (None, 197, 768)     0           [&#39;Transformer/EncoderBlock_8/MlpB
                                                                 lock/activation[0][0]&#39;]

 Transformer/EncoderBlock_8/Mlp  (None, 197, 192)    148032      [&#39;dropout_25[0][0]&#39;]
 Block/Dense_1 (QuantizedDense)

 dropout_26 (QuantizedDropout)  (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_8/MlpB
                                                                 lock/Dense_1[0][0]&#39;]

 Transformer/EncoderBlock_8/add  (None, 197, 192)    384         [&#39;Transformer/EncoderBlock_8/add_
 _2 (QuantizedAdd)                                               1[0][0]&#39;,
                                                                  &#39;dropout_26[0][0]&#39;]

 Transformer/EncoderBlock_9/Lay  (None, 197, 192)    768         [&#39;Transformer/EncoderBlock_8/add_
 erNorm_0 (QuantizedLayerNormal                                  2[0][0]&#39;]
 ization)

 Transformer/EncoderBlock_9/Mul  (None, 197, 192)    37058       [&#39;Transformer/EncoderBlock_9/Laye
 tiHeadDotProductAttention_1/qu                                  rNorm_0[0][0]&#39;]
 ery (QuantizedDense)

 Transformer/EncoderBlock_9/Mul  (None, 197, 192)    37058       [&#39;Transformer/EncoderBlock_9/Laye
 tiHeadDotProductAttention_1/ke                                  rNorm_0[0][0]&#39;]
 y (QuantizedDense)

 Transformer/EncoderBlock_9/Mul  (None, 197, 192)    37440       [&#39;Transformer/EncoderBlock_9/Laye
 tiHeadDotProductAttention_1/va                                  rNorm_0[0][0]&#39;]
 lue (QuantizedDense)

 Transformer/EncoderBlock_9/Mul  ((None, 197, 192),  384         [&#39;Transformer/EncoderBlock_9/Mult
 tiHeadDotProductAttention_1/at   (None, 3, 197, 197             iHeadDotProductAttention_1/query[
 tention (QuantizedAttention)   ))                               0][0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_9/Mult
                                                                 iHeadDotProductAttention_1/key[0]
                                                                 [0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_9/Mult
                                                                 iHeadDotProductAttention_1/value[
                                                                 0][0]&#39;]

 Transformer/EncoderBlock_9/Mul  (None, 197, 192)    37440       [&#39;Transformer/EncoderBlock_9/Mult
 tiHeadDotProductAttention_1/ou                                  iHeadDotProductAttention_1/attent
 t (QuantizedDense)                                              ion[0][0]&#39;]

 dropout_27 (QuantizedDropout)  (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_9/Mult
                                                                 iHeadDotProductAttention_1/out[0]
                                                                 [0]&#39;]

 Transformer/EncoderBlock_9/add  (None, 197, 192)    384         [&#39;dropout_27[0][0]&#39;,
 _1 (QuantizedAdd)                                                &#39;Transformer/EncoderBlock_8/add_
                                                                 2[0][0]&#39;]

 Transformer/EncoderBlock_9/Lay  (None, 197, 192)    768         [&#39;Transformer/EncoderBlock_9/add_
 erNorm_2 (QuantizedLayerNormal                                  1[0][0]&#39;]
 ization)

 Transformer/EncoderBlock_9/Mlp  (None, 197, 768)    148224      [&#39;Transformer/EncoderBlock_9/Laye
 Block/Dense_0 (QuantizedDense)                                  rNorm_2[0][0]&#39;]

 Transformer/EncoderBlock_9/Mlp  (None, 197, 768)    1536        [&#39;Transformer/EncoderBlock_9/MlpB
 Block/activation (QuantizedReL                                  lock/Dense_0[0][0]&#39;]
 U)

 dropout_28 (QuantizedDropout)  (None, 197, 768)     0           [&#39;Transformer/EncoderBlock_9/MlpB
                                                                 lock/activation[0][0]&#39;]

 Transformer/EncoderBlock_9/Mlp  (None, 197, 192)    148032      [&#39;dropout_28[0][0]&#39;]
 Block/Dense_1 (QuantizedDense)

 dropout_29 (QuantizedDropout)  (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_9/MlpB
                                                                 lock/Dense_1[0][0]&#39;]

 Transformer/EncoderBlock_9/add  (None, 197, 192)    384         [&#39;Transformer/EncoderBlock_9/add_
 _2 (QuantizedAdd)                                               1[0][0]&#39;,
                                                                  &#39;dropout_29[0][0]&#39;]

 Transformer/EncoderBlock_10/La  (None, 197, 192)    768         [&#39;Transformer/EncoderBlock_9/add_
 yerNorm_0 (QuantizedLayerNorma                                  2[0][0]&#39;]
 lization)

 Transformer/EncoderBlock_10/Mu  (None, 197, 192)    37058       [&#39;Transformer/EncoderBlock_10/Lay
 ltiHeadDotProductAttention_1/q                                  erNorm_0[0][0]&#39;]
 uery (QuantizedDense)

 Transformer/EncoderBlock_10/Mu  (None, 197, 192)    37058       [&#39;Transformer/EncoderBlock_10/Lay
 ltiHeadDotProductAttention_1/k                                  erNorm_0[0][0]&#39;]
 ey (QuantizedDense)

 Transformer/EncoderBlock_10/Mu  (None, 197, 192)    37440       [&#39;Transformer/EncoderBlock_10/Lay
 ltiHeadDotProductAttention_1/v                                  erNorm_0[0][0]&#39;]
 alue (QuantizedDense)

 Transformer/EncoderBlock_10/Mu  ((None, 197, 192),  384         [&#39;Transformer/EncoderBlock_10/Mul
 ltiHeadDotProductAttention_1/a   (None, 3, 197, 197             tiHeadDotProductAttention_1/query
 ttention (QuantizedAttention)  ))                               [0][0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_10/Mul
                                                                 tiHeadDotProductAttention_1/key[0
                                                                 ][0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_10/Mul
                                                                 tiHeadDotProductAttention_1/value
                                                                 [0][0]&#39;]

 Transformer/EncoderBlock_10/Mu  (None, 197, 192)    37440       [&#39;Transformer/EncoderBlock_10/Mul
 ltiHeadDotProductAttention_1/o                                  tiHeadDotProductAttention_1/atten
 ut (QuantizedDense)                                             tion[0][0]&#39;]

 dropout_30 (QuantizedDropout)  (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_10/Mul
                                                                 tiHeadDotProductAttention_1/out[0
                                                                 ][0]&#39;]

 Transformer/EncoderBlock_10/ad  (None, 197, 192)    384         [&#39;dropout_30[0][0]&#39;,
 d_1 (QuantizedAdd)                                               &#39;Transformer/EncoderBlock_9/add_
                                                                 2[0][0]&#39;]

 Transformer/EncoderBlock_10/La  (None, 197, 192)    768         [&#39;Transformer/EncoderBlock_10/add
 yerNorm_2 (QuantizedLayerNorma                                  _1[0][0]&#39;]
 lization)

 Transformer/EncoderBlock_10/Ml  (None, 197, 768)    148224      [&#39;Transformer/EncoderBlock_10/Lay
 pBlock/Dense_0 (QuantizedDense                                  erNorm_2[0][0]&#39;]
 )

 Transformer/EncoderBlock_10/Ml  (None, 197, 768)    1536        [&#39;Transformer/EncoderBlock_10/Mlp
 pBlock/activation (QuantizedRe                                  Block/Dense_0[0][0]&#39;]
 LU)

 dropout_31 (QuantizedDropout)  (None, 197, 768)     0           [&#39;Transformer/EncoderBlock_10/Mlp
                                                                 Block/activation[0][0]&#39;]

 Transformer/EncoderBlock_10/Ml  (None, 197, 192)    148032      [&#39;dropout_31[0][0]&#39;]
 pBlock/Dense_1 (QuantizedDense
 )

 dropout_32 (QuantizedDropout)  (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_10/Mlp
                                                                 Block/Dense_1[0][0]&#39;]

 Transformer/EncoderBlock_10/ad  (None, 197, 192)    384         [&#39;Transformer/EncoderBlock_10/add
 d_2 (QuantizedAdd)                                              _1[0][0]&#39;,
                                                                  &#39;dropout_32[0][0]&#39;]

 Transformer/EncoderBlock_11/La  (None, 197, 192)    768         [&#39;Transformer/EncoderBlock_10/add
 yerNorm_0 (QuantizedLayerNorma                                  _2[0][0]&#39;]
 lization)

 Transformer/EncoderBlock_11/Mu  (None, 197, 192)    37058       [&#39;Transformer/EncoderBlock_11/Lay
 ltiHeadDotProductAttention_1/q                                  erNorm_0[0][0]&#39;]
 uery (QuantizedDense)

 Transformer/EncoderBlock_11/Mu  (None, 197, 192)    37058       [&#39;Transformer/EncoderBlock_11/Lay
 ltiHeadDotProductAttention_1/k                                  erNorm_0[0][0]&#39;]
 ey (QuantizedDense)

 Transformer/EncoderBlock_11/Mu  (None, 197, 192)    37440       [&#39;Transformer/EncoderBlock_11/Lay
 ltiHeadDotProductAttention_1/v                                  erNorm_0[0][0]&#39;]
 alue (QuantizedDense)

 Transformer/EncoderBlock_11/Mu  ((None, 197, 192),  384         [&#39;Transformer/EncoderBlock_11/Mul
 ltiHeadDotProductAttention_1/a   (None, 3, 197, 197             tiHeadDotProductAttention_1/query
 ttention (QuantizedAttention)  ))                               [0][0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_11/Mul
                                                                 tiHeadDotProductAttention_1/key[0
                                                                 ][0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_11/Mul
                                                                 tiHeadDotProductAttention_1/value
                                                                 [0][0]&#39;]

 Transformer/EncoderBlock_11/Mu  (None, 197, 192)    37440       [&#39;Transformer/EncoderBlock_11/Mul
 ltiHeadDotProductAttention_1/o                                  tiHeadDotProductAttention_1/atten
 ut (QuantizedDense)                                             tion[0][0]&#39;]

 dropout_33 (QuantizedDropout)  (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_11/Mul
                                                                 tiHeadDotProductAttention_1/out[0
                                                                 ][0]&#39;]

 Transformer/EncoderBlock_11/ad  (None, 197, 192)    384         [&#39;dropout_33[0][0]&#39;,
 d_1 (QuantizedAdd)                                               &#39;Transformer/EncoderBlock_10/add
                                                                 _2[0][0]&#39;]

 Transformer/EncoderBlock_11/La  (None, 197, 192)    768         [&#39;Transformer/EncoderBlock_11/add
 yerNorm_2 (QuantizedLayerNorma                                  _1[0][0]&#39;]
 lization)

 Transformer/EncoderBlock_11/Ml  (None, 197, 768)    148224      [&#39;Transformer/EncoderBlock_11/Lay
 pBlock/Dense_0 (QuantizedDense                                  erNorm_2[0][0]&#39;]
 )

 Transformer/EncoderBlock_11/Ml  (None, 197, 768)    1536        [&#39;Transformer/EncoderBlock_11/Mlp
 pBlock/activation (QuantizedRe                                  Block/Dense_0[0][0]&#39;]
 LU)

 dropout_34 (QuantizedDropout)  (None, 197, 768)     0           [&#39;Transformer/EncoderBlock_11/Mlp
                                                                 Block/activation[0][0]&#39;]

 Transformer/EncoderBlock_11/Ml  (None, 197, 192)    148032      [&#39;dropout_34[0][0]&#39;]
 pBlock/Dense_1 (QuantizedDense
 )

 dropout_35 (QuantizedDropout)  (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_11/Mlp
                                                                 Block/Dense_1[0][0]&#39;]

 Transformer/EncoderBlock_11/ad  (None, 197, 192)    0           [&#39;Transformer/EncoderBlock_11/add
 d_2 (QuantizedAdd)                                              _1[0][0]&#39;,
                                                                  &#39;dropout_35[0][0]&#39;]

 Transformer/EncoderNorm (Quant  (None, 197, 192)    1152        [&#39;Transformer/EncoderBlock_11/add
 izedBatchNormalization)                                         _2[0][0]&#39;]

 ExtractToken (QuantizedExtract  (None, 192)         0           [&#39;Transformer/EncoderNorm[0][0]&#39;]
 Token)

 Head (QuantizedDense)          (None, 1000)         193000      [&#39;ExtractToken[0][0]&#39;]

 dequantizer_4 (Dequantizer)    [(None, 1000)]       0           [&#39;Head[0][0]&#39;]

==================================================================================================
Total params: 5,773,528
Trainable params: 5,717,800
Non-trainable params: 55,728
__________________________________________________________________________________________________
</pre></div>
</div>
<p>The <a class="reference external" href="../../api_reference/akida_models_apis.html#akida_models.bc_vit_ti16_imagenet_pretrained">bc_vit_ti16_imagenet_pretrained helper</a>
was obtained with the same 8-bit quantization scheme but with an additional QAT step to further
improve accuracy.</p>
</section>
<section id="quantization-aware-training-optional">
<h3>4.2 Quantization Aware Training (Optional)<a class="headerlink" href="#quantization-aware-training-optional" title="Permalink to this headline"></a></h3>
<p>In Section 4.1, we performed PTQ and converted the weights and activation outputs to 8-bit integer numbers.
In most cases, there is no accuracy drop observed after quantization, however in cases where an accurary
drop is observed, it is possible to further fine-tune this model using QAT.</p>
<p>The model that is obtained through <a class="reference external" href="../../user_guide/quantizeml.html">QuantizeML python package</a> is an
instance of Keras. This allows the model to be fine-tuned using the original dataset to regain accuracy.</p>
<p><a class="reference external" href="../../api_reference/akida_models_apis.html">Akida models python package</a>  provides pre-trained models
for vit_ti16 and deit_ti16 that have been trained using QAT method. It can be used in the following way:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">akida_models</span> <span class="kn">import</span> <span class="n">bc_vit_ti16_imagenet_pretrained</span>

<span class="c1"># Load the pre-trained quantized model</span>
<span class="n">model_quantized</span> <span class="o">=</span> <span class="n">bc_vit_ti16_imagenet_pretrained</span><span class="p">()</span>
<span class="n">model_quantized</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Downloading data from https://data.brainchip.com/models/AkidaV2/vit/bc_vit_ti16_224_i8_w8_a8.h5.

       0/24405400 [..............................] - ETA: 0s
   98304/24405400 [..............................] - ETA: 15s
  360448/24405400 [..............................] - ETA: 7s 
  892928/24405400 [&gt;.............................] - ETA: 4s
 1253376/24405400 [&gt;.............................] - ETA: 4s
 1622016/24405400 [&gt;.............................] - ETA: 3s
 2007040/24405400 [=&gt;............................] - ETA: 3s
 2359296/24405400 [=&gt;............................] - ETA: 3s
 2736128/24405400 [==&gt;...........................] - ETA: 3s
 3145728/24405400 [==&gt;...........................] - ETA: 3s
 3522560/24405400 [===&gt;..........................] - ETA: 3s
 3907584/24405400 [===&gt;..........................] - ETA: 3s
 4300800/24405400 [====&gt;.........................] - ETA: 2s
 4710400/24405400 [====&gt;.........................] - ETA: 2s
 5103616/24405400 [=====&gt;........................] - ETA: 2s
 5505024/24405400 [=====&gt;........................] - ETA: 2s
 5906432/24405400 [======&gt;.......................] - ETA: 2s
 6316032/24405400 [======&gt;.......................] - ETA: 2s
 6725632/24405400 [=======&gt;......................] - ETA: 2s
 7151616/24405400 [=======&gt;......................] - ETA: 2s
 7577600/24405400 [========&gt;.....................] - ETA: 2s
 7970816/24405400 [========&gt;.....................] - ETA: 2s
 8396800/24405400 [=========&gt;....................] - ETA: 2s
 8822784/24405400 [=========&gt;....................] - ETA: 2s
 9248768/24405400 [==========&gt;...................] - ETA: 2s
 9707520/24405400 [==========&gt;...................] - ETA: 1s
10149888/24405400 [===========&gt;..................] - ETA: 1s
10608640/24405400 [============&gt;.................] - ETA: 1s
11067392/24405400 [============&gt;.................] - ETA: 1s
11526144/24405400 [=============&gt;................] - ETA: 1s
12001280/24405400 [=============&gt;................] - ETA: 1s
12468224/24405400 [==============&gt;...............] - ETA: 1s
12935168/24405400 [==============&gt;...............] - ETA: 1s
13426688/24405400 [===============&gt;..............] - ETA: 1s
13901824/24405400 [================&gt;.............] - ETA: 1s
14376960/24405400 [================&gt;.............] - ETA: 1s
14852096/24405400 [=================&gt;............] - ETA: 1s
15343616/24405400 [=================&gt;............] - ETA: 1s
15835136/24405400 [==================&gt;...........] - ETA: 1s
16310272/24405400 [===================&gt;..........] - ETA: 0s
16801792/24405400 [===================&gt;..........] - ETA: 0s
17309696/24405400 [====================&gt;.........] - ETA: 0s
17817600/24405400 [====================&gt;.........] - ETA: 0s
18341888/24405400 [=====================&gt;........] - ETA: 0s
18849792/24405400 [======================&gt;.......] - ETA: 0s
19062784/24405400 [======================&gt;.......] - ETA: 0s
19603456/24405400 [=======================&gt;......] - ETA: 0s
19947520/24405400 [=======================&gt;......] - ETA: 0s
20324352/24405400 [=======================&gt;......] - ETA: 0s
20733952/24405400 [========================&gt;.....] - ETA: 0s
21127168/24405400 [========================&gt;.....] - ETA: 0s
21536768/24405400 [=========================&gt;....] - ETA: 0s
21929984/24405400 [=========================&gt;....] - ETA: 0s
22339584/24405400 [==========================&gt;...] - ETA: 0s
22749184/24405400 [==========================&gt;...] - ETA: 0s
23158784/24405400 [===========================&gt;..] - ETA: 0s
23584768/24405400 [===========================&gt;..] - ETA: 0s
24010752/24405400 [============================&gt;.] - ETA: 0s
24405400/24405400 [==============================] - 3s 0us/step
Model: &quot;vit-tiny&quot;
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to
==================================================================================================
 input (InputLayer)             [(None, 224, 224, 3  0           []
                                )]

 Rescale (QuantizedRescaling)   (None, 224, 224, 3)  0           [&#39;input[0][0]&#39;]

 Embedding (QuantizedConv2D)    (None, 14, 14, 192)  147648      [&#39;Rescale[0][0]&#39;]

 reshape (QuantizedReshape)     (None, 196, 192)     0           [&#39;Embedding[0][0]&#39;]

 ClassToken (QuantizedClassToke  (None, 197, 192)    192         [&#39;reshape[0][0]&#39;]
 n)

 Transformer/PosEmbed (Quantize  (None, 197, 192)    38208       [&#39;ClassToken[0][0]&#39;]
 dAddPositionEmbs)

 Transformer/EncoderBlock_0/Lay  (None, 197, 192)    768         [&#39;Transformer/PosEmbed[0][0]&#39;]
 erNorm_0 (QuantizedLayerNormal
 ization)

 Transformer/EncoderBlock_0/Mul  (None, 197, 192)    37058       [&#39;Transformer/EncoderBlock_0/Laye
 tiHeadDotProductAttention_1/qu                                  rNorm_0[0][0]&#39;]
 ery (QuantizedDense)

 Transformer/EncoderBlock_0/Mul  (None, 197, 192)    37058       [&#39;Transformer/EncoderBlock_0/Laye
 tiHeadDotProductAttention_1/ke                                  rNorm_0[0][0]&#39;]
 y (QuantizedDense)

 Transformer/EncoderBlock_0/Mul  (None, 197, 192)    37440       [&#39;Transformer/EncoderBlock_0/Laye
 tiHeadDotProductAttention_1/va                                  rNorm_0[0][0]&#39;]
 lue (QuantizedDense)

 Transformer/EncoderBlock_0/Mul  ((None, 197, 192),  384         [&#39;Transformer/EncoderBlock_0/Mult
 tiHeadDotProductAttention_1/at   (None, 3, 197, 197             iHeadDotProductAttention_1/query[
 tention (QuantizedAttention)   ))                               0][0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_0/Mult
                                                                 iHeadDotProductAttention_1/key[0]
                                                                 [0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_0/Mult
                                                                 iHeadDotProductAttention_1/value[
                                                                 0][0]&#39;]

 Transformer/EncoderBlock_0/Mul  (None, 197, 192)    37440       [&#39;Transformer/EncoderBlock_0/Mult
 tiHeadDotProductAttention_1/ou                                  iHeadDotProductAttention_1/attent
 t (QuantizedDense)                                              ion[0][0]&#39;]

 dropout (QuantizedDropout)     (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_0/Mult
                                                                 iHeadDotProductAttention_1/out[0]
                                                                 [0]&#39;]

 Transformer/EncoderBlock_0/add  (None, 197, 192)    384         [&#39;dropout[0][0]&#39;,
 _1 (QuantizedAdd)                                                &#39;Transformer/PosEmbed[0][0]&#39;]

 Transformer/EncoderBlock_0/Lay  (None, 197, 192)    768         [&#39;Transformer/EncoderBlock_0/add_
 erNorm_2 (QuantizedLayerNormal                                  1[0][0]&#39;]
 ization)

 Transformer/EncoderBlock_0/Mlp  (None, 197, 768)    148224      [&#39;Transformer/EncoderBlock_0/Laye
 Block/Dense_0 (QuantizedDense)                                  rNorm_2[0][0]&#39;]

 Transformer/EncoderBlock_0/Mlp  (None, 197, 768)    1536        [&#39;Transformer/EncoderBlock_0/MlpB
 Block/activation (QuantizedReL                                  lock/Dense_0[0][0]&#39;]
 U)

 dropout_1 (QuantizedDropout)   (None, 197, 768)     0           [&#39;Transformer/EncoderBlock_0/MlpB
                                                                 lock/activation[0][0]&#39;]

 Transformer/EncoderBlock_0/Mlp  (None, 197, 192)    148032      [&#39;dropout_1[0][0]&#39;]
 Block/Dense_1 (QuantizedDense)

 dropout_2 (QuantizedDropout)   (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_0/MlpB
                                                                 lock/Dense_1[0][0]&#39;]

 Transformer/EncoderBlock_0/add  (None, 197, 192)    384         [&#39;Transformer/EncoderBlock_0/add_
 _2 (QuantizedAdd)                                               1[0][0]&#39;,
                                                                  &#39;dropout_2[0][0]&#39;]

 Transformer/EncoderBlock_1/Lay  (None, 197, 192)    768         [&#39;Transformer/EncoderBlock_0/add_
 erNorm_0 (QuantizedLayerNormal                                  2[0][0]&#39;]
 ization)

 Transformer/EncoderBlock_1/Mul  (None, 197, 192)    37058       [&#39;Transformer/EncoderBlock_1/Laye
 tiHeadDotProductAttention_1/qu                                  rNorm_0[0][0]&#39;]
 ery (QuantizedDense)

 Transformer/EncoderBlock_1/Mul  (None, 197, 192)    37058       [&#39;Transformer/EncoderBlock_1/Laye
 tiHeadDotProductAttention_1/ke                                  rNorm_0[0][0]&#39;]
 y (QuantizedDense)

 Transformer/EncoderBlock_1/Mul  (None, 197, 192)    37440       [&#39;Transformer/EncoderBlock_1/Laye
 tiHeadDotProductAttention_1/va                                  rNorm_0[0][0]&#39;]
 lue (QuantizedDense)

 Transformer/EncoderBlock_1/Mul  ((None, 197, 192),  384         [&#39;Transformer/EncoderBlock_1/Mult
 tiHeadDotProductAttention_1/at   (None, 3, 197, 197             iHeadDotProductAttention_1/query[
 tention (QuantizedAttention)   ))                               0][0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_1/Mult
                                                                 iHeadDotProductAttention_1/key[0]
                                                                 [0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_1/Mult
                                                                 iHeadDotProductAttention_1/value[
                                                                 0][0]&#39;]

 Transformer/EncoderBlock_1/Mul  (None, 197, 192)    37440       [&#39;Transformer/EncoderBlock_1/Mult
 tiHeadDotProductAttention_1/ou                                  iHeadDotProductAttention_1/attent
 t (QuantizedDense)                                              ion[0][0]&#39;]

 dropout_3 (QuantizedDropout)   (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_1/Mult
                                                                 iHeadDotProductAttention_1/out[0]
                                                                 [0]&#39;]

 Transformer/EncoderBlock_1/add  (None, 197, 192)    384         [&#39;dropout_3[0][0]&#39;,
 _1 (QuantizedAdd)                                                &#39;Transformer/EncoderBlock_0/add_
                                                                 2[0][0]&#39;]

 Transformer/EncoderBlock_1/Lay  (None, 197, 192)    768         [&#39;Transformer/EncoderBlock_1/add_
 erNorm_2 (QuantizedLayerNormal                                  1[0][0]&#39;]
 ization)

 Transformer/EncoderBlock_1/Mlp  (None, 197, 768)    148224      [&#39;Transformer/EncoderBlock_1/Laye
 Block/Dense_0 (QuantizedDense)                                  rNorm_2[0][0]&#39;]

 Transformer/EncoderBlock_1/Mlp  (None, 197, 768)    1536        [&#39;Transformer/EncoderBlock_1/MlpB
 Block/activation (QuantizedReL                                  lock/Dense_0[0][0]&#39;]
 U)

 dropout_4 (QuantizedDropout)   (None, 197, 768)     0           [&#39;Transformer/EncoderBlock_1/MlpB
                                                                 lock/activation[0][0]&#39;]

 Transformer/EncoderBlock_1/Mlp  (None, 197, 192)    148032      [&#39;dropout_4[0][0]&#39;]
 Block/Dense_1 (QuantizedDense)

 dropout_5 (QuantizedDropout)   (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_1/MlpB
                                                                 lock/Dense_1[0][0]&#39;]

 Transformer/EncoderBlock_1/add  (None, 197, 192)    384         [&#39;Transformer/EncoderBlock_1/add_
 _2 (QuantizedAdd)                                               1[0][0]&#39;,
                                                                  &#39;dropout_5[0][0]&#39;]

 Transformer/EncoderBlock_2/Lay  (None, 197, 192)    768         [&#39;Transformer/EncoderBlock_1/add_
 erNorm_0 (QuantizedLayerNormal                                  2[0][0]&#39;]
 ization)

 Transformer/EncoderBlock_2/Mul  (None, 197, 192)    37058       [&#39;Transformer/EncoderBlock_2/Laye
 tiHeadDotProductAttention_1/qu                                  rNorm_0[0][0]&#39;]
 ery (QuantizedDense)

 Transformer/EncoderBlock_2/Mul  (None, 197, 192)    37058       [&#39;Transformer/EncoderBlock_2/Laye
 tiHeadDotProductAttention_1/ke                                  rNorm_0[0][0]&#39;]
 y (QuantizedDense)

 Transformer/EncoderBlock_2/Mul  (None, 197, 192)    37440       [&#39;Transformer/EncoderBlock_2/Laye
 tiHeadDotProductAttention_1/va                                  rNorm_0[0][0]&#39;]
 lue (QuantizedDense)

 Transformer/EncoderBlock_2/Mul  ((None, 197, 192),  384         [&#39;Transformer/EncoderBlock_2/Mult
 tiHeadDotProductAttention_1/at   (None, 3, 197, 197             iHeadDotProductAttention_1/query[
 tention (QuantizedAttention)   ))                               0][0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_2/Mult
                                                                 iHeadDotProductAttention_1/key[0]
                                                                 [0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_2/Mult
                                                                 iHeadDotProductAttention_1/value[
                                                                 0][0]&#39;]

 Transformer/EncoderBlock_2/Mul  (None, 197, 192)    37440       [&#39;Transformer/EncoderBlock_2/Mult
 tiHeadDotProductAttention_1/ou                                  iHeadDotProductAttention_1/attent
 t (QuantizedDense)                                              ion[0][0]&#39;]

 dropout_6 (QuantizedDropout)   (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_2/Mult
                                                                 iHeadDotProductAttention_1/out[0]
                                                                 [0]&#39;]

 Transformer/EncoderBlock_2/add  (None, 197, 192)    384         [&#39;dropout_6[0][0]&#39;,
 _1 (QuantizedAdd)                                                &#39;Transformer/EncoderBlock_1/add_
                                                                 2[0][0]&#39;]

 Transformer/EncoderBlock_2/Lay  (None, 197, 192)    768         [&#39;Transformer/EncoderBlock_2/add_
 erNorm_2 (QuantizedLayerNormal                                  1[0][0]&#39;]
 ization)

 Transformer/EncoderBlock_2/Mlp  (None, 197, 768)    148224      [&#39;Transformer/EncoderBlock_2/Laye
 Block/Dense_0 (QuantizedDense)                                  rNorm_2[0][0]&#39;]

 Transformer/EncoderBlock_2/Mlp  (None, 197, 768)    1536        [&#39;Transformer/EncoderBlock_2/MlpB
 Block/activation (QuantizedReL                                  lock/Dense_0[0][0]&#39;]
 U)

 dropout_7 (QuantizedDropout)   (None, 197, 768)     0           [&#39;Transformer/EncoderBlock_2/MlpB
                                                                 lock/activation[0][0]&#39;]

 Transformer/EncoderBlock_2/Mlp  (None, 197, 192)    148032      [&#39;dropout_7[0][0]&#39;]
 Block/Dense_1 (QuantizedDense)

 dropout_8 (QuantizedDropout)   (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_2/MlpB
                                                                 lock/Dense_1[0][0]&#39;]

 Transformer/EncoderBlock_2/add  (None, 197, 192)    384         [&#39;Transformer/EncoderBlock_2/add_
 _2 (QuantizedAdd)                                               1[0][0]&#39;,
                                                                  &#39;dropout_8[0][0]&#39;]

 Transformer/EncoderBlock_3/Lay  (None, 197, 192)    768         [&#39;Transformer/EncoderBlock_2/add_
 erNorm_0 (QuantizedLayerNormal                                  2[0][0]&#39;]
 ization)

 Transformer/EncoderBlock_3/Mul  (None, 197, 192)    37058       [&#39;Transformer/EncoderBlock_3/Laye
 tiHeadDotProductAttention_1/qu                                  rNorm_0[0][0]&#39;]
 ery (QuantizedDense)

 Transformer/EncoderBlock_3/Mul  (None, 197, 192)    37058       [&#39;Transformer/EncoderBlock_3/Laye
 tiHeadDotProductAttention_1/ke                                  rNorm_0[0][0]&#39;]
 y (QuantizedDense)

 Transformer/EncoderBlock_3/Mul  (None, 197, 192)    37440       [&#39;Transformer/EncoderBlock_3/Laye
 tiHeadDotProductAttention_1/va                                  rNorm_0[0][0]&#39;]
 lue (QuantizedDense)

 Transformer/EncoderBlock_3/Mul  ((None, 197, 192),  384         [&#39;Transformer/EncoderBlock_3/Mult
 tiHeadDotProductAttention_1/at   (None, 3, 197, 197             iHeadDotProductAttention_1/query[
 tention (QuantizedAttention)   ))                               0][0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_3/Mult
                                                                 iHeadDotProductAttention_1/key[0]
                                                                 [0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_3/Mult
                                                                 iHeadDotProductAttention_1/value[
                                                                 0][0]&#39;]

 Transformer/EncoderBlock_3/Mul  (None, 197, 192)    37440       [&#39;Transformer/EncoderBlock_3/Mult
 tiHeadDotProductAttention_1/ou                                  iHeadDotProductAttention_1/attent
 t (QuantizedDense)                                              ion[0][0]&#39;]

 dropout_9 (QuantizedDropout)   (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_3/Mult
                                                                 iHeadDotProductAttention_1/out[0]
                                                                 [0]&#39;]

 Transformer/EncoderBlock_3/add  (None, 197, 192)    384         [&#39;dropout_9[0][0]&#39;,
 _1 (QuantizedAdd)                                                &#39;Transformer/EncoderBlock_2/add_
                                                                 2[0][0]&#39;]

 Transformer/EncoderBlock_3/Lay  (None, 197, 192)    768         [&#39;Transformer/EncoderBlock_3/add_
 erNorm_2 (QuantizedLayerNormal                                  1[0][0]&#39;]
 ization)

 Transformer/EncoderBlock_3/Mlp  (None, 197, 768)    148224      [&#39;Transformer/EncoderBlock_3/Laye
 Block/Dense_0 (QuantizedDense)                                  rNorm_2[0][0]&#39;]

 Transformer/EncoderBlock_3/Mlp  (None, 197, 768)    1536        [&#39;Transformer/EncoderBlock_3/MlpB
 Block/activation (QuantizedReL                                  lock/Dense_0[0][0]&#39;]
 U)

 dropout_10 (QuantizedDropout)  (None, 197, 768)     0           [&#39;Transformer/EncoderBlock_3/MlpB
                                                                 lock/activation[0][0]&#39;]

 Transformer/EncoderBlock_3/Mlp  (None, 197, 192)    148032      [&#39;dropout_10[0][0]&#39;]
 Block/Dense_1 (QuantizedDense)

 dropout_11 (QuantizedDropout)  (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_3/MlpB
                                                                 lock/Dense_1[0][0]&#39;]

 Transformer/EncoderBlock_3/add  (None, 197, 192)    384         [&#39;Transformer/EncoderBlock_3/add_
 _2 (QuantizedAdd)                                               1[0][0]&#39;,
                                                                  &#39;dropout_11[0][0]&#39;]

 Transformer/EncoderBlock_4/Lay  (None, 197, 192)    768         [&#39;Transformer/EncoderBlock_3/add_
 erNorm_0 (QuantizedLayerNormal                                  2[0][0]&#39;]
 ization)

 Transformer/EncoderBlock_4/Mul  (None, 197, 192)    37058       [&#39;Transformer/EncoderBlock_4/Laye
 tiHeadDotProductAttention_1/qu                                  rNorm_0[0][0]&#39;]
 ery (QuantizedDense)

 Transformer/EncoderBlock_4/Mul  (None, 197, 192)    37058       [&#39;Transformer/EncoderBlock_4/Laye
 tiHeadDotProductAttention_1/ke                                  rNorm_0[0][0]&#39;]
 y (QuantizedDense)

 Transformer/EncoderBlock_4/Mul  (None, 197, 192)    37440       [&#39;Transformer/EncoderBlock_4/Laye
 tiHeadDotProductAttention_1/va                                  rNorm_0[0][0]&#39;]
 lue (QuantizedDense)

 Transformer/EncoderBlock_4/Mul  ((None, 197, 192),  384         [&#39;Transformer/EncoderBlock_4/Mult
 tiHeadDotProductAttention_1/at   (None, 3, 197, 197             iHeadDotProductAttention_1/query[
 tention (QuantizedAttention)   ))                               0][0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_4/Mult
                                                                 iHeadDotProductAttention_1/key[0]
                                                                 [0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_4/Mult
                                                                 iHeadDotProductAttention_1/value[
                                                                 0][0]&#39;]

 Transformer/EncoderBlock_4/Mul  (None, 197, 192)    37440       [&#39;Transformer/EncoderBlock_4/Mult
 tiHeadDotProductAttention_1/ou                                  iHeadDotProductAttention_1/attent
 t (QuantizedDense)                                              ion[0][0]&#39;]

 dropout_12 (QuantizedDropout)  (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_4/Mult
                                                                 iHeadDotProductAttention_1/out[0]
                                                                 [0]&#39;]

 Transformer/EncoderBlock_4/add  (None, 197, 192)    384         [&#39;dropout_12[0][0]&#39;,
 _1 (QuantizedAdd)                                                &#39;Transformer/EncoderBlock_3/add_
                                                                 2[0][0]&#39;]

 Transformer/EncoderBlock_4/Lay  (None, 197, 192)    768         [&#39;Transformer/EncoderBlock_4/add_
 erNorm_2 (QuantizedLayerNormal                                  1[0][0]&#39;]
 ization)

 Transformer/EncoderBlock_4/Mlp  (None, 197, 768)    148224      [&#39;Transformer/EncoderBlock_4/Laye
 Block/Dense_0 (QuantizedDense)                                  rNorm_2[0][0]&#39;]

 Transformer/EncoderBlock_4/Mlp  (None, 197, 768)    1536        [&#39;Transformer/EncoderBlock_4/MlpB
 Block/activation (QuantizedReL                                  lock/Dense_0[0][0]&#39;]
 U)

 dropout_13 (QuantizedDropout)  (None, 197, 768)     0           [&#39;Transformer/EncoderBlock_4/MlpB
                                                                 lock/activation[0][0]&#39;]

 Transformer/EncoderBlock_4/Mlp  (None, 197, 192)    148032      [&#39;dropout_13[0][0]&#39;]
 Block/Dense_1 (QuantizedDense)

 dropout_14 (QuantizedDropout)  (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_4/MlpB
                                                                 lock/Dense_1[0][0]&#39;]

 Transformer/EncoderBlock_4/add  (None, 197, 192)    384         [&#39;Transformer/EncoderBlock_4/add_
 _2 (QuantizedAdd)                                               1[0][0]&#39;,
                                                                  &#39;dropout_14[0][0]&#39;]

 Transformer/EncoderBlock_5/Lay  (None, 197, 192)    768         [&#39;Transformer/EncoderBlock_4/add_
 erNorm_0 (QuantizedLayerNormal                                  2[0][0]&#39;]
 ization)

 Transformer/EncoderBlock_5/Mul  (None, 197, 192)    37058       [&#39;Transformer/EncoderBlock_5/Laye
 tiHeadDotProductAttention_1/qu                                  rNorm_0[0][0]&#39;]
 ery (QuantizedDense)

 Transformer/EncoderBlock_5/Mul  (None, 197, 192)    37058       [&#39;Transformer/EncoderBlock_5/Laye
 tiHeadDotProductAttention_1/ke                                  rNorm_0[0][0]&#39;]
 y (QuantizedDense)

 Transformer/EncoderBlock_5/Mul  (None, 197, 192)    37440       [&#39;Transformer/EncoderBlock_5/Laye
 tiHeadDotProductAttention_1/va                                  rNorm_0[0][0]&#39;]
 lue (QuantizedDense)

 Transformer/EncoderBlock_5/Mul  ((None, 197, 192),  384         [&#39;Transformer/EncoderBlock_5/Mult
 tiHeadDotProductAttention_1/at   (None, 3, 197, 197             iHeadDotProductAttention_1/query[
 tention (QuantizedAttention)   ))                               0][0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_5/Mult
                                                                 iHeadDotProductAttention_1/key[0]
                                                                 [0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_5/Mult
                                                                 iHeadDotProductAttention_1/value[
                                                                 0][0]&#39;]

 Transformer/EncoderBlock_5/Mul  (None, 197, 192)    37440       [&#39;Transformer/EncoderBlock_5/Mult
 tiHeadDotProductAttention_1/ou                                  iHeadDotProductAttention_1/attent
 t (QuantizedDense)                                              ion[0][0]&#39;]

 dropout_15 (QuantizedDropout)  (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_5/Mult
                                                                 iHeadDotProductAttention_1/out[0]
                                                                 [0]&#39;]

 Transformer/EncoderBlock_5/add  (None, 197, 192)    384         [&#39;dropout_15[0][0]&#39;,
 _1 (QuantizedAdd)                                                &#39;Transformer/EncoderBlock_4/add_
                                                                 2[0][0]&#39;]

 Transformer/EncoderBlock_5/Lay  (None, 197, 192)    768         [&#39;Transformer/EncoderBlock_5/add_
 erNorm_2 (QuantizedLayerNormal                                  1[0][0]&#39;]
 ization)

 Transformer/EncoderBlock_5/Mlp  (None, 197, 768)    148224      [&#39;Transformer/EncoderBlock_5/Laye
 Block/Dense_0 (QuantizedDense)                                  rNorm_2[0][0]&#39;]

 Transformer/EncoderBlock_5/Mlp  (None, 197, 768)    1536        [&#39;Transformer/EncoderBlock_5/MlpB
 Block/activation (QuantizedReL                                  lock/Dense_0[0][0]&#39;]
 U)

 dropout_16 (QuantizedDropout)  (None, 197, 768)     0           [&#39;Transformer/EncoderBlock_5/MlpB
                                                                 lock/activation[0][0]&#39;]

 Transformer/EncoderBlock_5/Mlp  (None, 197, 192)    148032      [&#39;dropout_16[0][0]&#39;]
 Block/Dense_1 (QuantizedDense)

 dropout_17 (QuantizedDropout)  (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_5/MlpB
                                                                 lock/Dense_1[0][0]&#39;]

 Transformer/EncoderBlock_5/add  (None, 197, 192)    384         [&#39;Transformer/EncoderBlock_5/add_
 _2 (QuantizedAdd)                                               1[0][0]&#39;,
                                                                  &#39;dropout_17[0][0]&#39;]

 Transformer/EncoderBlock_6/Lay  (None, 197, 192)    768         [&#39;Transformer/EncoderBlock_5/add_
 erNorm_0 (QuantizedLayerNormal                                  2[0][0]&#39;]
 ization)

 Transformer/EncoderBlock_6/Mul  (None, 197, 192)    37058       [&#39;Transformer/EncoderBlock_6/Laye
 tiHeadDotProductAttention_1/qu                                  rNorm_0[0][0]&#39;]
 ery (QuantizedDense)

 Transformer/EncoderBlock_6/Mul  (None, 197, 192)    37058       [&#39;Transformer/EncoderBlock_6/Laye
 tiHeadDotProductAttention_1/ke                                  rNorm_0[0][0]&#39;]
 y (QuantizedDense)

 Transformer/EncoderBlock_6/Mul  (None, 197, 192)    37440       [&#39;Transformer/EncoderBlock_6/Laye
 tiHeadDotProductAttention_1/va                                  rNorm_0[0][0]&#39;]
 lue (QuantizedDense)

 Transformer/EncoderBlock_6/Mul  ((None, 197, 192),  384         [&#39;Transformer/EncoderBlock_6/Mult
 tiHeadDotProductAttention_1/at   (None, 3, 197, 197             iHeadDotProductAttention_1/query[
 tention (QuantizedAttention)   ))                               0][0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_6/Mult
                                                                 iHeadDotProductAttention_1/key[0]
                                                                 [0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_6/Mult
                                                                 iHeadDotProductAttention_1/value[
                                                                 0][0]&#39;]

 Transformer/EncoderBlock_6/Mul  (None, 197, 192)    37440       [&#39;Transformer/EncoderBlock_6/Mult
 tiHeadDotProductAttention_1/ou                                  iHeadDotProductAttention_1/attent
 t (QuantizedDense)                                              ion[0][0]&#39;]

 dropout_18 (QuantizedDropout)  (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_6/Mult
                                                                 iHeadDotProductAttention_1/out[0]
                                                                 [0]&#39;]

 Transformer/EncoderBlock_6/add  (None, 197, 192)    384         [&#39;dropout_18[0][0]&#39;,
 _1 (QuantizedAdd)                                                &#39;Transformer/EncoderBlock_5/add_
                                                                 2[0][0]&#39;]

 Transformer/EncoderBlock_6/Lay  (None, 197, 192)    768         [&#39;Transformer/EncoderBlock_6/add_
 erNorm_2 (QuantizedLayerNormal                                  1[0][0]&#39;]
 ization)

 Transformer/EncoderBlock_6/Mlp  (None, 197, 768)    148224      [&#39;Transformer/EncoderBlock_6/Laye
 Block/Dense_0 (QuantizedDense)                                  rNorm_2[0][0]&#39;]

 Transformer/EncoderBlock_6/Mlp  (None, 197, 768)    1536        [&#39;Transformer/EncoderBlock_6/MlpB
 Block/activation (QuantizedReL                                  lock/Dense_0[0][0]&#39;]
 U)

 dropout_19 (QuantizedDropout)  (None, 197, 768)     0           [&#39;Transformer/EncoderBlock_6/MlpB
                                                                 lock/activation[0][0]&#39;]

 Transformer/EncoderBlock_6/Mlp  (None, 197, 192)    148032      [&#39;dropout_19[0][0]&#39;]
 Block/Dense_1 (QuantizedDense)

 dropout_20 (QuantizedDropout)  (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_6/MlpB
                                                                 lock/Dense_1[0][0]&#39;]

 Transformer/EncoderBlock_6/add  (None, 197, 192)    384         [&#39;Transformer/EncoderBlock_6/add_
 _2 (QuantizedAdd)                                               1[0][0]&#39;,
                                                                  &#39;dropout_20[0][0]&#39;]

 Transformer/EncoderBlock_7/Lay  (None, 197, 192)    768         [&#39;Transformer/EncoderBlock_6/add_
 erNorm_0 (QuantizedLayerNormal                                  2[0][0]&#39;]
 ization)

 Transformer/EncoderBlock_7/Mul  (None, 197, 192)    37058       [&#39;Transformer/EncoderBlock_7/Laye
 tiHeadDotProductAttention_1/qu                                  rNorm_0[0][0]&#39;]
 ery (QuantizedDense)

 Transformer/EncoderBlock_7/Mul  (None, 197, 192)    37058       [&#39;Transformer/EncoderBlock_7/Laye
 tiHeadDotProductAttention_1/ke                                  rNorm_0[0][0]&#39;]
 y (QuantizedDense)

 Transformer/EncoderBlock_7/Mul  (None, 197, 192)    37440       [&#39;Transformer/EncoderBlock_7/Laye
 tiHeadDotProductAttention_1/va                                  rNorm_0[0][0]&#39;]
 lue (QuantizedDense)

 Transformer/EncoderBlock_7/Mul  ((None, 197, 192),  384         [&#39;Transformer/EncoderBlock_7/Mult
 tiHeadDotProductAttention_1/at   (None, 3, 197, 197             iHeadDotProductAttention_1/query[
 tention (QuantizedAttention)   ))                               0][0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_7/Mult
                                                                 iHeadDotProductAttention_1/key[0]
                                                                 [0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_7/Mult
                                                                 iHeadDotProductAttention_1/value[
                                                                 0][0]&#39;]

 Transformer/EncoderBlock_7/Mul  (None, 197, 192)    37440       [&#39;Transformer/EncoderBlock_7/Mult
 tiHeadDotProductAttention_1/ou                                  iHeadDotProductAttention_1/attent
 t (QuantizedDense)                                              ion[0][0]&#39;]

 dropout_21 (QuantizedDropout)  (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_7/Mult
                                                                 iHeadDotProductAttention_1/out[0]
                                                                 [0]&#39;]

 Transformer/EncoderBlock_7/add  (None, 197, 192)    384         [&#39;dropout_21[0][0]&#39;,
 _1 (QuantizedAdd)                                                &#39;Transformer/EncoderBlock_6/add_
                                                                 2[0][0]&#39;]

 Transformer/EncoderBlock_7/Lay  (None, 197, 192)    768         [&#39;Transformer/EncoderBlock_7/add_
 erNorm_2 (QuantizedLayerNormal                                  1[0][0]&#39;]
 ization)

 Transformer/EncoderBlock_7/Mlp  (None, 197, 768)    148224      [&#39;Transformer/EncoderBlock_7/Laye
 Block/Dense_0 (QuantizedDense)                                  rNorm_2[0][0]&#39;]

 Transformer/EncoderBlock_7/Mlp  (None, 197, 768)    1536        [&#39;Transformer/EncoderBlock_7/MlpB
 Block/activation (QuantizedReL                                  lock/Dense_0[0][0]&#39;]
 U)

 dropout_22 (QuantizedDropout)  (None, 197, 768)     0           [&#39;Transformer/EncoderBlock_7/MlpB
                                                                 lock/activation[0][0]&#39;]

 Transformer/EncoderBlock_7/Mlp  (None, 197, 192)    148032      [&#39;dropout_22[0][0]&#39;]
 Block/Dense_1 (QuantizedDense)

 dropout_23 (QuantizedDropout)  (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_7/MlpB
                                                                 lock/Dense_1[0][0]&#39;]

 Transformer/EncoderBlock_7/add  (None, 197, 192)    384         [&#39;Transformer/EncoderBlock_7/add_
 _2 (QuantizedAdd)                                               1[0][0]&#39;,
                                                                  &#39;dropout_23[0][0]&#39;]

 Transformer/EncoderBlock_8/Lay  (None, 197, 192)    768         [&#39;Transformer/EncoderBlock_7/add_
 erNorm_0 (QuantizedLayerNormal                                  2[0][0]&#39;]
 ization)

 Transformer/EncoderBlock_8/Mul  (None, 197, 192)    37058       [&#39;Transformer/EncoderBlock_8/Laye
 tiHeadDotProductAttention_1/qu                                  rNorm_0[0][0]&#39;]
 ery (QuantizedDense)

 Transformer/EncoderBlock_8/Mul  (None, 197, 192)    37058       [&#39;Transformer/EncoderBlock_8/Laye
 tiHeadDotProductAttention_1/ke                                  rNorm_0[0][0]&#39;]
 y (QuantizedDense)

 Transformer/EncoderBlock_8/Mul  (None, 197, 192)    37440       [&#39;Transformer/EncoderBlock_8/Laye
 tiHeadDotProductAttention_1/va                                  rNorm_0[0][0]&#39;]
 lue (QuantizedDense)

 Transformer/EncoderBlock_8/Mul  ((None, 197, 192),  384         [&#39;Transformer/EncoderBlock_8/Mult
 tiHeadDotProductAttention_1/at   (None, 3, 197, 197             iHeadDotProductAttention_1/query[
 tention (QuantizedAttention)   ))                               0][0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_8/Mult
                                                                 iHeadDotProductAttention_1/key[0]
                                                                 [0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_8/Mult
                                                                 iHeadDotProductAttention_1/value[
                                                                 0][0]&#39;]

 Transformer/EncoderBlock_8/Mul  (None, 197, 192)    37440       [&#39;Transformer/EncoderBlock_8/Mult
 tiHeadDotProductAttention_1/ou                                  iHeadDotProductAttention_1/attent
 t (QuantizedDense)                                              ion[0][0]&#39;]

 dropout_24 (QuantizedDropout)  (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_8/Mult
                                                                 iHeadDotProductAttention_1/out[0]
                                                                 [0]&#39;]

 Transformer/EncoderBlock_8/add  (None, 197, 192)    384         [&#39;dropout_24[0][0]&#39;,
 _1 (QuantizedAdd)                                                &#39;Transformer/EncoderBlock_7/add_
                                                                 2[0][0]&#39;]

 Transformer/EncoderBlock_8/Lay  (None, 197, 192)    768         [&#39;Transformer/EncoderBlock_8/add_
 erNorm_2 (QuantizedLayerNormal                                  1[0][0]&#39;]
 ization)

 Transformer/EncoderBlock_8/Mlp  (None, 197, 768)    148224      [&#39;Transformer/EncoderBlock_8/Laye
 Block/Dense_0 (QuantizedDense)                                  rNorm_2[0][0]&#39;]

 Transformer/EncoderBlock_8/Mlp  (None, 197, 768)    1536        [&#39;Transformer/EncoderBlock_8/MlpB
 Block/activation (QuantizedReL                                  lock/Dense_0[0][0]&#39;]
 U)

 dropout_25 (QuantizedDropout)  (None, 197, 768)     0           [&#39;Transformer/EncoderBlock_8/MlpB
                                                                 lock/activation[0][0]&#39;]

 Transformer/EncoderBlock_8/Mlp  (None, 197, 192)    148032      [&#39;dropout_25[0][0]&#39;]
 Block/Dense_1 (QuantizedDense)

 dropout_26 (QuantizedDropout)  (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_8/MlpB
                                                                 lock/Dense_1[0][0]&#39;]

 Transformer/EncoderBlock_8/add  (None, 197, 192)    384         [&#39;Transformer/EncoderBlock_8/add_
 _2 (QuantizedAdd)                                               1[0][0]&#39;,
                                                                  &#39;dropout_26[0][0]&#39;]

 Transformer/EncoderBlock_9/Lay  (None, 197, 192)    768         [&#39;Transformer/EncoderBlock_8/add_
 erNorm_0 (QuantizedLayerNormal                                  2[0][0]&#39;]
 ization)

 Transformer/EncoderBlock_9/Mul  (None, 197, 192)    37058       [&#39;Transformer/EncoderBlock_9/Laye
 tiHeadDotProductAttention_1/qu                                  rNorm_0[0][0]&#39;]
 ery (QuantizedDense)

 Transformer/EncoderBlock_9/Mul  (None, 197, 192)    37058       [&#39;Transformer/EncoderBlock_9/Laye
 tiHeadDotProductAttention_1/ke                                  rNorm_0[0][0]&#39;]
 y (QuantizedDense)

 Transformer/EncoderBlock_9/Mul  (None, 197, 192)    37440       [&#39;Transformer/EncoderBlock_9/Laye
 tiHeadDotProductAttention_1/va                                  rNorm_0[0][0]&#39;]
 lue (QuantizedDense)

 Transformer/EncoderBlock_9/Mul  ((None, 197, 192),  384         [&#39;Transformer/EncoderBlock_9/Mult
 tiHeadDotProductAttention_1/at   (None, 3, 197, 197             iHeadDotProductAttention_1/query[
 tention (QuantizedAttention)   ))                               0][0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_9/Mult
                                                                 iHeadDotProductAttention_1/key[0]
                                                                 [0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_9/Mult
                                                                 iHeadDotProductAttention_1/value[
                                                                 0][0]&#39;]

 Transformer/EncoderBlock_9/Mul  (None, 197, 192)    37440       [&#39;Transformer/EncoderBlock_9/Mult
 tiHeadDotProductAttention_1/ou                                  iHeadDotProductAttention_1/attent
 t (QuantizedDense)                                              ion[0][0]&#39;]

 dropout_27 (QuantizedDropout)  (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_9/Mult
                                                                 iHeadDotProductAttention_1/out[0]
                                                                 [0]&#39;]

 Transformer/EncoderBlock_9/add  (None, 197, 192)    384         [&#39;dropout_27[0][0]&#39;,
 _1 (QuantizedAdd)                                                &#39;Transformer/EncoderBlock_8/add_
                                                                 2[0][0]&#39;]

 Transformer/EncoderBlock_9/Lay  (None, 197, 192)    768         [&#39;Transformer/EncoderBlock_9/add_
 erNorm_2 (QuantizedLayerNormal                                  1[0][0]&#39;]
 ization)

 Transformer/EncoderBlock_9/Mlp  (None, 197, 768)    148224      [&#39;Transformer/EncoderBlock_9/Laye
 Block/Dense_0 (QuantizedDense)                                  rNorm_2[0][0]&#39;]

 Transformer/EncoderBlock_9/Mlp  (None, 197, 768)    1536        [&#39;Transformer/EncoderBlock_9/MlpB
 Block/activation (QuantizedReL                                  lock/Dense_0[0][0]&#39;]
 U)

 dropout_28 (QuantizedDropout)  (None, 197, 768)     0           [&#39;Transformer/EncoderBlock_9/MlpB
                                                                 lock/activation[0][0]&#39;]

 Transformer/EncoderBlock_9/Mlp  (None, 197, 192)    148032      [&#39;dropout_28[0][0]&#39;]
 Block/Dense_1 (QuantizedDense)

 dropout_29 (QuantizedDropout)  (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_9/MlpB
                                                                 lock/Dense_1[0][0]&#39;]

 Transformer/EncoderBlock_9/add  (None, 197, 192)    384         [&#39;Transformer/EncoderBlock_9/add_
 _2 (QuantizedAdd)                                               1[0][0]&#39;,
                                                                  &#39;dropout_29[0][0]&#39;]

 Transformer/EncoderBlock_10/La  (None, 197, 192)    768         [&#39;Transformer/EncoderBlock_9/add_
 yerNorm_0 (QuantizedLayerNorma                                  2[0][0]&#39;]
 lization)

 Transformer/EncoderBlock_10/Mu  (None, 197, 192)    37058       [&#39;Transformer/EncoderBlock_10/Lay
 ltiHeadDotProductAttention_1/q                                  erNorm_0[0][0]&#39;]
 uery (QuantizedDense)

 Transformer/EncoderBlock_10/Mu  (None, 197, 192)    37058       [&#39;Transformer/EncoderBlock_10/Lay
 ltiHeadDotProductAttention_1/k                                  erNorm_0[0][0]&#39;]
 ey (QuantizedDense)

 Transformer/EncoderBlock_10/Mu  (None, 197, 192)    37440       [&#39;Transformer/EncoderBlock_10/Lay
 ltiHeadDotProductAttention_1/v                                  erNorm_0[0][0]&#39;]
 alue (QuantizedDense)

 Transformer/EncoderBlock_10/Mu  ((None, 197, 192),  384         [&#39;Transformer/EncoderBlock_10/Mul
 ltiHeadDotProductAttention_1/a   (None, 3, 197, 197             tiHeadDotProductAttention_1/query
 ttention (QuantizedAttention)  ))                               [0][0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_10/Mul
                                                                 tiHeadDotProductAttention_1/key[0
                                                                 ][0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_10/Mul
                                                                 tiHeadDotProductAttention_1/value
                                                                 [0][0]&#39;]

 Transformer/EncoderBlock_10/Mu  (None, 197, 192)    37440       [&#39;Transformer/EncoderBlock_10/Mul
 ltiHeadDotProductAttention_1/o                                  tiHeadDotProductAttention_1/atten
 ut (QuantizedDense)                                             tion[0][0]&#39;]

 dropout_30 (QuantizedDropout)  (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_10/Mul
                                                                 tiHeadDotProductAttention_1/out[0
                                                                 ][0]&#39;]

 Transformer/EncoderBlock_10/ad  (None, 197, 192)    384         [&#39;dropout_30[0][0]&#39;,
 d_1 (QuantizedAdd)                                               &#39;Transformer/EncoderBlock_9/add_
                                                                 2[0][0]&#39;]

 Transformer/EncoderBlock_10/La  (None, 197, 192)    768         [&#39;Transformer/EncoderBlock_10/add
 yerNorm_2 (QuantizedLayerNorma                                  _1[0][0]&#39;]
 lization)

 Transformer/EncoderBlock_10/Ml  (None, 197, 768)    148224      [&#39;Transformer/EncoderBlock_10/Lay
 pBlock/Dense_0 (QuantizedDense                                  erNorm_2[0][0]&#39;]
 )

 Transformer/EncoderBlock_10/Ml  (None, 197, 768)    1536        [&#39;Transformer/EncoderBlock_10/Mlp
 pBlock/activation (QuantizedRe                                  Block/Dense_0[0][0]&#39;]
 LU)

 dropout_31 (QuantizedDropout)  (None, 197, 768)     0           [&#39;Transformer/EncoderBlock_10/Mlp
                                                                 Block/activation[0][0]&#39;]

 Transformer/EncoderBlock_10/Ml  (None, 197, 192)    148032      [&#39;dropout_31[0][0]&#39;]
 pBlock/Dense_1 (QuantizedDense
 )

 dropout_32 (QuantizedDropout)  (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_10/Mlp
                                                                 Block/Dense_1[0][0]&#39;]

 Transformer/EncoderBlock_10/ad  (None, 197, 192)    384         [&#39;Transformer/EncoderBlock_10/add
 d_2 (QuantizedAdd)                                              _1[0][0]&#39;,
                                                                  &#39;dropout_32[0][0]&#39;]

 Transformer/EncoderBlock_11/La  (None, 197, 192)    768         [&#39;Transformer/EncoderBlock_10/add
 yerNorm_0 (QuantizedLayerNorma                                  _2[0][0]&#39;]
 lization)

 Transformer/EncoderBlock_11/Mu  (None, 197, 192)    37058       [&#39;Transformer/EncoderBlock_11/Lay
 ltiHeadDotProductAttention_1/q                                  erNorm_0[0][0]&#39;]
 uery (QuantizedDense)

 Transformer/EncoderBlock_11/Mu  (None, 197, 192)    37058       [&#39;Transformer/EncoderBlock_11/Lay
 ltiHeadDotProductAttention_1/k                                  erNorm_0[0][0]&#39;]
 ey (QuantizedDense)

 Transformer/EncoderBlock_11/Mu  (None, 197, 192)    37440       [&#39;Transformer/EncoderBlock_11/Lay
 ltiHeadDotProductAttention_1/v                                  erNorm_0[0][0]&#39;]
 alue (QuantizedDense)

 Transformer/EncoderBlock_11/Mu  ((None, 197, 192),  384         [&#39;Transformer/EncoderBlock_11/Mul
 ltiHeadDotProductAttention_1/a   (None, 3, 197, 197             tiHeadDotProductAttention_1/query
 ttention (QuantizedAttention)  ))                               [0][0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_11/Mul
                                                                 tiHeadDotProductAttention_1/key[0
                                                                 ][0]&#39;,
                                                                  &#39;Transformer/EncoderBlock_11/Mul
                                                                 tiHeadDotProductAttention_1/value
                                                                 [0][0]&#39;]

 Transformer/EncoderBlock_11/Mu  (None, 197, 192)    37440       [&#39;Transformer/EncoderBlock_11/Mul
 ltiHeadDotProductAttention_1/o                                  tiHeadDotProductAttention_1/atten
 ut (QuantizedDense)                                             tion[0][0]&#39;]

 dropout_33 (QuantizedDropout)  (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_11/Mul
                                                                 tiHeadDotProductAttention_1/out[0
                                                                 ][0]&#39;]

 Transformer/EncoderBlock_11/ad  (None, 197, 192)    384         [&#39;dropout_33[0][0]&#39;,
 d_1 (QuantizedAdd)                                               &#39;Transformer/EncoderBlock_10/add
                                                                 _2[0][0]&#39;]

 Transformer/EncoderBlock_11/La  (None, 197, 192)    768         [&#39;Transformer/EncoderBlock_11/add
 yerNorm_2 (QuantizedLayerNorma                                  _1[0][0]&#39;]
 lization)

 Transformer/EncoderBlock_11/Ml  (None, 197, 768)    148224      [&#39;Transformer/EncoderBlock_11/Lay
 pBlock/Dense_0 (QuantizedDense                                  erNorm_2[0][0]&#39;]
 )

 Transformer/EncoderBlock_11/Ml  (None, 197, 768)    1536        [&#39;Transformer/EncoderBlock_11/Mlp
 pBlock/activation (QuantizedRe                                  Block/Dense_0[0][0]&#39;]
 LU)

 dropout_34 (QuantizedDropout)  (None, 197, 768)     0           [&#39;Transformer/EncoderBlock_11/Mlp
                                                                 Block/activation[0][0]&#39;]

 Transformer/EncoderBlock_11/Ml  (None, 197, 192)    148032      [&#39;dropout_34[0][0]&#39;]
 pBlock/Dense_1 (QuantizedDense
 )

 dropout_35 (QuantizedDropout)  (None, 197, 192)     0           [&#39;Transformer/EncoderBlock_11/Mlp
                                                                 Block/Dense_1[0][0]&#39;]

 Transformer/EncoderBlock_11/ad  (None, 197, 192)    0           [&#39;Transformer/EncoderBlock_11/add
 d_2 (QuantizedAdd)                                              _1[0][0]&#39;,
                                                                  &#39;dropout_35[0][0]&#39;]

 Transformer/EncoderNorm (Quant  (None, 197, 192)    1152        [&#39;Transformer/EncoderBlock_11/add
 izedBatchNormalization)                                         _2[0][0]&#39;]

 ExtractToken (QuantizedExtract  (None, 192)         0           [&#39;Transformer/EncoderNorm[0][0]&#39;]
 Token)

 Head (QuantizedDense)          (None, 1000)         193000      [&#39;ExtractToken[0][0]&#39;]

 dequantizer (Dequantizer)      (None, 1000)         0           [&#39;Head[0][0]&#39;]

==================================================================================================
Total params: 5,773,528
Trainable params: 5,717,800
Non-trainable params: 55,728
__________________________________________________________________________________________________
</pre></div>
</div>
</section>
</section>
<section id="conversion-to-akida">
<h2>5. Conversion to Akida<a class="headerlink" href="#conversion-to-akida" title="Permalink to this headline"></a></h2>
<p>A model quantized through <a class="reference external" href="../../user_guide/quantizeml.html">QuantizeML python package</a> is ready to be
converted to Akida. Once the quantized model has the desired accuracy <a class="reference external" href="../../user_guide/cnn2snn.html">CNN2SNN toolkit</a>
is used for conversion to Akida. There is no further optimization required and equivalent accuracy is
observed upon converting the model to Akida.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">cnn2snn</span> <span class="kn">import</span> <span class="n">convert</span>

<span class="c1"># Convert the model</span>
<span class="n">model_akida</span> <span class="o">=</span> <span class="n">convert</span><span class="p">(</span><span class="n">model_quantized</span><span class="p">)</span>
<span class="n">model_akida</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>                 Model Summary
________________________________________________
Input shape    Output shape  Sequences  Layers
================================================
[224, 224, 3]  [1, 1, 1000]  1          137
________________________________________________

___________________________________________________________________________________________________________________
Layer (type)                                                                      Output shape   Kernel shape

======================================= SW/Embedding-dequantizer (Software) =======================================

Embedding (Stem)                                                                  [1, 197, 192]  (16, 16, 3, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_0/LayerNorm_0 (MadNorm)                                  [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_0/MultiHeadDotProductAttention_1/query (Dense2D)         [1, 197, 192]  (192, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_0/MultiHeadDotProductAttention_1/key (Dense2D)           [1, 197, 192]  (192, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_0/MultiHeadDotProductAttention_1/value (Dense2D)         [1, 197, 192]  (192, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_0/MultiHeadDotProductAttention_1/attention (Attention)   [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_0/MultiHeadDotProductAttention_1/out (Dense2D)           [1, 197, 192]  (192, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_0/add_1 (Add)                                            [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_0/LayerNorm_2 (MadNorm)                                  [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_0/MlpBlock/Dense_0 (Dense2D)                             [1, 197, 768]  (192, 768)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_0/MlpBlock/Dense_1 (Dense2D)                             [1, 197, 192]  (768, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_0/add_2 (Add)                                            [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_1/LayerNorm_0 (MadNorm)                                  [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_1/MultiHeadDotProductAttention_1/query (Dense2D)         [1, 197, 192]  (192, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_1/MultiHeadDotProductAttention_1/key (Dense2D)           [1, 197, 192]  (192, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_1/MultiHeadDotProductAttention_1/value (Dense2D)         [1, 197, 192]  (192, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_1/MultiHeadDotProductAttention_1/attention (Attention)   [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_1/MultiHeadDotProductAttention_1/out (Dense2D)           [1, 197, 192]  (192, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_1/add_1 (Add)                                            [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_1/LayerNorm_2 (MadNorm)                                  [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_1/MlpBlock/Dense_0 (Dense2D)                             [1, 197, 768]  (192, 768)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_1/MlpBlock/Dense_1 (Dense2D)                             [1, 197, 192]  (768, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_1/add_2 (Add)                                            [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_2/LayerNorm_0 (MadNorm)                                  [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_2/MultiHeadDotProductAttention_1/query (Dense2D)         [1, 197, 192]  (192, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_2/MultiHeadDotProductAttention_1/key (Dense2D)           [1, 197, 192]  (192, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_2/MultiHeadDotProductAttention_1/value (Dense2D)         [1, 197, 192]  (192, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_2/MultiHeadDotProductAttention_1/attention (Attention)   [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_2/MultiHeadDotProductAttention_1/out (Dense2D)           [1, 197, 192]  (192, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_2/add_1 (Add)                                            [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_2/LayerNorm_2 (MadNorm)                                  [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_2/MlpBlock/Dense_0 (Dense2D)                             [1, 197, 768]  (192, 768)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_2/MlpBlock/Dense_1 (Dense2D)                             [1, 197, 192]  (768, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_2/add_2 (Add)                                            [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_3/LayerNorm_0 (MadNorm)                                  [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_3/MultiHeadDotProductAttention_1/query (Dense2D)         [1, 197, 192]  (192, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_3/MultiHeadDotProductAttention_1/key (Dense2D)           [1, 197, 192]  (192, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_3/MultiHeadDotProductAttention_1/value (Dense2D)         [1, 197, 192]  (192, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_3/MultiHeadDotProductAttention_1/attention (Attention)   [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_3/MultiHeadDotProductAttention_1/out (Dense2D)           [1, 197, 192]  (192, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_3/add_1 (Add)                                            [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_3/LayerNorm_2 (MadNorm)                                  [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_3/MlpBlock/Dense_0 (Dense2D)                             [1, 197, 768]  (192, 768)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_3/MlpBlock/Dense_1 (Dense2D)                             [1, 197, 192]  (768, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_3/add_2 (Add)                                            [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_4/LayerNorm_0 (MadNorm)                                  [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_4/MultiHeadDotProductAttention_1/query (Dense2D)         [1, 197, 192]  (192, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_4/MultiHeadDotProductAttention_1/key (Dense2D)           [1, 197, 192]  (192, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_4/MultiHeadDotProductAttention_1/value (Dense2D)         [1, 197, 192]  (192, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_4/MultiHeadDotProductAttention_1/attention (Attention)   [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_4/MultiHeadDotProductAttention_1/out (Dense2D)           [1, 197, 192]  (192, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_4/add_1 (Add)                                            [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_4/LayerNorm_2 (MadNorm)                                  [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_4/MlpBlock/Dense_0 (Dense2D)                             [1, 197, 768]  (192, 768)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_4/MlpBlock/Dense_1 (Dense2D)                             [1, 197, 192]  (768, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_4/add_2 (Add)                                            [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_5/LayerNorm_0 (MadNorm)                                  [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_5/MultiHeadDotProductAttention_1/query (Dense2D)         [1, 197, 192]  (192, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_5/MultiHeadDotProductAttention_1/key (Dense2D)           [1, 197, 192]  (192, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_5/MultiHeadDotProductAttention_1/value (Dense2D)         [1, 197, 192]  (192, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_5/MultiHeadDotProductAttention_1/attention (Attention)   [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_5/MultiHeadDotProductAttention_1/out (Dense2D)           [1, 197, 192]  (192, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_5/add_1 (Add)                                            [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_5/LayerNorm_2 (MadNorm)                                  [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_5/MlpBlock/Dense_0 (Dense2D)                             [1, 197, 768]  (192, 768)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_5/MlpBlock/Dense_1 (Dense2D)                             [1, 197, 192]  (768, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_5/add_2 (Add)                                            [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_6/LayerNorm_0 (MadNorm)                                  [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_6/MultiHeadDotProductAttention_1/query (Dense2D)         [1, 197, 192]  (192, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_6/MultiHeadDotProductAttention_1/key (Dense2D)           [1, 197, 192]  (192, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_6/MultiHeadDotProductAttention_1/value (Dense2D)         [1, 197, 192]  (192, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_6/MultiHeadDotProductAttention_1/attention (Attention)   [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_6/MultiHeadDotProductAttention_1/out (Dense2D)           [1, 197, 192]  (192, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_6/add_1 (Add)                                            [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_6/LayerNorm_2 (MadNorm)                                  [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_6/MlpBlock/Dense_0 (Dense2D)                             [1, 197, 768]  (192, 768)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_6/MlpBlock/Dense_1 (Dense2D)                             [1, 197, 192]  (768, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_6/add_2 (Add)                                            [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_7/LayerNorm_0 (MadNorm)                                  [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_7/MultiHeadDotProductAttention_1/query (Dense2D)         [1, 197, 192]  (192, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_7/MultiHeadDotProductAttention_1/key (Dense2D)           [1, 197, 192]  (192, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_7/MultiHeadDotProductAttention_1/value (Dense2D)         [1, 197, 192]  (192, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_7/MultiHeadDotProductAttention_1/attention (Attention)   [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_7/MultiHeadDotProductAttention_1/out (Dense2D)           [1, 197, 192]  (192, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_7/add_1 (Add)                                            [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_7/LayerNorm_2 (MadNorm)                                  [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_7/MlpBlock/Dense_0 (Dense2D)                             [1, 197, 768]  (192, 768)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_7/MlpBlock/Dense_1 (Dense2D)                             [1, 197, 192]  (768, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_7/add_2 (Add)                                            [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_8/LayerNorm_0 (MadNorm)                                  [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_8/MultiHeadDotProductAttention_1/query (Dense2D)         [1, 197, 192]  (192, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_8/MultiHeadDotProductAttention_1/key (Dense2D)           [1, 197, 192]  (192, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_8/MultiHeadDotProductAttention_1/value (Dense2D)         [1, 197, 192]  (192, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_8/MultiHeadDotProductAttention_1/attention (Attention)   [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_8/MultiHeadDotProductAttention_1/out (Dense2D)           [1, 197, 192]  (192, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_8/add_1 (Add)                                            [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_8/LayerNorm_2 (MadNorm)                                  [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_8/MlpBlock/Dense_0 (Dense2D)                             [1, 197, 768]  (192, 768)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_8/MlpBlock/Dense_1 (Dense2D)                             [1, 197, 192]  (768, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_8/add_2 (Add)                                            [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_9/LayerNorm_0 (MadNorm)                                  [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_9/MultiHeadDotProductAttention_1/query (Dense2D)         [1, 197, 192]  (192, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_9/MultiHeadDotProductAttention_1/key (Dense2D)           [1, 197, 192]  (192, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_9/MultiHeadDotProductAttention_1/value (Dense2D)         [1, 197, 192]  (192, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_9/MultiHeadDotProductAttention_1/attention (Attention)   [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_9/MultiHeadDotProductAttention_1/out (Dense2D)           [1, 197, 192]  (192, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_9/add_1 (Add)                                            [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_9/LayerNorm_2 (MadNorm)                                  [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_9/MlpBlock/Dense_0 (Dense2D)                             [1, 197, 768]  (192, 768)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_9/MlpBlock/Dense_1 (Dense2D)                             [1, 197, 192]  (768, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_9/add_2 (Add)                                            [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_10/LayerNorm_0 (MadNorm)                                 [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_10/MultiHeadDotProductAttention_1/query (Dense2D)        [1, 197, 192]  (192, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_10/MultiHeadDotProductAttention_1/key (Dense2D)          [1, 197, 192]  (192, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_10/MultiHeadDotProductAttention_1/value (Dense2D)        [1, 197, 192]  (192, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_10/MultiHeadDotProductAttention_1/attention (Attention)  [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_10/MultiHeadDotProductAttention_1/out (Dense2D)          [1, 197, 192]  (192, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_10/add_1 (Add)                                           [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_10/LayerNorm_2 (MadNorm)                                 [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_10/MlpBlock/Dense_0 (Dense2D)                            [1, 197, 768]  (192, 768)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_10/MlpBlock/Dense_1 (Dense2D)                            [1, 197, 192]  (768, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_10/add_2 (Add)                                           [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_11/LayerNorm_0 (MadNorm)                                 [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_11/MultiHeadDotProductAttention_1/query (Dense2D)        [1, 197, 192]  (192, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_11/MultiHeadDotProductAttention_1/key (Dense2D)          [1, 197, 192]  (192, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_11/MultiHeadDotProductAttention_1/value (Dense2D)        [1, 197, 192]  (192, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_11/MultiHeadDotProductAttention_1/attention (Attention)  [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_11/MultiHeadDotProductAttention_1/out (Dense2D)          [1, 197, 192]  (192, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_11/add_1 (Add)                                           [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_11/LayerNorm_2 (MadNorm)                                 [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_11/MlpBlock/Dense_0 (Dense2D)                            [1, 197, 768]  (192, 768)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_11/MlpBlock/Dense_1 (Dense2D)                            [1, 197, 192]  (768, 192)
___________________________________________________________________________________________________________________
Transformer/EncoderBlock_11/add_2 (Add)                                           [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
Transformer/EncoderNorm (BatchNormalization)                                      [1, 197, 192]  N/A
___________________________________________________________________________________________________________________
ExtractToken (ExtractToken)                                                       [1, 1, 192]    N/A
___________________________________________________________________________________________________________________
Head (Dense2D)                                                                    [1, 1, 1000]   (192, 1000)
___________________________________________________________________________________________________________________
dequantizer (Dequantizer)                                                         [1, 1, 1000]   N/A
___________________________________________________________________________________________________________________
</pre></div>
</div>
</section>
<section id="displaying-results-attention-maps">
<h2>6. Displaying results Attention Maps<a class="headerlink" href="#displaying-results-attention-maps" title="Permalink to this headline"></a></h2>
<p>Instead of showing predictions, here we propose to show attention maps on an image. This is
derived from <a class="reference external" href="https://arxiv.org/abs/2005.00928">Abnar et al. attention rollout</a> as shown in the
following <a class="reference external" href="https://keras.io/examples/vision/probing_vits/#method-ii-attention-rollout">Keras tutorial</a>. This aims to
highlight the model abilities to focus on relevant parts in the input image.</p>
<p>Just like for the <a class="reference external" href="plot_1_akidanet_imagenet.html#sphx-glr-examples-general-plot-1-akidanet-imagenet-py">AkidaNet example</a>, ImageNet
images are not publicly available, this example uses a set of 10 copyright free images that were
found on Google using ImageNet class names.</p>
<p>Get sample images and preprocess them:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">tensorflow.io</span> <span class="kn">import</span> <span class="n">read_file</span>
<span class="kn">from</span> <span class="nn">tensorflow.image</span> <span class="kn">import</span> <span class="n">decode_jpeg</span>

<span class="kn">from</span> <span class="nn">akida_models.imagenet</span> <span class="kn">import</span> <span class="n">preprocessing</span>

<span class="c1"># Model specification and hyperparameters</span>
<span class="n">NUM_CHANNELS</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">IMAGE_SIZE</span> <span class="o">=</span> <span class="mi">224</span>

<span class="n">NUM_IMAGES</span> <span class="o">=</span> <span class="mi">10</span>

<span class="c1"># Retrieve dataset file from Brainchip data server</span>
<span class="n">file_path</span> <span class="o">=</span> <span class="n">get_file</span><span class="p">(</span>
    <span class="s2">&quot;imagenet_like.zip&quot;</span><span class="p">,</span>
    <span class="s2">&quot;https://data.brainchip.com/dataset-mirror/imagenet_like/imagenet_like.zip&quot;</span><span class="p">,</span>
    <span class="n">cache_subdir</span><span class="o">=</span><span class="s1">&#39;datasets/imagenet_like&#39;</span><span class="p">,</span>
    <span class="n">extract</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">data_folder</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">file_path</span><span class="p">)</span>

<span class="c1"># Load images for test set</span>
<span class="n">x_test_files</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">NUM_IMAGES</span><span class="p">,</span> <span class="n">IMAGE_SIZE</span><span class="p">,</span> <span class="n">IMAGE_SIZE</span><span class="p">,</span> <span class="n">NUM_CHANNELS</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;uint8&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="nb">id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">NUM_IMAGES</span><span class="p">):</span>
    <span class="n">test_file</span> <span class="o">=</span> <span class="s1">&#39;image_&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">id</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">zfill</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;.jpg&#39;</span>
    <span class="n">x_test_files</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_file</span><span class="p">)</span>
    <span class="n">img_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_folder</span><span class="p">,</span> <span class="n">test_file</span><span class="p">)</span>
    <span class="n">base_image</span> <span class="o">=</span> <span class="n">read_file</span><span class="p">(</span><span class="n">img_path</span><span class="p">)</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">decode_jpeg</span><span class="p">(</span><span class="n">base_image</span><span class="p">,</span> <span class="n">channels</span><span class="o">=</span><span class="n">NUM_CHANNELS</span><span class="p">)</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">preprocess_image</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">IMAGE_SIZE</span><span class="p">)</span>
    <span class="n">x_test</span><span class="p">[</span><span class="nb">id</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">NUM_IMAGES</span><span class="si">}</span><span class="s1"> images loaded and preprocessed.&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>10 images loaded and preprocessed.
</pre></div>
</div>
<p>Build and display the attention map for one selected sample:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="kn">from</span> <span class="nn">quantizeml.layers</span> <span class="kn">import</span> <span class="n">ClassToken</span><span class="p">,</span> <span class="n">Attention</span>
<span class="kn">from</span> <span class="nn">quantizeml.tensors</span> <span class="kn">import</span> <span class="n">FixedPoint</span>
<span class="kn">from</span> <span class="nn">quantizeml.models.transforms.transforms_utils</span> <span class="kn">import</span> <span class="n">get_layers_by_type</span>


<span class="k">def</span> <span class="nf">build_attention_map</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">image</span><span class="p">):</span>
    <span class="c1"># Get the Attention layers list</span>
    <span class="n">attentions</span> <span class="o">=</span> <span class="n">get_layers_by_type</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">Attention</span><span class="p">)</span>

    <span class="c1"># Calculate the number of tokens and deduce the grid size</span>
    <span class="n">num_tokens</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">ly</span><span class="p">,</span> <span class="n">ClassToken</span><span class="p">)</span> <span class="k">for</span> <span class="n">ly</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">)</span>
    <span class="n">grid_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">attentions</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">output_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">num_tokens</span><span class="p">))</span>

    <span class="c1"># Get the attention weights from each transformer</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">la</span><span class="o">.</span><span class="n">output</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">la</span> <span class="ow">in</span> <span class="n">attentions</span><span class="p">]</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>

    <span class="c1"># Converts to float if needed</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span><span class="o">.</span><span class="n">to_float</span><span class="p">()</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">FixedPoint</span><span class="p">)</span> <span class="k">else</span> <span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">weights</span><span class="p">]</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>

    <span class="c1"># Heads number</span>
    <span class="n">num_heads</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">num_layers</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">reshaped</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">grid_size</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">grid_size</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>

    <span class="c1"># Average the attention weights across all heads</span>
    <span class="n">reshaped</span> <span class="o">=</span> <span class="n">reshaped</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># To account for residual connections, we add an identity matrix to the attention matrix and</span>
    <span class="c1"># re-normalize the weights.</span>
    <span class="n">reshaped</span> <span class="o">=</span> <span class="n">reshaped</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">reshaped</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">reshaped</span> <span class="o">=</span> <span class="n">reshaped</span> <span class="o">/</span> <span class="n">reshaped</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>

    <span class="c1"># Recursively multiply the weight matrices</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">reshaped</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">reshaped</span><span class="p">)):</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">reshaped</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span> <span class="o">-</span> <span class="n">n</span><span class="p">])</span>

    <span class="c1"># Attention from the output token to the input space</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">grid_size</span><span class="p">,</span> <span class="n">grid_size</span><span class="p">)</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="n">mask</span> <span class="o">/</span> <span class="n">mask</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="p">(</span><span class="n">image</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">image</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))[</span><span class="o">...</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">mask</span> <span class="o">*</span> <span class="n">image</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;uint8&quot;</span><span class="p">)</span>


<span class="c1"># Using a specific image for which attention map is easier to observe</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">x_test</span><span class="p">[</span><span class="mi">8</span><span class="p">]</span>

<span class="c1"># Compute the attention map</span>
<span class="n">attention_float</span> <span class="o">=</span> <span class="n">build_attention_map</span><span class="p">(</span><span class="n">model_keras</span><span class="p">,</span> <span class="n">image</span><span class="p">)</span>
<span class="n">attention_quantized</span> <span class="o">=</span> <span class="n">build_attention_map</span><span class="p">(</span><span class="n">model_quantized</span><span class="p">,</span> <span class="n">image</span><span class="p">)</span>

<span class="c1"># Display the attention map</span>
<span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">,</span> <span class="n">ax3</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Original&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>

<span class="n">ax2</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Float&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">attention_float</span><span class="p">)</span>

<span class="n">ax3</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Quantized&#39;</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">attention_quantized</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Attention masks&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img src="../../_images/sphx_glr_plot_7_vision_transformer_001.png" srcset="../../_images/sphx_glr_plot_7_vision_transformer_001.png" alt="Attention masks, Original, Float, Quantized" class = "sphx-glr-single-img"/><div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>1/1 [==============================] - ETA: 0s
1/1 [==============================] - 2s 2s/step

1/1 [==============================] - ETA: 0s
1/1 [==============================] - 33s 33s/step
</pre></div>
</div>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> ( 2 minutes  48.767 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-examples-general-plot-7-vision-transformer-py">
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/eff8033b2bd356f7a111980b79134365/plot_7_vision_transformer.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">plot_7_vision_transformer.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/05650343d25ad287b6cecb0e9a3a3ef5/plot_7_vision_transformer.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">plot_7_vision_transformer.ipynb</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="plot_6_segmentation.html" class="btn btn-neutral float-left" title="Segmentation tutorial" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../quantization/plot_0_advanced_quantizeml.html" class="btn btn-neutral float-right" title="Advanced QuantizeML tutorial" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, BrainChip Holdings Ltd. All Rights Reserved.</p>
  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>