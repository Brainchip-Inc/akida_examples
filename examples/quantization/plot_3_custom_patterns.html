<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Advanced ONNX models quantization &mdash; Akida Examples  documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/sg_gallery.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/sg_gallery-binder.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/sg_gallery-dataframe.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/sg_gallery-rendered-html.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/design-tabs.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Akida vision edge learning" href="../edge/plot_0_edge_learning_vision.html" />
    <link rel="prev" title="Off-the-shelf models quantization" href="plot_2_off_the_shelf_quantization.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #989898" >

          
          
          <a href="../../index.html">
            
              <img src="../../_static/MetaTF_logo.png" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                Akida, 2nd Generation
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../index.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../installation.html">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../installation.html#supported-configurations">Supported configurations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../installation.html#quick-installation">Quick installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../installation.html#running-examples">Running examples</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../user_guide/user_guide.html">User guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../user_guide/akida.html">Akida user guide</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/akida.html#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/akida.html#programming-interface">Programming interface</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida.html#the-akida-model">The Akida Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida.html#akida-layers">Akida layers</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/akida.html#model-hardware-mapping">Model Hardware Mapping</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida.html#devices">Devices</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida.html#model-mapping">Model mapping</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida.html#advanced-mapping-details-and-hardware-devices-usage">Advanced Mapping Details and Hardware Devices Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida.html#performance-measurement">Performance measurement</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida.html#command-line-interface-for-model-evaluation">Command-line interface for model evaluation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/akida.html#using-akida-edge-learning">Using Akida Edge learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida.html#learning-constraints">Learning constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida.html#compiling-a-layer">Compiling a layer</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../user_guide/quantizeml.html">QuantizeML toolkit</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/quantizeml.html#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/quantizeml.html#the-fixedpoint-representation">The FixedPoint representation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/quantizeml.html#quantization-flow">Quantization flow</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/quantizeml.html#compatibility-constraints">Compatibility constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/quantizeml.html#model-loading">Model loading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/quantizeml.html#command-line-interface">Command line interface</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/quantizeml.html#quantize-cli">quantize CLI</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/quantizeml.html#config-cli">config CLI</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/quantizeml.html#check-cli">check CLI</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/quantizeml.html#insert-rescaling-cli">insert_rescaling CLI</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/quantizeml.html#supported-layer-types">Supported layer types</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/quantizeml.html#keras-support">Keras support</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/quantizeml.html#onnx-support">ONNX support</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/quantizeml.html#analysis-module">Analysis module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/quantizeml.html#kernel-distribution">Kernel distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/quantizeml.html#quantization-error">Quantization error</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/quantizeml.html#metrics">Metrics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/quantizeml.html#command-line">Command line</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../user_guide/cnn2snn.html">CNN2SNN toolkit</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/cnn2snn.html#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/cnn2snn.html#conversion-flow">Conversion flow</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#conversion-compatibility">Conversion compatibility</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#command-line-interface">Command-line interface</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/cnn2snn.html#handling-akida-1-0-and-akida-2-0-specificities">Handling Akida 1.0 and Akida 2.0 specificities</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/cnn2snn.html#legacy-quantization-api">Legacy quantization API</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#typical-quantization-scenario">Typical quantization scenario</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#design-compatibility-constraints">Design compatibility constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#id3">Command-line interface</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#layers-considerations">Layers Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#tips-and-tricks">Tips and Tricks</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../user_guide/akida_models.html">Akida models zoo</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/akida_models.html#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/akida_models.html#command-line-interface-for-model-creation">Command-line interface for model creation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/akida_models.html#command-line-interface-for-model-training">Command-line interface for model training</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida_models.html#kws-training">KWS training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida_models.html#akidanet-training">AkidaNet training</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/akida_models.html#command-line-interface-for-model-evaluation">Command-line interface for model evaluation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/akida_models.html#command-line-interface-to-evaluate-model-macs">Command-line interface to evaluate model MACS</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/akida_models.html#command-line-interface-to-display-summary">Command-line interface to display summary</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/akida_models.html#id1">Layer Blocks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/akida_models.html#handling-akida-1-0-and-akida-2-0-specificities">Handling Akida 1.0 and Akida 2.0 specificities</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../user_guide/engine.html">Akida Engine</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/engine.html#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/engine.html#engine-directory-structure">Engine directory structure</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/engine.html#engine-api-overview">Engine API overview</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/engine.html#hardwaredriver">HardwareDriver</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/engine.html#hardwaredevice">HardwareDevice</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/engine.html#dense">Dense</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/engine.html#shape">Shape</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/engine.html#hwversion">HwVersion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/engine.html#sparse-and-input-conversion-functions">Sparse and Input conversion functions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/engine.html#other-headers-in-the-api">Other headers in the API</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../api_reference/api_reference.html">API reference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../api_reference/akida_apis.html">Akida runtime</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_apis.html#model">Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_apis.html#layer">Layer</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_apis.html#id1">Layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_apis.html#mapping">Mapping</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_apis.html#akida-layers">Akida layers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_apis.html#akida-v1-layers">Akida V1 layers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_apis.html#akida-v2-layers">Akida V2 layers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_apis.html#layer-parameters">Layer parameters</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_apis.html#layertype">LayerType</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_apis.html#padding">Padding</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_apis.html#pooltype">PoolType</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_apis.html#optimizers">Optimizers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_apis.html#sequence">Sequence</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_apis.html#id2">Sequence</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_apis.html#backendtype">BackendType</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_apis.html#pass">Pass</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_apis.html#device">Device</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_apis.html#id3">Device</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_apis.html#hwversion">HwVersion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_apis.html#hwdevice">HWDevice</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_apis.html#id4">HWDevice</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_apis.html#socdriver">SocDriver</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_apis.html#clockmode">ClockMode</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_apis.html#powermeter">PowerMeter</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_apis.html#np">NP</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_apis.html#mapmode">MapMode</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_apis.html#tools">Tools</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_apis.html#sparsity">Sparsity</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html">CNN2SNN</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#akida-version">Akida version</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#conversion">Conversion</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#legacy-quantization-api">Legacy quantization API</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#utils">Utils</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#calibration">Calibration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#transforms">Transforms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#constraint">Constraint</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#quantization">Quantization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#quantizers">Quantizers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#quantized-layers">Quantized layers</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../api_reference/quantizeml_apis.html">QuantizeML</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#layers">Layers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#reshaping">Reshaping</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#activations">Activations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#attention">Attention</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#normalization">Normalization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#convolution">Convolution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#depthwise-convolution">Depthwise convolution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#separable-convolution">Separable convolution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#dense">Dense</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#skip-connection">Skip connection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#pooling">Pooling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#shiftmax">Shiftmax</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#transformers">Transformers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#rescaling">Rescaling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#dropout">Dropout</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#quantizers">Quantizers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#calibration">Calibration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#recording">Recording</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#models">Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#transforms">Transforms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#quantization">Quantization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#quantization-parameters">Quantization parameters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#id1">Calibration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#utils">Utils</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#tensors">Tensors</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#qtensor">QTensor</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#fixedpoint">FixedPoint</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#qfloat">QFloat</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#onnx-support">ONNX support</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#id2">Layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#custom-patterns">Custom patterns</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#model-i-o">Model I/O</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#analysis">Analysis</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#kernel-distribution">Kernel distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#quantization-error">Quantization error</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/quantizeml_apis.html#metrics">Metrics</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../api_reference/akida_models_apis.html">Akida models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_models_apis.html#layer-blocks">Layer blocks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#cnn-blocks">CNN blocks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#transformers-blocks">Transformers blocks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#transposed-blocks">Transposed blocks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#detection-block">Detection block</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_models_apis.html#helpers">Helpers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#gamma-constraint">Gamma constraint</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#unfusing-separableconvolutional">Unfusing SeparableConvolutional</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#extract-samples">Extract samples</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_models_apis.html#knowledge-distillation">Knowledge distillation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_models_apis.html#macs">MACS</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_models_apis.html#model-i-o">Model I/O</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_models_apis.html#utils">Utils</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_models_apis.html#model-zoo">Model zoo</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#akidanet">AkidaNet</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#mobilenet">Mobilenet</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#ds-cnn">DS-CNN</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#vgg">VGG</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#yolo">YOLO</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#pointnet">PointNet++</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#gxnor">GXNOR</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#centernet">CenterNet</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#akidaunet">AkidaUNet</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#transformers">Transformers</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Examples</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../index.html#general-examples">General examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../general/plot_0_global_workflow.html">Global Akida workflow</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_0_global_workflow.html#create-and-train">1. Create and train</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_0_global_workflow.html#quantize">2. Quantize</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_0_global_workflow.html#convert">3. Convert</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../general/plot_1_akidanet_imagenet.html">AkidaNet/ImageNet inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_1_akidanet_imagenet.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_1_akidanet_imagenet.html#pretrained-quantized-model">2. Pretrained quantized model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_1_akidanet_imagenet.html#conversion-to-akida">3. Conversion to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_1_akidanet_imagenet.html#hardware-mapping-and-performance">4. Hardware mapping and performance</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../general/plot_2_ds_cnn_kws.html">DS-CNN/KWS inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_2_ds_cnn_kws.html#load-the-preprocessed-dataset">1. Load the preprocessed dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_2_ds_cnn_kws.html#load-a-pre-trained-native-keras-model">2. Load a pre-trained native Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_2_ds_cnn_kws.html#load-a-pre-trained-quantized-keras-model">3. Load a pre-trained quantized Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_2_ds_cnn_kws.html#conversion-to-akida">4. Conversion to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_2_ds_cnn_kws.html#confusion-matrix">5. Confusion matrix</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../general/plot_3_regression.html">Age estimation (regression) example</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_3_regression.html#load-the-utkface-dataset">1. Load the UTKFace Dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_3_regression.html#load-a-pre-trained-native-keras-model">2. Load a pre-trained native Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_3_regression.html#load-a-pre-trained-quantized-keras-model">3. Load a pre-trained quantized Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_3_regression.html#conversion-to-akida">4. Conversion to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_3_regression.html#estimate-age-on-a-single-image">5. Estimate age on a single image</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../general/plot_4_transfer_learning.html">Transfer learning with AkidaNet for PlantVillage</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_4_transfer_learning.html#transfer-learning-process">Transfer learning process</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_4_transfer_learning.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_4_transfer_learning.html#get-a-trained-akidanet-base-model">2. Get a trained AkidaNet base model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_4_transfer_learning.html#add-a-classification-head-to-the-model">3. Add a classification head to the model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_4_transfer_learning.html#train-for-a-few-epochs">4. Train for a few epochs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_4_transfer_learning.html#quantize-the-model">5. Quantize the model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_4_transfer_learning.html#compute-accuracy">6. Compute accuracy</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../general/plot_5_voc_yolo_detection.html">YOLO/PASCAL-VOC detection tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_5_voc_yolo_detection.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_5_voc_yolo_detection.html#preprocessing-tools">2. Preprocessing tools</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_5_voc_yolo_detection.html#model-architecture">3. Model architecture</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_5_voc_yolo_detection.html#training">4. Training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_5_voc_yolo_detection.html#performance">5. Performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_5_voc_yolo_detection.html#conversion-to-akida">6. Conversion to Akida</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../general/plot_6_segmentation.html">Segmentation tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_6_segmentation.html#load-the-dataset">1. Load the dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_6_segmentation.html#load-a-pre-trained-native-keras-model">2. Load a pre-trained native Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_6_segmentation.html#load-a-pre-trained-quantized-keras-model">3. Load a pre-trained quantized Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_6_segmentation.html#conversion-to-akida">4. Conversion to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_6_segmentation.html#segment-a-single-image">5. Segment a single image</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../general/plot_7_vision_transformer.html">Build Vision Transformers for Akida</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_7_vision_transformer.html#model-selection">1. Model selection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_7_vision_transformer.html#model-optimization-for-akida-hardware">2. Model optimization for Akida hardware</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_7_vision_transformer.html#model-training">3. Model Training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_7_vision_transformer.html#model-quantization">4. Model quantization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_7_vision_transformer.html#conversion-to-akida">5. Conversion to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_7_vision_transformer.html#displaying-results-attention-maps">6. Displaying results Attention Maps</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../general/plot_8_global_pytorch_workflow.html">PyTorch to Akida workflow</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_8_global_pytorch_workflow.html#create-and-train">1. Create and train</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_8_global_pytorch_workflow.html#export">2. Export</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_8_global_pytorch_workflow.html#quantize">3. Quantize</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_8_global_pytorch_workflow.html#convert">4. Convert</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 current"><a class="reference internal" href="../index.html#quantization">Quantization</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="plot_0_advanced_quantizeml.html">Advanced QuantizeML tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="plot_0_advanced_quantizeml.html#defining-a-quantization-scheme">1. Defining a quantization scheme</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_0_advanced_quantizeml.html#calibration">2. Calibration</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="plot_1_upgrading_to_2.0.html">Upgrading to Akida 2.0</a><ul>
<li class="toctree-l4"><a class="reference internal" href="plot_1_upgrading_to_2.0.html#workflow-differences">1. Workflow differences</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_1_upgrading_to_2.0.html#models-architecture-differences">2. Models architecture differences</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_1_upgrading_to_2.0.html#using-akidaversion">3. Using <code class="docutils literal notranslate"><span class="pre">AkidaVersion</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="plot_2_off_the_shelf_quantization.html">Off-the-shelf models quantization</a><ul>
<li class="toctree-l4"><a class="reference internal" href="plot_2_off_the_shelf_quantization.html#workflow-overview">1. Workflow overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_2_off_the_shelf_quantization.html#data-preparation">2. Data preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_2_off_the_shelf_quantization.html#download-and-export">3. Download and export</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_2_off_the_shelf_quantization.html#quantize">4. Quantize</a></li>
</ul>
</li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Advanced ONNX models quantization</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#get-model-and-data">1. Get model and data</a></li>
<li class="toctree-l4"><a class="reference internal" href="#quantize">2. Quantize</a></li>
<li class="toctree-l4"><a class="reference internal" href="#conversion">3. Conversion</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#edge-examples">Edge examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../edge/plot_0_edge_learning_vision.html">Akida vision edge learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../edge/plot_0_edge_learning_vision.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edge/plot_0_edge_learning_vision.html#prepare-akida-model-for-learning">2. Prepare Akida model for learning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edge/plot_0_edge_learning_vision.html#edge-learning-with-akida">3. Edge learning with Akida</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../edge/plot_1_edge_learning_kws.html">Akida edge learning for keyword spotting</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../edge/plot_1_edge_learning_kws.html#edge-learning-process">1. Edge learning process</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edge/plot_1_edge_learning_kws.html#dataset-preparation">2. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edge/plot_1_edge_learning_kws.html#prepare-akida-model-for-learning">3. Prepare Akida model for learning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edge/plot_1_edge_learning_kws.html#learn-with-akida-using-the-training-set">4. Learn with Akida using the training set</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edge/plot_1_edge_learning_kws.html#edge-learning">5. Edge learning</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../edge/plot_2_edge_learning_parameters.html">Tips to set Akida edge learning parameters</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../edge/plot_2_edge_learning_parameters.html#akida-learning-parameters">1. Akida learning parameters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edge/plot_2_edge_learning_parameters.html#create-akida-model">2. Create Akida model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edge/plot_2_edge_learning_parameters.html#estimate-the-required-number-of-weights-of-the-trainable-layer">3. Estimate the required number of weights of the trainable layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edge/plot_2_edge_learning_parameters.html#estimate-the-number-of-neurons-per-class">4. Estimate the number of neurons per class</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#deprecated-cnn2snn-tutorials">[Deprecated] CNN2SNN tutorials</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../cnn2snn/plot_1_advanced_cnn2snn.html">Advanced CNN2SNN tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../cnn2snn/plot_1_advanced_cnn2snn.html#design-a-cnn2snn-quantized-model">1. Design a CNN2SNN quantized model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cnn2snn/plot_1_advanced_cnn2snn.html#weight-quantizer-details">2. Weight Quantizer Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cnn2snn/plot_1_advanced_cnn2snn.html#understanding-quantized-activation">3. Understanding quantized activation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../cnn2snn/plot_1_advanced_cnn2snn.html#how-to-deal-with-too-high-scale-factors">4. How to deal with too high scale factors</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../model_zoo_performance.html">Model zoo performance</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../model_zoo_performance.html#akida-1-0-models">Akida 1.0 models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../model_zoo_performance.html#image-icon-ref-image-domain"> Image domain</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../model_zoo_performance.html#classification">Classification</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../model_zoo_performance.html#object-detection">Object detection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../model_zoo_performance.html#regression">Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../model_zoo_performance.html#face-recognition">Face recognition</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../model_zoo_performance.html#audio-icon-ref-audio-domain"> Audio domain</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../model_zoo_performance.html#keyword-spotting">Keyword spotting</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../model_zoo_performance.html#pointcloud-icon-ref-point-cloud"> Point cloud</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../model_zoo_performance.html#id1">Classification</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../model_zoo_performance.html#akida-2-0-models">Akida 2.0 models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../model_zoo_performance.html#id2"> Image domain</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../model_zoo_performance.html#id3">Classification</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../model_zoo_performance.html#id6">Object detection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../model_zoo_performance.html#id7">Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../model_zoo_performance.html#id8">Face recognition</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../model_zoo_performance.html#segmentation">Segmentation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../model_zoo_performance.html#id10"> Audio domain</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../model_zoo_performance.html#id11">Keyword spotting</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../model_zoo_performance.html#id12">Classification</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../model_zoo_performance.html#id14"> Point cloud</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../model_zoo_performance.html#id15">Classification</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../changelog.html">Changelog</a></li>
<li class="toctree-l1"><a class="reference external" href="https://support.brainchip.com/portal/home">Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../license.html">License</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #989898" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Akida Examples</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">Akida examples</a></li>
      <li class="breadcrumb-item active">Advanced ONNX models quantization</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#sphx-glr-download-examples-quantization-plot-3-custom-patterns-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code</p>
</div>
<section class="sphx-glr-example-title" id="advanced-onnx-models-quantization">
<span id="sphx-glr-examples-quantization-plot-3-custom-patterns-py"></span><h1>Advanced ONNX models quantization<a class="headerlink" href="#advanced-onnx-models-quantization" title="Permalink to this headline"></a></h1>
<p>Akida, like any specialized hardware accelerator, sacrifices very generalized computational
ability in favor of highly optimized implementations of a subset of key operations. While
we strive to make sure that Akida directly supports the most important models, it isn’t
feasible to support all possibilities. You may thus occasionally find yourself with a
model which is very nearly compatible with Akida, but which fails to convert due to just
a few incompatibilities. In this example, we will look at some simple workarounds and how
to implement them. The goal is to successfully convert the model to Akida without having
to retrain.</p>
<p>Preparing a model for Akida requires two steps: quantization, followed by conversion
for a specific target hardware device. We try to catch as many incompatibilities as
possible at the quantization step. However, some constraints depend on the specific
target device, and can only be caught at the conversion step. To illustrate, we will
simply walk through the process of preparing <a class="reference external" href="https://github.com/onnx/models/tree/main/archive/vision/classification/resnet#model">ResNet50</a> for
acceleration on Akida - we’ll run into several incompatibilities at different points
in that process, and see how to resolve them.</p>
<p>This example assumes a moderate level of experience with deep learning, and good familiarity
with the operations typically encountered in these types of models. For example, here we’ll
use the following workarounds:</p>
<ul class="simple">
<li><p>to avoid some incompatible sequences of operations we’ll insert layers with “identity”
convolution kernels,</p></li>
<li><p>in order to avoid an unusual kernel-size 1/stride 2 convolution, we’ll substitute those
kernels with equivalent size 3 kernels.</p></li>
</ul>
<section id="get-model-and-data">
<h2>1. Get model and data<a class="headerlink" href="#get-model-and-data" title="Permalink to this headline"></a></h2>
<p>Before diving into the model incompatibilities and how to resolve them, we’ll need to acquire
some sample data to test on, plus the pretrained model.</p>
<section id="data">
<h3>1.1 Data<a class="headerlink" href="#data" title="Permalink to this headline"></a></h3>
<p>Given that the reference model was trained on <a class="reference external" href="https://www.image-net.org/">ImageNet</a> dataset
(which is not publicly available), this tutorial uses a set of 10 copyright free images.
A helper function <code class="docutils literal notranslate"><span class="pre">imagenet.preprocessing.get_preprocessed_samples</span></code> loads
and preprocesses (decodes, crops and extracts a square 224x224x3 patch from an input image)
these images.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">akida_models.imagenet</span> <span class="kn">import</span> <span class="n">get_preprocessed_samples</span>
<span class="kn">from</span> <span class="nn">akida_models.imagenet.imagenet_utils</span> <span class="kn">import</span> <span class="n">IMAGENET_MEAN</span><span class="p">,</span> <span class="n">IMAGENET_STD</span>

<span class="c1"># Model specification and hyperparameters</span>
<span class="n">NUM_CHANNELS</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">IMAGE_SIZE</span> <span class="o">=</span> <span class="mi">224</span>

<span class="c1"># Load the preprocessed images and their corresponding labels for the test set</span>
<span class="n">x_test_raw</span><span class="p">,</span> <span class="n">labels_test</span> <span class="o">=</span> <span class="n">get_preprocessed_samples</span><span class="p">(</span><span class="n">IMAGE_SIZE</span><span class="p">,</span> <span class="n">NUM_CHANNELS</span><span class="p">)</span>
<span class="n">num_images</span> <span class="o">=</span> <span class="n">x_test_raw</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># Normalize images as models expects</span>
<span class="n">imagenet_mean_255</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">IMAGENET_MEAN</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span> <span class="o">*</span> <span class="mf">255.0</span>
<span class="n">imagenet_std_255</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">IMAGENET_STD</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span> <span class="o">*</span> <span class="mf">255.0</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="p">((</span><span class="n">x_test_raw</span> <span class="o">-</span> <span class="n">imagenet_mean_255</span><span class="p">)</span> <span class="o">/</span> <span class="n">imagenet_std_255</span><span class="p">)</span>

<span class="c1"># Transpose the channels to the first axis as per the default for ONNX models</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">num_images</span><span class="si">}</span><span class="s1"> images and their labels are loaded and preprocessed.&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>10 images and their labels are loaded and preprocessed.
</pre></div>
</div>
</section>
<section id="download-the-model">
<h3>1.2 Download the model<a class="headerlink" href="#download-the-model" title="Permalink to this headline"></a></h3>
<p>We download ResNet50 from the <a class="reference external" href="https://github.com/onnx/models/tree/main/archive/vision/classification">ONNX ZOO</a>,</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">onnx</span>
<span class="kn">import</span> <span class="nn">onnx.hub</span>
<span class="kn">from</span> <span class="nn">onnxruntime</span> <span class="kn">import</span> <span class="n">InferenceSession</span>

<span class="n">onnx_model</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;ResNet50&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Downloading ResNet50 to local path /root/.cache/onnx/hub/validated/vision/classification/resnet/model/af16a04a6ec48ac494065d4439fe9dea590d337b9ca6dc328160ccf04a217b9c_resnet50-v1-7.onnx
</pre></div>
</div>
</section>
<section id="evaluate-model-performance">
<h3>1.3 Evaluate model performance<a class="headerlink" href="#evaluate-model-performance" title="Permalink to this headline"></a></h3>
<p>The <a class="reference external" href="https://onnxruntime.ai">ONNXRuntime</a> package is a cross-platform
accelerator capable of loading and running models described in ONNX format.
We use this framework to evaluate the performance of the loaded ResNet50
model.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For example purposes, we only compute accuracy on 10 images.
Accuracy on the full ImageNet validation set is reported at the end.</p>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">evaluate_onnx_model</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="n">sess</span> <span class="o">=</span> <span class="n">InferenceSession</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">SerializeToString</span><span class="p">())</span>
    <span class="c1"># Calculate outputs by running images through the session</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="p">{</span><span class="n">model</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">input</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">name</span><span class="p">:</span> <span class="n">x_test</span><span class="p">})</span>
    <span class="c1"># The class with the highest score is what we choose as prediction</span>
    <span class="n">predicted</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
    <span class="c1"># Compute the number of valid predictions</span>
    <span class="k">return</span> <span class="nb">int</span><span class="p">((</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">labels_test</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>


<span class="c1"># Evaluate over test dataset</span>
<span class="n">correctly_classified_floating</span> <span class="o">=</span> <span class="n">evaluate_onnx_model</span><span class="p">(</span><span class="n">onnx_model</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Floating point model accuracy: </span><span class="si">{</span><span class="n">correctly_classified_floating</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">num_images</span><span class="si">}</span><span class="s1">.&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Floating point model accuracy: 10/10.
</pre></div>
</div>
</section>
</section>
<section id="quantize">
<h2>2. Quantize<a class="headerlink" href="#quantize" title="Permalink to this headline"></a></h2>
<p>Akida processes integer inputs, activations and weights. Therefore, the first step in
preparing a floating point model to run on Akida is to quantize it using <a class="reference external" href="../../api_reference/quantizeml_apis.html#quantizeml.models.quantize">QuantizeML quantize()</a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Please refer to the <a class="reference external" href="../../user_guide/quantizeml.html">QuantizeML toolkit user guide</a>
and the <a class="reference external" href="plot_0_advanced_quantizeml.html">Advanced QuantizeML tutorial</a> for further details.
In particular here, for simplicity, we pass only the small number of samples we already prepared
for calibration. Typically, you will want to use many more samples for calibration, say 1000 if
you have them available; and not drawn from your test data. The akida_models package provides a
helper function, <a class="reference external" href="../../api_reference/akida_models_apis.html#extract-samples">extract_samples()</a>
which may be helpful in preparing those.</p>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">quantizeml.models</span> <span class="kn">import</span> <span class="n">quantize</span>

<span class="n">model_quantized</span> <span class="o">=</span> <span class="n">quantize</span><span class="p">(</span><span class="n">onnx_model</span><span class="p">,</span> <span class="n">samples</span><span class="o">=</span><span class="n">x_test</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Calibrating with 10/10.0 samples
/usr/local/lib/python3.8/dist-packages/quantizeml/onnx_support/quantization/quantize.py:238: UserWarning: The following nodes were not quantized because their pattern was not found in the scope: [&#39;resnetv17_stage1_activation0 (Relu)&#39;, &#39;resnetv17_stage1_conv4_fwd (Conv)&#39;, &#39;resnetv17_stage1_relu2_fwd (Relu)&#39;, &#39;resnetv17_stage1_conv5_fwd (Conv)&#39;, &#39;resnetv17_stage1_relu3_fwd (Relu)&#39;, &#39;resnetv17_stage1_conv6_fwd (Conv)&#39;, &#39;resnetv17_stage1__plus1 (Add)&#39;, &#39;resnetv17_stage1_activation1 (Relu)&#39;, &#39;resnetv17_stage1_conv7_fwd (Conv)&#39;, &#39;resnetv17_stage1_relu4_fwd (Relu)&#39;, &#39;resnetv17_stage1_conv8_fwd (Conv)&#39;, &#39;resnetv17_stage1_relu5_fwd (Relu)&#39;, &#39;resnetv17_stage1_conv9_fwd (Conv)&#39;, &#39;resnetv17_stage1__plus2 (Add)&#39;, &#39;resnetv17_stage1_activation2 (Relu)&#39;, &#39;resnetv17_stage2_conv0_fwd (Conv)&#39;, &#39;resnetv17_stage2_relu0_fwd (Relu)&#39;, &#39;resnetv17_stage2_conv1_fwd (Conv)&#39;, &#39;resnetv17_stage2_relu1_fwd (Relu)&#39;, &#39;resnetv17_stage2_conv2_fwd (Conv)&#39;, &#39;resnetv17_stage2_conv3_fwd (Conv)&#39;, &#39;resnetv17_stage2__plus0 (Add)&#39;, &#39;resnetv17_stage2_activation0 (Relu)&#39;, &#39;resnetv17_stage2_conv4_fwd (Conv)&#39;, &#39;resnetv17_stage2_relu2_fwd (Relu)&#39;, &#39;resnetv17_stage2_conv5_fwd (Conv)&#39;, &#39;resnetv17_stage2_relu3_fwd (Relu)&#39;, &#39;resnetv17_stage2_conv6_fwd (Conv)&#39;, &#39;resnetv17_stage2__plus1 (Add)&#39;, &#39;resnetv17_stage2_activation1 (Relu)&#39;, &#39;resnetv17_stage2_conv7_fwd (Conv)&#39;, &#39;resnetv17_stage2_relu4_fwd (Relu)&#39;, &#39;resnetv17_stage2_conv8_fwd (Conv)&#39;, &#39;resnetv17_stage2_relu5_fwd (Relu)&#39;, &#39;resnetv17_stage2_conv9_fwd (Conv)&#39;, &#39;resnetv17_stage2__plus2 (Add)&#39;, &#39;resnetv17_stage2_activation2 (Relu)&#39;, &#39;resnetv17_stage2_conv10_fwd (Conv)&#39;, &#39;resnetv17_stage2_relu6_fwd (Relu)&#39;, &#39;resnetv17_stage2_conv11_fwd (Conv)&#39;, &#39;resnetv17_stage2_relu7_fwd (Relu)&#39;, &#39;resnetv17_stage2_conv12_fwd (Conv)&#39;, &#39;resnetv17_stage2__plus3 (Add)&#39;, &#39;resnetv17_stage2_activation3 (Relu)&#39;, &#39;resnetv17_stage3_conv0_fwd (Conv)&#39;, &#39;resnetv17_stage3_relu0_fwd (Relu)&#39;, &#39;resnetv17_stage3_conv1_fwd (Conv)&#39;, &#39;resnetv17_stage3_relu1_fwd (Relu)&#39;, &#39;resnetv17_stage3_conv2_fwd (Conv)&#39;, &#39;resnetv17_stage3_conv3_fwd (Conv)&#39;, &#39;resnetv17_stage3__plus0 (Add)&#39;, &#39;resnetv17_stage3_activation0 (Relu)&#39;, &#39;resnetv17_stage3_conv4_fwd (Conv)&#39;, &#39;resnetv17_stage3_relu2_fwd (Relu)&#39;, &#39;resnetv17_stage3_conv5_fwd (Conv)&#39;, &#39;resnetv17_stage3_relu3_fwd (Relu)&#39;, &#39;resnetv17_stage3_conv6_fwd (Conv)&#39;, &#39;resnetv17_stage3__plus1 (Add)&#39;, &#39;resnetv17_stage3_activation1 (Relu)&#39;, &#39;resnetv17_stage3_conv7_fwd (Conv)&#39;, &#39;resnetv17_stage3_relu4_fwd (Relu)&#39;, &#39;resnetv17_stage3_conv8_fwd (Conv)&#39;, &#39;resnetv17_stage3_relu5_fwd (Relu)&#39;, &#39;resnetv17_stage3_conv9_fwd (Conv)&#39;, &#39;resnetv17_stage3__plus2 (Add)&#39;, &#39;resnetv17_stage3_activation2 (Relu)&#39;, &#39;resnetv17_stage3_conv10_fwd (Conv)&#39;, &#39;resnetv17_stage3_relu6_fwd (Relu)&#39;, &#39;resnetv17_stage3_conv11_fwd (Conv)&#39;, &#39;resnetv17_stage3_relu7_fwd (Relu)&#39;, &#39;resnetv17_stage3_conv12_fwd (Conv)&#39;, &#39;resnetv17_stage3__plus3 (Add)&#39;, &#39;resnetv17_stage3_activation3 (Relu)&#39;, &#39;resnetv17_stage3_conv13_fwd (Conv)&#39;, &#39;resnetv17_stage3_relu8_fwd (Relu)&#39;, &#39;resnetv17_stage3_conv14_fwd (Conv)&#39;, &#39;resnetv17_stage3_relu9_fwd (Relu)&#39;, &#39;resnetv17_stage3_conv15_fwd (Conv)&#39;, &#39;resnetv17_stage3__plus4 (Add)&#39;, &#39;resnetv17_stage3_activation4 (Relu)&#39;, &#39;resnetv17_stage3_conv16_fwd (Conv)&#39;, &#39;resnetv17_stage3_relu10_fwd (Relu)&#39;, &#39;resnetv17_stage3_conv17_fwd (Conv)&#39;, &#39;resnetv17_stage3_relu11_fwd (Relu)&#39;, &#39;resnetv17_stage3_conv18_fwd (Conv)&#39;, &#39;resnetv17_stage3__plus5 (Add)&#39;, &#39;resnetv17_stage3_activation5 (Relu)&#39;, &#39;resnetv17_stage4_conv0_fwd (Conv)&#39;, &#39;resnetv17_stage4_relu0_fwd (Relu)&#39;, &#39;resnetv17_stage4_conv1_fwd (Conv)&#39;, &#39;resnetv17_stage4_relu1_fwd (Relu)&#39;, &#39;resnetv17_stage4_conv2_fwd (Conv)&#39;, &#39;resnetv17_stage4_conv3_fwd (Conv)&#39;, &#39;resnetv17_stage4__plus0 (Add)&#39;, &#39;resnetv17_stage4_activation0 (Relu)&#39;, &#39;resnetv17_stage4_conv4_fwd (Conv)&#39;, &#39;resnetv17_stage4_relu2_fwd (Relu)&#39;, &#39;resnetv17_stage4_conv5_fwd (Conv)&#39;, &#39;resnetv17_stage4_relu3_fwd (Relu)&#39;, &#39;resnetv17_stage4_conv6_fwd (Conv)&#39;, &#39;resnetv17_stage4__plus1 (Add)&#39;, &#39;resnetv17_stage4_activation1 (Relu)&#39;, &#39;resnetv17_stage4_conv7_fwd (Conv)&#39;, &#39;resnetv17_stage4_relu4_fwd (Relu)&#39;, &#39;resnetv17_stage4_conv8_fwd (Conv)&#39;, &#39;resnetv17_stage4_relu5_fwd (Relu)&#39;, &#39;resnetv17_stage4_conv9_fwd (Conv)&#39;, &#39;resnetv17_stage4__plus2 (Add)&#39;, &#39;resnetv17_stage4_activation2 (Relu)&#39;, &#39;resnetv17_pool1_fwd (GlobalAveragePool)&#39;, &#39;flatten_473 (Flatten)&#39;, &#39;resnetv17_dense0_fwd (Gemm)&#39;].
  warnings.warn(&quot;The following nodes were not quantized because their pattern &quot;
</pre></div>
</div>
<p>We can see that the model is not fully quantized, stopping at the first unrecognized
pattern (node <code class="docutils literal notranslate"><span class="pre">resnetv17_stage1_activation0</span> <span class="pre">(Relu)</span></code>). We know that Akida can definitely
handle ReLU activation functions, so we have to look more closely to understand the
problem. Analyzing the model, the ReLU immediately follows an <code class="docutils literal notranslate"><span class="pre">Add</span></code> operator. It is
this sequence of operations which is not supported by Akida.</p>
<figure class="align-center" id="id2">
<a class="reference external image-reference" href="../../_images/unsupported_activation.png"><img alt="Unsupported activation" src="../../_images/unsupported_activation.png" style="width: 794.5px; height: 251.99999999999997px;" /></a>
<figcaption>
<p><span class="caption-text">Unsupported pattern: [<code class="docutils literal notranslate"><span class="pre">Add</span></code>, <code class="docutils literal notranslate"><span class="pre">Relu</span></code>].</span><a class="headerlink" href="#id2" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<section id="about-patterns">
<h3>2.1 About Patterns<a class="headerlink" href="#about-patterns" title="Permalink to this headline"></a></h3>
<p>For efficiency, Akida hardware actually groups certain commonly occuring
operations together. For example, ReLU activation functions, where present,
are almost always applied on the outputs of the hard-working computational
layers (Convolutions, Depthwise Convolutions, Dense layers etc.). So the ReLU
on Akida is tied to those operations. While efficient, this does mean that
some sequences of operations will not by default be considered Akida-compatible,
even though the individual operations are known to be handled. That’s the
cause of the problem encountered here.</p>
<p>To properly see what’s going on, and to resolve the problem, we’ll need to
understand the concept of “patterns”. These are the objects that QuantizeML
uses to map ONNX models to their Akida equivalents. A pattern is a sequence of
continuous <a class="reference external" href="https://onnx.ai/onnx/operators/">ONNX operators</a> in a graph that
<strong>can be converted</strong> to an
<a class="reference external" href="../../api_reference/akida_apis.html#akida-v2-layers">Akida V2 layer</a>.
For example, the following model would be converted to an <a class="reference external" href="../../api_reference/akida_apis.html#akida.InputConv2D">akida.InputConv2D</a> layer:</p>
<figure class="align-center" id="id3">
<a class="reference external image-reference" href="../../_images/onnx_input_conv2d.png"><img alt="InputConv2D example model" src="../../_images/onnx_input_conv2d.png" style="width: 114.4px; height: 349.6px;" /></a>
<figcaption>
<p><span class="caption-text">One ONNX configuration that would map to an <a class="reference external" href="../../api_reference/akida_apis.html#akida.InputConv2D">InputConv2D</a>.</span><a class="headerlink" href="#id3" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<p>The sequence of operators [<code class="docutils literal notranslate"><span class="pre">Conv</span></code>, <code class="docutils literal notranslate"><span class="pre">Clip</span></code>, <code class="docutils literal notranslate"><span class="pre">MaxPool</span></code>] <strong>is one valid pattern</strong>
for conversion towards <a class="reference external" href="../../api_reference/akida_apis.html#akida.InputConv2D">InputConv2D</a>.</p>
<p>Crucially, we can check the list of the currently supported patterns:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">quantizeml.onnx_support.quantization.register_patterns</span> <span class="kn">import</span> <span class="n">PATTERNS_MAP</span>

<span class="nb">print</span><span class="p">(</span><span class="o">*</span><span class="n">PATTERNS_MAP</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>QuantizerPattern(pattern=(&#39;Conv&#39;, &#39;Relu&#39;, &#39;GlobalAveragePool&#39;), f=&lt;function get_qconv at 0x7f83e6af0280&gt;)
QuantizerPattern(pattern=(&#39;Conv&#39;, &#39;Relu&#39;, &#39;MaxPool&#39;), f=&lt;function get_qconv at 0x7f83e6af0280&gt;)
QuantizerPattern(pattern=(&#39;Conv&#39;, &#39;GlobalAveragePool&#39;), f=&lt;function get_qconv at 0x7f83e6af0280&gt;)
QuantizerPattern(pattern=(&#39;Conv&#39;, &#39;Relu&#39;), f=&lt;function get_qconv at 0x7f83e6af0280&gt;)
QuantizerPattern(pattern=(&#39;Conv&#39;,), f=&lt;function get_qconv at 0x7f83e6af0280&gt;)
QuantizerPattern(pattern=(&#39;DepthwiseConv&#39;, &#39;Relu&#39;), f=&lt;function get_qdepthwise at 0x7f83e6b01700&gt;)
QuantizerPattern(pattern=(&#39;DepthwiseConv&#39;,), f=&lt;function get_qdepthwise at 0x7f83e6b01700&gt;)
QuantizerPattern(pattern=(&#39;Flatten&#39;, &#39;Gemm&#39;, &#39;Relu&#39;), f=&lt;function get_qgemm at 0x7f83e6b01a60&gt;)
QuantizerPattern(pattern=(&#39;Flatten&#39;, &#39;Gemm&#39;), f=&lt;function get_qgemm at 0x7f83e6b01a60&gt;)
QuantizerPattern(pattern=(&#39;Gemm&#39;, &#39;Relu&#39;), f=&lt;function get_qgemm at 0x7f83e6b01a60&gt;)
QuantizerPattern(pattern=(&#39;Gemm&#39;,), f=&lt;function get_qgemm at 0x7f83e6b01a60&gt;)
QuantizerPattern(pattern=(&#39;Add&#39;,), f=&lt;function get_qadd at 0x7f83e6b01e50&gt;)
</pre></div>
</div>
<p>Looking at that list, it should be apparent that a <code class="docutils literal notranslate"><span class="pre">ReLU</span></code> operation on its own or
following an <code class="docutils literal notranslate"><span class="pre">Add</span></code> is not considered a compatible pattern.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Before the conversion the following changes are automatically done to allow the
QuantizeML toolkit to see an ONNX graph suitable for quantization:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>transforms the following operators for general purposes:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Conv</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">DepthwiseConv</span></code> when kernel size is 1 x Kx x Ky and <code class="docutils literal notranslate"><span class="pre">group</span></code> is required</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Clip</span></code> &gt; <code class="docutils literal notranslate"><span class="pre">Relu</span></code> (if <code class="docutils literal notranslate"><span class="pre">min</span> <span class="pre">=</span> <span class="pre">0.0</span></code>)</p></li>
</ul>
</li>
<li><p>uses <a class="reference external" href="https://onnxruntime.ai/docs/performance/model-optimizations/graph-optimizations.html">Graph Optimizations in ONNX Runtime</a>
to optimize the graph (e.g. fuse BatchNorm into convolutions).</p></li>
</ol>
</div></blockquote>
</div>
</section>
<section id="custom-quantization-patterns">
<h3>2.2. Custom quantization patterns<a class="headerlink" href="#custom-quantization-patterns" title="Permalink to this headline"></a></h3>
<p>The existing patterns won’t allow us to map an isolated ReLU operation. But, for example,
the ReLU operation can be mapped when following a Conv layer, and we can easily implement
a Conv layer that performs an identity operation on its inputs, just by setting the kernel
weights appropriately. We can implement this workaround by using custom quantization
patterns to extend <code class="docutils literal notranslate"><span class="pre">PATTERNS_MAP</span></code>.</p>
<p>Every pattern includes an ONNX layer that stores the ONNX graph information for the matching
sequence of nodes. QuantizeML also allows for a function to create a compatible layer from
an initially incompatible pattern. This pattern function has two input parameters: the graph
and the pattern-matched sequence of nodes extracted from it.</p>
<p>Once a pattern function is defined for an unsupported pattern, both can be appended
in the quantization context through the <code class="docutils literal notranslate"><span class="pre">custom_pattern_scope</span></code> function.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">quantizeml.onnx_support</span> <span class="kn">import</span> <span class="n">layers</span>
<span class="kn">from</span> <span class="nn">quantizeml.onnx_support.quantization</span> <span class="kn">import</span> <span class="n">custom_pattern_scope</span>


<span class="k">class</span> <span class="nc">IdentityQuantizedConv2D</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">QuantizedConv2D</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__build__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ts</span><span class="p">,</span> <span class="n">downscale</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="c1"># Produces a kernel such that the convolution does not modify the input.</span>
        <span class="n">identity_kernel</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">input_ts</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;float32&quot;</span><span class="p">)[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_weight</span><span class="p">(</span><span class="s2">&quot;kernel&quot;</span><span class="p">,</span> <span class="n">identity_kernel</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__build__</span><span class="p">(</span><span class="n">input_ts</span><span class="p">,</span> <span class="n">downscale</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">relu_pattern_fn</span><span class="p">(</span><span class="n">block_nodes</span><span class="p">,</span> <span class="n">graph</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Convert the incompatible patterns [&#39;Relu&#39;] and [&#39;Relu&#39;, &#39;GlobalAveragePool&#39;] into</span>
<span class="sd">    an IdentityQuantizedConv2D.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Note that as &#39;quantization_pattern_map&#39; is written, this function expects to receive</span>
    <span class="c1"># only the isolated (&#39;Relu&#39;) that matches in the graph.</span>
    <span class="n">block_ops</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">op_type</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">block_nodes</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">block_ops</span> <span class="o">==</span> <span class="p">[</span><span class="s1">&#39;Relu&#39;</span><span class="p">]:</span>
        <span class="k">return</span> <span class="n">IdentityQuantizedConv2D</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unrecognized pattern: </span><span class="si">{</span><span class="n">block_ops</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


<span class="c1"># Define a custom patterns map, as a new pattern and associated replacement function.</span>
<span class="n">relu_pattern_map</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;Relu&quot;</span><span class="p">:</span> <span class="n">relu_pattern_fn</span><span class="p">,</span>
<span class="p">}</span>

<span class="c1"># Include relu_pattern_map in the quantization context</span>
<span class="k">with</span> <span class="n">custom_pattern_scope</span><span class="p">(</span><span class="n">relu_pattern_map</span><span class="p">):</span>
    <span class="n">model_quantized</span> <span class="o">=</span> <span class="n">quantize</span><span class="p">(</span><span class="n">onnx_model</span><span class="p">,</span> <span class="n">samples</span><span class="o">=</span><span class="n">x_test</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Calibrating with 10/10.0 samples
/usr/local/lib/python3.8/dist-packages/quantizeml/onnx_support/quantization/quantize.py:238: UserWarning: The following nodes were not quantized because their pattern was not found in the scope: [&#39;resnetv17_pool1_fwd (GlobalAveragePool)&#39;, &#39;flatten_473 (Flatten)&#39;, &#39;resnetv17_dense0_fwd (Gemm)&#39;].
  warnings.warn(&quot;The following nodes were not quantized because their pattern &quot;
</pre></div>
</div>
<p>With the isolated ReLU fixed, we managed to quantize much more of the model, but
we hit a new problem, node <code class="docutils literal notranslate"><span class="pre">resnetv17_pool1_fwd</span> <span class="pre">(GlobalAveragePool)</span></code>. Looking back
at the list of compatible patterns, we can see that, like the ReLU, a GlobalAveragePooling
(GAP) operation cannot be handled in isolation, but is compatible when it follows
Conv or Conv + ReLU operations. The second of those will suit us better here,
that way we can combine it with our solution for the ReLU operation (because
the GAP here does indeed follow one of the isolated ReLU ops).</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">activation_pattern_fn</span><span class="p">(</span><span class="n">block_nodes</span><span class="p">,</span> <span class="n">graph</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Convert the incompatible patterns [&#39;Relu&#39;] and [&#39;Relu&#39;, &#39;GlobalAveragePool&#39;] into</span>
<span class="sd">    an IdentityQuantizedConv2D.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Note that as &#39;quantization_pattern_map&#39; is written, this function expects to receive</span>
    <span class="c1"># only the sequences (&#39;Relu&#39;) or (&#39;Relu&#39;, &#39;GlobalAveragePool&#39;).</span>
    <span class="n">block_ops</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">op_type</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">block_nodes</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">block_ops</span> <span class="o">==</span> <span class="p">[</span><span class="s1">&#39;Relu&#39;</span><span class="p">]:</span>
        <span class="k">return</span> <span class="n">IdentityQuantizedConv2D</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">block_ops</span> <span class="o">==</span> <span class="p">[</span><span class="s1">&#39;Relu&#39;</span><span class="p">,</span> <span class="s1">&#39;GlobalAveragePool&#39;</span><span class="p">]:</span>
        <span class="k">return</span> <span class="n">IdentityQuantizedConv2D</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">pool_type</span><span class="o">=</span><span class="s2">&quot;gap&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unrecognized pattern: </span><span class="si">{</span><span class="n">block_ops</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


<span class="c1"># Define quantization custom patterns map, as a set of patterns and associated replacement function.</span>
<span class="c1"># activation_pattern_fn was designed to handle two similar incompatibilities present in ResNet50.</span>
<span class="n">quantization_pattern_map</span> <span class="o">=</span> <span class="p">{</span>
    <span class="p">(</span><span class="s2">&quot;Relu&quot;</span><span class="p">,</span> <span class="s2">&quot;GlobalAveragePool&quot;</span><span class="p">):</span> <span class="n">activation_pattern_fn</span><span class="p">,</span>
    <span class="s2">&quot;Relu&quot;</span><span class="p">:</span> <span class="n">activation_pattern_fn</span><span class="p">,</span>
<span class="p">}</span>

<span class="c1"># Include quantization_pattern_map in the quantization context</span>
<span class="k">with</span> <span class="n">custom_pattern_scope</span><span class="p">(</span><span class="n">quantization_pattern_map</span><span class="p">):</span>
    <span class="n">model_quantized</span> <span class="o">=</span> <span class="n">quantize</span><span class="p">(</span><span class="n">onnx_model</span><span class="p">,</span> <span class="n">samples</span><span class="o">=</span><span class="n">x_test</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Calibrating with 10/10.0 samples
</pre></div>
</div>
<p>The full model is now quantized successfully.
At this point we can re-check its accuracy:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">correctly_classified</span> <span class="o">=</span> <span class="n">evaluate_onnx_model</span><span class="p">(</span><span class="n">model_quantized</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Quantized model accuracy: </span><span class="si">{</span><span class="n">correctly_classified</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">num_images</span><span class="si">}</span><span class="s1">.&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Quantized model accuracy: 10/10.
</pre></div>
</div>
</section>
</section>
<section id="conversion">
<h2>3. Conversion<a class="headerlink" href="#conversion" title="Permalink to this headline"></a></h2>
<section id="incompatibility-at-conversion">
<h3>3.1. Incompatibility at Conversion<a class="headerlink" href="#incompatibility-at-conversion" title="Permalink to this headline"></a></h3>
<p>As indicated above, while most imcompatibilities will be picked up at the
quantization step, some constraints are specific to the target hardware
device, and can only be applied at the conversion step. We can detect these
either with the <a class="reference external" href="../../api_reference/cnn2snn_apis.html#cnn2snn.check_model_compatibility">check_model_compatibility</a> tool,
or by trying to <a class="reference external" href="../../api_reference/cnn2snn_apis.html#cnn2snn.convert">convert the model into Akida</a>.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">cnn2snn</span> <span class="kn">import</span> <span class="n">convert</span>

<span class="k">try</span><span class="p">:</span>
    <span class="n">akida_model</span> <span class="o">=</span> <span class="n">convert</span><span class="p">(</span><span class="n">model_quantized</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ResNet50 is not fully accelerated by Akida. Reason: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>ResNet50 is not fully accelerated by Akida. Reason: Expect pads [2, 2, 3, 3] (found [3, 3, 3, 3]) in resnetv17_conv0_fwd.
</pre></div>
</div>
<p>This error is raised because the ResNet50 padding scheme is very specific and differs
from the Keras/Akida standard.</p>
<p>Ideally, we should aim to swap incompatible operations with mathematically
equivalent replacements. For issues of convolution kernel size or padding, we can
often achieve that by putting the kernel weights within a larger kernel, placed
eccentrically to compensate for any padding issues etc. More on that below - but
we can’t use that strategy here, because the kernel size for this layer (7x7) is
already the maximum supported by the Akida input layer. In this case, we’ll have to
try simply modifying the padding to be Akida-compatible. Because this is the input
layer, we could actually negate that change by padding the input image along two
edges before passing to Akida. However, precisely because this is the very start of
the network, and the consequence is only a single pixel of spatial offset, we might
expect that the impact on model performance will be negligible, and that’s precisely
what we find on testing. So let’s keep things simple in this case: simply replace the
incompatible values with compatible ones.</p>
<p>To achieve this, we’ll again customize the pattern functions to modify the model before
quantization. Rather than try to provide a general solution, we’ll hard code this for
the problem layer:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">quantizeml.onnx_support</span> <span class="kn">import</span> <span class="n">graph_tools</span>


<span class="k">def</span> <span class="nf">align_input_conv_with_akida</span><span class="p">(</span><span class="n">block_nodes</span><span class="p">,</span> <span class="n">graph</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Pattern function that handles convolutions incompatible with Akida</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Recover initial ONNXLayer from block nodes and graph</span>
    <span class="n">qconv</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">get_qconv</span><span class="p">(</span><span class="n">block_nodes</span><span class="p">,</span> <span class="n">graph</span><span class="p">)</span>

    <span class="c1"># Force the pads in first convolution to Akida compatible values</span>
    <span class="k">if</span> <span class="n">qconv</span><span class="o">.</span><span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;resnetv17_conv0_fwd&#39;</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Setting Akida pads in first convolution...&quot;</span><span class="p">)</span>
        <span class="c1"># Note: pads in convolution include spatial dimension</span>
        <span class="n">qconv</span><span class="o">.</span><span class="n">set_weight</span><span class="p">(</span><span class="s2">&quot;pads&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">]))</span>
        <span class="n">graph_tools</span><span class="o">.</span><span class="n">replace_field</span><span class="p">(</span><span class="n">qconv</span><span class="p">,</span> <span class="s2">&quot;pool_pads&quot;</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">qconv</span>


<span class="c1"># Infer intermediate shape: This is required for some custom pattern functions</span>
<span class="n">onnx_model_temp</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">shape_inference</span><span class="o">.</span><span class="n">infer_shapes</span><span class="p">(</span><span class="n">onnx_model</span><span class="p">)</span>

<span class="c1"># Quantize model with custom patterns</span>
<span class="n">quantization_pattern_map</span> <span class="o">=</span> <span class="p">{</span>
    <span class="p">(</span><span class="s2">&quot;Conv&quot;</span><span class="p">,</span> <span class="s2">&quot;Relu&quot;</span><span class="p">,</span> <span class="s2">&quot;MaxPool&quot;</span><span class="p">):</span> <span class="n">align_input_conv_with_akida</span><span class="p">,</span>
    <span class="p">(</span><span class="s2">&quot;Conv&quot;</span><span class="p">,</span> <span class="s2">&quot;Relu&quot;</span><span class="p">):</span> <span class="n">align_input_conv_with_akida</span><span class="p">,</span>
    <span class="p">(</span><span class="s2">&quot;Conv&quot;</span><span class="p">,):</span> <span class="n">align_input_conv_with_akida</span><span class="p">,</span>
    <span class="p">(</span><span class="s2">&quot;Relu&quot;</span><span class="p">,</span> <span class="s2">&quot;GlobalAveragePool&quot;</span><span class="p">):</span> <span class="n">activation_pattern_fn</span><span class="p">,</span>
    <span class="s2">&quot;Relu&quot;</span><span class="p">:</span> <span class="n">activation_pattern_fn</span><span class="p">,</span>
<span class="p">}</span>
<span class="k">with</span> <span class="n">custom_pattern_scope</span><span class="p">(</span><span class="n">quantization_pattern_map</span><span class="p">):</span>
    <span class="n">model_quantized</span> <span class="o">=</span> <span class="n">quantize</span><span class="p">(</span><span class="n">onnx_model_temp</span><span class="p">,</span> <span class="n">samples</span><span class="o">=</span><span class="n">x_test</span><span class="p">)</span>

<span class="c1"># Evaluate quantized model performance</span>
<span class="n">correctly_classified</span> <span class="o">=</span> <span class="n">evaluate_onnx_model</span><span class="p">(</span><span class="n">model_quantized</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Quantized model accuracy: </span><span class="si">{</span><span class="n">correctly_classified</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">num_images</span><span class="si">}</span><span class="s1">.&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Calibrating with 10/10.0 samples
Setting Akida pads in first convolution...
Quantized model accuracy: 10/10.
</pre></div>
</div>
</section>
<section id="successful-conversion">
<h3>3.2. Successful Conversion<a class="headerlink" href="#successful-conversion" title="Permalink to this headline"></a></h3>
<p>Time to check conversion again</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">akida_model</span> <span class="o">=</span> <span class="n">convert</span><span class="p">(</span><span class="n">model_quantized</span><span class="p">)</span>
</pre></div>
</div>
<p>Great - the model is now both quantized successfully, and can be
entirely converted for acceleration on Akida. To check its
performance, we need to bear in mind that</p>
<ol class="arabic simple">
<li><p>images must be numpy-raw, with an 8-bit unsigned integer data type and</p></li>
<li><p>the channel dimension must be in the last dimension.</p></li>
</ol>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Evaluate performance</span>
<span class="n">akida_accuracy</span> <span class="o">=</span> <span class="n">akida_model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">x_test_raw</span><span class="p">,</span> <span class="n">labels_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Akida model accuracy: </span><span class="si">{</span><span class="mi">100</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">akida_accuracy</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1"> %&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Akida model accuracy: 100.00 %
</pre></div>
</div>
</section>
<section id="performance-on-the-full-imagenet-validation-set">
<h3>3.3. Performance on the full ImageNet validation set<a class="headerlink" href="#performance-on-the-full-imagenet-validation-set" title="Permalink to this headline"></a></h3>
<p>The table below summarizes the obtained accuracy at the various stages using the full
ImageNet dataset. Note that forcing pads on the first layer decreases the performance
of the model by 0.445% - as noted, that change could be rendered lossless by padding
the input image prior to sending instead.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 45%" />
<col style="width: 17%" />
<col style="width: 21%" />
<col style="width: 17%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Float accuracy (before Akida adaptation)</p></th>
<th class="head"><p>Float accuracy</p></th>
<th class="head"><p>Quantized accuracy</p></th>
<th class="head"><p>Akida accuracy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>74.368</p></td>
<td><p>73.918</p></td>
<td><p>73.590</p></td>
<td><p>73.620</p></td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The images shown in this tutorial are produced through <a class="reference external" href="https://netron.app/">Netron</a>.</p>
</div>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (0 minutes 37.829 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-examples-quantization-plot-3-custom-patterns-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/0f6914e51d30caef6efa95eb7c8a1624/plot_3_custom_patterns.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">plot_3_custom_patterns.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/cd6ff7b8892e65a0d326fc44875d52ca/plot_3_custom_patterns.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">plot_3_custom_patterns.py</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="plot_2_off_the_shelf_quantization.html" class="btn btn-neutral float-left" title="Off-the-shelf models quantization" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../edge/plot_0_edge_learning_vision.html" class="btn btn-neutral float-right" title="Akida vision edge learning" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, BrainChip Holdings Ltd. All Rights Reserved.</p>
  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>