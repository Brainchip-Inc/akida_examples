

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>CNN conversion flow tutorial &mdash; Akida Examples  documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery-dataframe.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="DS-CNN/KWS inference" href="plot_ds_cnn_kws.html" />
    <link rel="prev" title="Regression tutorial" href="plot_regression.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #3f51b5" >
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/akida.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                Akida 1.8.9
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../index.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../installation.html#requirements">Requirements</a></li>
<li class="toctree-l2"><a class="reference internal" href="../installation.html#quick-installation">Quick installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../installation.html#running-examples">Running examples</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../user_guide/user_guide.html">User guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../user_guide/getting_started.html">Getting started</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/getting_started.html#for-beginners">For beginners</a></li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/getting_started.html#for-users-familiar-with-deep-learning">For users familiar with deep-learning</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../user_guide/aee.html">Akida user guide</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/aee.html#the-akida-execution-engine">The Akida Execution Engine</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#id1">1. The Spiking Neural Network model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#id2">2. Input data format</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#id3">3. Determine training mode</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#id4">4. Interpreting outputs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/aee.html#neural-network-model">Neural Network model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#specifying-the-neural-network-model">Specifying the Neural Network model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#saving-and-loading">Saving and loading</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#input-layer-types">Input layer types</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#data-processing-layer-types">Data-Processing layer types</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/aee.html#id5">Using Akida Unsupervised Learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#learning-constraints">Learning constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#compiling-a-layer">Compiling a layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#id6">Learning parameters</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../user_guide/cnn2snn.html">CNN2SNN toolkit</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/cnn2snn.html#overview">Overview</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#conversion-workflow">Conversion workflow</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#typical-training-scenario">Typical training scenario</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#design-compatibility-constraints">Design compatibility constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#quantization-compatibility-constraints">Quantization compatibility constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#command-line-interface">Command-line interface</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/cnn2snn.html#layers-considerations">Layers Considerations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#supported-layer-types">Supported layer types</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#cnn2snn-quantization-aware-layers">CNN2SNN Quantization-aware layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#training-only-layers">Training-Only Layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#first-layers">First Layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#id6">Final Layers</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/cnn2snn.html#tips-and-tricks">Tips and Tricks</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../user_guide/akida_models.html">Akida models zoo</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/akida_models.html#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/akida_models.html#command-line-interface-for-model-creation">Command-line interface for model creation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/akida_models.html#command-line-interface-for-model-training">Command-line interface for model training</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida_models.html#cifar10-training-and-tuning">CIFAR10 training and tuning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida_models.html#utk-face-training">UTK Face training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida_models.html#kws-training">KWS training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida_models.html#yolo-training">YOLO training</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/akida_models.html#id1">Layer Blocks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida_models.html#conv-block"><code class="docutils literal notranslate"><span class="pre">conv_block</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida_models.html#dense-block"><code class="docutils literal notranslate"><span class="pre">dense_block</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida_models.html#separable-conv-block"><code class="docutils literal notranslate"><span class="pre">separable_conv_block</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../user_guide/hw_constraints.html">Hardware constraints</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/hw_constraints.html#input-layer">Input layer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/hw_constraints.html#data-processing-layers">Data-Processing layers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/hw_constraints.html#convolutional-layer">Convolutional layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/hw_constraints.html#fully-connected-layer">Fully connected layer</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../api_reference/api_reference.html">API reference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../api_reference/aee_apis.html">Akida Execution Engine</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#model">Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#layer">Layer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#layerstatistics">LayerStatistics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#observer">Observer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#inputdata">InputData</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#inputconvolutional">InputConvolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#fullyconnected">FullyConnected</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#convolutional">Convolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#separableconvolutional">SeparableConvolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#dense">Dense</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#sparse">Sparse</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#coords-to-sparse">coords_to_sparse</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#dense-to-sparse">dense_to_sparse</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#packetize">packetize</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#backend">Backend</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#convolutionmode">ConvolutionMode</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#poolingtype">PoolingType</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#learningtype">LearningType</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/aee_apis.html#compatibility">Compatibility</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api_reference/cnn2snn_apis.html">CNN2SNN</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#quantize">quantize</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#quantize-layer">quantize_layer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#convert">convert</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#check-model-compatibility">check_model_compatibility</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#weightquantizer">WeightQuantizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#trainableweightquantizer">TrainableWeightQuantizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#maxquantizer">MaxQuantizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#maxperaxisquantizer">MaxPerAxisQuantizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#weightfloat">WeightFloat</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#quantizedconv2d">QuantizedConv2D</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#quantizeddepthwiseconv2d">QuantizedDepthwiseConv2D</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#quantizeddense">QuantizedDense</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#quantizedseparableconv2d">QuantizedSeparableConv2D</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#activationdiscreterelu">ActivationDiscreteRelu</a></li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/cnn2snn_apis.html#quantizedrelu">QuantizedReLU</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api_reference/akida_models_apis.html">Akida models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_models_apis.html#quantization-blocks">Quantization blocks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#conv-block">conv_block</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#separable-conv-block">separable_conv_block</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#dense-block">dense_block</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_models_apis.html#layer-blocks">Layer blocks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#id1">conv_block</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#id2">separable_conv_block</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#id3">dense_block</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../api_reference/akida_models_apis.html#model-zoo">Model zoo</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#mobilenet">Mobilenet</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#ds-cnn">DS-CNN</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#vgg">VGG</a></li>
<li class="toctree-l4"><a class="reference internal" href="../api_reference/akida_models_apis.html#yolo">YOLO</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Examples</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="plot_gxnor_mnist.html">GXNOR/MNIST inference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="plot_gxnor_mnist.html#loading-the-mnist-dataset">1. Loading the MNIST dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_gxnor_mnist.html#look-at-some-images-from-the-test-dataset">2. Look at some images from the test dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_gxnor_mnist.html#load-the-pre-trained-akida-model">3. Load the pre-trained Akida model</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_gxnor_mnist.html#classify-a-single-image">4. Classify a single image</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_gxnor_mnist.html#check-performance-across-a-number-of-samples">5. Check performance across a number of samples</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="plot_regression.html">Regression tutorial</a><ul>
<li class="toctree-l3"><a class="reference internal" href="plot_regression.html#load-the-dataset">1. Load the dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_regression.html#load-a-pre-trained-native-keras-model">2. Load a pre-trained native Keras model</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_regression.html#load-a-pre-trained-quantized-keras-model-satisfying-akida-nsoc-requirements">3. Load a pre-trained quantized Keras model satisfying Akida NSoC requirements</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_regression.html#conversion-to-akida">4. Conversion to Akida</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_regression.html#estimate-age-on-a-single-image">5. Estimate age on a single image</a></li>
</ul>
</li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">CNN conversion flow tutorial</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#load-and-reshape-mnist-dataset">1. Load and reshape MNIST dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="#model-definition">2. Model definition</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#model-training">3. Model training</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#model-quantization">4. Model quantization</a></li>
<li class="toctree-l3"><a class="reference internal" href="#model-fine-tuning-quantization-aware-training">5. Model fine tuning (quantization-aware training)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#model-conversion">6. Model conversion</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="plot_ds_cnn_kws.html">DS-CNN/KWS inference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="plot_ds_cnn_kws.html#load-the-preprocessed-dataset">1. Load the preprocessed dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_ds_cnn_kws.html#load-a-pre-trained-native-keras-model">2. Load a pre-trained native Keras model</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_ds_cnn_kws.html#load-a-pre-trained-quantized-keras-model-satisfying-akida-nsoc-requirements">3. Load a pre-trained quantized Keras model satisfying Akida NSoC requirements</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_ds_cnn_kws.html#conversion-to-akida">4. Conversion to Akida</a><ul>
<li class="toctree-l4"><a class="reference internal" href="plot_ds_cnn_kws.html#confusion-matrix">5. Confusion matrix</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="plot_ds_cnn_cifar10.html">DS-CNN CIFAR10 inference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="plot_ds_cnn_cifar10.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_ds_cnn_cifar10.html#create-a-keras-ds-cnn-model">2. Create a Keras DS-CNN model</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_ds_cnn_cifar10.html#quantized-model">3. Quantized model</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_ds_cnn_cifar10.html#pretrained-quantized-model">4. Pretrained quantized model</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_ds_cnn_cifar10.html#conversion-to-akida">5. Conversion to Akida</a><ul>
<li class="toctree-l4"><a class="reference internal" href="plot_ds_cnn_cifar10.html#convert-to-akida-model">5.1 Convert to Akida model</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_ds_cnn_cifar10.html#check-hardware-compliancy">5.2 Check hardware compliancy</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_ds_cnn_cifar10.html#check-performance">5.3 Check performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_ds_cnn_cifar10.html#show-predictions-for-a-random-image">5.4 Show predictions for a random image</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="plot_voc_yolo_detection.html">YOLO/PASCAL-VOC detection tutorial</a><ul>
<li class="toctree-l3"><a class="reference internal" href="plot_voc_yolo_detection.html#introduction">1. Introduction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="plot_voc_yolo_detection.html#object-detection">1.1 Object detection</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_voc_yolo_detection.html#yolo-key-concepts">1.2 YOLO key concepts</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="plot_voc_yolo_detection.html#preprocessing-tools">2. Preprocessing tools</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_voc_yolo_detection.html#model-architecture">3. Model architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_voc_yolo_detection.html#training">4. Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_voc_yolo_detection.html#performance">5. Performance</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_voc_yolo_detection.html#conversion-to-akida">6. Conversion to Akida</a><ul>
<li class="toctree-l4"><a class="reference internal" href="plot_voc_yolo_detection.html#convert-to-akida-model">6.1 Convert to Akida model</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_voc_yolo_detection.html#check-performance">6.1 Check performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_voc_yolo_detection.html#show-predictions-for-a-random-image">6.2 Show predictions for a random image</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="plot_transfer_learning.html">Transfer learning with MobileNet for cats vs. dogs</a><ul>
<li class="toctree-l3"><a class="reference internal" href="plot_transfer_learning.html#transfer-learning-process">Transfer learning process</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_transfer_learning.html#load-and-preprocess-data">1. Load and preprocess data</a><ul>
<li class="toctree-l4"><a class="reference internal" href="plot_transfer_learning.html#a-load-and-split-data">1.A - Load and split data</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_transfer_learning.html#b-preprocess-the-test-set">1.B - Preprocess the test set</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_transfer_learning.html#c-get-labels">1.C - Get labels</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="plot_transfer_learning.html#modify-a-pre-trained-base-keras-model">2. Modify a pre-trained base Keras model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="plot_transfer_learning.html#a-instantiate-a-keras-base-model">2.A - Instantiate a Keras base model</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_transfer_learning.html#b-modify-the-network-for-the-new-task">2.B - Modify the network for the new task</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="plot_transfer_learning.html#train-the-transferred-model-for-the-new-task">3. Train the transferred model for the new task</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_transfer_learning.html#quantize-the-top-layer">4 Quantize the top layer</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_transfer_learning.html#convert-to-akida">5. Convert to Akida</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_transfer_learning.html#plot-confusion-matrix">6. Plot confusion matrix</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="plot_mobilenet_imagenet.html">MobileNet/ImageNet inference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="plot_mobilenet_imagenet.html#load-cnn2snn-tool-dependencies">1. Load CNN2SNN tool dependencies</a></li>
<li class="toctree-l3"><a class="reference internal" href="plot_mobilenet_imagenet.html#load-test-images-from-imagenet">2. Load test images from ImageNet</a><ul>
<li class="toctree-l4"><a class="reference internal" href="plot_mobilenet_imagenet.html#load-test-images-and-preprocess-test-images">2.1 Load test images and preprocess test images</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_mobilenet_imagenet.html#load-labels">2.2 Load labels</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="plot_mobilenet_imagenet.html#create-a-quantized-keras-model">3. Create a quantized Keras model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="plot_mobilenet_imagenet.html#instantiate-keras-model">3.1 Instantiate Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_mobilenet_imagenet.html#check-performance-of-the-keras-model">3.2 Check performance of the Keras model</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="plot_mobilenet_imagenet.html#convert-keras-model-for-akida-nsoc">4. Convert Keras model for Akida NSoC</a><ul>
<li class="toctree-l4"><a class="reference internal" href="plot_mobilenet_imagenet.html#convert-keras-model-to-an-akida-compatible-model">4.1 Convert Keras model to an Akida compatible model</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_mobilenet_imagenet.html#test-performance-of-the-akida-model">4.2 Test performance of the Akida model</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_mobilenet_imagenet.html#show-predictions-for-a-random-test-image">4.3 Show predictions for a random test image</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/Brainchip-Inc/akida_examples/releases">Changelog</a></li>
<li class="toctree-l1"><a class="reference external" href="https://support.brainchip.com/portal/home">Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Akida Examples</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="index.html">Akida examples</a> &raquo;</li>
        
      <li>CNN conversion flow tutorial</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p>Click <a class="reference internal" href="#sphx-glr-download-examples-plot-cnn-flow-py"><span class="std std-ref">here</span></a>     to download the full example code</p>
</div>
<div class="sphx-glr-example-title section" id="cnn-conversion-flow-tutorial">
<span id="sphx-glr-examples-plot-cnn-flow-py"></span><h1>CNN conversion flow tutorial<a class="headerlink" href="#cnn-conversion-flow-tutorial" title="Permalink to this headline">¶</a></h1>
<p>This tutorial illustrates how to use the CNN2SNN toolkit to <strong>convert CNN
networks to SNN networks</strong> compatible with the <strong>Akida NSoC</strong> in a few steps.
You can refer to our <a class="reference external" href="https://doc.brainchipinc.com/user_guide/cnn2snn.html">CNN2SNN toolkit user guide</a> for further
explanation.</p>
<p>The CNN2SNN tool is based on Keras, TensorFlow high-level API for building and
training deep learning models.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Please refer to TensorFlow  <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/models">tf.keras.models</a>
module for model creation/import details and <a class="reference external" href="https://www.tensorflow.org/guide">TensorFlow
Guide</a> for details of how
TensorFlow works.</p>
<p>MNIST example below is light enough so you do not need a <a class="reference external" href="https://www.tensorflow.org/install/gpu">GPU</a> to run the CNN2SNN
tool.</p>
</div>
<a class="reference internal image-reference" href="../_images/cnn2snn_flow_small.jpg"><img alt="../_images/cnn2snn_flow_small.jpg" src="../_images/cnn2snn_flow_small.jpg" style="width: 725.9px; height: 170.1px;" /></a>
<div class="section" id="load-and-reshape-mnist-dataset">
<h2>1. Load and reshape MNIST dataset<a class="headerlink" href="#load-and-reshape-mnist-dataset" title="Permalink to this headline">¶</a></h2>
<p>After loading, we make 2 transformations on the dataset:</p>
<ol class="arabic simple">
<li><p>Reshape the sample content data (x values) into a num_samples x width x
height x channels matrix.</p></li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>At this point, we’ll set aside the raw data for testing our
converted model in the Akida Execution Engine later.</p>
</div>
<ol class="arabic simple" start="2">
<li><p>Rescale the 8-bit loaded data to the range 0-to-1 for training.</p></li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Input data normalization is a common step dealing with CNN
(rationale is to keep data in a range that works with selected
optimizers, some reading can be found
<a class="reference external" href="https://www.jeremyjordan.me/batch-normalization/">here</a>.</p>
<p>This shift makes almost no difference in the current example, but
for some datasets rescaling the absolute values (and also shifting
to zero-mean) can make a really major difference.</p>
<p>Also note that we store the scaling values <code class="docutils literal notranslate"><span class="pre">input_scaling</span></code> for
use when preparing the model for the Akida Execution Engine. The
implementation of the Akida neural network allows us to completely
skip the rescaling step (i.e. the Akida model should be fed with
the raw 8-bit values) but that does require information about what
scaling was applied prior to training - see below for more details.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow</span> <span class="kn">import</span> <span class="n">keras</span>

<span class="c1"># Load MNIST dataset</span>
<span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>

<span class="c1"># Reshape x-data</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">60000</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Set aside raw test data for use with Akida Execution Engine later</span>
<span class="n">raw_x_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;uint8&#39;</span><span class="p">)</span>
<span class="n">raw_y_test</span> <span class="o">=</span> <span class="n">y_test</span>

<span class="c1"># Rescale x-data</span>
<span class="n">a</span> <span class="o">=</span> <span class="mi">255</span>
<span class="n">b</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">input_scaling</span> <span class="o">=</span> <span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_train</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span> <span class="o">/</span> <span class="n">a</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_test</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span> <span class="o">/</span> <span class="n">a</span>
</pre></div>
</div>
</div>
<div class="section" id="model-definition">
<h2>2. Model definition<a class="headerlink" href="#model-definition" title="Permalink to this headline">¶</a></h2>
<p>Note that at this stage, there is nothing specific to the Akida NSoC.
This start point is very much a completely standard CNN as defined
within <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras">Keras</a>.</p>
<p>An appropriate model for MNIST (inspired by <a class="reference external" href="https://www.tensorflow.org/model_optimization/guide/quantization/training_example#train_a_model_for_mnist_without_quantization_aware_training">this
example</a>)
might look something like the following:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model_keras</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">MaxPool2D</span><span class="p">(),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">BatchNormalization</span><span class="p">(),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">MaxPool2D</span><span class="p">(</span><span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">BatchNormalization</span><span class="p">(),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="p">],</span> <span class="s1">&#39;mnistnet&#39;</span><span class="p">)</span>

<span class="n">model_keras</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Model: &quot;mnistnet&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d (Conv2D)              (None, 26, 26, 32)        320
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0
_________________________________________________________________
batch_normalization (BatchNo (None, 13, 13, 32)        128
_________________________________________________________________
re_lu (ReLU)                 (None, 13, 13, 32)        0
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 13, 13, 64)        18496
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0
_________________________________________________________________
batch_normalization_1 (Batch (None, 7, 7, 64)          256
_________________________________________________________________
re_lu_1 (ReLU)               (None, 7, 7, 64)          0
_________________________________________________________________
flatten (Flatten)            (None, 3136)              0
_________________________________________________________________
dense (Dense)                (None, 10)                31370
=================================================================
Total params: 50,570
Trainable params: 50,378
Non-trainable params: 192
_________________________________________________________________
</pre></div>
</div>
<p>The model defined above is compatible for conversion into an Akida model, i.e.
the model doesn’t include any layers or operations that aren’t Akida-compatible
(please refer to the <a class="reference external" href="../user_guide/cnn2snn.html">CNN2SNN toolkit</a> documentation for full
details):</p>
<ul class="simple">
<li><p>Standard Conv2D and Dense layers are supported</p></li>
<li><p>Hidden layers must be followed  by a ReLU layer.</p></li>
<li><p>BatchNormalization must always happen before activations.</p></li>
<li><p>Convolutional blocks can optionally be followed by a MaxPooling.</p></li>
</ul>
<div class="section" id="model-training">
<h3>3. Model training<a class="headerlink" href="#model-training" title="Permalink to this headline">¶</a></h3>
<p>Before going any further, train the model and get its performance.
The created model should have achieved a test accuracy a little over 99% after
10 epochs.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model_keras</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span>
    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>

<span class="n">model_keras</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">score</span> <span class="o">=</span> <span class="n">model_keras</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Test score:&#39;</span><span class="p">,</span> <span class="n">score</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Test accuracy:&#39;</span><span class="p">,</span> <span class="n">score</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Epoch 1/10

   1/1688 [..............................] - ETA: 0s - loss: 2.8241 - accuracy: 0.1562
  32/1688 [..............................] - ETA: 2s - loss: 0.8785 - accuracy: 0.7266
  63/1688 [&gt;.............................] - ETA: 2s - loss: 0.5734 - accuracy: 0.8214
  94/1688 [&gt;.............................] - ETA: 2s - loss: 0.4538 - accuracy: 0.8604
 126/1688 [=&gt;............................] - ETA: 2s - loss: 0.3775 - accuracy: 0.8852
 158/1688 [=&gt;............................] - ETA: 2s - loss: 0.3296 - accuracy: 0.8985
 191/1688 [==&gt;...........................] - ETA: 2s - loss: 0.2961 - accuracy: 0.9090
 225/1688 [==&gt;...........................] - ETA: 2s - loss: 0.2713 - accuracy: 0.9168
 258/1688 [===&gt;..........................] - ETA: 2s - loss: 0.2519 - accuracy: 0.9231
 291/1688 [====&gt;.........................] - ETA: 2s - loss: 0.2382 - accuracy: 0.9273
 322/1688 [====&gt;.........................] - ETA: 2s - loss: 0.2274 - accuracy: 0.9308
 354/1688 [=====&gt;........................] - ETA: 2s - loss: 0.2178 - accuracy: 0.9337
 386/1688 [=====&gt;........................] - ETA: 2s - loss: 0.2075 - accuracy: 0.9369
 418/1688 [======&gt;.......................] - ETA: 2s - loss: 0.1971 - accuracy: 0.9400
 450/1688 [======&gt;.......................] - ETA: 1s - loss: 0.1876 - accuracy: 0.9427
 482/1688 [=======&gt;......................] - ETA: 1s - loss: 0.1807 - accuracy: 0.9448
 515/1688 [========&gt;.....................] - ETA: 1s - loss: 0.1753 - accuracy: 0.9468
 549/1688 [========&gt;.....................] - ETA: 1s - loss: 0.1689 - accuracy: 0.9485
 581/1688 [=========&gt;....................] - ETA: 1s - loss: 0.1656 - accuracy: 0.9497
 614/1688 [=========&gt;....................] - ETA: 1s - loss: 0.1602 - accuracy: 0.9509
 646/1688 [==========&gt;...................] - ETA: 1s - loss: 0.1568 - accuracy: 0.9521
 678/1688 [===========&gt;..................] - ETA: 1s - loss: 0.1524 - accuracy: 0.9533
 709/1688 [===========&gt;..................] - ETA: 1s - loss: 0.1496 - accuracy: 0.9540
 741/1688 [============&gt;.................] - ETA: 1s - loss: 0.1467 - accuracy: 0.9551
 773/1688 [============&gt;.................] - ETA: 1s - loss: 0.1451 - accuracy: 0.9557
 805/1688 [=============&gt;................] - ETA: 1s - loss: 0.1417 - accuracy: 0.9567
 837/1688 [=============&gt;................] - ETA: 1s - loss: 0.1387 - accuracy: 0.9575
 868/1688 [==============&gt;...............] - ETA: 1s - loss: 0.1372 - accuracy: 0.9581
 900/1688 [==============&gt;...............] - ETA: 1s - loss: 0.1354 - accuracy: 0.9585
 932/1688 [===============&gt;..............] - ETA: 1s - loss: 0.1327 - accuracy: 0.9593
 964/1688 [================&gt;.............] - ETA: 1s - loss: 0.1302 - accuracy: 0.9601
 996/1688 [================&gt;.............] - ETA: 1s - loss: 0.1282 - accuracy: 0.9607
1028/1688 [=================&gt;............] - ETA: 1s - loss: 0.1267 - accuracy: 0.9611
1060/1688 [=================&gt;............] - ETA: 0s - loss: 0.1251 - accuracy: 0.9616
1092/1688 [==================&gt;...........] - ETA: 0s - loss: 0.1249 - accuracy: 0.9618
1124/1688 [==================&gt;...........] - ETA: 0s - loss: 0.1227 - accuracy: 0.9624
1156/1688 [===================&gt;..........] - ETA: 0s - loss: 0.1209 - accuracy: 0.9629
1187/1688 [====================&gt;.........] - ETA: 0s - loss: 0.1199 - accuracy: 0.9633
1219/1688 [====================&gt;.........] - ETA: 0s - loss: 0.1184 - accuracy: 0.9637
1251/1688 [=====================&gt;........] - ETA: 0s - loss: 0.1165 - accuracy: 0.9642
1283/1688 [=====================&gt;........] - ETA: 0s - loss: 0.1151 - accuracy: 0.9646
1315/1688 [======================&gt;.......] - ETA: 0s - loss: 0.1141 - accuracy: 0.9649
1347/1688 [======================&gt;.......] - ETA: 0s - loss: 0.1134 - accuracy: 0.9651
1379/1688 [=======================&gt;......] - ETA: 0s - loss: 0.1127 - accuracy: 0.9653
1411/1688 [========================&gt;.....] - ETA: 0s - loss: 0.1115 - accuracy: 0.9657
1443/1688 [========================&gt;.....] - ETA: 0s - loss: 0.1102 - accuracy: 0.9662
1476/1688 [=========================&gt;....] - ETA: 0s - loss: 0.1089 - accuracy: 0.9666
1509/1688 [=========================&gt;....] - ETA: 0s - loss: 0.1077 - accuracy: 0.9670
1540/1688 [==========================&gt;...] - ETA: 0s - loss: 0.1062 - accuracy: 0.9674
1572/1688 [==========================&gt;...] - ETA: 0s - loss: 0.1054 - accuracy: 0.9677
1604/1688 [===========================&gt;..] - ETA: 0s - loss: 0.1045 - accuracy: 0.9680
1635/1688 [============================&gt;.] - ETA: 0s - loss: 0.1042 - accuracy: 0.9682
1667/1688 [============================&gt;.] - ETA: 0s - loss: 0.1030 - accuracy: 0.9685
1688/1688 [==============================] - ETA: 0s - loss: 0.1024 - accuracy: 0.9687
1688/1688 [==============================] - 3s 2ms/step - loss: 0.1024 - accuracy: 0.9687 - val_loss: 0.0437 - val_accuracy: 0.9877
Epoch 2/10

   1/1688 [..............................] - ETA: 0s - loss: 0.0115 - accuracy: 1.0000
  35/1688 [..............................] - ETA: 2s - loss: 0.0246 - accuracy: 0.9920
  66/1688 [&gt;.............................] - ETA: 2s - loss: 0.0285 - accuracy: 0.9915
  98/1688 [&gt;.............................] - ETA: 2s - loss: 0.0288 - accuracy: 0.9904
 129/1688 [=&gt;............................] - ETA: 2s - loss: 0.0275 - accuracy: 0.9908
 161/1688 [=&gt;............................] - ETA: 2s - loss: 0.0278 - accuracy: 0.9905
 192/1688 [==&gt;...........................] - ETA: 2s - loss: 0.0301 - accuracy: 0.9902
 223/1688 [==&gt;...........................] - ETA: 2s - loss: 0.0298 - accuracy: 0.9908
 254/1688 [===&gt;..........................] - ETA: 2s - loss: 0.0326 - accuracy: 0.9900
 285/1688 [====&gt;.........................] - ETA: 2s - loss: 0.0342 - accuracy: 0.9899
 316/1688 [====&gt;.........................] - ETA: 2s - loss: 0.0366 - accuracy: 0.9892
 347/1688 [=====&gt;........................] - ETA: 2s - loss: 0.0384 - accuracy: 0.9888
 378/1688 [=====&gt;........................] - ETA: 2s - loss: 0.0398 - accuracy: 0.9883
 410/1688 [======&gt;.......................] - ETA: 2s - loss: 0.0404 - accuracy: 0.9880
 441/1688 [======&gt;.......................] - ETA: 2s - loss: 0.0398 - accuracy: 0.9882
 472/1688 [=======&gt;......................] - ETA: 1s - loss: 0.0404 - accuracy: 0.9879
 504/1688 [=======&gt;......................] - ETA: 1s - loss: 0.0404 - accuracy: 0.9879
 536/1688 [========&gt;.....................] - ETA: 1s - loss: 0.0395 - accuracy: 0.9881
 567/1688 [=========&gt;....................] - ETA: 1s - loss: 0.0395 - accuracy: 0.9879
 599/1688 [=========&gt;....................] - ETA: 1s - loss: 0.0390 - accuracy: 0.9881
 631/1688 [==========&gt;...................] - ETA: 1s - loss: 0.0393 - accuracy: 0.9879
 662/1688 [==========&gt;...................] - ETA: 1s - loss: 0.0393 - accuracy: 0.9879
 694/1688 [===========&gt;..................] - ETA: 1s - loss: 0.0399 - accuracy: 0.9877
 726/1688 [===========&gt;..................] - ETA: 1s - loss: 0.0412 - accuracy: 0.9873
 757/1688 [============&gt;.................] - ETA: 1s - loss: 0.0408 - accuracy: 0.9874
 790/1688 [=============&gt;................] - ETA: 1s - loss: 0.0407 - accuracy: 0.9875
 821/1688 [=============&gt;................] - ETA: 1s - loss: 0.0409 - accuracy: 0.9874
 852/1688 [==============&gt;...............] - ETA: 1s - loss: 0.0419 - accuracy: 0.9871
 884/1688 [==============&gt;...............] - ETA: 1s - loss: 0.0419 - accuracy: 0.9870
 915/1688 [===============&gt;..............] - ETA: 1s - loss: 0.0422 - accuracy: 0.9869
 946/1688 [===============&gt;..............] - ETA: 1s - loss: 0.0420 - accuracy: 0.9869
 977/1688 [================&gt;.............] - ETA: 1s - loss: 0.0414 - accuracy: 0.9871
1008/1688 [================&gt;.............] - ETA: 1s - loss: 0.0409 - accuracy: 0.9872
1040/1688 [=================&gt;............] - ETA: 1s - loss: 0.0414 - accuracy: 0.9871
1071/1688 [==================&gt;...........] - ETA: 0s - loss: 0.0412 - accuracy: 0.9871
1103/1688 [==================&gt;...........] - ETA: 0s - loss: 0.0409 - accuracy: 0.9872
1134/1688 [===================&gt;..........] - ETA: 0s - loss: 0.0406 - accuracy: 0.9872
1165/1688 [===================&gt;..........] - ETA: 0s - loss: 0.0410 - accuracy: 0.9872
1196/1688 [====================&gt;.........] - ETA: 0s - loss: 0.0410 - accuracy: 0.9871
1227/1688 [====================&gt;.........] - ETA: 0s - loss: 0.0418 - accuracy: 0.9869
1258/1688 [=====================&gt;........] - ETA: 0s - loss: 0.0421 - accuracy: 0.9869
1289/1688 [=====================&gt;........] - ETA: 0s - loss: 0.0425 - accuracy: 0.9868
1321/1688 [======================&gt;.......] - ETA: 0s - loss: 0.0425 - accuracy: 0.9868
1353/1688 [=======================&gt;......] - ETA: 0s - loss: 0.0427 - accuracy: 0.9867
1387/1688 [=======================&gt;......] - ETA: 0s - loss: 0.0426 - accuracy: 0.9868
1418/1688 [========================&gt;.....] - ETA: 0s - loss: 0.0426 - accuracy: 0.9867
1450/1688 [========================&gt;.....] - ETA: 0s - loss: 0.0427 - accuracy: 0.9867
1482/1688 [=========================&gt;....] - ETA: 0s - loss: 0.0425 - accuracy: 0.9868
1514/1688 [=========================&gt;....] - ETA: 0s - loss: 0.0422 - accuracy: 0.9869
1546/1688 [==========================&gt;...] - ETA: 0s - loss: 0.0423 - accuracy: 0.9868
1578/1688 [===========================&gt;..] - ETA: 0s - loss: 0.0429 - accuracy: 0.9866
1609/1688 [===========================&gt;..] - ETA: 0s - loss: 0.0432 - accuracy: 0.9866
1640/1688 [============================&gt;.] - ETA: 0s - loss: 0.0435 - accuracy: 0.9865
1672/1688 [============================&gt;.] - ETA: 0s - loss: 0.0431 - accuracy: 0.9866
1688/1688 [==============================] - 3s 2ms/step - loss: 0.0433 - accuracy: 0.9866 - val_loss: 0.0863 - val_accuracy: 0.9775
Epoch 3/10

   1/1688 [..............................] - ETA: 0s - loss: 0.0257 - accuracy: 0.9688
  33/1688 [..............................] - ETA: 2s - loss: 0.0275 - accuracy: 0.9896
  65/1688 [&gt;.............................] - ETA: 2s - loss: 0.0245 - accuracy: 0.9909
  96/1688 [&gt;.............................] - ETA: 2s - loss: 0.0250 - accuracy: 0.9906
 126/1688 [=&gt;............................] - ETA: 2s - loss: 0.0258 - accuracy: 0.9901
 157/1688 [=&gt;............................] - ETA: 2s - loss: 0.0250 - accuracy: 0.9906
 189/1688 [==&gt;...........................] - ETA: 2s - loss: 0.0266 - accuracy: 0.9904
 221/1688 [==&gt;...........................] - ETA: 2s - loss: 0.0254 - accuracy: 0.9910
 252/1688 [===&gt;..........................] - ETA: 2s - loss: 0.0253 - accuracy: 0.9911
 283/1688 [====&gt;.........................] - ETA: 2s - loss: 0.0258 - accuracy: 0.9909
 314/1688 [====&gt;.........................] - ETA: 2s - loss: 0.0255 - accuracy: 0.9911
 346/1688 [=====&gt;........................] - ETA: 2s - loss: 0.0258 - accuracy: 0.9914
 378/1688 [=====&gt;........................] - ETA: 2s - loss: 0.0257 - accuracy: 0.9915
 410/1688 [======&gt;.......................] - ETA: 2s - loss: 0.0255 - accuracy: 0.9915
 440/1688 [======&gt;.......................] - ETA: 2s - loss: 0.0258 - accuracy: 0.9915
 472/1688 [=======&gt;......................] - ETA: 1s - loss: 0.0268 - accuracy: 0.9913
 503/1688 [=======&gt;......................] - ETA: 1s - loss: 0.0270 - accuracy: 0.9912
 535/1688 [========&gt;.....................] - ETA: 1s - loss: 0.0267 - accuracy: 0.9914
 565/1688 [=========&gt;....................] - ETA: 1s - loss: 0.0263 - accuracy: 0.9915
 596/1688 [=========&gt;....................] - ETA: 1s - loss: 0.0265 - accuracy: 0.9914
 628/1688 [==========&gt;...................] - ETA: 1s - loss: 0.0261 - accuracy: 0.9915
 661/1688 [==========&gt;...................] - ETA: 1s - loss: 0.0267 - accuracy: 0.9914
 695/1688 [===========&gt;..................] - ETA: 1s - loss: 0.0268 - accuracy: 0.9914
 727/1688 [===========&gt;..................] - ETA: 1s - loss: 0.0266 - accuracy: 0.9915
 759/1688 [============&gt;.................] - ETA: 1s - loss: 0.0260 - accuracy: 0.9916
 791/1688 [=============&gt;................] - ETA: 1s - loss: 0.0256 - accuracy: 0.9918
 823/1688 [=============&gt;................] - ETA: 1s - loss: 0.0253 - accuracy: 0.9918
 853/1688 [==============&gt;...............] - ETA: 1s - loss: 0.0251 - accuracy: 0.9917
 885/1688 [==============&gt;...............] - ETA: 1s - loss: 0.0248 - accuracy: 0.9918
 917/1688 [===============&gt;..............] - ETA: 1s - loss: 0.0246 - accuracy: 0.9918
 948/1688 [===============&gt;..............] - ETA: 1s - loss: 0.0246 - accuracy: 0.9917
 979/1688 [================&gt;.............] - ETA: 1s - loss: 0.0249 - accuracy: 0.9918
1011/1688 [================&gt;.............] - ETA: 1s - loss: 0.0247 - accuracy: 0.9919
1042/1688 [=================&gt;............] - ETA: 1s - loss: 0.0251 - accuracy: 0.9918
1073/1688 [==================&gt;...........] - ETA: 0s - loss: 0.0254 - accuracy: 0.9918
1104/1688 [==================&gt;...........] - ETA: 0s - loss: 0.0255 - accuracy: 0.9917
1136/1688 [===================&gt;..........] - ETA: 0s - loss: 0.0259 - accuracy: 0.9915
1168/1688 [===================&gt;..........] - ETA: 0s - loss: 0.0263 - accuracy: 0.9913
1199/1688 [====================&gt;.........] - ETA: 0s - loss: 0.0259 - accuracy: 0.9915
1231/1688 [====================&gt;.........] - ETA: 0s - loss: 0.0259 - accuracy: 0.9914
1263/1688 [=====================&gt;........] - ETA: 0s - loss: 0.0261 - accuracy: 0.9915
1295/1688 [======================&gt;.......] - ETA: 0s - loss: 0.0261 - accuracy: 0.9915
1326/1688 [======================&gt;.......] - ETA: 0s - loss: 0.0267 - accuracy: 0.9913
1357/1688 [=======================&gt;......] - ETA: 0s - loss: 0.0271 - accuracy: 0.9911
1389/1688 [=======================&gt;......] - ETA: 0s - loss: 0.0274 - accuracy: 0.9910
1421/1688 [========================&gt;.....] - ETA: 0s - loss: 0.0272 - accuracy: 0.9910
1453/1688 [========================&gt;.....] - ETA: 0s - loss: 0.0273 - accuracy: 0.9910
1485/1688 [=========================&gt;....] - ETA: 0s - loss: 0.0276 - accuracy: 0.9909
1516/1688 [=========================&gt;....] - ETA: 0s - loss: 0.0276 - accuracy: 0.9910
1548/1688 [==========================&gt;...] - ETA: 0s - loss: 0.0283 - accuracy: 0.9908
1580/1688 [===========================&gt;..] - ETA: 0s - loss: 0.0284 - accuracy: 0.9906
1611/1688 [===========================&gt;..] - ETA: 0s - loss: 0.0283 - accuracy: 0.9907
1643/1688 [============================&gt;.] - ETA: 0s - loss: 0.0283 - accuracy: 0.9906
1674/1688 [============================&gt;.] - ETA: 0s - loss: 0.0282 - accuracy: 0.9907
1688/1688 [==============================] - 3s 2ms/step - loss: 0.0282 - accuracy: 0.9906 - val_loss: 0.0397 - val_accuracy: 0.9893
Epoch 4/10

   1/1688 [..............................] - ETA: 0s - loss: 6.4949e-04 - accuracy: 1.0000
  33/1688 [..............................] - ETA: 2s - loss: 0.0350 - accuracy: 0.9905    
  66/1688 [&gt;.............................] - ETA: 2s - loss: 0.0318 - accuracy: 0.9891
  98/1688 [&gt;.............................] - ETA: 2s - loss: 0.0291 - accuracy: 0.9898
 129/1688 [=&gt;............................] - ETA: 2s - loss: 0.0253 - accuracy: 0.9906
 160/1688 [=&gt;............................] - ETA: 2s - loss: 0.0242 - accuracy: 0.9914
 192/1688 [==&gt;...........................] - ETA: 2s - loss: 0.0228 - accuracy: 0.9922
 223/1688 [==&gt;...........................] - ETA: 2s - loss: 0.0222 - accuracy: 0.9922
 255/1688 [===&gt;..........................] - ETA: 2s - loss: 0.0206 - accuracy: 0.9926
 287/1688 [====&gt;.........................] - ETA: 2s - loss: 0.0199 - accuracy: 0.9929
 319/1688 [====&gt;.........................] - ETA: 2s - loss: 0.0195 - accuracy: 0.9931
 350/1688 [=====&gt;........................] - ETA: 2s - loss: 0.0187 - accuracy: 0.9935
 380/1688 [=====&gt;........................] - ETA: 2s - loss: 0.0183 - accuracy: 0.9936
 411/1688 [======&gt;.......................] - ETA: 2s - loss: 0.0177 - accuracy: 0.9938
 443/1688 [======&gt;.......................] - ETA: 1s - loss: 0.0172 - accuracy: 0.9940
 475/1688 [=======&gt;......................] - ETA: 1s - loss: 0.0167 - accuracy: 0.9942
 507/1688 [========&gt;.....................] - ETA: 1s - loss: 0.0171 - accuracy: 0.9941
 539/1688 [========&gt;.....................] - ETA: 1s - loss: 0.0174 - accuracy: 0.9939
 570/1688 [=========&gt;....................] - ETA: 1s - loss: 0.0176 - accuracy: 0.9940
 602/1688 [=========&gt;....................] - ETA: 1s - loss: 0.0182 - accuracy: 0.9938
 635/1688 [==========&gt;...................] - ETA: 1s - loss: 0.0193 - accuracy: 0.9935
 668/1688 [==========&gt;...................] - ETA: 1s - loss: 0.0190 - accuracy: 0.9936
 700/1688 [===========&gt;..................] - ETA: 1s - loss: 0.0188 - accuracy: 0.9937
 731/1688 [===========&gt;..................] - ETA: 1s - loss: 0.0186 - accuracy: 0.9937
 763/1688 [============&gt;.................] - ETA: 1s - loss: 0.0181 - accuracy: 0.9939
 795/1688 [=============&gt;................] - ETA: 1s - loss: 0.0180 - accuracy: 0.9940
 827/1688 [=============&gt;................] - ETA: 1s - loss: 0.0178 - accuracy: 0.9941
 860/1688 [==============&gt;...............] - ETA: 1s - loss: 0.0177 - accuracy: 0.9940
 892/1688 [==============&gt;...............] - ETA: 1s - loss: 0.0177 - accuracy: 0.9940
 923/1688 [===============&gt;..............] - ETA: 1s - loss: 0.0179 - accuracy: 0.9939
 954/1688 [===============&gt;..............] - ETA: 1s - loss: 0.0177 - accuracy: 0.9940
 986/1688 [================&gt;.............] - ETA: 1s - loss: 0.0176 - accuracy: 0.9940
1018/1688 [=================&gt;............] - ETA: 1s - loss: 0.0176 - accuracy: 0.9940
1049/1688 [=================&gt;............] - ETA: 1s - loss: 0.0178 - accuracy: 0.9939
1081/1688 [==================&gt;...........] - ETA: 0s - loss: 0.0180 - accuracy: 0.9938
1112/1688 [==================&gt;...........] - ETA: 0s - loss: 0.0179 - accuracy: 0.9938
1144/1688 [===================&gt;..........] - ETA: 0s - loss: 0.0180 - accuracy: 0.9939
1176/1688 [===================&gt;..........] - ETA: 0s - loss: 0.0180 - accuracy: 0.9939
1208/1688 [====================&gt;.........] - ETA: 0s - loss: 0.0184 - accuracy: 0.9938
1240/1688 [=====================&gt;........] - ETA: 0s - loss: 0.0184 - accuracy: 0.9938
1272/1688 [=====================&gt;........] - ETA: 0s - loss: 0.0187 - accuracy: 0.9937
1304/1688 [======================&gt;.......] - ETA: 0s - loss: 0.0189 - accuracy: 0.9937
1336/1688 [======================&gt;.......] - ETA: 0s - loss: 0.0187 - accuracy: 0.9938
1368/1688 [=======================&gt;......] - ETA: 0s - loss: 0.0185 - accuracy: 0.9939
1400/1688 [=======================&gt;......] - ETA: 0s - loss: 0.0185 - accuracy: 0.9939
1431/1688 [========================&gt;.....] - ETA: 0s - loss: 0.0185 - accuracy: 0.9939
1463/1688 [=========================&gt;....] - ETA: 0s - loss: 0.0190 - accuracy: 0.9937
1495/1688 [=========================&gt;....] - ETA: 0s - loss: 0.0195 - accuracy: 0.9936
1527/1688 [==========================&gt;...] - ETA: 0s - loss: 0.0197 - accuracy: 0.9936
1559/1688 [==========================&gt;...] - ETA: 0s - loss: 0.0197 - accuracy: 0.9936
1590/1688 [===========================&gt;..] - ETA: 0s - loss: 0.0200 - accuracy: 0.9935
1622/1688 [===========================&gt;..] - ETA: 0s - loss: 0.0200 - accuracy: 0.9935
1653/1688 [============================&gt;.] - ETA: 0s - loss: 0.0201 - accuracy: 0.9935
1686/1688 [============================&gt;.] - ETA: 0s - loss: 0.0202 - accuracy: 0.9935
1688/1688 [==============================] - 3s 2ms/step - loss: 0.0202 - accuracy: 0.9935 - val_loss: 0.0355 - val_accuracy: 0.9915
Epoch 5/10

   1/1688 [..............................] - ETA: 0s - loss: 0.0042 - accuracy: 1.0000
  35/1688 [..............................] - ETA: 2s - loss: 0.0145 - accuracy: 0.9929
  67/1688 [&gt;.............................] - ETA: 2s - loss: 0.0142 - accuracy: 0.9939
 100/1688 [&gt;.............................] - ETA: 2s - loss: 0.0160 - accuracy: 0.9934
 132/1688 [=&gt;............................] - ETA: 2s - loss: 0.0157 - accuracy: 0.9934
 164/1688 [=&gt;............................] - ETA: 2s - loss: 0.0144 - accuracy: 0.9937
 196/1688 [==&gt;...........................] - ETA: 2s - loss: 0.0159 - accuracy: 0.9935
 227/1688 [===&gt;..........................] - ETA: 2s - loss: 0.0150 - accuracy: 0.9941
 259/1688 [===&gt;..........................] - ETA: 2s - loss: 0.0142 - accuracy: 0.9947
 291/1688 [====&gt;.........................] - ETA: 2s - loss: 0.0138 - accuracy: 0.9950
 323/1688 [====&gt;.........................] - ETA: 2s - loss: 0.0154 - accuracy: 0.9947
 355/1688 [=====&gt;........................] - ETA: 2s - loss: 0.0174 - accuracy: 0.9939
 386/1688 [=====&gt;........................] - ETA: 2s - loss: 0.0172 - accuracy: 0.9941
 418/1688 [======&gt;.......................] - ETA: 2s - loss: 0.0187 - accuracy: 0.9935
 450/1688 [======&gt;.......................] - ETA: 1s - loss: 0.0189 - accuracy: 0.9933
 482/1688 [=======&gt;......................] - ETA: 1s - loss: 0.0189 - accuracy: 0.9932
 514/1688 [========&gt;.....................] - ETA: 1s - loss: 0.0184 - accuracy: 0.9934
 545/1688 [========&gt;.....................] - ETA: 1s - loss: 0.0181 - accuracy: 0.9935
 576/1688 [=========&gt;....................] - ETA: 1s - loss: 0.0179 - accuracy: 0.9936
 608/1688 [=========&gt;....................] - ETA: 1s - loss: 0.0173 - accuracy: 0.9938
 639/1688 [==========&gt;...................] - ETA: 1s - loss: 0.0168 - accuracy: 0.9940
 671/1688 [==========&gt;...................] - ETA: 1s - loss: 0.0167 - accuracy: 0.9941
 703/1688 [===========&gt;..................] - ETA: 1s - loss: 0.0167 - accuracy: 0.9943
 734/1688 [============&gt;.................] - ETA: 1s - loss: 0.0170 - accuracy: 0.9942
 766/1688 [============&gt;.................] - ETA: 1s - loss: 0.0169 - accuracy: 0.9942
 797/1688 [=============&gt;................] - ETA: 1s - loss: 0.0166 - accuracy: 0.9943
 829/1688 [=============&gt;................] - ETA: 1s - loss: 0.0167 - accuracy: 0.9943
 860/1688 [==============&gt;...............] - ETA: 1s - loss: 0.0168 - accuracy: 0.9943
 891/1688 [==============&gt;...............] - ETA: 1s - loss: 0.0167 - accuracy: 0.9943
 922/1688 [===============&gt;..............] - ETA: 1s - loss: 0.0171 - accuracy: 0.9942
 954/1688 [===============&gt;..............] - ETA: 1s - loss: 0.0172 - accuracy: 0.9942
 984/1688 [================&gt;.............] - ETA: 1s - loss: 0.0174 - accuracy: 0.9942
1014/1688 [=================&gt;............] - ETA: 1s - loss: 0.0172 - accuracy: 0.9943
1044/1688 [=================&gt;............] - ETA: 1s - loss: 0.0169 - accuracy: 0.9944
1074/1688 [==================&gt;...........] - ETA: 0s - loss: 0.0169 - accuracy: 0.9944
1104/1688 [==================&gt;...........] - ETA: 0s - loss: 0.0167 - accuracy: 0.9945
1134/1688 [===================&gt;..........] - ETA: 0s - loss: 0.0169 - accuracy: 0.9944
1165/1688 [===================&gt;..........] - ETA: 0s - loss: 0.0168 - accuracy: 0.9944
1198/1688 [====================&gt;.........] - ETA: 0s - loss: 0.0168 - accuracy: 0.9945
1231/1688 [====================&gt;.........] - ETA: 0s - loss: 0.0170 - accuracy: 0.9944
1263/1688 [=====================&gt;........] - ETA: 0s - loss: 0.0170 - accuracy: 0.9944
1294/1688 [=====================&gt;........] - ETA: 0s - loss: 0.0169 - accuracy: 0.9944
1325/1688 [======================&gt;.......] - ETA: 0s - loss: 0.0169 - accuracy: 0.9944
1356/1688 [=======================&gt;......] - ETA: 0s - loss: 0.0169 - accuracy: 0.9943
1388/1688 [=======================&gt;......] - ETA: 0s - loss: 0.0170 - accuracy: 0.9943
1419/1688 [========================&gt;.....] - ETA: 0s - loss: 0.0170 - accuracy: 0.9943
1451/1688 [========================&gt;.....] - ETA: 0s - loss: 0.0168 - accuracy: 0.9944
1483/1688 [=========================&gt;....] - ETA: 0s - loss: 0.0167 - accuracy: 0.9944
1515/1688 [=========================&gt;....] - ETA: 0s - loss: 0.0169 - accuracy: 0.9943
1547/1688 [==========================&gt;...] - ETA: 0s - loss: 0.0169 - accuracy: 0.9942
1579/1688 [===========================&gt;..] - ETA: 0s - loss: 0.0169 - accuracy: 0.9942
1611/1688 [===========================&gt;..] - ETA: 0s - loss: 0.0170 - accuracy: 0.9942
1643/1688 [============================&gt;.] - ETA: 0s - loss: 0.0171 - accuracy: 0.9942
1675/1688 [============================&gt;.] - ETA: 0s - loss: 0.0172 - accuracy: 0.9941
1688/1688 [==============================] - 3s 2ms/step - loss: 0.0175 - accuracy: 0.9941 - val_loss: 0.0666 - val_accuracy: 0.9828
Epoch 6/10

   1/1688 [..............................] - ETA: 0s - loss: 5.3251e-04 - accuracy: 1.0000
  36/1688 [..............................] - ETA: 2s - loss: 0.0164 - accuracy: 0.9965    
  69/1688 [&gt;.............................] - ETA: 2s - loss: 0.0127 - accuracy: 0.9968
 100/1688 [&gt;.............................] - ETA: 2s - loss: 0.0129 - accuracy: 0.9962
 131/1688 [=&gt;............................] - ETA: 2s - loss: 0.0124 - accuracy: 0.9959
 163/1688 [=&gt;............................] - ETA: 2s - loss: 0.0122 - accuracy: 0.9958
 194/1688 [==&gt;...........................] - ETA: 2s - loss: 0.0109 - accuracy: 0.9963
 225/1688 [==&gt;...........................] - ETA: 2s - loss: 0.0105 - accuracy: 0.9962
 257/1688 [===&gt;..........................] - ETA: 2s - loss: 0.0105 - accuracy: 0.9961
 288/1688 [====&gt;.........................] - ETA: 2s - loss: 0.0104 - accuracy: 0.9961
 319/1688 [====&gt;.........................] - ETA: 2s - loss: 0.0104 - accuracy: 0.9963
 351/1688 [=====&gt;........................] - ETA: 2s - loss: 0.0098 - accuracy: 0.9965
 383/1688 [=====&gt;........................] - ETA: 2s - loss: 0.0094 - accuracy: 0.9967
 415/1688 [======&gt;.......................] - ETA: 2s - loss: 0.0094 - accuracy: 0.9967
 447/1688 [======&gt;.......................] - ETA: 1s - loss: 0.0099 - accuracy: 0.9963
 478/1688 [=======&gt;......................] - ETA: 1s - loss: 0.0103 - accuracy: 0.9962
 510/1688 [========&gt;.....................] - ETA: 1s - loss: 0.0099 - accuracy: 0.9964
 542/1688 [========&gt;.....................] - ETA: 1s - loss: 0.0097 - accuracy: 0.9965
 574/1688 [=========&gt;....................] - ETA: 1s - loss: 0.0099 - accuracy: 0.9964
 606/1688 [=========&gt;....................] - ETA: 1s - loss: 0.0102 - accuracy: 0.9962
 637/1688 [==========&gt;...................] - ETA: 1s - loss: 0.0103 - accuracy: 0.9962
 669/1688 [==========&gt;...................] - ETA: 1s - loss: 0.0109 - accuracy: 0.9961
 700/1688 [===========&gt;..................] - ETA: 1s - loss: 0.0109 - accuracy: 0.9960
 732/1688 [============&gt;.................] - ETA: 1s - loss: 0.0119 - accuracy: 0.9958
 764/1688 [============&gt;.................] - ETA: 1s - loss: 0.0118 - accuracy: 0.9959
 796/1688 [=============&gt;................] - ETA: 1s - loss: 0.0117 - accuracy: 0.9958
 828/1688 [=============&gt;................] - ETA: 1s - loss: 0.0117 - accuracy: 0.9959
 860/1688 [==============&gt;...............] - ETA: 1s - loss: 0.0116 - accuracy: 0.9959
 893/1688 [==============&gt;...............] - ETA: 1s - loss: 0.0116 - accuracy: 0.9959
 924/1688 [===============&gt;..............] - ETA: 1s - loss: 0.0120 - accuracy: 0.9958
 956/1688 [===============&gt;..............] - ETA: 1s - loss: 0.0123 - accuracy: 0.9956
 987/1688 [================&gt;.............] - ETA: 1s - loss: 0.0122 - accuracy: 0.9956
1018/1688 [=================&gt;............] - ETA: 1s - loss: 0.0121 - accuracy: 0.9957
1049/1688 [=================&gt;............] - ETA: 1s - loss: 0.0120 - accuracy: 0.9957
1081/1688 [==================&gt;...........] - ETA: 0s - loss: 0.0119 - accuracy: 0.9957
1113/1688 [==================&gt;...........] - ETA: 0s - loss: 0.0121 - accuracy: 0.9956
1145/1688 [===================&gt;..........] - ETA: 0s - loss: 0.0120 - accuracy: 0.9957
1177/1688 [===================&gt;..........] - ETA: 0s - loss: 0.0123 - accuracy: 0.9956
1208/1688 [====================&gt;.........] - ETA: 0s - loss: 0.0123 - accuracy: 0.9957
1239/1688 [=====================&gt;........] - ETA: 0s - loss: 0.0125 - accuracy: 0.9956
1271/1688 [=====================&gt;........] - ETA: 0s - loss: 0.0125 - accuracy: 0.9956
1302/1688 [======================&gt;.......] - ETA: 0s - loss: 0.0128 - accuracy: 0.9955
1334/1688 [======================&gt;.......] - ETA: 0s - loss: 0.0128 - accuracy: 0.9955
1365/1688 [=======================&gt;......] - ETA: 0s - loss: 0.0127 - accuracy: 0.9955
1396/1688 [=======================&gt;......] - ETA: 0s - loss: 0.0130 - accuracy: 0.9955
1428/1688 [========================&gt;.....] - ETA: 0s - loss: 0.0129 - accuracy: 0.9955
1460/1688 [========================&gt;.....] - ETA: 0s - loss: 0.0129 - accuracy: 0.9955
1491/1688 [=========================&gt;....] - ETA: 0s - loss: 0.0127 - accuracy: 0.9955
1522/1688 [==========================&gt;...] - ETA: 0s - loss: 0.0126 - accuracy: 0.9956
1553/1688 [==========================&gt;...] - ETA: 0s - loss: 0.0125 - accuracy: 0.9956
1583/1688 [===========================&gt;..] - ETA: 0s - loss: 0.0124 - accuracy: 0.9957
1611/1688 [===========================&gt;..] - ETA: 0s - loss: 0.0125 - accuracy: 0.9955
1639/1688 [============================&gt;.] - ETA: 0s - loss: 0.0125 - accuracy: 0.9956
1669/1688 [============================&gt;.] - ETA: 0s - loss: 0.0126 - accuracy: 0.9956
1688/1688 [==============================] - 3s 2ms/step - loss: 0.0127 - accuracy: 0.9956 - val_loss: 0.0476 - val_accuracy: 0.9880
Epoch 7/10

   1/1688 [..............................] - ETA: 0s - loss: 0.0027 - accuracy: 1.0000
  35/1688 [..............................] - ETA: 2s - loss: 0.0056 - accuracy: 0.9982
  68/1688 [&gt;.............................] - ETA: 2s - loss: 0.0084 - accuracy: 0.9963
  99/1688 [&gt;.............................] - ETA: 2s - loss: 0.0090 - accuracy: 0.9962
 130/1688 [=&gt;............................] - ETA: 2s - loss: 0.0111 - accuracy: 0.9962
 162/1688 [=&gt;............................] - ETA: 2s - loss: 0.0128 - accuracy: 0.9956
 194/1688 [==&gt;...........................] - ETA: 2s - loss: 0.0118 - accuracy: 0.9960
 225/1688 [==&gt;...........................] - ETA: 2s - loss: 0.0107 - accuracy: 0.9962
 257/1688 [===&gt;..........................] - ETA: 2s - loss: 0.0102 - accuracy: 0.9964
 288/1688 [====&gt;.........................] - ETA: 2s - loss: 0.0096 - accuracy: 0.9966
 320/1688 [====&gt;.........................] - ETA: 2s - loss: 0.0092 - accuracy: 0.9968
 352/1688 [=====&gt;........................] - ETA: 2s - loss: 0.0085 - accuracy: 0.9971
 384/1688 [=====&gt;........................] - ETA: 2s - loss: 0.0089 - accuracy: 0.9968
 416/1688 [======&gt;.......................] - ETA: 2s - loss: 0.0090 - accuracy: 0.9968
 448/1688 [======&gt;.......................] - ETA: 1s - loss: 0.0089 - accuracy: 0.9969
 480/1688 [=======&gt;......................] - ETA: 1s - loss: 0.0089 - accuracy: 0.9970
 512/1688 [========&gt;.....................] - ETA: 1s - loss: 0.0086 - accuracy: 0.9971
 544/1688 [========&gt;.....................] - ETA: 1s - loss: 0.0082 - accuracy: 0.9973
 576/1688 [=========&gt;....................] - ETA: 1s - loss: 0.0080 - accuracy: 0.9974
 607/1688 [=========&gt;....................] - ETA: 1s - loss: 0.0082 - accuracy: 0.9972
 639/1688 [==========&gt;...................] - ETA: 1s - loss: 0.0082 - accuracy: 0.9972
 670/1688 [==========&gt;...................] - ETA: 1s - loss: 0.0083 - accuracy: 0.9972
 701/1688 [===========&gt;..................] - ETA: 1s - loss: 0.0082 - accuracy: 0.9972
 734/1688 [============&gt;.................] - ETA: 1s - loss: 0.0079 - accuracy: 0.9974
 766/1688 [============&gt;.................] - ETA: 1s - loss: 0.0077 - accuracy: 0.9974
 798/1688 [=============&gt;................] - ETA: 1s - loss: 0.0079 - accuracy: 0.9973
 830/1688 [=============&gt;................] - ETA: 1s - loss: 0.0078 - accuracy: 0.9974
 862/1688 [==============&gt;...............] - ETA: 1s - loss: 0.0079 - accuracy: 0.9973
 894/1688 [==============&gt;...............] - ETA: 1s - loss: 0.0080 - accuracy: 0.9973
 925/1688 [===============&gt;..............] - ETA: 1s - loss: 0.0088 - accuracy: 0.9970
 957/1688 [================&gt;.............] - ETA: 1s - loss: 0.0088 - accuracy: 0.9970
 989/1688 [================&gt;.............] - ETA: 1s - loss: 0.0091 - accuracy: 0.9969
1020/1688 [=================&gt;............] - ETA: 1s - loss: 0.0093 - accuracy: 0.9969
1052/1688 [=================&gt;............] - ETA: 1s - loss: 0.0094 - accuracy: 0.9970
1084/1688 [==================&gt;...........] - ETA: 0s - loss: 0.0092 - accuracy: 0.9970
1116/1688 [==================&gt;...........] - ETA: 0s - loss: 0.0091 - accuracy: 0.9971
1148/1688 [===================&gt;..........] - ETA: 0s - loss: 0.0090 - accuracy: 0.9971
1179/1688 [===================&gt;..........] - ETA: 0s - loss: 0.0091 - accuracy: 0.9970
1210/1688 [====================&gt;.........] - ETA: 0s - loss: 0.0091 - accuracy: 0.9970
1241/1688 [=====================&gt;........] - ETA: 0s - loss: 0.0093 - accuracy: 0.9969
1272/1688 [=====================&gt;........] - ETA: 0s - loss: 0.0092 - accuracy: 0.9969
1304/1688 [======================&gt;.......] - ETA: 0s - loss: 0.0093 - accuracy: 0.9968
1336/1688 [======================&gt;.......] - ETA: 0s - loss: 0.0093 - accuracy: 0.9968
1368/1688 [=======================&gt;......] - ETA: 0s - loss: 0.0094 - accuracy: 0.9968
1400/1688 [=======================&gt;......] - ETA: 0s - loss: 0.0093 - accuracy: 0.9968
1432/1688 [========================&gt;.....] - ETA: 0s - loss: 0.0094 - accuracy: 0.9968
1463/1688 [=========================&gt;....] - ETA: 0s - loss: 0.0096 - accuracy: 0.9966
1494/1688 [=========================&gt;....] - ETA: 0s - loss: 0.0096 - accuracy: 0.9966
1526/1688 [==========================&gt;...] - ETA: 0s - loss: 0.0096 - accuracy: 0.9966
1557/1688 [==========================&gt;...] - ETA: 0s - loss: 0.0096 - accuracy: 0.9966
1589/1688 [===========================&gt;..] - ETA: 0s - loss: 0.0096 - accuracy: 0.9966
1620/1688 [===========================&gt;..] - ETA: 0s - loss: 0.0095 - accuracy: 0.9967
1651/1688 [============================&gt;.] - ETA: 0s - loss: 0.0094 - accuracy: 0.9967
1682/1688 [============================&gt;.] - ETA: 0s - loss: 0.0094 - accuracy: 0.9967
1688/1688 [==============================] - 3s 2ms/step - loss: 0.0094 - accuracy: 0.9967 - val_loss: 0.0521 - val_accuracy: 0.9885
Epoch 8/10

   1/1688 [..............................] - ETA: 0s - loss: 0.0037 - accuracy: 1.0000
  33/1688 [..............................] - ETA: 2s - loss: 0.0077 - accuracy: 0.9962
  65/1688 [&gt;.............................] - ETA: 2s - loss: 0.0063 - accuracy: 0.9971
  96/1688 [&gt;.............................] - ETA: 2s - loss: 0.0054 - accuracy: 0.9977
 128/1688 [=&gt;............................] - ETA: 2s - loss: 0.0047 - accuracy: 0.9980
 160/1688 [=&gt;............................] - ETA: 2s - loss: 0.0053 - accuracy: 0.9980
 192/1688 [==&gt;...........................] - ETA: 2s - loss: 0.0056 - accuracy: 0.9982
 223/1688 [==&gt;...........................] - ETA: 2s - loss: 0.0051 - accuracy: 0.9985
 255/1688 [===&gt;..........................] - ETA: 2s - loss: 0.0050 - accuracy: 0.9985
 287/1688 [====&gt;.........................] - ETA: 2s - loss: 0.0066 - accuracy: 0.9984
 319/1688 [====&gt;.........................] - ETA: 2s - loss: 0.0063 - accuracy: 0.9984
 351/1688 [=====&gt;........................] - ETA: 2s - loss: 0.0059 - accuracy: 0.9986
 382/1688 [=====&gt;........................] - ETA: 2s - loss: 0.0057 - accuracy: 0.9986
 413/1688 [======&gt;.......................] - ETA: 2s - loss: 0.0059 - accuracy: 0.9984
 444/1688 [======&gt;.......................] - ETA: 1s - loss: 0.0060 - accuracy: 0.9982
 476/1688 [=======&gt;......................] - ETA: 1s - loss: 0.0064 - accuracy: 0.9980
 507/1688 [========&gt;.....................] - ETA: 1s - loss: 0.0065 - accuracy: 0.9980
 539/1688 [========&gt;.....................] - ETA: 1s - loss: 0.0069 - accuracy: 0.9980
 571/1688 [=========&gt;....................] - ETA: 1s - loss: 0.0068 - accuracy: 0.9980
 603/1688 [=========&gt;....................] - ETA: 1s - loss: 0.0072 - accuracy: 0.9978
 634/1688 [==========&gt;...................] - ETA: 1s - loss: 0.0072 - accuracy: 0.9978
 666/1688 [==========&gt;...................] - ETA: 1s - loss: 0.0070 - accuracy: 0.9979
 696/1688 [===========&gt;..................] - ETA: 1s - loss: 0.0071 - accuracy: 0.9979
 729/1688 [===========&gt;..................] - ETA: 1s - loss: 0.0073 - accuracy: 0.9979
 761/1688 [============&gt;.................] - ETA: 1s - loss: 0.0073 - accuracy: 0.9978
 793/1688 [=============&gt;................] - ETA: 1s - loss: 0.0072 - accuracy: 0.9978
 824/1688 [=============&gt;................] - ETA: 1s - loss: 0.0072 - accuracy: 0.9978
 855/1688 [==============&gt;...............] - ETA: 1s - loss: 0.0078 - accuracy: 0.9976
 887/1688 [==============&gt;...............] - ETA: 1s - loss: 0.0077 - accuracy: 0.9976
 918/1688 [===============&gt;..............] - ETA: 1s - loss: 0.0077 - accuracy: 0.9976
 949/1688 [===============&gt;..............] - ETA: 1s - loss: 0.0076 - accuracy: 0.9976
 980/1688 [================&gt;.............] - ETA: 1s - loss: 0.0076 - accuracy: 0.9976
1012/1688 [================&gt;.............] - ETA: 1s - loss: 0.0079 - accuracy: 0.9975
1044/1688 [=================&gt;............] - ETA: 1s - loss: 0.0078 - accuracy: 0.9975
1076/1688 [==================&gt;...........] - ETA: 0s - loss: 0.0078 - accuracy: 0.9975
1108/1688 [==================&gt;...........] - ETA: 0s - loss: 0.0080 - accuracy: 0.9975
1140/1688 [===================&gt;..........] - ETA: 0s - loss: 0.0079 - accuracy: 0.9975
1171/1688 [===================&gt;..........] - ETA: 0s - loss: 0.0079 - accuracy: 0.9975
1203/1688 [====================&gt;.........] - ETA: 0s - loss: 0.0080 - accuracy: 0.9975
1234/1688 [====================&gt;.........] - ETA: 0s - loss: 0.0079 - accuracy: 0.9975
1265/1688 [=====================&gt;........] - ETA: 0s - loss: 0.0078 - accuracy: 0.9976
1296/1688 [======================&gt;.......] - ETA: 0s - loss: 0.0078 - accuracy: 0.9976
1328/1688 [======================&gt;.......] - ETA: 0s - loss: 0.0079 - accuracy: 0.9975
1360/1688 [=======================&gt;......] - ETA: 0s - loss: 0.0079 - accuracy: 0.9975
1392/1688 [=======================&gt;......] - ETA: 0s - loss: 0.0079 - accuracy: 0.9975
1425/1688 [========================&gt;.....] - ETA: 0s - loss: 0.0081 - accuracy: 0.9974
1457/1688 [========================&gt;.....] - ETA: 0s - loss: 0.0080 - accuracy: 0.9974
1489/1688 [=========================&gt;....] - ETA: 0s - loss: 0.0079 - accuracy: 0.9974
1520/1688 [==========================&gt;...] - ETA: 0s - loss: 0.0079 - accuracy: 0.9974
1552/1688 [==========================&gt;...] - ETA: 0s - loss: 0.0077 - accuracy: 0.9975
1583/1688 [===========================&gt;..] - ETA: 0s - loss: 0.0076 - accuracy: 0.9975
1615/1688 [===========================&gt;..] - ETA: 0s - loss: 0.0076 - accuracy: 0.9976
1647/1688 [============================&gt;.] - ETA: 0s - loss: 0.0076 - accuracy: 0.9976
1679/1688 [============================&gt;.] - ETA: 0s - loss: 0.0076 - accuracy: 0.9976
1688/1688 [==============================] - 3s 2ms/step - loss: 0.0077 - accuracy: 0.9975 - val_loss: 0.0450 - val_accuracy: 0.9905
Epoch 9/10

   1/1688 [..............................] - ETA: 0s - loss: 0.0066 - accuracy: 1.0000
  34/1688 [..............................] - ETA: 2s - loss: 0.0054 - accuracy: 0.9991
  66/1688 [&gt;.............................] - ETA: 2s - loss: 0.0062 - accuracy: 0.9981
  97/1688 [&gt;.............................] - ETA: 2s - loss: 0.0058 - accuracy: 0.9981
 129/1688 [=&gt;............................] - ETA: 2s - loss: 0.0055 - accuracy: 0.9983
 161/1688 [=&gt;............................] - ETA: 2s - loss: 0.0063 - accuracy: 0.9979
 192/1688 [==&gt;...........................] - ETA: 2s - loss: 0.0067 - accuracy: 0.9977
 224/1688 [==&gt;...........................] - ETA: 2s - loss: 0.0068 - accuracy: 0.9976
 255/1688 [===&gt;..........................] - ETA: 2s - loss: 0.0074 - accuracy: 0.9975
 286/1688 [====&gt;.........................] - ETA: 2s - loss: 0.0074 - accuracy: 0.9975
 318/1688 [====&gt;.........................] - ETA: 2s - loss: 0.0071 - accuracy: 0.9976
 349/1688 [=====&gt;........................] - ETA: 2s - loss: 0.0068 - accuracy: 0.9978
 381/1688 [=====&gt;........................] - ETA: 2s - loss: 0.0063 - accuracy: 0.9979
 413/1688 [======&gt;.......................] - ETA: 2s - loss: 0.0061 - accuracy: 0.9980
 446/1688 [======&gt;.......................] - ETA: 1s - loss: 0.0063 - accuracy: 0.9978
 477/1688 [=======&gt;......................] - ETA: 1s - loss: 0.0062 - accuracy: 0.9978
 509/1688 [========&gt;.....................] - ETA: 1s - loss: 0.0059 - accuracy: 0.9980
 540/1688 [========&gt;.....................] - ETA: 1s - loss: 0.0061 - accuracy: 0.9979
 572/1688 [=========&gt;....................] - ETA: 1s - loss: 0.0059 - accuracy: 0.9980
 604/1688 [=========&gt;....................] - ETA: 1s - loss: 0.0060 - accuracy: 0.9979
 636/1688 [==========&gt;...................] - ETA: 1s - loss: 0.0060 - accuracy: 0.9979
 668/1688 [==========&gt;...................] - ETA: 1s - loss: 0.0061 - accuracy: 0.9979
 700/1688 [===========&gt;..................] - ETA: 1s - loss: 0.0061 - accuracy: 0.9979
 731/1688 [===========&gt;..................] - ETA: 1s - loss: 0.0061 - accuracy: 0.9979
 763/1688 [============&gt;.................] - ETA: 1s - loss: 0.0060 - accuracy: 0.9979
 795/1688 [=============&gt;................] - ETA: 1s - loss: 0.0059 - accuracy: 0.9980
 827/1688 [=============&gt;................] - ETA: 1s - loss: 0.0059 - accuracy: 0.9979
 859/1688 [==============&gt;...............] - ETA: 1s - loss: 0.0060 - accuracy: 0.9979
 891/1688 [==============&gt;...............] - ETA: 1s - loss: 0.0059 - accuracy: 0.9979
 922/1688 [===============&gt;..............] - ETA: 1s - loss: 0.0059 - accuracy: 0.9979
 954/1688 [===============&gt;..............] - ETA: 1s - loss: 0.0060 - accuracy: 0.9978
 986/1688 [================&gt;.............] - ETA: 1s - loss: 0.0060 - accuracy: 0.9978
1019/1688 [=================&gt;............] - ETA: 1s - loss: 0.0060 - accuracy: 0.9978
1050/1688 [=================&gt;............] - ETA: 1s - loss: 0.0061 - accuracy: 0.9978
1082/1688 [==================&gt;...........] - ETA: 0s - loss: 0.0062 - accuracy: 0.9978
1114/1688 [==================&gt;...........] - ETA: 0s - loss: 0.0063 - accuracy: 0.9977
1146/1688 [===================&gt;..........] - ETA: 0s - loss: 0.0063 - accuracy: 0.9977
1176/1688 [===================&gt;..........] - ETA: 0s - loss: 0.0064 - accuracy: 0.9977
1207/1688 [====================&gt;.........] - ETA: 0s - loss: 0.0064 - accuracy: 0.9976
1238/1688 [=====================&gt;........] - ETA: 0s - loss: 0.0065 - accuracy: 0.9976
1269/1688 [=====================&gt;........] - ETA: 0s - loss: 0.0065 - accuracy: 0.9977
1301/1688 [======================&gt;.......] - ETA: 0s - loss: 0.0065 - accuracy: 0.9977
1332/1688 [======================&gt;.......] - ETA: 0s - loss: 0.0065 - accuracy: 0.9977
1364/1688 [=======================&gt;......] - ETA: 0s - loss: 0.0065 - accuracy: 0.9977
1396/1688 [=======================&gt;......] - ETA: 0s - loss: 0.0065 - accuracy: 0.9977
1428/1688 [========================&gt;.....] - ETA: 0s - loss: 0.0064 - accuracy: 0.9977
1460/1688 [========================&gt;.....] - ETA: 0s - loss: 0.0064 - accuracy: 0.9977
1491/1688 [=========================&gt;....] - ETA: 0s - loss: 0.0064 - accuracy: 0.9977
1523/1688 [==========================&gt;...] - ETA: 0s - loss: 0.0065 - accuracy: 0.9977
1553/1688 [==========================&gt;...] - ETA: 0s - loss: 0.0065 - accuracy: 0.9977
1585/1688 [===========================&gt;..] - ETA: 0s - loss: 0.0067 - accuracy: 0.9976
1617/1688 [===========================&gt;..] - ETA: 0s - loss: 0.0068 - accuracy: 0.9976
1649/1688 [============================&gt;.] - ETA: 0s - loss: 0.0068 - accuracy: 0.9976
1680/1688 [============================&gt;.] - ETA: 0s - loss: 0.0071 - accuracy: 0.9975
1688/1688 [==============================] - 3s 2ms/step - loss: 0.0071 - accuracy: 0.9975 - val_loss: 0.0937 - val_accuracy: 0.9805
Epoch 10/10

   1/1688 [..............................] - ETA: 0s - loss: 4.9000e-05 - accuracy: 1.0000
  36/1688 [..............................] - ETA: 2s - loss: 0.0153 - accuracy: 0.9948    
  70/1688 [&gt;.............................] - ETA: 2s - loss: 0.0113 - accuracy: 0.9964
 102/1688 [&gt;.............................] - ETA: 2s - loss: 0.0093 - accuracy: 0.9969
 134/1688 [=&gt;............................] - ETA: 2s - loss: 0.0088 - accuracy: 0.9972
 165/1688 [=&gt;............................] - ETA: 2s - loss: 0.0075 - accuracy: 0.9977
 196/1688 [==&gt;...........................] - ETA: 2s - loss: 0.0067 - accuracy: 0.9981
 227/1688 [===&gt;..........................] - ETA: 2s - loss: 0.0060 - accuracy: 0.9982
 258/1688 [===&gt;..........................] - ETA: 2s - loss: 0.0054 - accuracy: 0.9984
 289/1688 [====&gt;.........................] - ETA: 2s - loss: 0.0052 - accuracy: 0.9985
 320/1688 [====&gt;.........................] - ETA: 2s - loss: 0.0052 - accuracy: 0.9984
 351/1688 [=====&gt;........................] - ETA: 2s - loss: 0.0050 - accuracy: 0.9985
 382/1688 [=====&gt;........................] - ETA: 2s - loss: 0.0051 - accuracy: 0.9984
 413/1688 [======&gt;.......................] - ETA: 2s - loss: 0.0054 - accuracy: 0.9983
 445/1688 [======&gt;.......................] - ETA: 1s - loss: 0.0054 - accuracy: 0.9983
 478/1688 [=======&gt;......................] - ETA: 1s - loss: 0.0052 - accuracy: 0.9984
 511/1688 [========&gt;.....................] - ETA: 1s - loss: 0.0051 - accuracy: 0.9984
 543/1688 [========&gt;.....................] - ETA: 1s - loss: 0.0049 - accuracy: 0.9985
 575/1688 [=========&gt;....................] - ETA: 1s - loss: 0.0048 - accuracy: 0.9985
 606/1688 [=========&gt;....................] - ETA: 1s - loss: 0.0049 - accuracy: 0.9984
 637/1688 [==========&gt;...................] - ETA: 1s - loss: 0.0048 - accuracy: 0.9984
 669/1688 [==========&gt;...................] - ETA: 1s - loss: 0.0047 - accuracy: 0.9985
 701/1688 [===========&gt;..................] - ETA: 1s - loss: 0.0049 - accuracy: 0.9984
 732/1688 [============&gt;.................] - ETA: 1s - loss: 0.0052 - accuracy: 0.9982
 763/1688 [============&gt;.................] - ETA: 1s - loss: 0.0054 - accuracy: 0.9982
 795/1688 [=============&gt;................] - ETA: 1s - loss: 0.0057 - accuracy: 0.9981
 826/1688 [=============&gt;................] - ETA: 1s - loss: 0.0058 - accuracy: 0.9980
 858/1688 [==============&gt;...............] - ETA: 1s - loss: 0.0060 - accuracy: 0.9980
 890/1688 [==============&gt;...............] - ETA: 1s - loss: 0.0068 - accuracy: 0.9978
 922/1688 [===============&gt;..............] - ETA: 1s - loss: 0.0068 - accuracy: 0.9977
 954/1688 [===============&gt;..............] - ETA: 1s - loss: 0.0068 - accuracy: 0.9977
 988/1688 [================&gt;.............] - ETA: 1s - loss: 0.0071 - accuracy: 0.9975
1020/1688 [=================&gt;............] - ETA: 1s - loss: 0.0069 - accuracy: 0.9976
1051/1688 [=================&gt;............] - ETA: 1s - loss: 0.0069 - accuracy: 0.9976
1082/1688 [==================&gt;...........] - ETA: 0s - loss: 0.0069 - accuracy: 0.9976
1114/1688 [==================&gt;...........] - ETA: 0s - loss: 0.0068 - accuracy: 0.9976
1145/1688 [===================&gt;..........] - ETA: 0s - loss: 0.0068 - accuracy: 0.9976
1177/1688 [===================&gt;..........] - ETA: 0s - loss: 0.0067 - accuracy: 0.9976
1209/1688 [====================&gt;.........] - ETA: 0s - loss: 0.0066 - accuracy: 0.9976
1240/1688 [=====================&gt;........] - ETA: 0s - loss: 0.0067 - accuracy: 0.9976
1271/1688 [=====================&gt;........] - ETA: 0s - loss: 0.0067 - accuracy: 0.9976
1303/1688 [======================&gt;.......] - ETA: 0s - loss: 0.0068 - accuracy: 0.9976
1335/1688 [======================&gt;.......] - ETA: 0s - loss: 0.0068 - accuracy: 0.9976
1367/1688 [=======================&gt;......] - ETA: 0s - loss: 0.0069 - accuracy: 0.9975
1399/1688 [=======================&gt;......] - ETA: 0s - loss: 0.0070 - accuracy: 0.9975
1431/1688 [========================&gt;.....] - ETA: 0s - loss: 0.0072 - accuracy: 0.9974
1463/1688 [=========================&gt;....] - ETA: 0s - loss: 0.0075 - accuracy: 0.9974
1494/1688 [=========================&gt;....] - ETA: 0s - loss: 0.0075 - accuracy: 0.9974
1526/1688 [==========================&gt;...] - ETA: 0s - loss: 0.0076 - accuracy: 0.9974
1558/1688 [==========================&gt;...] - ETA: 0s - loss: 0.0076 - accuracy: 0.9974
1589/1688 [===========================&gt;..] - ETA: 0s - loss: 0.0076 - accuracy: 0.9974
1620/1688 [===========================&gt;..] - ETA: 0s - loss: 0.0076 - accuracy: 0.9974
1651/1688 [============================&gt;.] - ETA: 0s - loss: 0.0075 - accuracy: 0.9974
1682/1688 [============================&gt;.] - ETA: 0s - loss: 0.0076 - accuracy: 0.9974
1688/1688 [==============================] - 3s 2ms/step - loss: 0.0075 - accuracy: 0.9974 - val_loss: 0.0439 - val_accuracy: 0.9902
Test score: 0.03948387876152992
Test accuracy: 0.9902999997138977
</pre></div>
</div>
</div>
</div>
<div class="section" id="model-quantization">
<h2>4. Model quantization<a class="headerlink" href="#model-quantization" title="Permalink to this headline">¶</a></h2>
<p>We can now turn to quantization to get a discretized version of the model,
where the weights and activations are quantized so as to be suitable for
implementation in the Akida NSoC.</p>
<p>For this, we just have to quantize the Keras model using the
<a class="reference external" href="../api_reference/cnn2snn_apis.html#quantize">quantize</a>
function. Here, we decide to quantize to the maximum allowed bitwidths for
the first layer weights (8-bit), the subsequent layer weights (4-bit) and the
activations (4-bit).</p>
<p>The quantized model is a Keras model where the neural layers (Conv2D, Dense)
and the ReLU layers are replaced with custom CNN2SNN quantized layers
(QuantizedConv2D, QuantizedDense, QuantizedReLU). All Keras API functions
can be applied on this new model: <code class="docutils literal notranslate"><span class="pre">summary()</span></code>, <code class="docutils literal notranslate"><span class="pre">compile()</span></code>, <code class="docutils literal notranslate"><span class="pre">fit()</span></code>. etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="docutils literal notranslate"><span class="pre">quantize</span></code> function folds the batch normalization layers into
the corresponding neural layer. The new weights are computed
according to this folding operation.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The CNN2SNN toolkit provides the
<a class="reference external" href="../api_reference/cnn2snn_apis.html#check-model-compatibility">check_model_compatibility</a>
function to ensure that the quantized model is compatible with the
Akida NSoC. If the model is not fully compatible, substitutes will
be needed for the relevant layers/operations (guidelines included
in the documentation).</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">cnn2snn</span> <span class="kn">import</span> <span class="n">quantize</span><span class="p">,</span> <span class="n">check_model_compatibility</span>

<span class="n">model_quantized</span> <span class="o">=</span> <span class="n">quantize</span><span class="p">(</span><span class="n">model_keras</span><span class="p">,</span>
                           <span class="n">input_weight_quantization</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
                           <span class="n">weight_quantization</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                           <span class="n">activ_quantization</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">model_quantized</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model compatible for Akida conversion:&quot;</span><span class="p">,</span>
      <span class="n">check_model_compatibility</span><span class="p">(</span><span class="n">model_quantized</span><span class="p">,</span> <span class="n">input_is_sparse</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Model: &quot;model&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_3 (InputLayer)         [(None, 28, 28, 1)]       0
_________________________________________________________________
conv2d (QuantizedConv2D)     (None, 26, 26, 32)        320
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0
_________________________________________________________________
re_lu (ActivationDiscreteRel (None, 13, 13, 32)        0
_________________________________________________________________
conv2d_1 (QuantizedConv2D)   (None, 13, 13, 64)        18496
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0
_________________________________________________________________
re_lu_1 (ActivationDiscreteR (None, 7, 7, 64)          0
_________________________________________________________________
flatten (Flatten)            (None, 3136)              0
_________________________________________________________________
dense (QuantizedDense)       (None, 10)                31370
=================================================================
Total params: 50,186
Trainable params: 50,186
Non-trainable params: 0
_________________________________________________________________
Model compatible for Akida conversion: True
</pre></div>
</div>
<p>Check the quantized model accuracy.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model_quantized</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span>
    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>

<span class="n">score</span> <span class="o">=</span> <span class="n">model_quantized</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Test accuracy after 8-4-4 quantization:&#39;</span><span class="p">,</span> <span class="n">score</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Test accuracy after 8-4-4 quantization: 0.9894000291824341
</pre></div>
</div>
<p>Since we used the maximum allowed bitwidths for weights and activations, the
accuracy of the quantized model is equivalent to the one of the base model,
but for lower bitwidth, the quantization  usually introduces a performance drop.</p>
<p>Let’s try this time with 2-bit for weights and 1-bit for activations.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model_quantized</span> <span class="o">=</span> <span class="n">quantize</span><span class="p">(</span><span class="n">model_keras</span><span class="p">,</span>
                           <span class="n">weight_quantization</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                           <span class="n">activ_quantization</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">model_quantized</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span>
    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>

<span class="n">score</span> <span class="o">=</span> <span class="n">model_quantized</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Test accuracy after 2-2-1 quantization:&#39;</span><span class="p">,</span> <span class="n">score</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="c1"># To recover the original model accuracy, a quantization-aware training phase</span>
<span class="c1"># is required.</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Test accuracy after 2-2-1 quantization: 0.2386000007390976
</pre></div>
</div>
</div>
<div class="section" id="model-fine-tuning-quantization-aware-training">
<h2>5. Model fine tuning (quantization-aware training)<a class="headerlink" href="#model-fine-tuning-quantization-aware-training" title="Permalink to this headline">¶</a></h2>
<p>This quantization-aware training (fine tuning) allows to cover the
performance drop due to the quantization step.</p>
<p>Note that since this step is a fine tuning, the number of epochs can be
lowered, compared to the training from scratch of the standard model.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model_quantized</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">score</span> <span class="o">=</span> <span class="n">model_quantized</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Test accuracy after fine tuning:&#39;</span><span class="p">,</span> <span class="n">score</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Epoch 1/5

   1/1688 [..............................] - ETA: 0s - loss: 2.1594 - accuracy: 0.2188
  26/1688 [..............................] - ETA: 3s - loss: 1.3231 - accuracy: 0.5553
  52/1688 [..............................] - ETA: 3s - loss: 0.8645 - accuracy: 0.7163
  78/1688 [&gt;.............................] - ETA: 3s - loss: 0.6624 - accuracy: 0.7849
 104/1688 [&gt;.............................] - ETA: 3s - loss: 0.5415 - accuracy: 0.8245
 129/1688 [=&gt;............................] - ETA: 3s - loss: 0.4668 - accuracy: 0.8486
 155/1688 [=&gt;............................] - ETA: 3s - loss: 0.4159 - accuracy: 0.8661
 180/1688 [==&gt;...........................] - ETA: 2s - loss: 0.3748 - accuracy: 0.8795
 206/1688 [==&gt;...........................] - ETA: 2s - loss: 0.3393 - accuracy: 0.8909
 232/1688 [===&gt;..........................] - ETA: 2s - loss: 0.3161 - accuracy: 0.8982
 258/1688 [===&gt;..........................] - ETA: 2s - loss: 0.2992 - accuracy: 0.9044
 284/1688 [====&gt;.........................] - ETA: 2s - loss: 0.2815 - accuracy: 0.9097
 311/1688 [====&gt;.........................] - ETA: 2s - loss: 0.2676 - accuracy: 0.9144
 337/1688 [====&gt;.........................] - ETA: 2s - loss: 0.2569 - accuracy: 0.9187
 363/1688 [=====&gt;........................] - ETA: 2s - loss: 0.2474 - accuracy: 0.9215
 390/1688 [=====&gt;........................] - ETA: 2s - loss: 0.2365 - accuracy: 0.9249
 416/1688 [======&gt;.......................] - ETA: 2s - loss: 0.2278 - accuracy: 0.9274
 442/1688 [======&gt;.......................] - ETA: 2s - loss: 0.2204 - accuracy: 0.9294
 468/1688 [=======&gt;......................] - ETA: 2s - loss: 0.2132 - accuracy: 0.9318
 494/1688 [=======&gt;......................] - ETA: 2s - loss: 0.2069 - accuracy: 0.9338
 520/1688 [========&gt;.....................] - ETA: 2s - loss: 0.2023 - accuracy: 0.9353
 547/1688 [========&gt;.....................] - ETA: 2s - loss: 0.1958 - accuracy: 0.9374
 573/1688 [=========&gt;....................] - ETA: 2s - loss: 0.1914 - accuracy: 0.9389
 600/1688 [=========&gt;....................] - ETA: 2s - loss: 0.1858 - accuracy: 0.9407
 627/1688 [==========&gt;...................] - ETA: 2s - loss: 0.1805 - accuracy: 0.9422
 653/1688 [==========&gt;...................] - ETA: 2s - loss: 0.1761 - accuracy: 0.9436
 679/1688 [===========&gt;..................] - ETA: 1s - loss: 0.1727 - accuracy: 0.9449
 704/1688 [===========&gt;..................] - ETA: 1s - loss: 0.1689 - accuracy: 0.9462
 730/1688 [===========&gt;..................] - ETA: 1s - loss: 0.1648 - accuracy: 0.9473
 756/1688 [============&gt;.................] - ETA: 1s - loss: 0.1623 - accuracy: 0.9481
 782/1688 [============&gt;.................] - ETA: 1s - loss: 0.1597 - accuracy: 0.9491
 808/1688 [=============&gt;................] - ETA: 1s - loss: 0.1576 - accuracy: 0.9499
 834/1688 [=============&gt;................] - ETA: 1s - loss: 0.1549 - accuracy: 0.9508
 860/1688 [==============&gt;...............] - ETA: 1s - loss: 0.1529 - accuracy: 0.9513
 886/1688 [==============&gt;...............] - ETA: 1s - loss: 0.1510 - accuracy: 0.9520
 912/1688 [===============&gt;..............] - ETA: 1s - loss: 0.1491 - accuracy: 0.9526
 938/1688 [===============&gt;..............] - ETA: 1s - loss: 0.1468 - accuracy: 0.9533
 964/1688 [================&gt;.............] - ETA: 1s - loss: 0.1445 - accuracy: 0.9540
 990/1688 [================&gt;.............] - ETA: 1s - loss: 0.1426 - accuracy: 0.9548
1016/1688 [=================&gt;............] - ETA: 1s - loss: 0.1402 - accuracy: 0.9555
1042/1688 [=================&gt;............] - ETA: 1s - loss: 0.1385 - accuracy: 0.9560
1068/1688 [=================&gt;............] - ETA: 1s - loss: 0.1366 - accuracy: 0.9565
1094/1688 [==================&gt;...........] - ETA: 1s - loss: 0.1347 - accuracy: 0.9571
1120/1688 [==================&gt;...........] - ETA: 1s - loss: 0.1327 - accuracy: 0.9576
1146/1688 [===================&gt;..........] - ETA: 1s - loss: 0.1304 - accuracy: 0.9583
1172/1688 [===================&gt;..........] - ETA: 1s - loss: 0.1289 - accuracy: 0.9588
1198/1688 [====================&gt;.........] - ETA: 0s - loss: 0.1275 - accuracy: 0.9592
1224/1688 [====================&gt;.........] - ETA: 0s - loss: 0.1255 - accuracy: 0.9598
1251/1688 [=====================&gt;........] - ETA: 0s - loss: 0.1236 - accuracy: 0.9604
1277/1688 [=====================&gt;........] - ETA: 0s - loss: 0.1230 - accuracy: 0.9607
1303/1688 [======================&gt;.......] - ETA: 0s - loss: 0.1220 - accuracy: 0.9610
1329/1688 [======================&gt;.......] - ETA: 0s - loss: 0.1209 - accuracy: 0.9612
1355/1688 [=======================&gt;......] - ETA: 0s - loss: 0.1204 - accuracy: 0.9613
1381/1688 [=======================&gt;......] - ETA: 0s - loss: 0.1194 - accuracy: 0.9617
1406/1688 [=======================&gt;......] - ETA: 0s - loss: 0.1184 - accuracy: 0.9620
1432/1688 [========================&gt;.....] - ETA: 0s - loss: 0.1177 - accuracy: 0.9621
1458/1688 [========================&gt;.....] - ETA: 0s - loss: 0.1168 - accuracy: 0.9624
1483/1688 [=========================&gt;....] - ETA: 0s - loss: 0.1155 - accuracy: 0.9628
1508/1688 [=========================&gt;....] - ETA: 0s - loss: 0.1143 - accuracy: 0.9632
1534/1688 [==========================&gt;...] - ETA: 0s - loss: 0.1136 - accuracy: 0.9635
1560/1688 [==========================&gt;...] - ETA: 0s - loss: 0.1123 - accuracy: 0.9639
1586/1688 [===========================&gt;..] - ETA: 0s - loss: 0.1112 - accuracy: 0.9644
1612/1688 [===========================&gt;..] - ETA: 0s - loss: 0.1104 - accuracy: 0.9647
1638/1688 [============================&gt;.] - ETA: 0s - loss: 0.1097 - accuracy: 0.9648
1664/1688 [============================&gt;.] - ETA: 0s - loss: 0.1087 - accuracy: 0.9652
1688/1688 [==============================] - 4s 2ms/step - loss: 0.1075 - accuracy: 0.9656 - val_loss: 0.0713 - val_accuracy: 0.9818
Epoch 2/5

   1/1688 [..............................] - ETA: 0s - loss: 0.0304 - accuracy: 1.0000
  28/1688 [..............................] - ETA: 2s - loss: 0.0426 - accuracy: 0.9855
  55/1688 [..............................] - ETA: 3s - loss: 0.0422 - accuracy: 0.9830
  82/1688 [&gt;.............................] - ETA: 3s - loss: 0.0395 - accuracy: 0.9848
 109/1688 [&gt;.............................] - ETA: 2s - loss: 0.0452 - accuracy: 0.9848
 135/1688 [=&gt;............................] - ETA: 2s - loss: 0.0498 - accuracy: 0.9838
 161/1688 [=&gt;............................] - ETA: 2s - loss: 0.0491 - accuracy: 0.9839
 187/1688 [==&gt;...........................] - ETA: 2s - loss: 0.0473 - accuracy: 0.9840
 213/1688 [==&gt;...........................] - ETA: 2s - loss: 0.0483 - accuracy: 0.9837
 239/1688 [===&gt;..........................] - ETA: 2s - loss: 0.0479 - accuracy: 0.9839
 266/1688 [===&gt;..........................] - ETA: 2s - loss: 0.0469 - accuracy: 0.9838
 292/1688 [====&gt;.........................] - ETA: 2s - loss: 0.0459 - accuracy: 0.9841
 318/1688 [====&gt;.........................] - ETA: 2s - loss: 0.0480 - accuracy: 0.9833
 344/1688 [=====&gt;........................] - ETA: 2s - loss: 0.0486 - accuracy: 0.9834
 370/1688 [=====&gt;........................] - ETA: 2s - loss: 0.0485 - accuracy: 0.9835
 396/1688 [======&gt;.......................] - ETA: 2s - loss: 0.0476 - accuracy: 0.9838
 421/1688 [======&gt;.......................] - ETA: 2s - loss: 0.0482 - accuracy: 0.9837
 447/1688 [======&gt;.......................] - ETA: 2s - loss: 0.0482 - accuracy: 0.9838
 473/1688 [=======&gt;......................] - ETA: 2s - loss: 0.0477 - accuracy: 0.9839
 500/1688 [=======&gt;......................] - ETA: 2s - loss: 0.0474 - accuracy: 0.9840
 526/1688 [========&gt;.....................] - ETA: 2s - loss: 0.0478 - accuracy: 0.9841
 552/1688 [========&gt;.....................] - ETA: 2s - loss: 0.0486 - accuracy: 0.9840
 578/1688 [=========&gt;....................] - ETA: 2s - loss: 0.0493 - accuracy: 0.9837
 604/1688 [=========&gt;....................] - ETA: 2s - loss: 0.0484 - accuracy: 0.9839
 630/1688 [==========&gt;...................] - ETA: 2s - loss: 0.0498 - accuracy: 0.9834
 656/1688 [==========&gt;...................] - ETA: 2s - loss: 0.0501 - accuracy: 0.9833
 682/1688 [===========&gt;..................] - ETA: 1s - loss: 0.0500 - accuracy: 0.9831
 708/1688 [===========&gt;..................] - ETA: 1s - loss: 0.0494 - accuracy: 0.9834
 734/1688 [============&gt;.................] - ETA: 1s - loss: 0.0503 - accuracy: 0.9830
 760/1688 [============&gt;.................] - ETA: 1s - loss: 0.0516 - accuracy: 0.9830
 786/1688 [============&gt;.................] - ETA: 1s - loss: 0.0515 - accuracy: 0.9828
 812/1688 [=============&gt;................] - ETA: 1s - loss: 0.0514 - accuracy: 0.9829
 838/1688 [=============&gt;................] - ETA: 1s - loss: 0.0515 - accuracy: 0.9828
 864/1688 [==============&gt;...............] - ETA: 1s - loss: 0.0512 - accuracy: 0.9830
 891/1688 [==============&gt;...............] - ETA: 1s - loss: 0.0514 - accuracy: 0.9830
 917/1688 [===============&gt;..............] - ETA: 1s - loss: 0.0511 - accuracy: 0.9831
 943/1688 [===============&gt;..............] - ETA: 1s - loss: 0.0515 - accuracy: 0.9830
 969/1688 [================&gt;.............] - ETA: 1s - loss: 0.0518 - accuracy: 0.9830
 994/1688 [================&gt;.............] - ETA: 1s - loss: 0.0519 - accuracy: 0.9827
1020/1688 [=================&gt;............] - ETA: 1s - loss: 0.0516 - accuracy: 0.9829
1046/1688 [=================&gt;............] - ETA: 1s - loss: 0.0519 - accuracy: 0.9828
1072/1688 [==================&gt;...........] - ETA: 1s - loss: 0.0527 - accuracy: 0.9827
1098/1688 [==================&gt;...........] - ETA: 1s - loss: 0.0526 - accuracy: 0.9826
1124/1688 [==================&gt;...........] - ETA: 1s - loss: 0.0522 - accuracy: 0.9827
1150/1688 [===================&gt;..........] - ETA: 1s - loss: 0.0520 - accuracy: 0.9828
1176/1688 [===================&gt;..........] - ETA: 0s - loss: 0.0522 - accuracy: 0.9828
1201/1688 [====================&gt;.........] - ETA: 0s - loss: 0.0520 - accuracy: 0.9829
1227/1688 [====================&gt;.........] - ETA: 0s - loss: 0.0525 - accuracy: 0.9827
1253/1688 [=====================&gt;........] - ETA: 0s - loss: 0.0521 - accuracy: 0.9828
1279/1688 [=====================&gt;........] - ETA: 0s - loss: 0.0523 - accuracy: 0.9827
1305/1688 [======================&gt;.......] - ETA: 0s - loss: 0.0520 - accuracy: 0.9829
1331/1688 [======================&gt;.......] - ETA: 0s - loss: 0.0519 - accuracy: 0.9828
1356/1688 [=======================&gt;......] - ETA: 0s - loss: 0.0521 - accuracy: 0.9829
1381/1688 [=======================&gt;......] - ETA: 0s - loss: 0.0517 - accuracy: 0.9829
1407/1688 [========================&gt;.....] - ETA: 0s - loss: 0.0517 - accuracy: 0.9829
1432/1688 [========================&gt;.....] - ETA: 0s - loss: 0.0521 - accuracy: 0.9829
1459/1688 [========================&gt;.....] - ETA: 0s - loss: 0.0521 - accuracy: 0.9829
1485/1688 [=========================&gt;....] - ETA: 0s - loss: 0.0524 - accuracy: 0.9827
1512/1688 [=========================&gt;....] - ETA: 0s - loss: 0.0520 - accuracy: 0.9828
1538/1688 [==========================&gt;...] - ETA: 0s - loss: 0.0519 - accuracy: 0.9828
1564/1688 [==========================&gt;...] - ETA: 0s - loss: 0.0517 - accuracy: 0.9829
1590/1688 [===========================&gt;..] - ETA: 0s - loss: 0.0517 - accuracy: 0.9829
1616/1688 [===========================&gt;..] - ETA: 0s - loss: 0.0514 - accuracy: 0.9829
1642/1688 [============================&gt;.] - ETA: 0s - loss: 0.0516 - accuracy: 0.9829
1667/1688 [============================&gt;.] - ETA: 0s - loss: 0.0515 - accuracy: 0.9830
1688/1688 [==============================] - 4s 2ms/step - loss: 0.0519 - accuracy: 0.9829 - val_loss: 0.0601 - val_accuracy: 0.9832
Epoch 3/5

   1/1688 [..............................] - ETA: 0s - loss: 4.5511e-04 - accuracy: 1.0000
  28/1688 [..............................] - ETA: 3s - loss: 0.0273 - accuracy: 0.9888    
  54/1688 [..............................] - ETA: 3s - loss: 0.0212 - accuracy: 0.9925
  80/1688 [&gt;.............................] - ETA: 3s - loss: 0.0271 - accuracy: 0.9910
 105/1688 [&gt;.............................] - ETA: 3s - loss: 0.0302 - accuracy: 0.9893
 131/1688 [=&gt;............................] - ETA: 3s - loss: 0.0282 - accuracy: 0.9897
 157/1688 [=&gt;............................] - ETA: 2s - loss: 0.0330 - accuracy: 0.9883
 183/1688 [==&gt;...........................] - ETA: 2s - loss: 0.0348 - accuracy: 0.9887
 209/1688 [==&gt;...........................] - ETA: 2s - loss: 0.0366 - accuracy: 0.9886
 235/1688 [===&gt;..........................] - ETA: 2s - loss: 0.0362 - accuracy: 0.9883
 261/1688 [===&gt;..........................] - ETA: 2s - loss: 0.0369 - accuracy: 0.9881
 287/1688 [====&gt;.........................] - ETA: 2s - loss: 0.0371 - accuracy: 0.9876
 313/1688 [====&gt;.........................] - ETA: 2s - loss: 0.0379 - accuracy: 0.9874
 339/1688 [=====&gt;........................] - ETA: 2s - loss: 0.0367 - accuracy: 0.9876
 365/1688 [=====&gt;........................] - ETA: 2s - loss: 0.0368 - accuracy: 0.9878
 391/1688 [=====&gt;........................] - ETA: 2s - loss: 0.0368 - accuracy: 0.9878
 418/1688 [======&gt;.......................] - ETA: 2s - loss: 0.0383 - accuracy: 0.9871
 444/1688 [======&gt;.......................] - ETA: 2s - loss: 0.0391 - accuracy: 0.9867
 469/1688 [=======&gt;......................] - ETA: 2s - loss: 0.0403 - accuracy: 0.9863
 495/1688 [=======&gt;......................] - ETA: 2s - loss: 0.0411 - accuracy: 0.9860
 521/1688 [========&gt;.....................] - ETA: 2s - loss: 0.0433 - accuracy: 0.9856
 547/1688 [========&gt;.....................] - ETA: 2s - loss: 0.0432 - accuracy: 0.9857
 573/1688 [=========&gt;....................] - ETA: 2s - loss: 0.0434 - accuracy: 0.9858
 599/1688 [=========&gt;....................] - ETA: 2s - loss: 0.0427 - accuracy: 0.9860
 625/1688 [==========&gt;...................] - ETA: 2s - loss: 0.0434 - accuracy: 0.9857
 651/1688 [==========&gt;...................] - ETA: 2s - loss: 0.0437 - accuracy: 0.9857
 676/1688 [===========&gt;..................] - ETA: 1s - loss: 0.0438 - accuracy: 0.9858
 702/1688 [===========&gt;..................] - ETA: 1s - loss: 0.0442 - accuracy: 0.9857
 728/1688 [===========&gt;..................] - ETA: 1s - loss: 0.0455 - accuracy: 0.9854
 754/1688 [============&gt;.................] - ETA: 1s - loss: 0.0458 - accuracy: 0.9852
 780/1688 [============&gt;.................] - ETA: 1s - loss: 0.0451 - accuracy: 0.9855
 807/1688 [=============&gt;................] - ETA: 1s - loss: 0.0451 - accuracy: 0.9854
 833/1688 [=============&gt;................] - ETA: 1s - loss: 0.0446 - accuracy: 0.9856
 859/1688 [==============&gt;...............] - ETA: 1s - loss: 0.0447 - accuracy: 0.9856
 885/1688 [==============&gt;...............] - ETA: 1s - loss: 0.0447 - accuracy: 0.9856
 910/1688 [===============&gt;..............] - ETA: 1s - loss: 0.0442 - accuracy: 0.9856
 936/1688 [===============&gt;..............] - ETA: 1s - loss: 0.0444 - accuracy: 0.9856
 962/1688 [================&gt;.............] - ETA: 1s - loss: 0.0439 - accuracy: 0.9857
 988/1688 [================&gt;.............] - ETA: 1s - loss: 0.0441 - accuracy: 0.9857
1013/1688 [=================&gt;............] - ETA: 1s - loss: 0.0440 - accuracy: 0.9857
1039/1688 [=================&gt;............] - ETA: 1s - loss: 0.0439 - accuracy: 0.9857
1065/1688 [=================&gt;............] - ETA: 1s - loss: 0.0448 - accuracy: 0.9855
1091/1688 [==================&gt;...........] - ETA: 1s - loss: 0.0447 - accuracy: 0.9854
1117/1688 [==================&gt;...........] - ETA: 1s - loss: 0.0442 - accuracy: 0.9856
1143/1688 [===================&gt;..........] - ETA: 1s - loss: 0.0443 - accuracy: 0.9856
1168/1688 [===================&gt;..........] - ETA: 1s - loss: 0.0446 - accuracy: 0.9856
1195/1688 [====================&gt;.........] - ETA: 0s - loss: 0.0444 - accuracy: 0.9856
1221/1688 [====================&gt;.........] - ETA: 0s - loss: 0.0440 - accuracy: 0.9857
1248/1688 [=====================&gt;........] - ETA: 0s - loss: 0.0437 - accuracy: 0.9859
1274/1688 [=====================&gt;........] - ETA: 0s - loss: 0.0439 - accuracy: 0.9859
1300/1688 [======================&gt;.......] - ETA: 0s - loss: 0.0447 - accuracy: 0.9857
1326/1688 [======================&gt;.......] - ETA: 0s - loss: 0.0445 - accuracy: 0.9857
1351/1688 [=======================&gt;......] - ETA: 0s - loss: 0.0443 - accuracy: 0.9858
1377/1688 [=======================&gt;......] - ETA: 0s - loss: 0.0440 - accuracy: 0.9859
1403/1688 [=======================&gt;......] - ETA: 0s - loss: 0.0438 - accuracy: 0.9859
1429/1688 [========================&gt;.....] - ETA: 0s - loss: 0.0437 - accuracy: 0.9860
1456/1688 [========================&gt;.....] - ETA: 0s - loss: 0.0441 - accuracy: 0.9859
1482/1688 [=========================&gt;....] - ETA: 0s - loss: 0.0443 - accuracy: 0.9859
1508/1688 [=========================&gt;....] - ETA: 0s - loss: 0.0443 - accuracy: 0.9858
1534/1688 [==========================&gt;...] - ETA: 0s - loss: 0.0444 - accuracy: 0.9857
1560/1688 [==========================&gt;...] - ETA: 0s - loss: 0.0445 - accuracy: 0.9857
1586/1688 [===========================&gt;..] - ETA: 0s - loss: 0.0451 - accuracy: 0.9856
1612/1688 [===========================&gt;..] - ETA: 0s - loss: 0.0452 - accuracy: 0.9855
1638/1688 [============================&gt;.] - ETA: 0s - loss: 0.0451 - accuracy: 0.9856
1664/1688 [============================&gt;.] - ETA: 0s - loss: 0.0452 - accuracy: 0.9855
1688/1688 [==============================] - 4s 2ms/step - loss: 0.0452 - accuracy: 0.9855 - val_loss: 0.0759 - val_accuracy: 0.9803
Epoch 4/5

   1/1688 [..............................] - ETA: 0s - loss: 0.0385 - accuracy: 0.9688
  27/1688 [..............................] - ETA: 3s - loss: 0.0398 - accuracy: 0.9907
  53/1688 [..............................] - ETA: 3s - loss: 0.0407 - accuracy: 0.9876
  79/1688 [&gt;.............................] - ETA: 3s - loss: 0.0380 - accuracy: 0.9877
 105/1688 [&gt;.............................] - ETA: 3s - loss: 0.0372 - accuracy: 0.9878
 130/1688 [=&gt;............................] - ETA: 3s - loss: 0.0321 - accuracy: 0.9899
 156/1688 [=&gt;............................] - ETA: 2s - loss: 0.0309 - accuracy: 0.9904
 182/1688 [==&gt;...........................] - ETA: 2s - loss: 0.0321 - accuracy: 0.9899
 209/1688 [==&gt;...........................] - ETA: 2s - loss: 0.0318 - accuracy: 0.9895
 234/1688 [===&gt;..........................] - ETA: 2s - loss: 0.0315 - accuracy: 0.9899
 260/1688 [===&gt;..........................] - ETA: 2s - loss: 0.0315 - accuracy: 0.9899
 286/1688 [====&gt;.........................] - ETA: 2s - loss: 0.0321 - accuracy: 0.9894
 312/1688 [====&gt;.........................] - ETA: 2s - loss: 0.0319 - accuracy: 0.9895
 338/1688 [=====&gt;........................] - ETA: 2s - loss: 0.0317 - accuracy: 0.9898
 364/1688 [=====&gt;........................] - ETA: 2s - loss: 0.0320 - accuracy: 0.9895
 390/1688 [=====&gt;........................] - ETA: 2s - loss: 0.0316 - accuracy: 0.9897
 415/1688 [======&gt;.......................] - ETA: 2s - loss: 0.0314 - accuracy: 0.9898
 441/1688 [======&gt;.......................] - ETA: 2s - loss: 0.0330 - accuracy: 0.9893
 467/1688 [=======&gt;......................] - ETA: 2s - loss: 0.0335 - accuracy: 0.9891
 493/1688 [=======&gt;......................] - ETA: 2s - loss: 0.0335 - accuracy: 0.9890
 520/1688 [========&gt;.....................] - ETA: 2s - loss: 0.0339 - accuracy: 0.9890
 546/1688 [========&gt;.....................] - ETA: 2s - loss: 0.0335 - accuracy: 0.9891
 572/1688 [=========&gt;....................] - ETA: 2s - loss: 0.0338 - accuracy: 0.9890
 598/1688 [=========&gt;....................] - ETA: 2s - loss: 0.0338 - accuracy: 0.9888
 624/1688 [==========&gt;...................] - ETA: 2s - loss: 0.0339 - accuracy: 0.9888
 650/1688 [==========&gt;...................] - ETA: 2s - loss: 0.0343 - accuracy: 0.9888
 677/1688 [===========&gt;..................] - ETA: 1s - loss: 0.0342 - accuracy: 0.9888
 704/1688 [===========&gt;..................] - ETA: 1s - loss: 0.0342 - accuracy: 0.9887
 730/1688 [===========&gt;..................] - ETA: 1s - loss: 0.0343 - accuracy: 0.9887
 756/1688 [============&gt;.................] - ETA: 1s - loss: 0.0348 - accuracy: 0.9886
 782/1688 [============&gt;.................] - ETA: 1s - loss: 0.0359 - accuracy: 0.9883
 808/1688 [=============&gt;................] - ETA: 1s - loss: 0.0367 - accuracy: 0.9881
 834/1688 [=============&gt;................] - ETA: 1s - loss: 0.0364 - accuracy: 0.9881
 860/1688 [==============&gt;...............] - ETA: 1s - loss: 0.0361 - accuracy: 0.9882
 886/1688 [==============&gt;...............] - ETA: 1s - loss: 0.0370 - accuracy: 0.9880
 912/1688 [===============&gt;..............] - ETA: 1s - loss: 0.0368 - accuracy: 0.9881
 938/1688 [===============&gt;..............] - ETA: 1s - loss: 0.0367 - accuracy: 0.9881
 964/1688 [================&gt;.............] - ETA: 1s - loss: 0.0366 - accuracy: 0.9880
 990/1688 [================&gt;.............] - ETA: 1s - loss: 0.0366 - accuracy: 0.9880
1016/1688 [=================&gt;............] - ETA: 1s - loss: 0.0367 - accuracy: 0.9879
1042/1688 [=================&gt;............] - ETA: 1s - loss: 0.0371 - accuracy: 0.9879
1068/1688 [=================&gt;............] - ETA: 1s - loss: 0.0375 - accuracy: 0.9877
1093/1688 [==================&gt;...........] - ETA: 1s - loss: 0.0382 - accuracy: 0.9875
1119/1688 [==================&gt;...........] - ETA: 1s - loss: 0.0378 - accuracy: 0.9876
1145/1688 [===================&gt;..........] - ETA: 1s - loss: 0.0375 - accuracy: 0.9877
1171/1688 [===================&gt;..........] - ETA: 1s - loss: 0.0381 - accuracy: 0.9876
1197/1688 [====================&gt;.........] - ETA: 0s - loss: 0.0384 - accuracy: 0.9875
1224/1688 [====================&gt;.........] - ETA: 0s - loss: 0.0388 - accuracy: 0.9874
1250/1688 [=====================&gt;........] - ETA: 0s - loss: 0.0389 - accuracy: 0.9873
1276/1688 [=====================&gt;........] - ETA: 0s - loss: 0.0387 - accuracy: 0.9874
1301/1688 [======================&gt;.......] - ETA: 0s - loss: 0.0387 - accuracy: 0.9874
1327/1688 [======================&gt;.......] - ETA: 0s - loss: 0.0383 - accuracy: 0.9875
1353/1688 [=======================&gt;......] - ETA: 0s - loss: 0.0381 - accuracy: 0.9875
1379/1688 [=======================&gt;......] - ETA: 0s - loss: 0.0382 - accuracy: 0.9875
1405/1688 [=======================&gt;......] - ETA: 0s - loss: 0.0379 - accuracy: 0.9875
1431/1688 [========================&gt;.....] - ETA: 0s - loss: 0.0381 - accuracy: 0.9875
1457/1688 [========================&gt;.....] - ETA: 0s - loss: 0.0381 - accuracy: 0.9875
1484/1688 [=========================&gt;....] - ETA: 0s - loss: 0.0382 - accuracy: 0.9875
1510/1688 [=========================&gt;....] - ETA: 0s - loss: 0.0380 - accuracy: 0.9876
1536/1688 [==========================&gt;...] - ETA: 0s - loss: 0.0377 - accuracy: 0.9876
1562/1688 [==========================&gt;...] - ETA: 0s - loss: 0.0379 - accuracy: 0.9875
1588/1688 [===========================&gt;..] - ETA: 0s - loss: 0.0380 - accuracy: 0.9875
1614/1688 [===========================&gt;..] - ETA: 0s - loss: 0.0384 - accuracy: 0.9874
1640/1688 [============================&gt;.] - ETA: 0s - loss: 0.0389 - accuracy: 0.9873
1666/1688 [============================&gt;.] - ETA: 0s - loss: 0.0390 - accuracy: 0.9873
1688/1688 [==============================] - 4s 2ms/step - loss: 0.0392 - accuracy: 0.9872 - val_loss: 0.0570 - val_accuracy: 0.9845
Epoch 5/5

   1/1688 [..............................] - ETA: 0s - loss: 0.1100 - accuracy: 0.9688
  28/1688 [..............................] - ETA: 2s - loss: 0.0314 - accuracy: 0.9877
  53/1688 [..............................] - ETA: 3s - loss: 0.0410 - accuracy: 0.9858
  78/1688 [&gt;.............................] - ETA: 3s - loss: 0.0340 - accuracy: 0.9876
 104/1688 [&gt;.............................] - ETA: 3s - loss: 0.0360 - accuracy: 0.9877
 130/1688 [=&gt;............................] - ETA: 3s - loss: 0.0351 - accuracy: 0.9875
 156/1688 [=&gt;............................] - ETA: 2s - loss: 0.0379 - accuracy: 0.9876
 182/1688 [==&gt;...........................] - ETA: 2s - loss: 0.0372 - accuracy: 0.9876
 208/1688 [==&gt;...........................] - ETA: 2s - loss: 0.0391 - accuracy: 0.9872
 234/1688 [===&gt;..........................] - ETA: 2s - loss: 0.0374 - accuracy: 0.9880
 260/1688 [===&gt;..........................] - ETA: 2s - loss: 0.0371 - accuracy: 0.9882
 286/1688 [====&gt;.........................] - ETA: 2s - loss: 0.0369 - accuracy: 0.9883
 312/1688 [====&gt;.........................] - ETA: 2s - loss: 0.0355 - accuracy: 0.9888
 338/1688 [=====&gt;........................] - ETA: 2s - loss: 0.0341 - accuracy: 0.9893
 364/1688 [=====&gt;........................] - ETA: 2s - loss: 0.0343 - accuracy: 0.9890
 389/1688 [=====&gt;........................] - ETA: 2s - loss: 0.0339 - accuracy: 0.9890
 415/1688 [======&gt;.......................] - ETA: 2s - loss: 0.0348 - accuracy: 0.9886
 441/1688 [======&gt;.......................] - ETA: 2s - loss: 0.0342 - accuracy: 0.9887
 467/1688 [=======&gt;......................] - ETA: 2s - loss: 0.0342 - accuracy: 0.9887
 494/1688 [=======&gt;......................] - ETA: 2s - loss: 0.0340 - accuracy: 0.9887
 519/1688 [========&gt;.....................] - ETA: 2s - loss: 0.0338 - accuracy: 0.9887
 544/1688 [========&gt;.....................] - ETA: 2s - loss: 0.0337 - accuracy: 0.9887
 570/1688 [=========&gt;....................] - ETA: 2s - loss: 0.0337 - accuracy: 0.9889
 596/1688 [=========&gt;....................] - ETA: 2s - loss: 0.0343 - accuracy: 0.9889
 622/1688 [==========&gt;...................] - ETA: 2s - loss: 0.0342 - accuracy: 0.9889
 648/1688 [==========&gt;...................] - ETA: 2s - loss: 0.0335 - accuracy: 0.9891
 674/1688 [==========&gt;...................] - ETA: 1s - loss: 0.0341 - accuracy: 0.9889
 700/1688 [===========&gt;..................] - ETA: 1s - loss: 0.0334 - accuracy: 0.9890
 726/1688 [===========&gt;..................] - ETA: 1s - loss: 0.0343 - accuracy: 0.9887
 752/1688 [============&gt;.................] - ETA: 1s - loss: 0.0338 - accuracy: 0.9887
 778/1688 [============&gt;.................] - ETA: 1s - loss: 0.0335 - accuracy: 0.9888
 804/1688 [=============&gt;................] - ETA: 1s - loss: 0.0332 - accuracy: 0.9888
 830/1688 [=============&gt;................] - ETA: 1s - loss: 0.0332 - accuracy: 0.9889
 856/1688 [==============&gt;...............] - ETA: 1s - loss: 0.0331 - accuracy: 0.9890
 882/1688 [==============&gt;...............] - ETA: 1s - loss: 0.0328 - accuracy: 0.9891
 908/1688 [===============&gt;..............] - ETA: 1s - loss: 0.0331 - accuracy: 0.9890
 935/1688 [===============&gt;..............] - ETA: 1s - loss: 0.0331 - accuracy: 0.9891
 961/1688 [================&gt;.............] - ETA: 1s - loss: 0.0334 - accuracy: 0.9890
 986/1688 [================&gt;.............] - ETA: 1s - loss: 0.0332 - accuracy: 0.9891
1011/1688 [================&gt;.............] - ETA: 1s - loss: 0.0335 - accuracy: 0.9891
1037/1688 [=================&gt;............] - ETA: 1s - loss: 0.0334 - accuracy: 0.9891
1063/1688 [=================&gt;............] - ETA: 1s - loss: 0.0338 - accuracy: 0.9890
1088/1688 [==================&gt;...........] - ETA: 1s - loss: 0.0342 - accuracy: 0.9889
1114/1688 [==================&gt;...........] - ETA: 1s - loss: 0.0345 - accuracy: 0.9889
1140/1688 [===================&gt;..........] - ETA: 1s - loss: 0.0348 - accuracy: 0.9888
1166/1688 [===================&gt;..........] - ETA: 1s - loss: 0.0350 - accuracy: 0.9887
1192/1688 [====================&gt;.........] - ETA: 0s - loss: 0.0353 - accuracy: 0.9887
1218/1688 [====================&gt;.........] - ETA: 0s - loss: 0.0359 - accuracy: 0.9886
1244/1688 [=====================&gt;........] - ETA: 0s - loss: 0.0354 - accuracy: 0.9887
1269/1688 [=====================&gt;........] - ETA: 0s - loss: 0.0354 - accuracy: 0.9887
1296/1688 [======================&gt;.......] - ETA: 0s - loss: 0.0358 - accuracy: 0.9887
1323/1688 [======================&gt;.......] - ETA: 0s - loss: 0.0359 - accuracy: 0.9887
1349/1688 [======================&gt;.......] - ETA: 0s - loss: 0.0367 - accuracy: 0.9885
1375/1688 [=======================&gt;......] - ETA: 0s - loss: 0.0366 - accuracy: 0.9884
1401/1688 [=======================&gt;......] - ETA: 0s - loss: 0.0363 - accuracy: 0.9885
1427/1688 [========================&gt;.....] - ETA: 0s - loss: 0.0362 - accuracy: 0.9886
1453/1688 [========================&gt;.....] - ETA: 0s - loss: 0.0364 - accuracy: 0.9885
1479/1688 [=========================&gt;....] - ETA: 0s - loss: 0.0363 - accuracy: 0.9885
1505/1688 [=========================&gt;....] - ETA: 0s - loss: 0.0364 - accuracy: 0.9885
1531/1688 [==========================&gt;...] - ETA: 0s - loss: 0.0361 - accuracy: 0.9885
1557/1688 [==========================&gt;...] - ETA: 0s - loss: 0.0360 - accuracy: 0.9886
1583/1688 [===========================&gt;..] - ETA: 0s - loss: 0.0358 - accuracy: 0.9886
1609/1688 [===========================&gt;..] - ETA: 0s - loss: 0.0359 - accuracy: 0.9886
1635/1688 [============================&gt;.] - ETA: 0s - loss: 0.0359 - accuracy: 0.9885
1661/1688 [============================&gt;.] - ETA: 0s - loss: 0.0360 - accuracy: 0.9885
1687/1688 [============================&gt;.] - ETA: 0s - loss: 0.0359 - accuracy: 0.9886
1688/1688 [==============================] - 4s 2ms/step - loss: 0.0359 - accuracy: 0.9886 - val_loss: 0.0557 - val_accuracy: 0.9888
Test accuracy after fine tuning: 0.9846000075340271
</pre></div>
</div>
</div>
<div class="section" id="model-conversion">
<h2>6. Model conversion<a class="headerlink" href="#model-conversion" title="Permalink to this headline">¶</a></h2>
<p>After having obtained a quantized model with satisfactory performance, it can
be converted to a model suitable to be used in the Akida NSoC in inference
mode. The <a class="reference external" href="../api_reference/cnn2snn_apis.html#convert">convert</a>
function returns a model in Akida format, ready for the Akida NSoC or the
Akida Execution Engine.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>One needs to supply the coefficients used to rescale the input
dataset before the training - here <code class="docutils literal notranslate"><span class="pre">input_scaling</span></code>.</p>
</div>
<p>As with Keras, the summary() method provides a textual representation of the
Akida model.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">cnn2snn</span> <span class="kn">import</span> <span class="n">convert</span>

<span class="n">model_akida</span> <span class="o">=</span> <span class="n">convert</span><span class="p">(</span><span class="n">model_quantized</span><span class="p">,</span> <span class="n">input_scaling</span><span class="o">=</span><span class="n">input_scaling</span><span class="p">)</span>
<span class="n">model_akida</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>

<span class="n">results</span> <span class="o">=</span> <span class="n">model_akida</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">raw_x_test</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="p">(</span><span class="n">raw_y_test</span> <span class="o">==</span> <span class="n">results</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Test accuracy after conversion:&#39;</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">)</span>

<span class="c1"># For non-regression purpose</span>
<span class="k">assert</span> <span class="n">accuracy</span> <span class="o">&gt;</span> <span class="mf">0.97</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>                        Model Summary
_____________________________________________________________
Layer (type)                 Output shape  Kernel shape
=============================================================
conv2d (InputConvolutional)  [13, 13, 32]  (3, 3, 1, 32)
_____________________________________________________________
conv2d_1 (Convolutional)     [7, 7, 64]    (3, 3, 32, 64)
_____________________________________________________________
dense (FullyConnected)       [1, 1, 10]    (1, 1, 3136, 10)
_____________________________________________________________
Input shape: 28, 28, 1
Backend type: Software - 1.8.9


Test accuracy after conversion: 0.9843
</pre></div>
</div>
<p>Depending on the number of samples you run, you should find a
performance of around 98% (better results can be achieved using more
epochs for training).</p>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> ( 0 minutes  53.184 seconds)</p>
<div class="sphx-glr-footer class sphx-glr-footer-example docutils container" id="sphx-glr-download-examples-plot-cnn-flow-py">
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/5e3bf4dfd4795a473358be232d14448e/plot_cnn_flow.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">plot_cnn_flow.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/7d452896f8a0cf5503747c045ac9c320/plot_cnn_flow.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">plot_cnn_flow.ipynb</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="plot_ds_cnn_kws.html" class="btn btn-neutral float-right" title="DS-CNN/KWS inference" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="plot_regression.html" class="btn btn-neutral float-left" title="Regression tutorial" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright Copyright 2020, BrainChip Holdings Ltd. All Rights Reserved.

    </p>
  </div> 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>