<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Advanced CNN2SNN tutorial &mdash; Akida Examples  documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/sg_gallery.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/sg_gallery-binder.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/sg_gallery-dataframe.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/sg_gallery-rendered-html.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Akida vision edge learning" href="../edge/plot_0_edge_learning_vision.html" />
    <link rel="prev" title="CNN conversion flow tutorial" href="plot_0_cnn_flow.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #78b3ff" >
            <a href="../../index.html">
            <img src="../../_static/akida.svg" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                MetaTF 2.0.4
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../index.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../installation.html">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../installation.html#requirements">Requirements</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../installation.html#quick-installation">Quick installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../installation.html#running-examples">Running examples</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../user_guide/user_guide.html">User guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../user_guide/getting_started.html">Getting started</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/getting_started.html#for-beginners">For beginners</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/getting_started.html#for-users-familiar-with-deep-learning">For users familiar with deep-learning</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../user_guide/aee.html">Akida user guide</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/aee.html#introduction">Introduction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/aee.html#akida-layers">Akida layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/aee.html#input-format">Input Format</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/aee.html#a-versatile-machine-learning-framework">A versatile machine learning framework</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/aee.html#the-sequential-model">The Sequential model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/aee.html#specifying-the-model">Specifying the model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/aee.html#accessing-layer-parameters-and-weights">Accessing layer parameters and weights</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/aee.html#inference">Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/aee.html#saving-and-loading">Saving and loading</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/aee.html#input-layer-types">Input layer types</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/aee.html#data-processing-layer-types">Data-Processing layer types</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/aee.html#model-hardware-mapping">Model Hardware Mapping</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/aee.html#devices">Devices</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/aee.html#model-mapping">Model mapping</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/aee.html#advanced-mapping-details-and-hardware-devices-usage">Advanced Mapping Details and Hardware Devices Usage</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/aee.html#id1">Using Akida Edge learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/aee.html#learning-constraints">Learning constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/aee.html#compiling-a-layer">Compiling a layer</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../user_guide/cnn2snn.html">CNN2SNN toolkit</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/cnn2snn.html#overview">Overview</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#conversion-workflow">Conversion workflow</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#typical-training-scenario">Typical training scenario</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#design-compatibility-constraints">Design compatibility constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#quantization-compatibility-constraints">Quantization compatibility constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#command-line-interface">Command-line interface</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/cnn2snn.html#layers-considerations">Layers Considerations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#supported-layer-types">Supported layer types</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#cnn2snn-quantization-aware-layers">CNN2SNN Quantization-aware layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#training-only-layers">Training-Only Layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#first-layers">First Layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#id6">Final Layers</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/cnn2snn.html#tips-and-tricks">Tips and Tricks</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../user_guide/akida_models.html">Akida models zoo</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/akida_models.html#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/akida_models.html#command-line-interface-for-model-creation">Command-line interface for model creation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/akida_models.html#command-line-interface-for-model-training">Command-line interface for model training</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida_models.html#cifar10-training-and-tuning">CIFAR10 training and tuning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida_models.html#utk-face-training">UTK Face training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida_models.html#kws-training">KWS training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida_models.html#yolo-training">YOLO training</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/akida_models.html#command-line-interface-for-model-evaluation">Command-line interface for model evaluation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/akida_models.html#id1">Layer Blocks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida_models.html#conv-block"><code class="docutils literal notranslate"><span class="pre">conv_block</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida_models.html#dense-block"><code class="docutils literal notranslate"><span class="pre">dense_block</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida_models.html#separable-conv-block"><code class="docutils literal notranslate"><span class="pre">separable_conv_block</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../user_guide/hw_constraints.html">Hardware constraints</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/hw_constraints.html#akida-nsoc-pre-production">Akida NSoC (Pre-production)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/hw_constraints.html#inputconvolutional">InputConvolutional</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/hw_constraints.html#convolutional">Convolutional</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/hw_constraints.html#separableconvolutional">SeparableConvolutional</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/hw_constraints.html#fullyconnected">FullyConnected</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/hw_constraints.html#akida-nsoc-production">Akida NSoC (Production)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/hw_constraints.html#id1">InputConvolutional</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/hw_constraints.html#id2">Convolutional</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/hw_constraints.html#id3">SeparableConvolutional</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/hw_constraints.html#id4">FullyConnected</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../user_guide/compatibility.html">Akida versions compatibility</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/compatibility.html#upgrading-models-with-legacy-quantizers">Upgrading models with legacy quantizers</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../api_reference/api_reference.html">API reference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../api_reference/aee_apis.html">Akida Execution Engine</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#model">Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#layer">Layer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#layerstatistics">LayerStatistics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#inputdata">InputData</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#inputconvolutional">InputConvolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#fullyconnected">FullyConnected</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#convolutional">Convolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#separableconvolutional">SeparableConvolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#concat">Concat</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#backendtype">BackendType</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#padding">Padding</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#pooltype">PoolType</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#learningtype">LearningType</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#hwversion">HwVersion</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#compatibility">Compatibility</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#device">Device</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#hwdevice">HWDevice</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#socdriver">SocDriver</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#sequence">Sequence</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#program">Program</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#np">NP</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#soc">soc</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#powermeter">PowerMeter</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html">CNN2SNN</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#tool-functions">Tool functions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#quantize">quantize</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#quantize-layer">quantize_layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#convert">convert</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#check-model-compatibility">check_model_compatibility</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#load-quantized-model">load_quantized_model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#load-partial-weights">load_partial_weights</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#quantizers">Quantizers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#weightquantizer">WeightQuantizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#linearweightquantizer">LinearWeightQuantizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#stdweightquantizer">StdWeightQuantizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#trainablestdweightquantizer">TrainableStdWeightQuantizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#maxquantizer">MaxQuantizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#maxperaxisquantizer">MaxPerAxisQuantizer</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#quantized-layers">Quantized layers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#quantizedconv2d">QuantizedConv2D</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#quantizeddepthwiseconv2d">QuantizedDepthwiseConv2D</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#quantizeddense">QuantizedDense</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#quantizedseparableconv2d">QuantizedSeparableConv2D</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#quantizedactivation">QuantizedActivation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#activationdiscreterelu">ActivationDiscreteRelu</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#quantizedrelu">QuantizedReLU</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../api_reference/akida_models_apis.html">Akida models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_models_apis.html#layer-blocks">Layer blocks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#conv-block">conv_block</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#separable-conv-block">separable_conv_block</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#dense-block">dense_block</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_models_apis.html#helpers">Helpers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#batchnormalization-gamma-constraint">BatchNormalization gamma constraint</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_models_apis.html#knowledge-distillation">Knowledge distillation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_models_apis.html#pruning">Pruning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_models_apis.html#model-zoo">Model zoo</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#mobilenet">Mobilenet</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#ds-cnn">DS-CNN</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#vgg">VGG</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#yolo">YOLO</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#convtiny">ConvTiny</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#pointnet">PointNet++</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#gxnor">GXNOR</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Examples</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../index.html#general-examples">General examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../general/plot_0_gxnor_mnist.html">GXNOR/MNIST inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_0_gxnor_mnist.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_0_gxnor_mnist.html#create-a-keras-gxnor-model">2. Create a Keras GXNOR model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_0_gxnor_mnist.html#conversion-to-akida">3. Conversion to Akida</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../general/plot_1_ds_cnn_cifar10.html">DS-CNN CIFAR10 inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_1_ds_cnn_cifar10.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_1_ds_cnn_cifar10.html#create-a-keras-ds-cnn-model">2. Create a Keras DS-CNN model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_1_ds_cnn_cifar10.html#quantized-model">3. Quantized model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_1_ds_cnn_cifar10.html#pretrained-quantized-model">4. Pretrained quantized model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_1_ds_cnn_cifar10.html#conversion-to-akida">5. Conversion to Akida</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../general/plot_2_mobilenet_imagenet.html">MobileNet/ImageNet inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_2_mobilenet_imagenet.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_2_mobilenet_imagenet.html#create-a-keras-mobilenet-model">2. Create a Keras MobileNet model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_2_mobilenet_imagenet.html#quantized-model">3. Quantized model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_2_mobilenet_imagenet.html#pretrained-quantized-model">4. Pretrained quantized model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_2_mobilenet_imagenet.html#conversion-to-akida">5. Conversion to Akida</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../general/plot_3_ds_cnn_kws.html">DS-CNN/KWS inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_3_ds_cnn_kws.html#load-the-preprocessed-dataset">1. Load the preprocessed dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_3_ds_cnn_kws.html#load-a-pre-trained-native-keras-model">2. Load a pre-trained native Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_3_ds_cnn_kws.html#load-a-pre-trained-quantized-keras-model-satisfying-akida-nsoc-requirements">3. Load a pre-trained quantized Keras model satisfying Akida NSoC requirements</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_3_ds_cnn_kws.html#conversion-to-akida">4. Conversion to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_3_ds_cnn_kws.html#confusion-matrix">5. Confusion matrix</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../general/plot_4_regression.html">Regression tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_4_regression.html#load-the-dataset">1. Load the dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_4_regression.html#load-a-pre-trained-native-keras-model">2. Load a pre-trained native Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_4_regression.html#load-a-pre-trained-quantized-keras-model-satisfying-akida-nsoc-requirements">3. Load a pre-trained quantized Keras model satisfying Akida NSoC requirements</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_4_regression.html#conversion-to-akida">4. Conversion to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_4_regression.html#estimate-age-on-a-single-image">5. Estimate age on a single image</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../general/plot_5_transfer_learning.html">Transfer learning with MobileNet for cats vs. dogs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_5_transfer_learning.html#transfer-learning-process">Transfer learning process</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_5_transfer_learning.html#load-and-preprocess-data">1. Load and preprocess data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_5_transfer_learning.html#modify-a-pre-trained-base-keras-model">2. Modify a pre-trained base Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_5_transfer_learning.html#train-the-transferred-model-for-the-new-task">3. Train the transferred model for the new task</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_5_transfer_learning.html#quantize-the-top-layer">4 Quantize the top layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_5_transfer_learning.html#convert-to-akida">5. Convert to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_5_transfer_learning.html#plot-confusion-matrix">6. Plot confusion matrix</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../general/plot_6_voc_yolo_detection.html">YOLO/PASCAL-VOC detection tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_6_voc_yolo_detection.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_6_voc_yolo_detection.html#preprocessing-tools">2. Preprocessing tools</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_6_voc_yolo_detection.html#model-architecture">3. Model architecture</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_6_voc_yolo_detection.html#training">4. Training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_6_voc_yolo_detection.html#performance">5. Performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_6_voc_yolo_detection.html#conversion-to-akida">6. Conversion to Akida</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 current"><a class="reference internal" href="../index.html#cnn2snn-tutorials">CNN2SNN tutorials</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="plot_0_cnn_flow.html">CNN conversion flow tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="plot_0_cnn_flow.html#load-and-reshape-mnist-dataset">1. Load and reshape MNIST dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_0_cnn_flow.html#model-definition">2. Model definition</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_0_cnn_flow.html#model-training">3. Model training</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_0_cnn_flow.html#model-quantization">4. Model quantization</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_0_cnn_flow.html#model-fine-tuning-quantization-aware-training">5. Model fine tuning (quantization-aware training)</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_0_cnn_flow.html#model-conversion">6. Model conversion</a></li>
</ul>
</li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Advanced CNN2SNN tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#design-a-cnn2snn-quantized-model">1. Design a CNN2SNN quantized model</a></li>
<li class="toctree-l4"><a class="reference internal" href="#weight-quantizer-details">2. Weight Quantizer Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="#quantized-activation-layer-details">3. Quantized Activation Layer Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="#how-to-deal-with-too-high-scale-factors">4. How to deal with too high scale factors</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#edge-examples">Edge examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../edge/plot_0_edge_learning_vision.html">Akida vision edge learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../edge/plot_0_edge_learning_vision.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edge/plot_0_edge_learning_vision.html#prepare-akida-model-for-learning">2. Prepare Akida model for learning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edge/plot_0_edge_learning_vision.html#edge-learning-with-akida">3. Edge learning with Akida</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../edge/plot_1_edge_learning_kws.html">Akida edge learning for keyword spotting</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../edge/plot_1_edge_learning_kws.html#edge-learning-process">1. Edge learning process</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edge/plot_1_edge_learning_kws.html#dataset-preparation">2. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edge/plot_1_edge_learning_kws.html#prepare-akida-model-for-learning">3. Prepare Akida model for learning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edge/plot_1_edge_learning_kws.html#learn-with-akida-using-the-training-set">4. Learn with Akida using the training set</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edge/plot_1_edge_learning_kws.html#edge-learning">5. Edge learning</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../edge/plot_2_edge_learning_parameters.html">Tips to set Akida learning parameters</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../edge/plot_2_edge_learning_parameters.html#akida-learning-parameters">1. Akida learning parameters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edge/plot_2_edge_learning_parameters.html#create-akida-model">2. Create Akida model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edge/plot_2_edge_learning_parameters.html#estimate-the-required-number-of-weights-of-the-trainable-layer">3. Estimate the required number of weights of the trainable layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edge/plot_2_edge_learning_parameters.html#estimate-the-number-of-neurons-per-class">4. Estimate the number of neurons per class</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../zoo_performances.html">Model zoo performances</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../zoo_performances.html#image-icon-ref-image-domain"> Image domain</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../zoo_performances.html#classification">Classification</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zoo_performances.html#object-detection">Object detection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zoo_performances.html#regression">Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zoo_performances.html#face-recognition">Face recognition</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../zoo_performances.html#audio-icon-ref-audio-domain"> Audio domain</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../zoo_performances.html#keyword-spotting">Keyword spotting</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../zoo_performances.html#time-icon-ref-time-domain"> Time domain</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../zoo_performances.html#fault-detection">Fault detection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zoo_performances.html#id1">Classification</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../zoo_performances.html#pointcloud-icon-ref-point-cloud"> Point cloud</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../zoo_performances.html#id2">Classification</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/Brainchip-Inc/akida_examples/releases">Changelog</a></li>
<li class="toctree-l1"><a class="reference external" href="https://support.brainchip.com/portal/home">Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../license.html">License</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #78b3ff" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Akida Examples</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../index.html">Akida examples</a> &raquo;</li>
      <li>Advanced CNN2SNN tutorial</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p>Click <a class="reference internal" href="#sphx-glr-download-examples-cnn2snn-plot-1-advanced-cnn2snn-py"><span class="std std-ref">here</span></a>
to download the full example code</p>
</div>
<section class="sphx-glr-example-title" id="advanced-cnn2snn-tutorial">
<span id="sphx-glr-examples-cnn2snn-plot-1-advanced-cnn2snn-py"></span><h1>Advanced CNN2SNN tutorial<a class="headerlink" href="#advanced-cnn2snn-tutorial" title="Permalink to this headline"></a></h1>
<p>This tutorial gives insights about CNN2SNN for users who want to go deeper
into the quantization possibilities of Keras models. We recommend first looking
at the <a class="reference external" href="../../user_guide/cnn2snn.html">user guide</a>  and the
<a class="reference external" href="plot_0_cnn_flow.html">CNN2SNN conversion flow tutorial</a> to get started with
CNN2SNN.</p>
<p>The CNN2SNN toolkit offers an easy-to-use set of functions to get a quantized
model from a native Keras model and to convert it to an Akida model compatible
with the Akida NSoC. The <a class="reference external" href="../../api_reference/cnn2snn_apis.html#quantize">quantize</a>
and <a class="reference external" href="../../api_reference/cnn2snn_apis.html#quantize-layer">quantize_layer</a>
high-level functions replace native Keras layers into custom CNN2SNN quantized
layers which are derived from their Keras equivalents. However, these functions
are not designed to choose how the weights and activations are quantized. This
tutorial will present an alternative low-level method to define models with
customizable quantization of weights and activations.</p>
<section id="design-a-cnn2snn-quantized-model">
<h2>1. Design a CNN2SNN quantized model<a class="headerlink" href="#design-a-cnn2snn-quantized-model" title="Permalink to this headline"></a></h2>
<p>Unlike the standard CNN2SNN flow where a native Keras model is quantized
using the <code class="docutils literal notranslate"><span class="pre">quantize</span></code> and <code class="docutils literal notranslate"><span class="pre">quantize_layer</span></code> functions, a customizable
quantized model must be directly created using quantized layers.</p>
<p>The CNN2SNN toolkit supplies custom quantized layers to replace native
Keras neural layers (Conv2D, SeparableConv2D and Dense) and
activations (ReLU).</p>
<section id="quantized-neural-layers">
<h3>Quantized neural layers<a class="headerlink" href="#quantized-neural-layers" title="Permalink to this headline"></a></h3>
<p>The CNN2SNN quantized neural layers are:</p>
<ul class="simple">
<li><p><strong>QuantizedConv2D</strong>, derived from <code class="docutils literal notranslate"><span class="pre">keras.Conv2D</span></code></p></li>
<li><p><strong>QuantizedSeparableConv2D</strong>, derived from <code class="docutils literal notranslate"><span class="pre">keras.SeparableConv2D</span></code></p></li>
<li><p><strong>QuantizedDense</strong>, derived from <code class="docutils literal notranslate"><span class="pre">keras.Dense</span></code></p></li>
</ul>
<p>They are similar to their Keras counterparts, but have an additional
argument: <code class="docutils literal notranslate"><span class="pre">quantizer</span></code>. This parameter expects a <em>WeightQuantizer</em> object
that defines how the weights are discretized for a given bitwidth. Some
quantizers are proposed in the CNN2SNN API:</p>
<ul class="simple">
<li><p><strong>StdWeightQuantizer</strong> and <strong>TrainableStdWeightQuantizer</strong>: these two
quantizers use the standard deviation of the weights to compute
the range on which weights are discretized. The <em>StdWeightQuantizer</em> uses
a range equal to a fixed number of standard deviations. The trainable
version uses a variable number of standard deviations where this number
is a trainable parameter of the model.</p></li>
<li><p><strong>MaxQuantizer</strong> and <strong>MaxPerAxisQuantizer</strong>: these discretize on
a range based on the maximum of the absolute value of the weights. The
<em>MaxQuantizer</em> discretizes all weights within a layer based on their global
maximum, whereas the <em>MaxPerAxisQuantizer</em> discretizes each feature kernel,
in practice the last dimension of the weights tensor, independently based
on its local maximum.</p></li>
</ul>
<p>If those quantizers do not fit your specific needs, you can
create your own (cf. <a class="reference internal" href="#weight-quantizer-section"><span class="std std-ref">2. Weight Quantizer Details</span></a>).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <cite>QuantizedSeparableConv2D</cite> layer can accept two quantizers:
one <code class="docutils literal notranslate"><span class="pre">quantizer</span></code> for the pointwise convolution and a
<code class="docutils literal notranslate"><span class="pre">quantizer_dw</span></code> for the depthwise convolution. If the latter is
not defined, it is set by default to the same value as
<code class="docutils literal notranslate"><span class="pre">quantizer</span></code>.</p>
<p>For Akida compatibility, the depthwise quantizer must be a
per-tensor quantizer (i.e. all weights within the depthwise kernel
are quantized together) and not a per-axis quantizer (i.e. each
feature kernel is quantized independently). See more details
<a class="reference external" href="https://www.tensorflow.org/lite/performance/quantization_spec#per-axis_vs_per-tensor">here</a>.</p>
</div>
</section>
<section id="quantized-activation-layers">
<h3>Quantized activation layers<a class="headerlink" href="#quantized-activation-layers" title="Permalink to this headline"></a></h3>
<p>Similarly, a quantized activation layer returns values that are discretized
on a uniform grid. Two quantized activation layers are provided to replace
the native ReLU layers:</p>
<ul class="simple">
<li><p><strong>ActivationDiscreteRelu</strong>: a linear quantizer for ReLU, clipped at value 6.</p></li>
<li><p><strong>QuantizedRelu</strong>: a trainable activation layer where the activation threshold
and the max clipping value are learned.</p></li>
</ul>
<p>It is also possible to define a custom quantized activation layer. Details
are given in the section <a class="reference internal" href="#activation-section"><span class="std std-ref">3. Quantized Activation Layer Details</span></a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="docutils literal notranslate"><span class="pre">quantize</span></code> function is a high-level helper that automatically
replaces the neural layers with their corresponding quantized
counterparts, using
<a class="reference external" href="../../api_reference/cnn2snn_apis.html#maxperaxisquantizer">MaxPerAxisQuantizer</a>.
The ReLU layers are substituted by
<a class="reference external" href="../../api_reference/cnn2snn_apis.html#activationdiscreterelu">ActivationDiscreteRelu</a>
layers.</p>
</div>
</section>
<section id="load-pre-trained-weights-from-a-native-keras-model">
<h3>Load pre-trained weights from a native Keras model<a class="headerlink" href="#load-pre-trained-weights-from-a-native-keras-model" title="Permalink to this headline"></a></h3>
<p>In a standard quantization-aware training workflow, the pre-trained weights
from a native Keras model are loaded into the equivalent quantized model.
Weight quantizers and activation layers, such as the
<em>TrainableWeightQuantizer</em> and the <em>QuantizedReLU</em>, have trainable variables
(also called “weights” in Keras). For example, if a <em>Conv2D</em> layer with two
weights (convolutional weights and bias) is replaced by a <em>QuantizedConv2D</em>
with a TrainableWeightQuantizer, the new quantized layer has then three
weights: convolutional weights, bias and the quantizer variable.
Thus, the total number of weights in the quantized CNN2SNN model is larger,
compared to the equivalent native Keras model. Directly loading pre-trained
weights from the native Keras model using the Keras <code class="docutils literal notranslate"><span class="pre">load_weights</span></code> function
will then fail, as it expects that both source and destination models have
the same number of weights.</p>
<p>To circumvent this issue, the <code class="docutils literal notranslate"><span class="pre">cnn2snn.load_partial_weights</span></code> function loads
weights, even with extra variables, in the new model, provided that the
layer names in the two models are identical. We therefore recommend using the
same names in both native and quantized models.</p>
</section>
<section id="create-a-quantized-model">
<h3>Create a quantized model<a class="headerlink" href="#create-a-quantized-model" title="Permalink to this headline"></a></h3>
<p>Here, we illustrate how to create a quantized model, equivalent to a native
Keras model. We use the weight quantizers
and quantized activation layers available in the CNN2SNN package. Although
we present only one weight quantizer and one quantized activation, a quantized
model can be a mix of any quantizers and activations. For instance, every
neural layer can have a different weight quantizer with different parameters.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">Sequential</span><span class="p">,</span> <span class="n">Input</span><span class="p">,</span> <span class="n">layers</span>

<span class="c1"># Create a native Keras toy model</span>
<span class="n">model_keras</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">([</span>

    <span class="c1"># Input layer</span>
    <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>

    <span class="c1"># Conv + MaxPool + BatchNorm + ReLU</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">MaxPool2D</span><span class="p">(),</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">BatchNormalization</span><span class="p">(),</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>

    <span class="c1"># Flatten + Dense + Softmax</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">Softmax</span><span class="p">()</span>
<span class="p">])</span>

<span class="n">model_keras</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential_58&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d_1 (Conv2D)            (None, 26, 26, 8)         80
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 13, 13, 8)         0
_________________________________________________________________
batch_normalization_2 (Batch (None, 13, 13, 8)         32
_________________________________________________________________
re_lu_2 (ReLU)               (None, 13, 13, 8)         0
_________________________________________________________________
flatten_1 (Flatten)          (None, 1352)              0
_________________________________________________________________
dense_1 (Dense)              (None, 10)                13530
_________________________________________________________________
softmax (Softmax)            (None, 10)                0
=================================================================
Total params: 13,642
Trainable params: 13,626
Non-trainable params: 16
_________________________________________________________________
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">cnn2snn</span> <span class="kn">import</span> <span class="n">quantization_layers</span> <span class="k">as</span> <span class="n">qlayers</span>
<span class="kn">from</span> <span class="nn">cnn2snn</span> <span class="kn">import</span> <span class="n">quantization_ops</span> <span class="k">as</span> <span class="n">qops</span>

<span class="c1"># Prepare weight quantizers</span>
<span class="n">q1</span> <span class="o">=</span> <span class="n">qops</span><span class="o">.</span><span class="n">MaxQuantizer</span><span class="p">(</span><span class="n">bitwidth</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">q2</span> <span class="o">=</span> <span class="n">qops</span><span class="o">.</span><span class="n">MaxQuantizer</span><span class="p">(</span><span class="n">bitwidth</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

<span class="c1"># Get layer names to set them in the quantized model</span>
<span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="n">layer</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">model_keras</span><span class="o">.</span><span class="n">layers</span><span class="p">]</span>

<span class="c1"># Create a quantized model, equivalent to the native Keras model</span>
<span class="n">model_quantized</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">([</span>

    <span class="c1"># Input layer</span>
    <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>

    <span class="c1"># Conv + MaxPool + BatchNorm + ReLU</span>
    <span class="n">qlayers</span><span class="o">.</span><span class="n">QuantizedConv2D</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">quantizer</span><span class="o">=</span><span class="n">q1</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">names</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">MaxPool2D</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">names</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">BatchNormalization</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">names</span><span class="p">[</span><span class="mi">2</span><span class="p">]),</span>
    <span class="n">qlayers</span><span class="o">.</span><span class="n">QuantizedReLU</span><span class="p">(</span><span class="n">bitwidth</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">names</span><span class="p">[</span><span class="mi">3</span><span class="p">]),</span>

    <span class="c1"># Flatten + Dense + Softmax</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">names</span><span class="p">[</span><span class="mi">4</span><span class="p">]),</span>
    <span class="n">qlayers</span><span class="o">.</span><span class="n">QuantizedDense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">quantizer</span><span class="o">=</span><span class="n">q2</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">names</span><span class="p">[</span><span class="mi">5</span><span class="p">]),</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">names</span><span class="p">[</span><span class="mi">6</span><span class="p">]),</span>
<span class="p">])</span>

<span class="n">model_quantized</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential_59&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d_1 (QuantizedConv2D)   (None, 26, 26, 8)         80
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 13, 13, 8)         0
_________________________________________________________________
batch_normalization_2 (Batch (None, 13, 13, 8)         32
_________________________________________________________________
re_lu_2 (QuantizedReLU)      (None, 13, 13, 8)         2
_________________________________________________________________
flatten_1 (Flatten)          (None, 1352)              0
_________________________________________________________________
dense_1 (QuantizedDense)     (None, 10)                13530
_________________________________________________________________
softmax (Softmax)            (None, 10)                0
=================================================================
Total params: 13,644
Trainable params: 13,628
Non-trainable params: 16
_________________________________________________________________
</pre></div>
</div>
<p>As detailed in the summary, the <em>QuantizedReLU</em> layer has two trainable
parameters. The quantized model has then two parameters more than the
native Keras model. To load weights from the native model, we then
use the provided <code class="docutils literal notranslate"><span class="pre">load_partial_weights</span></code> function. Remember that both
models must have the same layer names.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">cnn2snn</span> <span class="kn">import</span> <span class="n">load_partial_weights</span>

<span class="n">load_partial_weights</span><span class="p">(</span><span class="n">model_quantized</span><span class="p">,</span> <span class="n">model_keras</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="weight-quantizer-details">
<span id="weight-quantizer-section"></span><h2>2. Weight Quantizer Details<a class="headerlink" href="#weight-quantizer-details" title="Permalink to this headline"></a></h2>
<section id="how-a-weight-quantizer-works">
<h3>How a weight quantizer works<a class="headerlink" href="#how-a-weight-quantizer-works" title="Permalink to this headline"></a></h3>
<p>The purpose of a weight quantizer is to compute a tensor of discretized
weights. It can be split into two operations:</p>
<ul class="simple">
<li><p>an optional transformation applied on the weights, e.g. a shift, a
non-linear transformation, …</p></li>
<li><p>the quantization of the weights.</p></li>
</ul>
<p>For Akida compatibility, the weights must be discretized on a symmetric grid
defined by two parameters:</p>
<ul class="simple">
<li><p>the <strong>bitwidth</strong> defines the number of unique values the weights can take.
We define <em>kmax = 2^(bitwidth-1) - 1</em>, being the maximum integer value of
the symmetric quantization scheme. For instance, a 4-bit quantizer must
return weights on a grid of 15 values, between -7 and 7. Here, <em>kmax = 7</em>.</p></li>
<li><p>the symmetric range on which the weights will be discretized (let’s say
between <em>-lim</em> and <em>lim</em>). Instead of working with the range, we use the
<strong>scale factor</strong> which is defined by <em>sf = kmax / lim</em>, where <em>sf</em> is the
scale factor. For instance with a 4-bit quantizer, the discretized weights
will be on the grid [<em>-7/sf, -6/sf, …, -1/sf, 0, 1/sf, …, 6/sf, 7/sf</em>].
The maximum discrete value <em>7/sf</em> is equal to <em>lim</em>, the limit of the range
(see figure below).</p></li>
</ul>
<a class="reference internal image-reference" href="../../_images/custom_quantizer.jpg"><img alt="../../_images/custom_quantizer.jpg" src="../../_images/custom_quantizer.jpg" style="width: 759.5px; height: 284.2px;" /></a>
<p>When training, the weight quantization is applied during the forward pass:
the weights are quantized and then used for the convolution or the fully
connected operation. However, during the back-propagation phase, the gradient
is computed as if there were no quantization and the weights are updated
based on their original values before quantization. This is usually called
the “Straight-Through Estimator” (STE) and it can be done using the
<code class="docutils literal notranslate"><span class="pre">tf.stop_gradient</span></code> function.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Remember that the weights are stored as standard float values in
the model. To get the quantized weights, you must first retrieve
the standard weights, using <code class="docutils literal notranslate"><span class="pre">get_weights()</span></code>. Then, you can apply
the <code class="docutils literal notranslate"><span class="pre">quantize</span></code> function of the weight quantizer to obtain the
discretized weights. Finally, if you want to get the integer
quantized values (between <em>-kmax</em> and <em>kmax</em>), you must multiply
the discretized weights by the scale factor.</p>
</div>
</section>
<section id="how-to-create-a-custom-weight-quantizer">
<h3>How to create a custom weight quantizer<a class="headerlink" href="#how-to-create-a-custom-weight-quantizer" title="Permalink to this headline"></a></h3>
<p>The CNN2SNN API proposes a way to create a custom weight quantizer. It must
be derived from the <code class="docutils literal notranslate"><span class="pre">WeightQuantizer</span></code> base class and must override two
methods:</p>
<ul class="simple">
<li><p>the <code class="docutils literal notranslate"><span class="pre">scale_factor(w)</span></code> method, returning the scale factor based on the
input weights. The output must be a scalar or vectorial TensorFlow tensor.
Per-tensor quantization will give a single scalar value, whereas
per-axis quantization will yield a vector with a scale factor for each
feature kernel.</p></li>
<li><p>the <code class="docutils literal notranslate"><span class="pre">quantize(w)</span></code> method, returning the discretized weights based on the
scale factor and the bitwidth. A Tensorflow tensor must be returned. The
two operations (optional transformation and quantization) are performed in
here.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To be able to correctly train a quantized model, it is important
to implement the STE estimator in the <code class="docutils literal notranslate"><span class="pre">quantize</span></code> function, by
using <code class="docutils literal notranslate"><span class="pre">tf.stop_gradient</span></code> at the quantization operation.</p>
</div>
<p>If there is no need for the optional transformation in the custom quantizer,
the CNN2SNN toolkit gives a <code class="docutils literal notranslate"><span class="pre">LinearWeightQuantizer</span></code> that skips this
step. The <code class="docutils literal notranslate"><span class="pre">quantize</span></code> function is already provided and only the
<code class="docutils literal notranslate"><span class="pre">scale_factor</span></code> function must be overridden.</p>
</section>
<section id="why-use-a-different-quantizer">
<h3>Why use a different quantizer<a class="headerlink" href="#why-use-a-different-quantizer" title="Permalink to this headline"></a></h3>
<p>Let’s now see a use case where it is interesting to consider the behaviour of
different quantizers. The <em>MaxQuantizer</em> used in the <em>QuantizedDense</em> layer
of our above model discretizes the weights based on their maximum value. The
default <em>MaxPerAxisQuantizer</em> has a similar behaviour with an additional
per-axis quantization design. If weights contain outliers, that are very
large weights in absolute value, this quantization scheme based on maximum
value can be inappropriate. Let’s look at it in practice: we retrieve the
weights of the QuantizedDense layer and compute the discretized counterparts
using the <em>MaxQuantizer</em> of the layer.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Retrieve weights and quantizer of the QuantizedDense layer</span>
<span class="n">dense_name</span> <span class="o">=</span> <span class="n">names</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span>
<span class="n">quantizer</span> <span class="o">=</span> <span class="n">model_quantized</span><span class="o">.</span><span class="n">get_layer</span><span class="p">(</span><span class="n">dense_name</span><span class="p">)</span><span class="o">.</span><span class="n">quantizer</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">model_quantized</span><span class="o">.</span><span class="n">get_layer</span><span class="p">(</span><span class="n">dense_name</span><span class="p">)</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># Artificially add outliers</span>
<span class="n">w</span><span class="p">[:</span><span class="mi">5</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="mf">0.5</span>

<span class="c1"># Compute discretized weights</span>
<span class="n">wq</span> <span class="o">=</span> <span class="n">quantizer</span><span class="o">.</span><span class="n">quantize</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">w</span><span class="p">))</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>


<span class="c1"># Show original and discretized weights histograms</span>
<span class="k">def</span> <span class="nf">plot_discretized_weights</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">wq</span><span class="p">):</span>
    <span class="n">xlim</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.095</span><span class="p">,</span> <span class="mf">0.53</span><span class="p">]</span>
    <span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">xlim</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">get_yaxis</span><span class="p">()</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">title</span><span class="o">.</span><span class="n">set_text</span><span class="p">(</span><span class="s2">&quot;Original weights distribution&quot;</span><span class="p">)</span>

    <span class="n">vals</span><span class="p">,</span> <span class="n">counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">wq</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">vals</span><span class="p">,</span> <span class="n">counts</span><span class="p">,</span> <span class="mf">0.005</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">xlim</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">get_yaxis</span><span class="p">()</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">title</span><span class="o">.</span><span class="n">set_text</span><span class="p">(</span><span class="s2">&quot;Discretized weights distribution&quot;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>


<span class="n">plot_discretized_weights</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">wq</span><span class="p">)</span>
</pre></div>
</div>
<img src="../../_images/sphx_glr_plot_1_advanced_cnn2snn_001.png" srcset="../../_images/sphx_glr_plot_1_advanced_cnn2snn_001.png" alt="Original weights distribution, Discretized weights distribution" class = "sphx-glr-single-img"/><p>The graphs above illustrate that a <em>MaxQuantizer</em> applied on weights with
outliers will keep the full range of weights to discretize. In this use case,
the large majority of weights is between -0.1 and 0.1, and are discretized
on only three quantization values. The outliers at 0.5 are preserved after
quantization. If outlier weights don’t represent much information in the
layer, it can be preferable to use another weight quantizer which “forgets”
them.</p>
<p>The <em>StdWeightQuantizer</em> is a good alternative for this use case: the
quantization range is based on the standard deviation of the original
weights. Outliers have little impact on the standard deviation of the
weights. Then the outliers can be out of the range based on the standard
deviation.</p>
<p>In this tutorial, instead of directly using the <em>StdWeightQuantizer</em>, we
present how to create a quantizer. The custom quantizer created below is a
simplified version of the <em>StdWeightQuantizer</em>. It is derived from the
<a class="reference external" href="../../api_reference/cnn2snn_apis.html#linearweightquantizer">LinearWeightQuantizer</a>.
As mentioned above, the <code class="docutils literal notranslate"><span class="pre">quantize</span></code> function is already implemented in
<em>LinearWeightQuantizer</em>. Only the <code class="docutils literal notranslate"><span class="pre">scale_factor</span></code> function must be
overridden.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define a custom weight quantizer</span>
<span class="k">class</span> <span class="nc">CustomStdQuantizer</span><span class="p">(</span><span class="n">qops</span><span class="o">.</span><span class="n">LinearWeightQuantizer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;This is a custom weight quantizer that defines the scale factor based</span>
<span class="sd">    on the standard deviation of the weights.</span>

<span class="sd">    The weights in range (-2*std, 2*std) are quantized into (2**bitwidth - 1)</span>
<span class="sd">    levels and the weights outside this range are clipped to ±2*std.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">scale_factor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
        <span class="n">std_dev</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">reduce_std</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">kmax_</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">std_dev</span><span class="p">)</span>


<span class="n">quantizer_std</span> <span class="o">=</span> <span class="n">CustomStdQuantizer</span><span class="p">(</span><span class="n">bitwidth</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

<span class="c1"># Compute discretized weights</span>
<span class="n">wq</span> <span class="o">=</span> <span class="n">quantizer_std</span><span class="o">.</span><span class="n">quantize</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">w</span><span class="p">))</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="c1"># Show original and discretized weights histograms</span>
<span class="n">plot_discretized_weights</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">wq</span><span class="p">)</span>
</pre></div>
</div>
<img src="../../_images/sphx_glr_plot_1_advanced_cnn2snn_002.png" srcset="../../_images/sphx_glr_plot_1_advanced_cnn2snn_002.png" alt="Original weights distribution, Discretized weights distribution" class = "sphx-glr-single-img"/><p>The two graphs above show that using a quantizer based on the standard
deviation can remove the outliers and give a finer discretization of the
weights between -0.1 and 0.1. In this toy example, the <em>MaxQuantizer</em>
discretizes the “small” weights on 3 quantization values, whereas the
<em>CustomStdQuantizer</em> discretizes them on about 13-14 quantization values.
Depending on the need to preserve the outliers or not, one quantizer or
the other is preferable.</p>
<p>In our experience, the <em>MaxPerAxisQuantizer</em> yields better results in most
use cases, especially for post-training quantization, which is why it is the
default quantizer.</p>
</section>
</section>
<section id="quantized-activation-layer-details">
<span id="activation-section"></span><h2>3. Quantized Activation Layer Details<a class="headerlink" href="#quantized-activation-layer-details" title="Permalink to this headline"></a></h2>
<section id="how-a-quantized-activation-works">
<h3>How a quantized activation works<a class="headerlink" href="#how-a-quantized-activation-works" title="Permalink to this headline"></a></h3>
<p>A quantized activation layer works as a ReLU layer with an additional
quantization step. It can then be seen as a succession of two operations:</p>
<ul class="simple">
<li><p>a linear activation function, clipped between zero and a maximum
activation value</p></li>
<li><p>the quantization, which is a ceiling operation. The activations will be
uniformly quantized between zero and the maximum activation value.</p></li>
</ul>
<p>The linear activation function is defined by (cf. the blue line in the graph
below):</p>
<ul class="simple">
<li><p>the activation threshold: the value above which a neuron fires</p></li>
<li><p>the maximum activation value: any activation above will be clipped</p></li>
<li><p>the slope of the linear function: unlike a ReLU function with a fixed
slope of 1, the CNN2SNN quantized activation accepts a different value.</p></li>
</ul>
<p>The quantization operation is defined by one parameter: the bitwidth. The
activation function is quantized using the ceiling operator on
<em>2^bitwidth - 1</em> positive activation levels. For instance, a 4-bit quantized
activation gives 15 activation levels (plus the zero activation) uniformly
distributed between zero and the maximum activation value (cf. the orange
line in the graph).</p>
<a class="reference internal image-reference" href="../../_images/custom_activation.jpg"><img alt="../../_images/custom_activation.jpg" src="../../_images/custom_activation.jpg" style="width: 861.75px; height: 428.25px;" /></a>
<p>During training, the ceiling quantization is performed in the forward pass:
the activations are discretized and then transferred to the next layer.
However, during the back-propagation phase, the gradient is computed as if
there were no quantization: only the gradient of the clipped linear
activation function (blue line above) is back-propagated. Like for weight
quantizers, this STE estimator is done using the <code class="docutils literal notranslate"><span class="pre">tf.stop_gradient</span></code>
function.</p>
</section>
<section id="how-to-create-a-custom-quantized-activation-layer">
<h3>How to create a custom quantized activation layer<a class="headerlink" href="#how-to-create-a-custom-quantized-activation-layer" title="Permalink to this headline"></a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">QuantizedActivation</span></code> base class lets users easily create custom
quantized activation layers. Three property functions must be overridden
and return scalar Tensorflow objects (tf.constant, tf.Variable):</p>
<ul class="simple">
<li><p>the <code class="docutils literal notranslate"><span class="pre">threshold</span></code> property, returning the activation threshold</p></li>
<li><p>the <code class="docutils literal notranslate"><span class="pre">step_height</span></code> property, returning the step height between two
activation levels. It is defined as the maximum activation value divided
by the number of activation levels (i.e. <em>2^bitwidth - 1</em>)</p></li>
<li><p>the <code class="docutils literal notranslate"><span class="pre">step_width</span></code> property, returning the step width as shown in the
above figure. It is computed as: <em>max_value / slope / (2^bitwidth - 1)</em></p></li>
</ul>
<p>Note that the slope of the linear activation function is equal to
<em>step_height/step_width</em>.</p>
</section>
<section id="why-use-a-different-quantized-activation">
<h3>Why use a different quantized activation<a class="headerlink" href="#why-use-a-different-quantized-activation" title="Permalink to this headline"></a></h3>
<p>The default <em>ActivationDiscreteRelu</em> layer does not allow choosing a maximum
activation value. For instance, a 4-bit <em>ActivationDiscreteRelu</em> layer clips
activations to 6. In use cases where input potentials are rather small,
let’s say smaller than 3, clipping to 6 means that the input potentials will
be quantized only on the first half of the possible activation levels. Let’s
see an example.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create an ActivationDiscreteRelu layer</span>
<span class="n">act_layer</span> <span class="o">=</span> <span class="n">qlayers</span><span class="o">.</span><span class="n">ActivationDiscreteRelu</span><span class="p">(</span><span class="n">bitwidth</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Activation step height: </span><span class="si">{</span><span class="n">act_layer</span><span class="o">.</span><span class="n">step_height</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Compute quantized activations for input potentials between -1 and 7</span>
<span class="n">input_potentials</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">activations</span> <span class="o">=</span> <span class="n">act_layer</span><span class="p">(</span><span class="n">input_potentials</span><span class="p">)</span>

<span class="c1"># Plot quantized activations</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">input_potentials</span><span class="p">,</span> <span class="n">activations</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="s1">&#39;.&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Quantized activations with ActivationDiscreteRelu&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img src="../../_images/sphx_glr_plot_1_advanced_cnn2snn_003.png" srcset="../../_images/sphx_glr_plot_1_advanced_cnn2snn_003.png" alt="Quantized activations with ActivationDiscreteRelu" class = "sphx-glr-single-img"/><p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Activation step height: 0.40
</pre></div>
</div>
<p>We can see that, with input potentials smaller than 3, shown by the dotted
vertical line, the output quantized activations only takes 7 levels, with a
step height of 0.4. We don’t benefit from all the quantization levels.</p>
<p>One option is to define a custom quantized activation layer where we can set
the maximum activation value. In our use case, we can set it to 3, in order
to take advantage of all the quantization levels by reducing the step height.
We suppose a slope of 1 and a threshold of half the step width (as set in
<em>ActivationDiscreteRelu</em>). We then override the three property functions.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">CustomQuantizedActivation</span><span class="p">(</span><span class="n">qlayers</span><span class="o">.</span><span class="n">QuantizedActivation</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bitwidth</span><span class="p">,</span> <span class="n">max_value</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">bitwidth</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_value</span> <span class="o">=</span> <span class="n">max_value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">step_height_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">max_value</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">levels</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">step_height</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">step_height_</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">step_width</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">step_height_</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">threshold</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">step_height_</span>

    <span class="k">def</span> <span class="nf">get_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">config</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span>
        <span class="n">config</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">&#39;max_value&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_value</span><span class="p">})</span>
        <span class="k">return</span> <span class="n">config</span>


<span class="c1"># Create a custom quantized activation layer</span>
<span class="n">custom_act_layer</span> <span class="o">=</span> <span class="n">CustomQuantizedActivation</span><span class="p">(</span><span class="n">bitwidth</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">max_value</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Custom activation step height: &quot;</span>
      <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">custom_act_layer</span><span class="o">.</span><span class="n">step_height</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Compute new quantized activations</span>
<span class="n">new_activations</span> <span class="o">=</span> <span class="n">custom_act_layer</span><span class="p">(</span><span class="n">input_potentials</span><span class="p">)</span>

<span class="c1"># Plot new quantized activations</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">input_potentials</span><span class="p">,</span> <span class="n">activations</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="s1">&#39;.&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">input_potentials</span><span class="p">,</span> <span class="n">new_activations</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="s1">&#39;.&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s2">&quot;ActivationDiscreteRelu&quot;</span><span class="p">,</span> <span class="s2">&quot;CustomQuantizedActivation&quot;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Quantized activations with CustomQuantizedActivation&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img src="../../_images/sphx_glr_plot_1_advanced_cnn2snn_004.png" srcset="../../_images/sphx_glr_plot_1_advanced_cnn2snn_004.png" alt="Quantized activations with CustomQuantizedActivation" class = "sphx-glr-single-img"/><p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Custom activation step height: 0.20
</pre></div>
</div>
<p>The quantized activations are clipped to 3 as expected, and the step height
is now 0.2. The activations between 0 and 3 are then discretized on 15
activation levels, versus 7 with <em>ActivationDiscreteRelu</em>. This new layer
gives a finer discretization and is better adjusted to our use case with
small potentials.</p>
<p>Besides, in the <em>QuantizedReLU</em> layer provided in the CNN2SNN toolkit, there
are two trainable variables that learn the activation threshold and the
step width. The step height is set to the step width, to preserve a slope of
1, as in the standard ReLU layer. This can be a suitable activation layer for
use cases where the maximum activation value is not known. The layer can learn
what are the best values to adapt to the input potentials.</p>
</section>
</section>
<section id="how-to-deal-with-too-high-scale-factors">
<span id="high-scale-factors"></span><h2>4. How to deal with too high scale factors<a class="headerlink" href="#how-to-deal-with-too-high-scale-factors" title="Permalink to this headline"></a></h2>
<p>A quantized Keras model may have sometimes very high scale factors, i.e. very
small weights, in the neural layers. During conversion into an Akida model,
these scale factors are used to compute the Akida fire thresholds and steps
required for Akida inference. However, these fire thresholds and steps are
limited in memory on NSoC. It may happen that their values are too big to fit
into memory and then a Runtime Error occurs at Akida inference, e.g. <code class="docutils literal notranslate"><span class="pre">Runtime</span>
<span class="pre">Error:</span> <span class="pre">Error</span> <span class="pre">when</span> <span class="pre">programming</span> <span class="pre">layer</span> <span class="pre">'separable_8':</span> <span class="pre">Backend</span> <span class="pre">Hardware(CNP):</span>
<span class="pre">1246278</span> <span class="pre">cannot</span> <span class="pre">fit</span> <span class="pre">in</span> <span class="pre">a</span> <span class="pre">20</span> <span class="pre">bits</span> <span class="pre">unsigned</span> <span class="pre">integer</span></code>.</p>
<p>If you’re facing this issue, it is necessary to retrain your Keras model to
avoid too high scale factors in the neural layers. One possible reason for
these high scale factors is the presence of very small gammas in
BatchNormalization (BN) layers. Indeed, when folding BN layers into their
preceding neural layers, the weights corresponding to tiny BN gammas become in
turn very small, which leads to high scale factors. The akida_models package
provides a tool to add constraint on BN gammas: the gammas are clipped to a
minimum value of 1e-2: the gammas cannot be smaller than this threshold. The
code snippet below illustrates how to use the provided tool. Note that it must
be applied on the Keras float (or quantized) model before folding BN layers.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">akida_models</span> <span class="kn">import</span> <span class="n">add_gamma_constraint</span>

<span class="c1"># Add BN gamma constraint on all BN layers of the model</span>
<span class="n">model_keras_with_gamma_constraint</span> <span class="o">=</span> <span class="n">add_gamma_constraint</span><span class="p">(</span><span class="n">model_keras</span><span class="p">)</span>
</pre></div>
</div>
<p>The new model can then be trained using <code class="docutils literal notranslate"><span class="pre">compile()</span></code> and <code class="docutils literal notranslate"><span class="pre">fit()</span></code> and
quantized if needed. The trained model will not have BN gammas less than 1e-2,
which is valuable to avoid very high scale factors.</p>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> ( 0 minutes  0.480 seconds)</p>
<div class="sphx-glr-footer class sphx-glr-footer-example docutils container" id="sphx-glr-download-examples-cnn2snn-plot-1-advanced-cnn2snn-py">
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/fd3541eadb2a3e014de84358069874b2/plot_1_advanced_cnn2snn.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">plot_1_advanced_cnn2snn.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/dd6b94a0aebdb62243f3a2f0bf975bf5/plot_1_advanced_cnn2snn.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">plot_1_advanced_cnn2snn.ipynb</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="plot_0_cnn_flow.html" class="btn btn-neutral float-left" title="CNN conversion flow tutorial" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../edge/plot_0_edge_learning_vision.html" class="btn btn-neutral float-right" title="Akida vision edge learning" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, BrainChip Holdings Ltd. All Rights Reserved.</p>
  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>