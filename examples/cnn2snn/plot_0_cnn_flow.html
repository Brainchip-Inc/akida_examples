<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>CNN conversion flow tutorial &mdash; Akida Examples  documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/sg_gallery.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/sg_gallery-binder.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/sg_gallery-dataframe.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/sg_gallery-rendered-html.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Advanced CNN2SNN tutorial" href="plot_1_advanced_cnn2snn.html" />
    <link rel="prev" title="YOLO/PASCAL-VOC detection tutorial" href="../general/plot_5_voc_yolo_detection.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #78b3ff" >
            <a href="../../index.html">
            <img src="../../_static/akida.svg" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                MetaTF 2.1.6
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../index.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../installation.html">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../installation.html#requirements">Requirements</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../installation.html#quick-installation">Quick installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../installation.html#running-examples">Running examples</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../user_guide/user_guide.html">User guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../user_guide/getting_started.html">Getting started</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/getting_started.html#for-beginners">For beginners</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/getting_started.html#for-users-familiar-with-deep-learning">For users familiar with deep-learning</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../user_guide/akida.html">Akida user guide</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/akida.html#introduction">Introduction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida.html#akida-layers">Akida layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida.html#input-format">Input Format</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida.html#a-versatile-machine-learning-framework">A versatile machine learning framework</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/akida.html#the-sequential-model">The Sequential model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida.html#specifying-the-model">Specifying the model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida.html#accessing-layer-parameters-and-weights">Accessing layer parameters and weights</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida.html#inference">Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida.html#saving-and-loading">Saving and loading</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida.html#input-layer-types">Input layer types</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida.html#data-processing-layer-types">Data-Processing layer types</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/akida.html#model-hardware-mapping">Model Hardware Mapping</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida.html#devices">Devices</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida.html#model-mapping">Model mapping</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida.html#advanced-mapping-details-and-hardware-devices-usage">Advanced Mapping Details and Hardware Devices Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida.html#performances-measurement">Performances measurement</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/akida.html#id1">Using Akida Edge learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida.html#learning-constraints">Learning constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida.html#compiling-a-layer">Compiling a layer</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../user_guide/cnn2snn.html">CNN2SNN toolkit</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/cnn2snn.html#overview">Overview</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#conversion-workflow">Conversion workflow</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#typical-training-scenario">Typical training scenario</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#design-compatibility-constraints">Design compatibility constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#quantization-compatibility-constraints">Quantization compatibility constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#command-line-interface">Command-line interface</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/cnn2snn.html#layers-considerations">Layers Considerations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#supported-layer-types">Supported layer types</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#cnn2snn-quantization-aware-layers">CNN2SNN Quantization-aware layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#training-only-layers">Training-Only Layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#first-layers">First Layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#id6">Final Layers</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/cnn2snn.html#tips-and-tricks">Tips and Tricks</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../user_guide/akida_models.html">Akida models zoo</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/akida_models.html#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/akida_models.html#command-line-interface-for-model-creation">Command-line interface for model creation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/akida_models.html#command-line-interface-for-model-training">Command-line interface for model training</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida_models.html#utk-face-training">UTK Face training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida_models.html#kws-training">KWS training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida_models.html#yolo-training">YOLO training</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/akida_models.html#command-line-interface-for-model-evaluation">Command-line interface for model evaluation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/akida_models.html#id1">Layer Blocks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida_models.html#conv-block"><code class="docutils literal notranslate"><span class="pre">conv_block</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida_models.html#dense-block"><code class="docutils literal notranslate"><span class="pre">dense_block</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/akida_models.html#separable-conv-block"><code class="docutils literal notranslate"><span class="pre">separable_conv_block</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../user_guide/hw_constraints.html">Hardware constraints</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/hw_constraints.html#inputconvolutional">InputConvolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/hw_constraints.html#convolutional">Convolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/hw_constraints.html#separableconvolutional">SeparableConvolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/hw_constraints.html#fullyconnected">FullyConnected</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../user_guide/compatibility.html">Akida versions compatibility</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/compatibility.html#upgrading-models-with-legacy-quantizers">Upgrading models with legacy quantizers</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../api_reference/api_reference.html">API reference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../api_reference/akida_apis.html">Akida runtime</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_apis.html#model">Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_apis.html#layer">Layer</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_apis.html#id1">Layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_apis.html#mapping">Mapping</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_apis.html#inputdata">InputData</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_apis.html#inputconvolutional">InputConvolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_apis.html#fullyconnected">FullyConnected</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_apis.html#convolutional">Convolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_apis.html#separableconvolutional">SeparableConvolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_apis.html#concat">Concat</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_apis.html#layer-parameters">Layer parameters</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_apis.html#layertype">LayerType</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_apis.html#padding">Padding</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_apis.html#pooltype">PoolType</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_apis.html#learningtype">LearningType</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_apis.html#sequence">Sequence</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_apis.html#id2">Sequence</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_apis.html#backendtype">BackendType</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_apis.html#pass">Pass</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_apis.html#device">Device</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_apis.html#id3">Device</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_apis.html#hwversion">HwVersion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_apis.html#hwdevice">HWDevice</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_apis.html#id4">HWDevice</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_apis.html#socdriver">SocDriver</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_apis.html#clockmode">ClockMode</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_apis.html#powermeter">PowerMeter</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_apis.html#np">NP</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_apis.html#tools">Tools</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_apis.html#sparsity">Sparsity</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_apis.html#compatibility">Compatibility</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html">CNN2SNN</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#tool-functions">Tool functions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#quantize">quantize</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#quantize-layer">quantize_layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#convert">convert</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#check-model-compatibility">check_model_compatibility</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#load-quantized-model">load_quantized_model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#transforms">Transforms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#calibration">Calibration</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#quantizers">Quantizers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#weightquantizer">WeightQuantizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#linearweightquantizer">LinearWeightQuantizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#stdweightquantizer">StdWeightQuantizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#stdperaxisquantizer">StdPerAxisQuantizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#maxquantizer">MaxQuantizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#maxperaxisquantizer">MaxPerAxisQuantizer</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#quantized-layers">Quantized layers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#quantizedconv2d">QuantizedConv2D</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#quantizeddense">QuantizedDense</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#quantizedseparableconv2d">QuantizedSeparableConv2D</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#quantizedactivation">QuantizedActivation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#activationdiscreterelu">ActivationDiscreteRelu</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#quantizedrelu">QuantizedReLU</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../api_reference/akida_models_apis.html">Akida models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_models_apis.html#layer-blocks">Layer blocks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#conv-block">conv_block</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#separable-conv-block">separable_conv_block</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#dense-block">dense_block</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_models_apis.html#helpers">Helpers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#batchnormalization-gamma-constraint">BatchNormalization gamma constraint</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_models_apis.html#knowledge-distillation">Knowledge distillation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_models_apis.html#pruning">Pruning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_models_apis.html#training">Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/akida_models_apis.html#model-zoo">Model zoo</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#akidanet">AkidaNet</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#mobilenet">Mobilenet</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#ds-cnn">DS-CNN</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#vgg">VGG</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#yolo">YOLO</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#convtiny">ConvTiny</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#pointnet">PointNet++</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api_reference/akida_models_apis.html#gxnor">GXNOR</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Examples</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../index.html#general-examples">General examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../general/plot_0_gxnor_mnist.html">GXNOR/MNIST inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_0_gxnor_mnist.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_0_gxnor_mnist.html#create-a-keras-gxnor-model">2. Create a Keras GXNOR model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_0_gxnor_mnist.html#conversion-to-akida">3. Conversion to Akida</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../general/plot_1_akidanet_imagenet.html">AkidaNet/ImageNet inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_1_akidanet_imagenet.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_1_akidanet_imagenet.html#create-a-keras-akidanet-model">2. Create a Keras AkidaNet model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_1_akidanet_imagenet.html#quantized-model">3. Quantized model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_1_akidanet_imagenet.html#pretrained-quantized-model">4. Pretrained quantized model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_1_akidanet_imagenet.html#conversion-to-akida">5. Conversion to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_1_akidanet_imagenet.html#hardware-mapping-and-performance">6. Hardware mapping and performance</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../general/plot_2_ds_cnn_kws.html">DS-CNN/KWS inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_2_ds_cnn_kws.html#load-the-preprocessed-dataset">1. Load the preprocessed dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_2_ds_cnn_kws.html#load-a-pre-trained-native-keras-model">2. Load a pre-trained native Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_2_ds_cnn_kws.html#load-a-pre-trained-quantized-keras-model-satisfying-akida-nsoc-requirements">3. Load a pre-trained quantized Keras model satisfying Akida NSoC requirements</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_2_ds_cnn_kws.html#conversion-to-akida">4. Conversion to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_2_ds_cnn_kws.html#confusion-matrix">5. Confusion matrix</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../general/plot_3_regression.html">Regression tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_3_regression.html#load-the-dataset">1. Load the dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_3_regression.html#load-a-pre-trained-native-keras-model">2. Load a pre-trained native Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_3_regression.html#load-a-pre-trained-quantized-keras-model-satisfying-akida-nsoc-requirements">3. Load a pre-trained quantized Keras model satisfying Akida NSoC requirements</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_3_regression.html#conversion-to-akida">4. Conversion to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_3_regression.html#estimate-age-on-a-single-image">5. Estimate age on a single image</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../general/plot_4_transfer_learning.html">Transfer learning with AkidaNet for PlantVillage</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_4_transfer_learning.html#transfer-learning-process">Transfer learning process</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_4_transfer_learning.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_4_transfer_learning.html#get-a-trained-akidanet-base-model">2. Get a trained AkidaNet base model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_4_transfer_learning.html#add-a-float-classification-head-to-the-model">3. Add a float classification head to the model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_4_transfer_learning.html#freeze-the-base-model">4. Freeze the base model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_4_transfer_learning.html#train-for-a-few-epochs">5. Train for a few epochs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_4_transfer_learning.html#quantize-the-classification-head">6. Quantize the classification head</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_4_transfer_learning.html#compute-accuracy">7. Compute accuracy</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../general/plot_5_voc_yolo_detection.html">YOLO/PASCAL-VOC detection tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_5_voc_yolo_detection.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_5_voc_yolo_detection.html#preprocessing-tools">2. Preprocessing tools</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_5_voc_yolo_detection.html#model-architecture">3. Model architecture</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_5_voc_yolo_detection.html#training">4. Training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_5_voc_yolo_detection.html#performance">5. Performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../general/plot_5_voc_yolo_detection.html#conversion-to-akida">6. Conversion to Akida</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 current"><a class="reference internal" href="../index.html#cnn2snn-tutorials">CNN2SNN tutorials</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">CNN conversion flow tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#load-and-reshape-mnist-dataset">1. Load and reshape MNIST dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="#model-definition">2. Model definition</a></li>
<li class="toctree-l4"><a class="reference internal" href="#model-training">3. Model training</a></li>
<li class="toctree-l4"><a class="reference internal" href="#model-quantization">4. Model quantization</a></li>
<li class="toctree-l4"><a class="reference internal" href="#model-fine-tuning-quantization-aware-training">5. Model fine tuning (quantization-aware training)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#model-conversion">6. Model conversion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="plot_1_advanced_cnn2snn.html">Advanced CNN2SNN tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="plot_1_advanced_cnn2snn.html#design-a-cnn2snn-quantized-model">1. Design a CNN2SNN quantized model</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_1_advanced_cnn2snn.html#weight-quantizer-details">2. Weight Quantizer Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_1_advanced_cnn2snn.html#understanding-quantized-activation">3. Understanding quantized activation</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_1_advanced_cnn2snn.html#how-to-deal-with-too-high-scale-factors">4. How to deal with too high scale factors</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#edge-examples">Edge examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../edge/plot_0_edge_learning_vision.html">Akida vision edge learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../edge/plot_0_edge_learning_vision.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edge/plot_0_edge_learning_vision.html#prepare-akida-model-for-learning">2. Prepare Akida model for learning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edge/plot_0_edge_learning_vision.html#edge-learning-with-akida">3. Edge learning with Akida</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../edge/plot_1_edge_learning_kws.html">Akida edge learning for keyword spotting</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../edge/plot_1_edge_learning_kws.html#edge-learning-process">1. Edge learning process</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edge/plot_1_edge_learning_kws.html#dataset-preparation">2. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edge/plot_1_edge_learning_kws.html#prepare-akida-model-for-learning">3. Prepare Akida model for learning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edge/plot_1_edge_learning_kws.html#learn-with-akida-using-the-training-set">4. Learn with Akida using the training set</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edge/plot_1_edge_learning_kws.html#edge-learning">5. Edge learning</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../edge/plot_2_edge_learning_parameters.html">Tips to set Akida learning parameters</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../edge/plot_2_edge_learning_parameters.html#akida-learning-parameters">1. Akida learning parameters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edge/plot_2_edge_learning_parameters.html#create-akida-model">2. Create Akida model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edge/plot_2_edge_learning_parameters.html#estimate-the-required-number-of-weights-of-the-trainable-layer">3. Estimate the required number of weights of the trainable layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../edge/plot_2_edge_learning_parameters.html#estimate-the-number-of-neurons-per-class">4. Estimate the number of neurons per class</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../zoo_performances.html">Model zoo performances</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../zoo_performances.html#image-icon-ref-image-domain"> Image domain</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../zoo_performances.html#classification">Classification</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zoo_performances.html#object-detection">Object detection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zoo_performances.html#regression">Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zoo_performances.html#face-recognition">Face recognition</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../zoo_performances.html#audio-icon-ref-audio-domain"> Audio domain</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../zoo_performances.html#keyword-spotting">Keyword spotting</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../zoo_performances.html#time-icon-ref-time-domain"> Time domain</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../zoo_performances.html#fault-detection">Fault detection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zoo_performances.html#id1">Classification</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../zoo_performances.html#pointcloud-icon-ref-point-cloud"> Point cloud</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../zoo_performances.html#id2">Classification</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/Brainchip-Inc/akida_examples/releases">Changelog</a></li>
<li class="toctree-l1"><a class="reference external" href="https://support.brainchip.com/portal/home">Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../license.html">License</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #78b3ff" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Akida Examples</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../index.html">Akida examples</a> &raquo;</li>
      <li>CNN conversion flow tutorial</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p>Click <a class="reference internal" href="#sphx-glr-download-examples-cnn2snn-plot-0-cnn-flow-py"><span class="std std-ref">here</span></a>
to download the full example code</p>
</div>
<section class="sphx-glr-example-title" id="cnn-conversion-flow-tutorial">
<span id="sphx-glr-examples-cnn2snn-plot-0-cnn-flow-py"></span><h1>CNN conversion flow tutorial<a class="headerlink" href="#cnn-conversion-flow-tutorial" title="Permalink to this headline"></a></h1>
<p>This tutorial illustrates how to use the CNN2SNN toolkit to <strong>convert CNN
networks to SNN networks</strong> compatible with the <strong>Akida NSoC</strong> in a few steps.
You can refer to our <a class="reference external" href="../../user_guide/cnn2snn.html">CNN2SNN toolkit user guide</a> for further
explanation.</p>
<p>The CNN2SNN tool is based on Keras, TensorFlow high-level API for building and
training deep learning models.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Please refer to TensorFlow  <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/models">tf.keras.models</a>
module for model creation/import details and <a class="reference external" href="https://www.tensorflow.org/guide">TensorFlow
Guide</a> for details of how
TensorFlow works.</p>
<p>MNIST example below is light enough so you do not need a <a class="reference external" href="https://www.tensorflow.org/install/gpu">GPU</a> to run the CNN2SNN
tool.</p>
</div>
<a class="reference internal image-reference" href="../../_images/cnn2snn_flow_small.jpg"><img alt="../../_images/cnn2snn_flow_small.jpg" src="../../_images/cnn2snn_flow_small.jpg" style="width: 725.9px; height: 170.1px;" /></a>
<section id="load-and-reshape-mnist-dataset">
<h2>1. Load and reshape MNIST dataset<a class="headerlink" href="#load-and-reshape-mnist-dataset" title="Permalink to this headline"></a></h2>
<p>After loading, we make 2 transformations on the dataset:</p>
<ol class="arabic simple">
<li><p>Reshape the sample content data (x values) into a num_samples x width x
height x channels matrix.</p></li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>At this point, we’ll set aside the raw data for testing our
converted model in the Akida runtime later.</p>
</div>
<ol class="arabic simple" start="2">
<li><p>Rescale the 8-bit loaded data to the range 0-to-1 for training.</p></li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Input data normalization is a common step dealing with CNN
(rationale is to keep data in a range that works with selected
optimizers, some reading can be found
<a class="reference external" href="https://www.jeremyjordan.me/batch-normalization/">here</a>.</p>
<p>This shift makes almost no difference in the current example, but
for some datasets rescaling the absolute values (and also shifting
to zero-mean) can make a really major difference.</p>
<p>Also note that we store the scaling values <code class="docutils literal notranslate"><span class="pre">input_scaling</span></code> for
use when preparing the model for the Akida neuromorphic IP. The
implementation of the Akida neural network allows us to completely
skip the rescaling step (i.e. the Akida model should be fed with
the raw 8-bit values) but that does require information about what
scaling was applied prior to training - see below for more details.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow</span> <span class="kn">import</span> <span class="n">keras</span>

<span class="c1"># Load MNIST dataset</span>
<span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>

<span class="c1"># Reshape x-data</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">60000</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Set aside raw test data for use with Akida runtime later</span>
<span class="n">raw_x_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;uint8&#39;</span><span class="p">)</span>
<span class="n">raw_y_test</span> <span class="o">=</span> <span class="n">y_test</span>

<span class="c1"># Rescale x-data</span>
<span class="n">a</span> <span class="o">=</span> <span class="mi">255</span>
<span class="n">b</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">input_scaling</span> <span class="o">=</span> <span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_train</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span> <span class="o">/</span> <span class="n">a</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_test</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span> <span class="o">/</span> <span class="n">a</span>
</pre></div>
</div>
</section>
<section id="model-definition">
<h2>2. Model definition<a class="headerlink" href="#model-definition" title="Permalink to this headline"></a></h2>
<p>Note that at this stage, there is nothing specific to the Akida NSoC.
This start point is very much a completely standard CNN as defined
within <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras">Keras</a>.</p>
<p>An appropriate model for MNIST (inspired by <a class="reference external" href="https://www.tensorflow.org/model_optimization/guide/quantization/training_example#train_a_model_for_mnist_without_quantization_aware_training">this
example</a>)
might look something like the following:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model_keras</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span>
        <span class="n">filters</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">BatchNormalization</span><span class="p">(),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">SeparableConv2D</span><span class="p">(</span>
        <span class="n">filters</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">BatchNormalization</span><span class="p">(),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="p">],</span> <span class="s1">&#39;mnistnet&#39;</span><span class="p">)</span>

<span class="n">model_keras</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Model: &quot;mnistnet&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d (Conv2D)              (None, 13, 13, 32)        320
_________________________________________________________________
batch_normalization (BatchNo (None, 13, 13, 32)        128
_________________________________________________________________
re_lu (ReLU)                 (None, 13, 13, 32)        0
_________________________________________________________________
separable_conv2d (SeparableC (None, 7, 7, 64)          2400
_________________________________________________________________
batch_normalization_1 (Batch (None, 7, 7, 64)          256
_________________________________________________________________
re_lu_1 (ReLU)               (None, 7, 7, 64)          0
_________________________________________________________________
flatten (Flatten)            (None, 3136)              0
_________________________________________________________________
dense (Dense)                (None, 10)                31370
=================================================================
Total params: 34,474
Trainable params: 34,282
Non-trainable params: 192
_________________________________________________________________
</pre></div>
</div>
<p>The model defined above is compatible for conversion into an Akida model, i.e.
the model doesn’t include any layers or operations that aren’t Akida-compatible
(please refer to the <a class="reference external" href="../../user_guide/cnn2snn.html">CNN2SNN toolkit</a> documentation for full
details):</p>
<ul class="simple">
<li><p>Standard Conv2D and Dense layers are supported</p></li>
<li><p>Hidden layers must be followed  by a ReLU layer.</p></li>
<li><p>BatchNormalization must always happen before activations.</p></li>
<li><p>Convolutional blocks can optionally be followed by a MaxPooling.</p></li>
</ul>
<p>The CNN2SNN toolkit provides the
<a class="reference external" href="../../api_reference/cnn2snn_apis.html#check-model-compatibility">check_model_compatibility</a>
function to ensure that the model can be converted into an Akida model. If
the model is not fully compatible, substitutes will be needed for the
relevant layers/operations (guidelines included in the documentation).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">cnn2snn</span> <span class="kn">import</span> <span class="n">check_model_compatibility</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model compatible for Akida conversion:&quot;</span><span class="p">,</span>
      <span class="n">check_model_compatibility</span><span class="p">(</span><span class="n">model_keras</span><span class="p">))</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Model compatible for Akida conversion: True
</pre></div>
</div>
</section>
<section id="model-training">
<h2>3. Model training<a class="headerlink" href="#model-training" title="Permalink to this headline"></a></h2>
<p>Before going any further, train the model and get its performance.
The created model should have achieved a test accuracy over 98% after 10
epochs.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model_keras</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span>
    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>

<span class="n">model_keras</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">score</span> <span class="o">=</span> <span class="n">model_keras</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Test accuracy:&#39;</span><span class="p">,</span> <span class="n">score</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Epoch 1/10

   1/1688 [..............................] - ETA: 10:54 - loss: 2.6340 - accuracy: 0.1562
  33/1688 [..............................] - ETA: 2s - loss: 1.2527 - accuracy: 0.5909   
  67/1688 [&gt;.............................] - ETA: 2s - loss: 0.9135 - accuracy: 0.7015
 101/1688 [&gt;.............................] - ETA: 2s - loss: 0.7370 - accuracy: 0.7649
 134/1688 [=&gt;............................] - ETA: 2s - loss: 0.6362 - accuracy: 0.7990
 167/1688 [=&gt;............................] - ETA: 2s - loss: 0.5699 - accuracy: 0.8196
 201/1688 [==&gt;...........................] - ETA: 2s - loss: 0.5256 - accuracy: 0.8350
 234/1688 [===&gt;..........................] - ETA: 2s - loss: 0.4882 - accuracy: 0.8490
 269/1688 [===&gt;..........................] - ETA: 2s - loss: 0.4573 - accuracy: 0.8591
 302/1688 [====&gt;.........................] - ETA: 2s - loss: 0.4326 - accuracy: 0.8668
 335/1688 [====&gt;.........................] - ETA: 2s - loss: 0.4094 - accuracy: 0.8737
 369/1688 [=====&gt;........................] - ETA: 2s - loss: 0.3859 - accuracy: 0.8805
 402/1688 [======&gt;.......................] - ETA: 1s - loss: 0.3746 - accuracy: 0.8839
 436/1688 [======&gt;.......................] - ETA: 1s - loss: 0.3590 - accuracy: 0.8883
 470/1688 [=======&gt;......................] - ETA: 1s - loss: 0.3447 - accuracy: 0.8930
 504/1688 [=======&gt;......................] - ETA: 1s - loss: 0.3314 - accuracy: 0.8970
 538/1688 [========&gt;.....................] - ETA: 1s - loss: 0.3193 - accuracy: 0.9012
 572/1688 [=========&gt;....................] - ETA: 1s - loss: 0.3109 - accuracy: 0.9041
 606/1688 [=========&gt;....................] - ETA: 1s - loss: 0.3028 - accuracy: 0.9063
 639/1688 [==========&gt;...................] - ETA: 1s - loss: 0.2945 - accuracy: 0.9094
 673/1688 [==========&gt;...................] - ETA: 1s - loss: 0.2861 - accuracy: 0.9120
 707/1688 [===========&gt;..................] - ETA: 1s - loss: 0.2797 - accuracy: 0.9141
 741/1688 [============&gt;.................] - ETA: 1s - loss: 0.2710 - accuracy: 0.9170
 774/1688 [============&gt;.................] - ETA: 1s - loss: 0.2668 - accuracy: 0.9187
 808/1688 [=============&gt;................] - ETA: 1s - loss: 0.2604 - accuracy: 0.9208
 842/1688 [=============&gt;................] - ETA: 1s - loss: 0.2547 - accuracy: 0.9228
 876/1688 [==============&gt;...............] - ETA: 1s - loss: 0.2494 - accuracy: 0.9242
 909/1688 [===============&gt;..............] - ETA: 1s - loss: 0.2457 - accuracy: 0.9255
 943/1688 [===============&gt;..............] - ETA: 1s - loss: 0.2413 - accuracy: 0.9269
 976/1688 [================&gt;.............] - ETA: 1s - loss: 0.2373 - accuracy: 0.9281
1010/1688 [================&gt;.............] - ETA: 1s - loss: 0.2328 - accuracy: 0.9294
1043/1688 [=================&gt;............] - ETA: 0s - loss: 0.2289 - accuracy: 0.9305
1076/1688 [==================&gt;...........] - ETA: 0s - loss: 0.2248 - accuracy: 0.9318
1109/1688 [==================&gt;...........] - ETA: 0s - loss: 0.2215 - accuracy: 0.9329
1142/1688 [===================&gt;..........] - ETA: 0s - loss: 0.2190 - accuracy: 0.9336
1175/1688 [===================&gt;..........] - ETA: 0s - loss: 0.2165 - accuracy: 0.9345
1209/1688 [====================&gt;.........] - ETA: 0s - loss: 0.2134 - accuracy: 0.9354
1242/1688 [=====================&gt;........] - ETA: 0s - loss: 0.2104 - accuracy: 0.9363
1276/1688 [=====================&gt;........] - ETA: 0s - loss: 0.2074 - accuracy: 0.9372
1310/1688 [======================&gt;.......] - ETA: 0s - loss: 0.2049 - accuracy: 0.9379
1344/1688 [======================&gt;.......] - ETA: 0s - loss: 0.2027 - accuracy: 0.9386
1377/1688 [=======================&gt;......] - ETA: 0s - loss: 0.2005 - accuracy: 0.9393
1412/1688 [========================&gt;.....] - ETA: 0s - loss: 0.1985 - accuracy: 0.9399
1446/1688 [========================&gt;.....] - ETA: 0s - loss: 0.1962 - accuracy: 0.9406
1480/1688 [=========================&gt;....] - ETA: 0s - loss: 0.1942 - accuracy: 0.9413
1513/1688 [=========================&gt;....] - ETA: 0s - loss: 0.1920 - accuracy: 0.9418
1547/1688 [==========================&gt;...] - ETA: 0s - loss: 0.1902 - accuracy: 0.9424
1580/1688 [===========================&gt;..] - ETA: 0s - loss: 0.1881 - accuracy: 0.9431
1612/1688 [===========================&gt;..] - ETA: 0s - loss: 0.1867 - accuracy: 0.9435
1645/1688 [============================&gt;.] - ETA: 0s - loss: 0.1849 - accuracy: 0.9440
1677/1688 [============================&gt;.] - ETA: 0s - loss: 0.1829 - accuracy: 0.9445
1688/1688 [==============================] - 3s 2ms/step - loss: 0.1823 - accuracy: 0.9448 - val_loss: 0.0957 - val_accuracy: 0.9722
Epoch 2/10

   1/1688 [..............................] - ETA: 2s - loss: 0.0069 - accuracy: 1.0000
  36/1688 [..............................] - ETA: 2s - loss: 0.0826 - accuracy: 0.9740
  69/1688 [&gt;.............................] - ETA: 2s - loss: 0.0785 - accuracy: 0.9755
 103/1688 [&gt;.............................] - ETA: 2s - loss: 0.0817 - accuracy: 0.9745
 137/1688 [=&gt;............................] - ETA: 2s - loss: 0.0802 - accuracy: 0.9754
 171/1688 [==&gt;...........................] - ETA: 2s - loss: 0.0802 - accuracy: 0.9768
 204/1688 [==&gt;...........................] - ETA: 2s - loss: 0.0780 - accuracy: 0.9773
 236/1688 [===&gt;..........................] - ETA: 2s - loss: 0.0731 - accuracy: 0.9784
 270/1688 [===&gt;..........................] - ETA: 2s - loss: 0.0759 - accuracy: 0.9770
 304/1688 [====&gt;.........................] - ETA: 2s - loss: 0.0729 - accuracy: 0.9780
 338/1688 [=====&gt;........................] - ETA: 2s - loss: 0.0714 - accuracy: 0.9781
 372/1688 [=====&gt;........................] - ETA: 1s - loss: 0.0733 - accuracy: 0.9771
 406/1688 [======&gt;.......................] - ETA: 1s - loss: 0.0727 - accuracy: 0.9772
 440/1688 [======&gt;.......................] - ETA: 1s - loss: 0.0707 - accuracy: 0.9778
 472/1688 [=======&gt;......................] - ETA: 1s - loss: 0.0721 - accuracy: 0.9775
 505/1688 [=======&gt;......................] - ETA: 1s - loss: 0.0701 - accuracy: 0.9779
 537/1688 [========&gt;.....................] - ETA: 1s - loss: 0.0700 - accuracy: 0.9778
 570/1688 [=========&gt;....................] - ETA: 1s - loss: 0.0705 - accuracy: 0.9775
 604/1688 [=========&gt;....................] - ETA: 1s - loss: 0.0702 - accuracy: 0.9776
 637/1688 [==========&gt;...................] - ETA: 1s - loss: 0.0709 - accuracy: 0.9775
 671/1688 [==========&gt;...................] - ETA: 1s - loss: 0.0702 - accuracy: 0.9777
 704/1688 [===========&gt;..................] - ETA: 1s - loss: 0.0706 - accuracy: 0.9777
 737/1688 [============&gt;.................] - ETA: 1s - loss: 0.0704 - accuracy: 0.9778
 769/1688 [============&gt;.................] - ETA: 1s - loss: 0.0698 - accuracy: 0.9779
 803/1688 [=============&gt;................] - ETA: 1s - loss: 0.0712 - accuracy: 0.9776
 836/1688 [=============&gt;................] - ETA: 1s - loss: 0.0714 - accuracy: 0.9775
 870/1688 [==============&gt;...............] - ETA: 1s - loss: 0.0721 - accuracy: 0.9773
 904/1688 [===============&gt;..............] - ETA: 1s - loss: 0.0721 - accuracy: 0.9773
 937/1688 [===============&gt;..............] - ETA: 1s - loss: 0.0728 - accuracy: 0.9772
 970/1688 [================&gt;.............] - ETA: 1s - loss: 0.0730 - accuracy: 0.9773
1003/1688 [================&gt;.............] - ETA: 1s - loss: 0.0733 - accuracy: 0.9772
1036/1688 [=================&gt;............] - ETA: 0s - loss: 0.0723 - accuracy: 0.9776
1069/1688 [=================&gt;............] - ETA: 0s - loss: 0.0722 - accuracy: 0.9776
1102/1688 [==================&gt;...........] - ETA: 0s - loss: 0.0722 - accuracy: 0.9775
1136/1688 [===================&gt;..........] - ETA: 0s - loss: 0.0722 - accuracy: 0.9775
1170/1688 [===================&gt;..........] - ETA: 0s - loss: 0.0720 - accuracy: 0.9776
1204/1688 [====================&gt;.........] - ETA: 0s - loss: 0.0718 - accuracy: 0.9777
1237/1688 [====================&gt;.........] - ETA: 0s - loss: 0.0720 - accuracy: 0.9776
1271/1688 [=====================&gt;........] - ETA: 0s - loss: 0.0725 - accuracy: 0.9774
1304/1688 [======================&gt;.......] - ETA: 0s - loss: 0.0734 - accuracy: 0.9772
1338/1688 [======================&gt;.......] - ETA: 0s - loss: 0.0735 - accuracy: 0.9772
1372/1688 [=======================&gt;......] - ETA: 0s - loss: 0.0743 - accuracy: 0.9771
1407/1688 [========================&gt;.....] - ETA: 0s - loss: 0.0742 - accuracy: 0.9772
1439/1688 [========================&gt;.....] - ETA: 0s - loss: 0.0743 - accuracy: 0.9771
1472/1688 [=========================&gt;....] - ETA: 0s - loss: 0.0739 - accuracy: 0.9773
1505/1688 [=========================&gt;....] - ETA: 0s - loss: 0.0737 - accuracy: 0.9774
1538/1688 [==========================&gt;...] - ETA: 0s - loss: 0.0733 - accuracy: 0.9774
1572/1688 [==========================&gt;...] - ETA: 0s - loss: 0.0733 - accuracy: 0.9773
1606/1688 [===========================&gt;..] - ETA: 0s - loss: 0.0733 - accuracy: 0.9773
1640/1688 [============================&gt;.] - ETA: 0s - loss: 0.0732 - accuracy: 0.9772
1673/1688 [============================&gt;.] - ETA: 0s - loss: 0.0734 - accuracy: 0.9773
1688/1688 [==============================] - 3s 2ms/step - loss: 0.0734 - accuracy: 0.9773 - val_loss: 0.0645 - val_accuracy: 0.9798
Epoch 3/10

   1/1688 [..............................] - ETA: 2s - loss: 0.0751 - accuracy: 0.9688
  37/1688 [..............................] - ETA: 2s - loss: 0.0609 - accuracy: 0.9797
  73/1688 [&gt;.............................] - ETA: 2s - loss: 0.0585 - accuracy: 0.9807
 109/1688 [&gt;.............................] - ETA: 2s - loss: 0.0527 - accuracy: 0.9828
 144/1688 [=&gt;............................] - ETA: 2s - loss: 0.0486 - accuracy: 0.9839
 177/1688 [==&gt;...........................] - ETA: 2s - loss: 0.0492 - accuracy: 0.9838
 210/1688 [==&gt;...........................] - ETA: 2s - loss: 0.0514 - accuracy: 0.9833
 243/1688 [===&gt;..........................] - ETA: 2s - loss: 0.0519 - accuracy: 0.9832
 276/1688 [===&gt;..........................] - ETA: 2s - loss: 0.0509 - accuracy: 0.9838
 309/1688 [====&gt;.........................] - ETA: 2s - loss: 0.0529 - accuracy: 0.9832
 342/1688 [=====&gt;........................] - ETA: 2s - loss: 0.0521 - accuracy: 0.9833
 376/1688 [=====&gt;........................] - ETA: 1s - loss: 0.0518 - accuracy: 0.9833
 410/1688 [======&gt;.......................] - ETA: 1s - loss: 0.0531 - accuracy: 0.9825
 443/1688 [======&gt;.......................] - ETA: 1s - loss: 0.0538 - accuracy: 0.9822
 476/1688 [=======&gt;......................] - ETA: 1s - loss: 0.0537 - accuracy: 0.9822
 510/1688 [========&gt;.....................] - ETA: 1s - loss: 0.0532 - accuracy: 0.9824
 543/1688 [========&gt;.....................] - ETA: 1s - loss: 0.0529 - accuracy: 0.9826
 575/1688 [=========&gt;....................] - ETA: 1s - loss: 0.0523 - accuracy: 0.9827
 609/1688 [=========&gt;....................] - ETA: 1s - loss: 0.0524 - accuracy: 0.9827
 643/1688 [==========&gt;...................] - ETA: 1s - loss: 0.0529 - accuracy: 0.9827
 675/1688 [==========&gt;...................] - ETA: 1s - loss: 0.0522 - accuracy: 0.9831
 708/1688 [===========&gt;..................] - ETA: 1s - loss: 0.0526 - accuracy: 0.9832
 741/1688 [============&gt;.................] - ETA: 1s - loss: 0.0521 - accuracy: 0.9832
 774/1688 [============&gt;.................] - ETA: 1s - loss: 0.0518 - accuracy: 0.9833
 808/1688 [=============&gt;................] - ETA: 1s - loss: 0.0524 - accuracy: 0.9832
 841/1688 [=============&gt;................] - ETA: 1s - loss: 0.0524 - accuracy: 0.9832
 874/1688 [==============&gt;...............] - ETA: 1s - loss: 0.0526 - accuracy: 0.9831
 906/1688 [===============&gt;..............] - ETA: 1s - loss: 0.0526 - accuracy: 0.9831
 940/1688 [===============&gt;..............] - ETA: 1s - loss: 0.0530 - accuracy: 0.9831
 972/1688 [================&gt;.............] - ETA: 1s - loss: 0.0533 - accuracy: 0.9831
1005/1688 [================&gt;.............] - ETA: 1s - loss: 0.0532 - accuracy: 0.9832
1039/1688 [=================&gt;............] - ETA: 0s - loss: 0.0530 - accuracy: 0.9833
1073/1688 [==================&gt;...........] - ETA: 0s - loss: 0.0533 - accuracy: 0.9832
1106/1688 [==================&gt;...........] - ETA: 0s - loss: 0.0534 - accuracy: 0.9830
1139/1688 [===================&gt;..........] - ETA: 0s - loss: 0.0528 - accuracy: 0.9832
1174/1688 [===================&gt;..........] - ETA: 0s - loss: 0.0531 - accuracy: 0.9832
1208/1688 [====================&gt;.........] - ETA: 0s - loss: 0.0528 - accuracy: 0.9832
1241/1688 [=====================&gt;........] - ETA: 0s - loss: 0.0528 - accuracy: 0.9833
1274/1688 [=====================&gt;........] - ETA: 0s - loss: 0.0530 - accuracy: 0.9833
1307/1688 [======================&gt;.......] - ETA: 0s - loss: 0.0526 - accuracy: 0.9834
1341/1688 [======================&gt;.......] - ETA: 0s - loss: 0.0530 - accuracy: 0.9833
1375/1688 [=======================&gt;......] - ETA: 0s - loss: 0.0531 - accuracy: 0.9833
1409/1688 [========================&gt;.....] - ETA: 0s - loss: 0.0532 - accuracy: 0.9832
1443/1688 [========================&gt;.....] - ETA: 0s - loss: 0.0528 - accuracy: 0.9833
1474/1688 [=========================&gt;....] - ETA: 0s - loss: 0.0525 - accuracy: 0.9834
1507/1688 [=========================&gt;....] - ETA: 0s - loss: 0.0528 - accuracy: 0.9832
1539/1688 [==========================&gt;...] - ETA: 0s - loss: 0.0527 - accuracy: 0.9832
1573/1688 [==========================&gt;...] - ETA: 0s - loss: 0.0535 - accuracy: 0.9831
1605/1688 [===========================&gt;..] - ETA: 0s - loss: 0.0536 - accuracy: 0.9831
1639/1688 [============================&gt;.] - ETA: 0s - loss: 0.0536 - accuracy: 0.9830
1672/1688 [============================&gt;.] - ETA: 0s - loss: 0.0534 - accuracy: 0.9831
1688/1688 [==============================] - 3s 2ms/step - loss: 0.0532 - accuracy: 0.9831 - val_loss: 0.0728 - val_accuracy: 0.9812
Epoch 4/10

   1/1688 [..............................] - ETA: 2s - loss: 0.0058 - accuracy: 1.0000
  38/1688 [..............................] - ETA: 2s - loss: 0.0316 - accuracy: 0.9901
  71/1688 [&gt;.............................] - ETA: 2s - loss: 0.0439 - accuracy: 0.9864
 105/1688 [&gt;.............................] - ETA: 2s - loss: 0.0393 - accuracy: 0.9869
 137/1688 [=&gt;............................] - ETA: 2s - loss: 0.0377 - accuracy: 0.9875
 170/1688 [==&gt;...........................] - ETA: 2s - loss: 0.0356 - accuracy: 0.9888
 203/1688 [==&gt;...........................] - ETA: 2s - loss: 0.0363 - accuracy: 0.9885
 237/1688 [===&gt;..........................] - ETA: 2s - loss: 0.0378 - accuracy: 0.9880
 271/1688 [===&gt;..........................] - ETA: 2s - loss: 0.0374 - accuracy: 0.9885
 305/1688 [====&gt;.........................] - ETA: 2s - loss: 0.0366 - accuracy: 0.9889
 338/1688 [=====&gt;........................] - ETA: 2s - loss: 0.0386 - accuracy: 0.9884
 370/1688 [=====&gt;........................] - ETA: 1s - loss: 0.0381 - accuracy: 0.9883
 404/1688 [======&gt;.......................] - ETA: 1s - loss: 0.0377 - accuracy: 0.9886
 438/1688 [======&gt;.......................] - ETA: 1s - loss: 0.0360 - accuracy: 0.9891
 470/1688 [=======&gt;......................] - ETA: 1s - loss: 0.0359 - accuracy: 0.9889
 502/1688 [=======&gt;......................] - ETA: 1s - loss: 0.0370 - accuracy: 0.9887
 535/1688 [========&gt;.....................] - ETA: 1s - loss: 0.0370 - accuracy: 0.9888
 569/1688 [=========&gt;....................] - ETA: 1s - loss: 0.0368 - accuracy: 0.9885
 602/1688 [=========&gt;....................] - ETA: 1s - loss: 0.0372 - accuracy: 0.9884
 635/1688 [==========&gt;...................] - ETA: 1s - loss: 0.0375 - accuracy: 0.9883
 669/1688 [==========&gt;...................] - ETA: 1s - loss: 0.0374 - accuracy: 0.9884
 703/1688 [===========&gt;..................] - ETA: 1s - loss: 0.0380 - accuracy: 0.9882
 736/1688 [============&gt;.................] - ETA: 1s - loss: 0.0388 - accuracy: 0.9880
 770/1688 [============&gt;.................] - ETA: 1s - loss: 0.0389 - accuracy: 0.9881
 803/1688 [=============&gt;................] - ETA: 1s - loss: 0.0387 - accuracy: 0.9880
 836/1688 [=============&gt;................] - ETA: 1s - loss: 0.0386 - accuracy: 0.9880
 870/1688 [==============&gt;...............] - ETA: 1s - loss: 0.0393 - accuracy: 0.9877
 904/1688 [===============&gt;..............] - ETA: 1s - loss: 0.0401 - accuracy: 0.9876
 937/1688 [===============&gt;..............] - ETA: 1s - loss: 0.0403 - accuracy: 0.9874
 970/1688 [================&gt;.............] - ETA: 1s - loss: 0.0405 - accuracy: 0.9873
1003/1688 [================&gt;.............] - ETA: 1s - loss: 0.0405 - accuracy: 0.9873
1035/1688 [=================&gt;............] - ETA: 0s - loss: 0.0403 - accuracy: 0.9873
1069/1688 [=================&gt;............] - ETA: 0s - loss: 0.0400 - accuracy: 0.9873
1102/1688 [==================&gt;...........] - ETA: 0s - loss: 0.0397 - accuracy: 0.9874
1136/1688 [===================&gt;..........] - ETA: 0s - loss: 0.0401 - accuracy: 0.9873
1169/1688 [===================&gt;..........] - ETA: 0s - loss: 0.0406 - accuracy: 0.9871
1202/1688 [====================&gt;.........] - ETA: 0s - loss: 0.0404 - accuracy: 0.9872
1235/1688 [====================&gt;.........] - ETA: 0s - loss: 0.0403 - accuracy: 0.9873
1269/1688 [=====================&gt;........] - ETA: 0s - loss: 0.0409 - accuracy: 0.9870
1301/1688 [======================&gt;.......] - ETA: 0s - loss: 0.0409 - accuracy: 0.9871
1334/1688 [======================&gt;.......] - ETA: 0s - loss: 0.0410 - accuracy: 0.9870
1367/1688 [=======================&gt;......] - ETA: 0s - loss: 0.0411 - accuracy: 0.9870
1400/1688 [=======================&gt;......] - ETA: 0s - loss: 0.0413 - accuracy: 0.9871
1433/1688 [========================&gt;.....] - ETA: 0s - loss: 0.0415 - accuracy: 0.9870
1466/1688 [=========================&gt;....] - ETA: 0s - loss: 0.0411 - accuracy: 0.9871
1499/1688 [=========================&gt;....] - ETA: 0s - loss: 0.0412 - accuracy: 0.9871
1532/1688 [==========================&gt;...] - ETA: 0s - loss: 0.0411 - accuracy: 0.9871
1565/1688 [==========================&gt;...] - ETA: 0s - loss: 0.0414 - accuracy: 0.9870
1599/1688 [===========================&gt;..] - ETA: 0s - loss: 0.0418 - accuracy: 0.9869
1632/1688 [============================&gt;.] - ETA: 0s - loss: 0.0420 - accuracy: 0.9869
1665/1688 [============================&gt;.] - ETA: 0s - loss: 0.0420 - accuracy: 0.9868
1688/1688 [==============================] - 3s 2ms/step - loss: 0.0420 - accuracy: 0.9869 - val_loss: 0.0513 - val_accuracy: 0.9863
Epoch 5/10

   1/1688 [..............................] - ETA: 2s - loss: 0.0014 - accuracy: 1.0000
  35/1688 [..............................] - ETA: 2s - loss: 0.0216 - accuracy: 0.9937
  69/1688 [&gt;.............................] - ETA: 2s - loss: 0.0253 - accuracy: 0.9928
 102/1688 [&gt;.............................] - ETA: 2s - loss: 0.0326 - accuracy: 0.9905
 136/1688 [=&gt;............................] - ETA: 2s - loss: 0.0338 - accuracy: 0.9908
 169/1688 [==&gt;...........................] - ETA: 2s - loss: 0.0315 - accuracy: 0.9915
 203/1688 [==&gt;...........................] - ETA: 2s - loss: 0.0306 - accuracy: 0.9914
 237/1688 [===&gt;..........................] - ETA: 2s - loss: 0.0282 - accuracy: 0.9922
 270/1688 [===&gt;..........................] - ETA: 2s - loss: 0.0287 - accuracy: 0.9922
 303/1688 [====&gt;.........................] - ETA: 2s - loss: 0.0288 - accuracy: 0.9922
 337/1688 [====&gt;.........................] - ETA: 2s - loss: 0.0291 - accuracy: 0.9917
 370/1688 [=====&gt;........................] - ETA: 1s - loss: 0.0300 - accuracy: 0.9914
 404/1688 [======&gt;.......................] - ETA: 1s - loss: 0.0292 - accuracy: 0.9915
 438/1688 [======&gt;.......................] - ETA: 1s - loss: 0.0293 - accuracy: 0.9912
 471/1688 [=======&gt;......................] - ETA: 1s - loss: 0.0301 - accuracy: 0.9907
 505/1688 [=======&gt;......................] - ETA: 1s - loss: 0.0304 - accuracy: 0.9907
 537/1688 [========&gt;.....................] - ETA: 1s - loss: 0.0305 - accuracy: 0.9907
 570/1688 [=========&gt;....................] - ETA: 1s - loss: 0.0301 - accuracy: 0.9909
 604/1688 [=========&gt;....................] - ETA: 1s - loss: 0.0296 - accuracy: 0.9910
 637/1688 [==========&gt;...................] - ETA: 1s - loss: 0.0302 - accuracy: 0.9908
 669/1688 [==========&gt;...................] - ETA: 1s - loss: 0.0298 - accuracy: 0.9909
 701/1688 [===========&gt;..................] - ETA: 1s - loss: 0.0303 - accuracy: 0.9908
 735/1688 [============&gt;.................] - ETA: 1s - loss: 0.0312 - accuracy: 0.9905
 769/1688 [============&gt;.................] - ETA: 1s - loss: 0.0308 - accuracy: 0.9905
 803/1688 [=============&gt;................] - ETA: 1s - loss: 0.0311 - accuracy: 0.9905
 836/1688 [=============&gt;................] - ETA: 1s - loss: 0.0314 - accuracy: 0.9903
 869/1688 [==============&gt;...............] - ETA: 1s - loss: 0.0320 - accuracy: 0.9900
 902/1688 [===============&gt;..............] - ETA: 1s - loss: 0.0319 - accuracy: 0.9901
 936/1688 [===============&gt;..............] - ETA: 1s - loss: 0.0322 - accuracy: 0.9898
 969/1688 [================&gt;.............] - ETA: 1s - loss: 0.0319 - accuracy: 0.9898
1002/1688 [================&gt;.............] - ETA: 1s - loss: 0.0319 - accuracy: 0.9899
1037/1688 [=================&gt;............] - ETA: 0s - loss: 0.0324 - accuracy: 0.9897
1071/1688 [==================&gt;...........] - ETA: 0s - loss: 0.0330 - accuracy: 0.9895
1105/1688 [==================&gt;...........] - ETA: 0s - loss: 0.0332 - accuracy: 0.9894
1138/1688 [===================&gt;..........] - ETA: 0s - loss: 0.0330 - accuracy: 0.9894
1170/1688 [===================&gt;..........] - ETA: 0s - loss: 0.0331 - accuracy: 0.9894
1203/1688 [====================&gt;.........] - ETA: 0s - loss: 0.0331 - accuracy: 0.9894
1236/1688 [====================&gt;.........] - ETA: 0s - loss: 0.0333 - accuracy: 0.9894
1268/1688 [=====================&gt;........] - ETA: 0s - loss: 0.0334 - accuracy: 0.9893
1302/1688 [======================&gt;.......] - ETA: 0s - loss: 0.0336 - accuracy: 0.9892
1335/1688 [======================&gt;.......] - ETA: 0s - loss: 0.0339 - accuracy: 0.9891
1368/1688 [=======================&gt;......] - ETA: 0s - loss: 0.0341 - accuracy: 0.9890
1401/1688 [=======================&gt;......] - ETA: 0s - loss: 0.0344 - accuracy: 0.9889
1435/1688 [========================&gt;.....] - ETA: 0s - loss: 0.0346 - accuracy: 0.9888
1468/1688 [=========================&gt;....] - ETA: 0s - loss: 0.0348 - accuracy: 0.9887
1503/1688 [=========================&gt;....] - ETA: 0s - loss: 0.0345 - accuracy: 0.9888
1536/1688 [==========================&gt;...] - ETA: 0s - loss: 0.0346 - accuracy: 0.9887
1567/1688 [==========================&gt;...] - ETA: 0s - loss: 0.0348 - accuracy: 0.9886
1599/1688 [===========================&gt;..] - ETA: 0s - loss: 0.0349 - accuracy: 0.9886
1632/1688 [============================&gt;.] - ETA: 0s - loss: 0.0351 - accuracy: 0.9885
1666/1688 [============================&gt;.] - ETA: 0s - loss: 0.0355 - accuracy: 0.9884
1688/1688 [==============================] - 3s 2ms/step - loss: 0.0355 - accuracy: 0.9884 - val_loss: 0.0629 - val_accuracy: 0.9838
Epoch 6/10

   1/1688 [..............................] - ETA: 2s - loss: 0.0047 - accuracy: 1.0000
  37/1688 [..............................] - ETA: 2s - loss: 0.0422 - accuracy: 0.9873
  71/1688 [&gt;.............................] - ETA: 2s - loss: 0.0325 - accuracy: 0.9903
 104/1688 [&gt;.............................] - ETA: 2s - loss: 0.0284 - accuracy: 0.9916
 136/1688 [=&gt;............................] - ETA: 2s - loss: 0.0256 - accuracy: 0.9917
 170/1688 [==&gt;...........................] - ETA: 2s - loss: 0.0247 - accuracy: 0.9919
 204/1688 [==&gt;...........................] - ETA: 2s - loss: 0.0230 - accuracy: 0.9925
 237/1688 [===&gt;..........................] - ETA: 2s - loss: 0.0243 - accuracy: 0.9920
 271/1688 [===&gt;..........................] - ETA: 2s - loss: 0.0235 - accuracy: 0.9922
 305/1688 [====&gt;.........................] - ETA: 2s - loss: 0.0237 - accuracy: 0.9920
 339/1688 [=====&gt;........................] - ETA: 2s - loss: 0.0247 - accuracy: 0.9918
 371/1688 [=====&gt;........................] - ETA: 1s - loss: 0.0245 - accuracy: 0.9922
 404/1688 [======&gt;.......................] - ETA: 1s - loss: 0.0239 - accuracy: 0.9923
 438/1688 [======&gt;.......................] - ETA: 1s - loss: 0.0230 - accuracy: 0.9926
 471/1688 [=======&gt;......................] - ETA: 1s - loss: 0.0227 - accuracy: 0.9926
 504/1688 [=======&gt;......................] - ETA: 1s - loss: 0.0231 - accuracy: 0.9924
 537/1688 [========&gt;.....................] - ETA: 1s - loss: 0.0230 - accuracy: 0.9927
 569/1688 [=========&gt;....................] - ETA: 1s - loss: 0.0236 - accuracy: 0.9925
 602/1688 [=========&gt;....................] - ETA: 1s - loss: 0.0247 - accuracy: 0.9921
 635/1688 [==========&gt;...................] - ETA: 1s - loss: 0.0249 - accuracy: 0.9921
 669/1688 [==========&gt;...................] - ETA: 1s - loss: 0.0255 - accuracy: 0.9921
 702/1688 [===========&gt;..................] - ETA: 1s - loss: 0.0258 - accuracy: 0.9920
 735/1688 [============&gt;.................] - ETA: 1s - loss: 0.0260 - accuracy: 0.9919
 768/1688 [============&gt;.................] - ETA: 1s - loss: 0.0261 - accuracy: 0.9917
 801/1688 [=============&gt;................] - ETA: 1s - loss: 0.0262 - accuracy: 0.9917
 835/1688 [=============&gt;................] - ETA: 1s - loss: 0.0261 - accuracy: 0.9917
 868/1688 [==============&gt;...............] - ETA: 1s - loss: 0.0263 - accuracy: 0.9917
 902/1688 [===============&gt;..............] - ETA: 1s - loss: 0.0261 - accuracy: 0.9917
 935/1688 [===============&gt;..............] - ETA: 1s - loss: 0.0259 - accuracy: 0.9917
 968/1688 [================&gt;.............] - ETA: 1s - loss: 0.0259 - accuracy: 0.9917
1002/1688 [================&gt;.............] - ETA: 1s - loss: 0.0258 - accuracy: 0.9918
1035/1688 [=================&gt;............] - ETA: 0s - loss: 0.0259 - accuracy: 0.9916
1070/1688 [==================&gt;...........] - ETA: 0s - loss: 0.0259 - accuracy: 0.9916
1102/1688 [==================&gt;...........] - ETA: 0s - loss: 0.0260 - accuracy: 0.9915
1135/1688 [===================&gt;..........] - ETA: 0s - loss: 0.0260 - accuracy: 0.9916
1167/1688 [===================&gt;..........] - ETA: 0s - loss: 0.0260 - accuracy: 0.9915
1201/1688 [====================&gt;.........] - ETA: 0s - loss: 0.0258 - accuracy: 0.9915
1233/1688 [====================&gt;.........] - ETA: 0s - loss: 0.0261 - accuracy: 0.9914
1266/1688 [=====================&gt;........] - ETA: 0s - loss: 0.0262 - accuracy: 0.9914
1299/1688 [======================&gt;.......] - ETA: 0s - loss: 0.0265 - accuracy: 0.9912
1331/1688 [======================&gt;.......] - ETA: 0s - loss: 0.0267 - accuracy: 0.9912
1364/1688 [=======================&gt;......] - ETA: 0s - loss: 0.0268 - accuracy: 0.9912
1397/1688 [=======================&gt;......] - ETA: 0s - loss: 0.0267 - accuracy: 0.9912
1430/1688 [========================&gt;.....] - ETA: 0s - loss: 0.0267 - accuracy: 0.9912
1464/1688 [=========================&gt;....] - ETA: 0s - loss: 0.0273 - accuracy: 0.9911
1498/1688 [=========================&gt;....] - ETA: 0s - loss: 0.0273 - accuracy: 0.9911
1532/1688 [==========================&gt;...] - ETA: 0s - loss: 0.0277 - accuracy: 0.9910
1565/1688 [==========================&gt;...] - ETA: 0s - loss: 0.0279 - accuracy: 0.9909
1598/1688 [===========================&gt;..] - ETA: 0s - loss: 0.0279 - accuracy: 0.9908
1631/1688 [===========================&gt;..] - ETA: 0s - loss: 0.0281 - accuracy: 0.9908
1664/1688 [============================&gt;.] - ETA: 0s - loss: 0.0282 - accuracy: 0.9907
1688/1688 [==============================] - 3s 2ms/step - loss: 0.0284 - accuracy: 0.9906 - val_loss: 0.0633 - val_accuracy: 0.9842
Epoch 7/10

   1/1688 [..............................] - ETA: 3s - loss: 0.0051 - accuracy: 1.0000
  37/1688 [..............................] - ETA: 2s - loss: 0.0167 - accuracy: 0.9949
  71/1688 [&gt;.............................] - ETA: 2s - loss: 0.0131 - accuracy: 0.9960
 104/1688 [&gt;.............................] - ETA: 2s - loss: 0.0141 - accuracy: 0.9955
 137/1688 [=&gt;............................] - ETA: 2s - loss: 0.0151 - accuracy: 0.9954
 170/1688 [==&gt;...........................] - ETA: 2s - loss: 0.0166 - accuracy: 0.9947
 204/1688 [==&gt;...........................] - ETA: 2s - loss: 0.0161 - accuracy: 0.9951
 237/1688 [===&gt;..........................] - ETA: 2s - loss: 0.0182 - accuracy: 0.9942
 271/1688 [===&gt;..........................] - ETA: 2s - loss: 0.0198 - accuracy: 0.9939
 305/1688 [====&gt;.........................] - ETA: 2s - loss: 0.0204 - accuracy: 0.9931
 338/1688 [=====&gt;........................] - ETA: 2s - loss: 0.0203 - accuracy: 0.9931
 372/1688 [=====&gt;........................] - ETA: 1s - loss: 0.0202 - accuracy: 0.9929
 406/1688 [======&gt;.......................] - ETA: 1s - loss: 0.0212 - accuracy: 0.9927
 441/1688 [======&gt;.......................] - ETA: 1s - loss: 0.0218 - accuracy: 0.9924
 472/1688 [=======&gt;......................] - ETA: 1s - loss: 0.0224 - accuracy: 0.9920
 506/1688 [=======&gt;......................] - ETA: 1s - loss: 0.0221 - accuracy: 0.9923
 538/1688 [========&gt;.....................] - ETA: 1s - loss: 0.0215 - accuracy: 0.9926
 572/1688 [=========&gt;....................] - ETA: 1s - loss: 0.0219 - accuracy: 0.9924
 606/1688 [=========&gt;....................] - ETA: 1s - loss: 0.0229 - accuracy: 0.9920
 639/1688 [==========&gt;...................] - ETA: 1s - loss: 0.0225 - accuracy: 0.9921
 673/1688 [==========&gt;...................] - ETA: 1s - loss: 0.0224 - accuracy: 0.9922
 705/1688 [===========&gt;..................] - ETA: 1s - loss: 0.0221 - accuracy: 0.9923
 739/1688 [============&gt;.................] - ETA: 1s - loss: 0.0222 - accuracy: 0.9922
 772/1688 [============&gt;.................] - ETA: 1s - loss: 0.0221 - accuracy: 0.9921
 804/1688 [=============&gt;................] - ETA: 1s - loss: 0.0221 - accuracy: 0.9920
 837/1688 [=============&gt;................] - ETA: 1s - loss: 0.0225 - accuracy: 0.9920
 871/1688 [==============&gt;...............] - ETA: 1s - loss: 0.0223 - accuracy: 0.9921
 905/1688 [===============&gt;..............] - ETA: 1s - loss: 0.0224 - accuracy: 0.9921
 938/1688 [===============&gt;..............] - ETA: 1s - loss: 0.0231 - accuracy: 0.9919
 971/1688 [================&gt;.............] - ETA: 1s - loss: 0.0235 - accuracy: 0.9919
1004/1688 [================&gt;.............] - ETA: 1s - loss: 0.0235 - accuracy: 0.9918
1038/1688 [=================&gt;............] - ETA: 0s - loss: 0.0238 - accuracy: 0.9918
1072/1688 [==================&gt;...........] - ETA: 0s - loss: 0.0243 - accuracy: 0.9916
1105/1688 [==================&gt;...........] - ETA: 0s - loss: 0.0242 - accuracy: 0.9917
1138/1688 [===================&gt;..........] - ETA: 0s - loss: 0.0245 - accuracy: 0.9917
1172/1688 [===================&gt;..........] - ETA: 0s - loss: 0.0244 - accuracy: 0.9917
1205/1688 [====================&gt;.........] - ETA: 0s - loss: 0.0244 - accuracy: 0.9917
1238/1688 [=====================&gt;........] - ETA: 0s - loss: 0.0247 - accuracy: 0.9915
1271/1688 [=====================&gt;........] - ETA: 0s - loss: 0.0246 - accuracy: 0.9916
1303/1688 [======================&gt;.......] - ETA: 0s - loss: 0.0246 - accuracy: 0.9916
1337/1688 [======================&gt;.......] - ETA: 0s - loss: 0.0244 - accuracy: 0.9916
1370/1688 [=======================&gt;......] - ETA: 0s - loss: 0.0249 - accuracy: 0.9915
1404/1688 [=======================&gt;......] - ETA: 0s - loss: 0.0249 - accuracy: 0.9916
1437/1688 [========================&gt;.....] - ETA: 0s - loss: 0.0249 - accuracy: 0.9916
1471/1688 [=========================&gt;....] - ETA: 0s - loss: 0.0250 - accuracy: 0.9915
1503/1688 [=========================&gt;....] - ETA: 0s - loss: 0.0249 - accuracy: 0.9916
1537/1688 [==========================&gt;...] - ETA: 0s - loss: 0.0252 - accuracy: 0.9915
1570/1688 [==========================&gt;...] - ETA: 0s - loss: 0.0257 - accuracy: 0.9913
1603/1688 [===========================&gt;..] - ETA: 0s - loss: 0.0259 - accuracy: 0.9913
1635/1688 [============================&gt;.] - ETA: 0s - loss: 0.0258 - accuracy: 0.9913
1668/1688 [============================&gt;.] - ETA: 0s - loss: 0.0260 - accuracy: 0.9912
1688/1688 [==============================] - 3s 2ms/step - loss: 0.0261 - accuracy: 0.9912 - val_loss: 0.0599 - val_accuracy: 0.9848
Epoch 8/10

   1/1688 [..............................] - ETA: 2s - loss: 5.8221e-04 - accuracy: 1.0000
  35/1688 [..............................] - ETA: 2s - loss: 0.0105 - accuracy: 0.9973    
  69/1688 [&gt;.............................] - ETA: 2s - loss: 0.0096 - accuracy: 0.9977
 102/1688 [&gt;.............................] - ETA: 2s - loss: 0.0089 - accuracy: 0.9982
 134/1688 [=&gt;............................] - ETA: 2s - loss: 0.0099 - accuracy: 0.9970
 167/1688 [=&gt;............................] - ETA: 2s - loss: 0.0112 - accuracy: 0.9964
 201/1688 [==&gt;...........................] - ETA: 2s - loss: 0.0119 - accuracy: 0.9963
 234/1688 [===&gt;..........................] - ETA: 2s - loss: 0.0133 - accuracy: 0.9959
 266/1688 [===&gt;..........................] - ETA: 2s - loss: 0.0139 - accuracy: 0.9957
 299/1688 [====&gt;.........................] - ETA: 2s - loss: 0.0136 - accuracy: 0.9957
 332/1688 [====&gt;.........................] - ETA: 2s - loss: 0.0135 - accuracy: 0.9959
 364/1688 [=====&gt;........................] - ETA: 2s - loss: 0.0137 - accuracy: 0.9958
 398/1688 [======&gt;.......................] - ETA: 1s - loss: 0.0139 - accuracy: 0.9957
 432/1688 [======&gt;.......................] - ETA: 1s - loss: 0.0144 - accuracy: 0.9954
 465/1688 [=======&gt;......................] - ETA: 1s - loss: 0.0142 - accuracy: 0.9954
 498/1688 [=======&gt;......................] - ETA: 1s - loss: 0.0142 - accuracy: 0.9953
 530/1688 [========&gt;.....................] - ETA: 1s - loss: 0.0142 - accuracy: 0.9953
 564/1688 [=========&gt;....................] - ETA: 1s - loss: 0.0156 - accuracy: 0.9947
 597/1688 [=========&gt;....................] - ETA: 1s - loss: 0.0162 - accuracy: 0.9946
 629/1688 [==========&gt;...................] - ETA: 1s - loss: 0.0164 - accuracy: 0.9945
 663/1688 [==========&gt;...................] - ETA: 1s - loss: 0.0167 - accuracy: 0.9942
 695/1688 [===========&gt;..................] - ETA: 1s - loss: 0.0176 - accuracy: 0.9938
 727/1688 [===========&gt;..................] - ETA: 1s - loss: 0.0180 - accuracy: 0.9937
 760/1688 [============&gt;.................] - ETA: 1s - loss: 0.0183 - accuracy: 0.9935
 793/1688 [=============&gt;................] - ETA: 1s - loss: 0.0183 - accuracy: 0.9935
 827/1688 [=============&gt;................] - ETA: 1s - loss: 0.0186 - accuracy: 0.9933
 859/1688 [==============&gt;...............] - ETA: 1s - loss: 0.0185 - accuracy: 0.9934
 892/1688 [==============&gt;...............] - ETA: 1s - loss: 0.0186 - accuracy: 0.9933
 925/1688 [===============&gt;..............] - ETA: 1s - loss: 0.0182 - accuracy: 0.9935
 960/1688 [================&gt;.............] - ETA: 1s - loss: 0.0186 - accuracy: 0.9934
 993/1688 [================&gt;.............] - ETA: 1s - loss: 0.0189 - accuracy: 0.9933
1026/1688 [=================&gt;............] - ETA: 1s - loss: 0.0193 - accuracy: 0.9932
1058/1688 [=================&gt;............] - ETA: 0s - loss: 0.0194 - accuracy: 0.9932
1090/1688 [==================&gt;...........] - ETA: 0s - loss: 0.0202 - accuracy: 0.9928
1123/1688 [==================&gt;...........] - ETA: 0s - loss: 0.0203 - accuracy: 0.9928
1156/1688 [===================&gt;..........] - ETA: 0s - loss: 0.0202 - accuracy: 0.9928
1190/1688 [====================&gt;.........] - ETA: 0s - loss: 0.0202 - accuracy: 0.9928
1223/1688 [====================&gt;.........] - ETA: 0s - loss: 0.0201 - accuracy: 0.9928
1256/1688 [=====================&gt;........] - ETA: 0s - loss: 0.0201 - accuracy: 0.9929
1289/1688 [=====================&gt;........] - ETA: 0s - loss: 0.0198 - accuracy: 0.9930
1322/1688 [======================&gt;.......] - ETA: 0s - loss: 0.0198 - accuracy: 0.9930
1356/1688 [=======================&gt;......] - ETA: 0s - loss: 0.0199 - accuracy: 0.9930
1389/1688 [=======================&gt;......] - ETA: 0s - loss: 0.0197 - accuracy: 0.9930
1423/1688 [========================&gt;.....] - ETA: 0s - loss: 0.0198 - accuracy: 0.9930
1457/1688 [========================&gt;.....] - ETA: 0s - loss: 0.0201 - accuracy: 0.9929
1491/1688 [=========================&gt;....] - ETA: 0s - loss: 0.0202 - accuracy: 0.9928
1524/1688 [==========================&gt;...] - ETA: 0s - loss: 0.0203 - accuracy: 0.9928
1557/1688 [==========================&gt;...] - ETA: 0s - loss: 0.0206 - accuracy: 0.9928
1591/1688 [===========================&gt;..] - ETA: 0s - loss: 0.0206 - accuracy: 0.9928
1625/1688 [===========================&gt;..] - ETA: 0s - loss: 0.0211 - accuracy: 0.9926
1657/1688 [============================&gt;.] - ETA: 0s - loss: 0.0212 - accuracy: 0.9926
1688/1688 [==============================] - 3s 2ms/step - loss: 0.0217 - accuracy: 0.9924 - val_loss: 0.0518 - val_accuracy: 0.9878
Epoch 9/10

   1/1688 [..............................] - ETA: 2s - loss: 0.0245 - accuracy: 1.0000
  37/1688 [..............................] - ETA: 2s - loss: 0.0184 - accuracy: 0.9941
  70/1688 [&gt;.............................] - ETA: 2s - loss: 0.0145 - accuracy: 0.9955
 103/1688 [&gt;.............................] - ETA: 2s - loss: 0.0133 - accuracy: 0.9961
 136/1688 [=&gt;............................] - ETA: 2s - loss: 0.0127 - accuracy: 0.9963
 170/1688 [==&gt;...........................] - ETA: 2s - loss: 0.0121 - accuracy: 0.9965
 203/1688 [==&gt;...........................] - ETA: 2s - loss: 0.0128 - accuracy: 0.9962
 237/1688 [===&gt;..........................] - ETA: 2s - loss: 0.0136 - accuracy: 0.9958
 271/1688 [===&gt;..........................] - ETA: 2s - loss: 0.0132 - accuracy: 0.9960
 305/1688 [====&gt;.........................] - ETA: 2s - loss: 0.0128 - accuracy: 0.9960
 338/1688 [=====&gt;........................] - ETA: 2s - loss: 0.0125 - accuracy: 0.9961
 371/1688 [=====&gt;........................] - ETA: 1s - loss: 0.0129 - accuracy: 0.9958
 403/1688 [======&gt;.......................] - ETA: 1s - loss: 0.0135 - accuracy: 0.9955
 437/1688 [======&gt;.......................] - ETA: 1s - loss: 0.0140 - accuracy: 0.9953
 470/1688 [=======&gt;......................] - ETA: 1s - loss: 0.0140 - accuracy: 0.9952
 503/1688 [=======&gt;......................] - ETA: 1s - loss: 0.0140 - accuracy: 0.9952
 536/1688 [========&gt;.....................] - ETA: 1s - loss: 0.0146 - accuracy: 0.9949
 569/1688 [=========&gt;....................] - ETA: 1s - loss: 0.0143 - accuracy: 0.9951
 602/1688 [=========&gt;....................] - ETA: 1s - loss: 0.0146 - accuracy: 0.9949
 636/1688 [==========&gt;...................] - ETA: 1s - loss: 0.0143 - accuracy: 0.9950
 669/1688 [==========&gt;...................] - ETA: 1s - loss: 0.0144 - accuracy: 0.9949
 702/1688 [===========&gt;..................] - ETA: 1s - loss: 0.0145 - accuracy: 0.9948
 734/1688 [============&gt;.................] - ETA: 1s - loss: 0.0146 - accuracy: 0.9947
 767/1688 [============&gt;.................] - ETA: 1s - loss: 0.0148 - accuracy: 0.9947
 799/1688 [=============&gt;................] - ETA: 1s - loss: 0.0152 - accuracy: 0.9945
 832/1688 [=============&gt;................] - ETA: 1s - loss: 0.0155 - accuracy: 0.9943
 865/1688 [==============&gt;...............] - ETA: 1s - loss: 0.0153 - accuracy: 0.9945
 898/1688 [==============&gt;...............] - ETA: 1s - loss: 0.0152 - accuracy: 0.9945
 932/1688 [===============&gt;..............] - ETA: 1s - loss: 0.0150 - accuracy: 0.9946
 966/1688 [================&gt;.............] - ETA: 1s - loss: 0.0154 - accuracy: 0.9946
1000/1688 [================&gt;.............] - ETA: 1s - loss: 0.0158 - accuracy: 0.9945
1033/1688 [=================&gt;............] - ETA: 0s - loss: 0.0159 - accuracy: 0.9944
1066/1688 [=================&gt;............] - ETA: 0s - loss: 0.0157 - accuracy: 0.9945
1099/1688 [==================&gt;...........] - ETA: 0s - loss: 0.0157 - accuracy: 0.9944
1132/1688 [===================&gt;..........] - ETA: 0s - loss: 0.0163 - accuracy: 0.9942
1164/1688 [===================&gt;..........] - ETA: 0s - loss: 0.0164 - accuracy: 0.9941
1198/1688 [====================&gt;.........] - ETA: 0s - loss: 0.0171 - accuracy: 0.9940
1231/1688 [====================&gt;.........] - ETA: 0s - loss: 0.0174 - accuracy: 0.9939
1264/1688 [=====================&gt;........] - ETA: 0s - loss: 0.0178 - accuracy: 0.9938
1298/1688 [======================&gt;.......] - ETA: 0s - loss: 0.0177 - accuracy: 0.9939
1332/1688 [======================&gt;.......] - ETA: 0s - loss: 0.0175 - accuracy: 0.9939
1365/1688 [=======================&gt;......] - ETA: 0s - loss: 0.0178 - accuracy: 0.9937
1397/1688 [=======================&gt;......] - ETA: 0s - loss: 0.0180 - accuracy: 0.9936
1430/1688 [========================&gt;.....] - ETA: 0s - loss: 0.0186 - accuracy: 0.9935
1464/1688 [=========================&gt;....] - ETA: 0s - loss: 0.0191 - accuracy: 0.9934
1498/1688 [=========================&gt;....] - ETA: 0s - loss: 0.0194 - accuracy: 0.9933
1531/1688 [==========================&gt;...] - ETA: 0s - loss: 0.0195 - accuracy: 0.9933
1565/1688 [==========================&gt;...] - ETA: 0s - loss: 0.0194 - accuracy: 0.9933
1599/1688 [===========================&gt;..] - ETA: 0s - loss: 0.0195 - accuracy: 0.9932
1633/1688 [============================&gt;.] - ETA: 0s - loss: 0.0198 - accuracy: 0.9931
1666/1688 [============================&gt;.] - ETA: 0s - loss: 0.0196 - accuracy: 0.9932
1688/1688 [==============================] - 3s 2ms/step - loss: 0.0196 - accuracy: 0.9932 - val_loss: 0.0552 - val_accuracy: 0.9875
Epoch 10/10

   1/1688 [..............................] - ETA: 2s - loss: 0.0347 - accuracy: 0.9688
  37/1688 [..............................] - ETA: 2s - loss: 0.0146 - accuracy: 0.9958
  72/1688 [&gt;.............................] - ETA: 2s - loss: 0.0148 - accuracy: 0.9952
 105/1688 [&gt;.............................] - ETA: 2s - loss: 0.0149 - accuracy: 0.9955
 138/1688 [=&gt;............................] - ETA: 2s - loss: 0.0162 - accuracy: 0.9955
 170/1688 [==&gt;...........................] - ETA: 2s - loss: 0.0143 - accuracy: 0.9961
 204/1688 [==&gt;...........................] - ETA: 2s - loss: 0.0139 - accuracy: 0.9960
 238/1688 [===&gt;..........................] - ETA: 2s - loss: 0.0130 - accuracy: 0.9963
 272/1688 [===&gt;..........................] - ETA: 2s - loss: 0.0128 - accuracy: 0.9964
 306/1688 [====&gt;.........................] - ETA: 2s - loss: 0.0127 - accuracy: 0.9962
 340/1688 [=====&gt;........................] - ETA: 2s - loss: 0.0126 - accuracy: 0.9960
 373/1688 [=====&gt;........................] - ETA: 1s - loss: 0.0123 - accuracy: 0.9962
 406/1688 [======&gt;.......................] - ETA: 1s - loss: 0.0120 - accuracy: 0.9963
 439/1688 [======&gt;.......................] - ETA: 1s - loss: 0.0117 - accuracy: 0.9964
 473/1688 [=======&gt;......................] - ETA: 1s - loss: 0.0117 - accuracy: 0.9964
 508/1688 [========&gt;.....................] - ETA: 1s - loss: 0.0115 - accuracy: 0.9964
 541/1688 [========&gt;.....................] - ETA: 1s - loss: 0.0121 - accuracy: 0.9962
 574/1688 [=========&gt;....................] - ETA: 1s - loss: 0.0120 - accuracy: 0.9964
 606/1688 [=========&gt;....................] - ETA: 1s - loss: 0.0119 - accuracy: 0.9963
 639/1688 [==========&gt;...................] - ETA: 1s - loss: 0.0122 - accuracy: 0.9961
 672/1688 [==========&gt;...................] - ETA: 1s - loss: 0.0120 - accuracy: 0.9961
 705/1688 [===========&gt;..................] - ETA: 1s - loss: 0.0118 - accuracy: 0.9962
 737/1688 [============&gt;.................] - ETA: 1s - loss: 0.0121 - accuracy: 0.9961
 770/1688 [============&gt;.................] - ETA: 1s - loss: 0.0121 - accuracy: 0.9960
 803/1688 [=============&gt;................] - ETA: 1s - loss: 0.0121 - accuracy: 0.9960
 837/1688 [=============&gt;................] - ETA: 1s - loss: 0.0122 - accuracy: 0.9959
 871/1688 [==============&gt;...............] - ETA: 1s - loss: 0.0122 - accuracy: 0.9959
 904/1688 [===============&gt;..............] - ETA: 1s - loss: 0.0123 - accuracy: 0.9958
 938/1688 [===============&gt;..............] - ETA: 1s - loss: 0.0125 - accuracy: 0.9958
 973/1688 [================&gt;.............] - ETA: 1s - loss: 0.0128 - accuracy: 0.9956
1007/1688 [================&gt;.............] - ETA: 1s - loss: 0.0130 - accuracy: 0.9955
1040/1688 [=================&gt;............] - ETA: 0s - loss: 0.0130 - accuracy: 0.9955
1074/1688 [==================&gt;...........] - ETA: 0s - loss: 0.0132 - accuracy: 0.9954
1108/1688 [==================&gt;...........] - ETA: 0s - loss: 0.0134 - accuracy: 0.9953
1141/1688 [===================&gt;..........] - ETA: 0s - loss: 0.0138 - accuracy: 0.9953
1174/1688 [===================&gt;..........] - ETA: 0s - loss: 0.0140 - accuracy: 0.9953
1207/1688 [====================&gt;.........] - ETA: 0s - loss: 0.0138 - accuracy: 0.9953
1241/1688 [=====================&gt;........] - ETA: 0s - loss: 0.0138 - accuracy: 0.9954
1273/1688 [=====================&gt;........] - ETA: 0s - loss: 0.0139 - accuracy: 0.9953
1306/1688 [======================&gt;.......] - ETA: 0s - loss: 0.0138 - accuracy: 0.9953
1339/1688 [======================&gt;.......] - ETA: 0s - loss: 0.0143 - accuracy: 0.9952
1372/1688 [=======================&gt;......] - ETA: 0s - loss: 0.0144 - accuracy: 0.9951
1405/1688 [=======================&gt;......] - ETA: 0s - loss: 0.0145 - accuracy: 0.9951
1438/1688 [========================&gt;.....] - ETA: 0s - loss: 0.0148 - accuracy: 0.9949
1471/1688 [=========================&gt;....] - ETA: 0s - loss: 0.0149 - accuracy: 0.9948
1505/1688 [=========================&gt;....] - ETA: 0s - loss: 0.0150 - accuracy: 0.9948
1539/1688 [==========================&gt;...] - ETA: 0s - loss: 0.0149 - accuracy: 0.9948
1571/1688 [==========================&gt;...] - ETA: 0s - loss: 0.0150 - accuracy: 0.9947
1604/1688 [===========================&gt;..] - ETA: 0s - loss: 0.0150 - accuracy: 0.9947
1638/1688 [============================&gt;.] - ETA: 0s - loss: 0.0150 - accuracy: 0.9947
1671/1688 [============================&gt;.] - ETA: 0s - loss: 0.0152 - accuracy: 0.9947
1688/1688 [==============================] - 3s 2ms/step - loss: 0.0152 - accuracy: 0.9947 - val_loss: 0.0599 - val_accuracy: 0.9867
Test accuracy: 0.9857000112533569
</pre></div>
</div>
</section>
<section id="model-quantization">
<h2>4. Model quantization<a class="headerlink" href="#model-quantization" title="Permalink to this headline"></a></h2>
<p>We can now turn to quantization to get a discretized version of the model,
where the weights and activations are quantized so as to be suitable for
implementation in the Akida NSoC.</p>
<p>For this, we just have to quantize the Keras model using the
<a class="reference external" href="../../api_reference/cnn2snn_apis.html#quantize">quantize</a>
function. Here, we decide to quantize to the maximum allowed bitwidths for
the first layer weights (8-bit), the subsequent layer weights (4-bit) and the
activations (4-bit).</p>
<p>The quantized model is a Keras model where the neural layers (Conv2D, Dense)
and the ReLU layers are replaced with custom CNN2SNN quantized layers
(QuantizedConv2D, QuantizedDense, QuantizedReLU). All Keras API functions
can be applied on this new model: <code class="docutils literal notranslate"><span class="pre">summary()</span></code>, <code class="docutils literal notranslate"><span class="pre">compile()</span></code>, <code class="docutils literal notranslate"><span class="pre">fit()</span></code>. etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="docutils literal notranslate"><span class="pre">quantize</span></code> function folds the batch normalization layers into
the corresponding neural layer. The new weights are computed
according to this folding operation.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">cnn2snn</span> <span class="kn">import</span> <span class="n">quantize</span>

<span class="n">model_quantized</span> <span class="o">=</span> <span class="n">quantize</span><span class="p">(</span><span class="n">model_keras</span><span class="p">,</span>
                           <span class="n">input_weight_quantization</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
                           <span class="n">weight_quantization</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                           <span class="n">activ_quantization</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">model_quantized</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential_48&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d (QuantizedConv2D)     (None, 13, 13, 32)        320
_________________________________________________________________
re_lu (QuantizedReLU)        (None, 13, 13, 32)        0
_________________________________________________________________
separable_conv2d (QuantizedS (None, 7, 7, 64)          2400
_________________________________________________________________
re_lu_1 (QuantizedReLU)      (None, 7, 7, 64)          0
_________________________________________________________________
flatten (Flatten)            (None, 3136)              0
_________________________________________________________________
dense (QuantizedDense)       (None, 10)                31370
=================================================================
Total params: 34,090
Trainable params: 34,090
Non-trainable params: 0
_________________________________________________________________
</pre></div>
</div>
<p>Check the quantized model accuracy.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model_quantized</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span>
    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>

<span class="n">score</span> <span class="o">=</span> <span class="n">model_quantized</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Test accuracy after 8-4-4 quantization:&#39;</span><span class="p">,</span> <span class="n">score</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Test accuracy after 8-4-4 quantization: 0.9822999835014343
</pre></div>
</div>
<p>Since we used the maximum allowed bitwidths for weights and activations, the
accuracy of the quantized model is equivalent to the one of the base model,
but for lower bitwidth, the quantization  usually introduces a performance drop.</p>
<p>Let’s try to quantize specific layers to a lower bitwidth. The CNN2SNN
toolkit provides the
<a class="reference external" href="../../api_reference/cnn2snn_apis.html#quantize-layer">quantize_layer</a>
function: each layer can be individually quantized.</p>
<p>Here, we quantize the “re_lu_1” layer to binary activations (bitwidth=1)
and the “dense” layer with 2-bit weights.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">cnn2snn</span> <span class="kn">import</span> <span class="n">quantize_layer</span>

<span class="n">model_quantized</span> <span class="o">=</span> <span class="n">quantize_layer</span><span class="p">(</span><span class="n">model_quantized</span><span class="p">,</span> <span class="s2">&quot;re_lu_1&quot;</span><span class="p">,</span> <span class="n">bitwidth</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">model_quantized</span> <span class="o">=</span> <span class="n">quantize_layer</span><span class="p">(</span><span class="n">model_quantized</span><span class="p">,</span> <span class="s2">&quot;dense&quot;</span><span class="p">,</span> <span class="n">bitwidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">model_quantized</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span>
    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>

<span class="n">score</span> <span class="o">=</span> <span class="n">model_quantized</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Test accuracy after low bitwidth quantization:&#39;</span><span class="p">,</span> <span class="n">score</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="c1"># To recover the original model accuracy, a quantization-aware training phase</span>
<span class="c1"># is required.</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Test accuracy after low bitwidth quantization: 0.2134999930858612
</pre></div>
</div>
</section>
<section id="model-fine-tuning-quantization-aware-training">
<h2>5. Model fine tuning (quantization-aware training)<a class="headerlink" href="#model-fine-tuning-quantization-aware-training" title="Permalink to this headline"></a></h2>
<p>This quantization-aware training (fine tuning) allows to cover the
performance drop due to the quantization step.</p>
<p>Note that since this step is a fine tuning, the number of epochs can be
lowered, compared to the training from scratch of the standard model.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model_quantized</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">score</span> <span class="o">=</span> <span class="n">model_quantized</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Test accuracy after fine tuning:&#39;</span><span class="p">,</span> <span class="n">score</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Epoch 1/5

   1/1688 [..............................] - ETA: 16:00 - loss: 2.1215 - accuracy: 0.1562
  24/1688 [..............................] - ETA: 3s - loss: 1.9833 - accuracy: 0.2721   
  47/1688 [..............................] - ETA: 3s - loss: 1.8320 - accuracy: 0.3338
  70/1688 [&gt;.............................] - ETA: 3s - loss: 1.6471 - accuracy: 0.4094
  93/1688 [&gt;.............................] - ETA: 3s - loss: 1.4550 - accuracy: 0.4829
 116/1688 [=&gt;............................] - ETA: 3s - loss: 1.3076 - accuracy: 0.5493
 138/1688 [=&gt;............................] - ETA: 3s - loss: 1.1645 - accuracy: 0.6024
 161/1688 [=&gt;............................] - ETA: 3s - loss: 1.0667 - accuracy: 0.6398
 183/1688 [==&gt;...........................] - ETA: 3s - loss: 0.9960 - accuracy: 0.6689
 206/1688 [==&gt;...........................] - ETA: 3s - loss: 0.9271 - accuracy: 0.6963
 228/1688 [===&gt;..........................] - ETA: 3s - loss: 0.8757 - accuracy: 0.7179
 251/1688 [===&gt;..........................] - ETA: 3s - loss: 0.8341 - accuracy: 0.7356
 274/1688 [===&gt;..........................] - ETA: 3s - loss: 0.7883 - accuracy: 0.7522
 296/1688 [====&gt;.........................] - ETA: 3s - loss: 0.7583 - accuracy: 0.7649
 319/1688 [====&gt;.........................] - ETA: 3s - loss: 0.7247 - accuracy: 0.7768
 341/1688 [=====&gt;........................] - ETA: 3s - loss: 0.6991 - accuracy: 0.7868
 364/1688 [=====&gt;........................] - ETA: 2s - loss: 0.6755 - accuracy: 0.7964
 387/1688 [=====&gt;........................] - ETA: 2s - loss: 0.6584 - accuracy: 0.8047
 410/1688 [======&gt;.......................] - ETA: 2s - loss: 0.6346 - accuracy: 0.8130
 433/1688 [======&gt;.......................] - ETA: 2s - loss: 0.6142 - accuracy: 0.8205
 456/1688 [=======&gt;......................] - ETA: 2s - loss: 0.5978 - accuracy: 0.8270
 479/1688 [=======&gt;......................] - ETA: 2s - loss: 0.5830 - accuracy: 0.8327
 502/1688 [=======&gt;......................] - ETA: 2s - loss: 0.5698 - accuracy: 0.8380
 525/1688 [========&gt;.....................] - ETA: 2s - loss: 0.5599 - accuracy: 0.8422
 548/1688 [========&gt;.....................] - ETA: 2s - loss: 0.5514 - accuracy: 0.8463
 571/1688 [=========&gt;....................] - ETA: 2s - loss: 0.5421 - accuracy: 0.8503
 593/1688 [=========&gt;....................] - ETA: 2s - loss: 0.5310 - accuracy: 0.8544
 617/1688 [=========&gt;....................] - ETA: 2s - loss: 0.5191 - accuracy: 0.8584
 639/1688 [==========&gt;...................] - ETA: 2s - loss: 0.5072 - accuracy: 0.8622
 660/1688 [==========&gt;...................] - ETA: 2s - loss: 0.4997 - accuracy: 0.8646
 682/1688 [===========&gt;..................] - ETA: 2s - loss: 0.4953 - accuracy: 0.8671
 703/1688 [===========&gt;..................] - ETA: 2s - loss: 0.4892 - accuracy: 0.8697
 726/1688 [===========&gt;..................] - ETA: 2s - loss: 0.4843 - accuracy: 0.8722
 749/1688 [============&gt;.................] - ETA: 2s - loss: 0.4796 - accuracy: 0.8745
 772/1688 [============&gt;.................] - ETA: 2s - loss: 0.4725 - accuracy: 0.8771
 796/1688 [=============&gt;................] - ETA: 2s - loss: 0.4704 - accuracy: 0.8794
 819/1688 [=============&gt;................] - ETA: 1s - loss: 0.4627 - accuracy: 0.8817
 841/1688 [=============&gt;................] - ETA: 1s - loss: 0.4556 - accuracy: 0.8839
 864/1688 [==============&gt;...............] - ETA: 1s - loss: 0.4492 - accuracy: 0.8860
 887/1688 [==============&gt;...............] - ETA: 1s - loss: 0.4413 - accuracy: 0.8882
 909/1688 [===============&gt;..............] - ETA: 1s - loss: 0.4369 - accuracy: 0.8898
 932/1688 [===============&gt;..............] - ETA: 1s - loss: 0.4313 - accuracy: 0.8915
 955/1688 [===============&gt;..............] - ETA: 1s - loss: 0.4251 - accuracy: 0.8931
 978/1688 [================&gt;.............] - ETA: 1s - loss: 0.4211 - accuracy: 0.8947
1001/1688 [================&gt;.............] - ETA: 1s - loss: 0.4135 - accuracy: 0.8966
1024/1688 [=================&gt;............] - ETA: 1s - loss: 0.4102 - accuracy: 0.8979
1047/1688 [=================&gt;............] - ETA: 1s - loss: 0.4050 - accuracy: 0.8996
1070/1688 [==================&gt;...........] - ETA: 1s - loss: 0.4010 - accuracy: 0.9010
1093/1688 [==================&gt;...........] - ETA: 1s - loss: 0.3977 - accuracy: 0.9022
1117/1688 [==================&gt;...........] - ETA: 1s - loss: 0.3940 - accuracy: 0.9035
1139/1688 [===================&gt;..........] - ETA: 1s - loss: 0.3902 - accuracy: 0.9047
1162/1688 [===================&gt;..........] - ETA: 1s - loss: 0.3860 - accuracy: 0.9061
1184/1688 [====================&gt;.........] - ETA: 1s - loss: 0.3849 - accuracy: 0.9069
1207/1688 [====================&gt;.........] - ETA: 1s - loss: 0.3839 - accuracy: 0.9078
1229/1688 [====================&gt;.........] - ETA: 1s - loss: 0.3839 - accuracy: 0.9086
1251/1688 [=====================&gt;........] - ETA: 0s - loss: 0.3816 - accuracy: 0.9096
1273/1688 [=====================&gt;........] - ETA: 0s - loss: 0.3794 - accuracy: 0.9105
1296/1688 [======================&gt;.......] - ETA: 0s - loss: 0.3745 - accuracy: 0.9116
1318/1688 [======================&gt;.......] - ETA: 0s - loss: 0.3706 - accuracy: 0.9126
1341/1688 [======================&gt;.......] - ETA: 0s - loss: 0.3695 - accuracy: 0.9134
1364/1688 [=======================&gt;......] - ETA: 0s - loss: 0.3672 - accuracy: 0.9142
1386/1688 [=======================&gt;......] - ETA: 0s - loss: 0.3647 - accuracy: 0.9150
1409/1688 [========================&gt;.....] - ETA: 0s - loss: 0.3608 - accuracy: 0.9159
1431/1688 [========================&gt;.....] - ETA: 0s - loss: 0.3589 - accuracy: 0.9166
1454/1688 [========================&gt;.....] - ETA: 0s - loss: 0.3564 - accuracy: 0.9174
1476/1688 [=========================&gt;....] - ETA: 0s - loss: 0.3546 - accuracy: 0.9183
1499/1688 [=========================&gt;....] - ETA: 0s - loss: 0.3543 - accuracy: 0.9189
1522/1688 [==========================&gt;...] - ETA: 0s - loss: 0.3508 - accuracy: 0.9198
1545/1688 [==========================&gt;...] - ETA: 0s - loss: 0.3505 - accuracy: 0.9203
1567/1688 [==========================&gt;...] - ETA: 0s - loss: 0.3489 - accuracy: 0.9210
1590/1688 [===========================&gt;..] - ETA: 0s - loss: 0.3480 - accuracy: 0.9215
1613/1688 [===========================&gt;..] - ETA: 0s - loss: 0.3482 - accuracy: 0.9220
1636/1688 [============================&gt;.] - ETA: 0s - loss: 0.3479 - accuracy: 0.9225
1659/1688 [============================&gt;.] - ETA: 0s - loss: 0.3465 - accuracy: 0.9231
1681/1688 [============================&gt;.] - ETA: 0s - loss: 0.3465 - accuracy: 0.9236
1688/1688 [==============================] - 5s 2ms/step - loss: 0.3460 - accuracy: 0.9238 - val_loss: 0.2731 - val_accuracy: 0.9690
Epoch 2/5

   1/1688 [..............................] - ETA: 4s - loss: 0.1303 - accuracy: 0.9688
  24/1688 [..............................] - ETA: 3s - loss: 0.1989 - accuracy: 0.9688
  47/1688 [..............................] - ETA: 3s - loss: 0.1786 - accuracy: 0.9707
  70/1688 [&gt;.............................] - ETA: 3s - loss: 0.1731 - accuracy: 0.9719
  93/1688 [&gt;.............................] - ETA: 3s - loss: 0.2097 - accuracy: 0.9691
 116/1688 [=&gt;............................] - ETA: 3s - loss: 0.2039 - accuracy: 0.9690
 139/1688 [=&gt;............................] - ETA: 3s - loss: 0.1947 - accuracy: 0.9712
 162/1688 [=&gt;............................] - ETA: 3s - loss: 0.2117 - accuracy: 0.9688
 184/1688 [==&gt;...........................] - ETA: 3s - loss: 0.2067 - accuracy: 0.9688
 207/1688 [==&gt;...........................] - ETA: 3s - loss: 0.2047 - accuracy: 0.9694
 229/1688 [===&gt;..........................] - ETA: 3s - loss: 0.2051 - accuracy: 0.9697
 251/1688 [===&gt;..........................] - ETA: 3s - loss: 0.2021 - accuracy: 0.9697
 274/1688 [===&gt;..........................] - ETA: 3s - loss: 0.2074 - accuracy: 0.9688
 297/1688 [====&gt;.........................] - ETA: 3s - loss: 0.2046 - accuracy: 0.9692
 320/1688 [====&gt;.........................] - ETA: 3s - loss: 0.2079 - accuracy: 0.9690
 342/1688 [=====&gt;........................] - ETA: 3s - loss: 0.2072 - accuracy: 0.9696
 365/1688 [=====&gt;........................] - ETA: 2s - loss: 0.2103 - accuracy: 0.9688
 388/1688 [=====&gt;........................] - ETA: 2s - loss: 0.2087 - accuracy: 0.9691
 411/1688 [======&gt;.......................] - ETA: 2s - loss: 0.2037 - accuracy: 0.9696
 433/1688 [======&gt;.......................] - ETA: 2s - loss: 0.2026 - accuracy: 0.9697
 456/1688 [=======&gt;......................] - ETA: 2s - loss: 0.1967 - accuracy: 0.9702
 479/1688 [=======&gt;......................] - ETA: 2s - loss: 0.1977 - accuracy: 0.9701
 501/1688 [=======&gt;......................] - ETA: 2s - loss: 0.1964 - accuracy: 0.9706
 524/1688 [========&gt;.....................] - ETA: 2s - loss: 0.1990 - accuracy: 0.9706
 547/1688 [========&gt;.....................] - ETA: 2s - loss: 0.2032 - accuracy: 0.9702
 570/1688 [=========&gt;....................] - ETA: 2s - loss: 0.2045 - accuracy: 0.9700
 593/1688 [=========&gt;....................] - ETA: 2s - loss: 0.2029 - accuracy: 0.9701
 615/1688 [=========&gt;....................] - ETA: 2s - loss: 0.2023 - accuracy: 0.9701
 638/1688 [==========&gt;...................] - ETA: 2s - loss: 0.2028 - accuracy: 0.9701
 661/1688 [==========&gt;...................] - ETA: 2s - loss: 0.2039 - accuracy: 0.9700
 683/1688 [===========&gt;..................] - ETA: 2s - loss: 0.2075 - accuracy: 0.9700
 706/1688 [===========&gt;..................] - ETA: 2s - loss: 0.2095 - accuracy: 0.9699
 728/1688 [===========&gt;..................] - ETA: 2s - loss: 0.2087 - accuracy: 0.9700
 751/1688 [============&gt;.................] - ETA: 2s - loss: 0.2092 - accuracy: 0.9702
 773/1688 [============&gt;.................] - ETA: 2s - loss: 0.2079 - accuracy: 0.9702
 796/1688 [=============&gt;................] - ETA: 2s - loss: 0.2085 - accuracy: 0.9701
 819/1688 [=============&gt;................] - ETA: 1s - loss: 0.2089 - accuracy: 0.9701
 841/1688 [=============&gt;................] - ETA: 1s - loss: 0.2077 - accuracy: 0.9701
 863/1688 [==============&gt;...............] - ETA: 1s - loss: 0.2067 - accuracy: 0.9703
 885/1688 [==============&gt;...............] - ETA: 1s - loss: 0.2097 - accuracy: 0.9704
 907/1688 [===============&gt;..............] - ETA: 1s - loss: 0.2126 - accuracy: 0.9702
 929/1688 [===============&gt;..............] - ETA: 1s - loss: 0.2125 - accuracy: 0.9701
 952/1688 [===============&gt;..............] - ETA: 1s - loss: 0.2111 - accuracy: 0.9701
 975/1688 [================&gt;.............] - ETA: 1s - loss: 0.2136 - accuracy: 0.9700
 997/1688 [================&gt;.............] - ETA: 1s - loss: 0.2159 - accuracy: 0.9698
1019/1688 [=================&gt;............] - ETA: 1s - loss: 0.2165 - accuracy: 0.9698
1042/1688 [=================&gt;............] - ETA: 1s - loss: 0.2162 - accuracy: 0.9698
1065/1688 [=================&gt;............] - ETA: 1s - loss: 0.2153 - accuracy: 0.9699
1087/1688 [==================&gt;...........] - ETA: 1s - loss: 0.2182 - accuracy: 0.9698
1110/1688 [==================&gt;...........] - ETA: 1s - loss: 0.2181 - accuracy: 0.9697
1133/1688 [===================&gt;..........] - ETA: 1s - loss: 0.2199 - accuracy: 0.9696
1156/1688 [===================&gt;..........] - ETA: 1s - loss: 0.2218 - accuracy: 0.9696
1179/1688 [===================&gt;..........] - ETA: 1s - loss: 0.2212 - accuracy: 0.9696
1202/1688 [====================&gt;.........] - ETA: 1s - loss: 0.2225 - accuracy: 0.9693
1224/1688 [====================&gt;.........] - ETA: 1s - loss: 0.2210 - accuracy: 0.9696
1245/1688 [=====================&gt;........] - ETA: 1s - loss: 0.2223 - accuracy: 0.9695
1268/1688 [=====================&gt;........] - ETA: 0s - loss: 0.2231 - accuracy: 0.9694
1291/1688 [=====================&gt;........] - ETA: 0s - loss: 0.2232 - accuracy: 0.9694
1314/1688 [======================&gt;.......] - ETA: 0s - loss: 0.2225 - accuracy: 0.9696
1337/1688 [======================&gt;.......] - ETA: 0s - loss: 0.2221 - accuracy: 0.9697
1360/1688 [=======================&gt;......] - ETA: 0s - loss: 0.2215 - accuracy: 0.9699
1384/1688 [=======================&gt;......] - ETA: 0s - loss: 0.2228 - accuracy: 0.9697
1407/1688 [========================&gt;.....] - ETA: 0s - loss: 0.2225 - accuracy: 0.9697
1429/1688 [========================&gt;.....] - ETA: 0s - loss: 0.2253 - accuracy: 0.9695
1452/1688 [========================&gt;.....] - ETA: 0s - loss: 0.2258 - accuracy: 0.9695
1475/1688 [=========================&gt;....] - ETA: 0s - loss: 0.2268 - accuracy: 0.9694
1497/1688 [=========================&gt;....] - ETA: 0s - loss: 0.2269 - accuracy: 0.9694
1520/1688 [==========================&gt;...] - ETA: 0s - loss: 0.2259 - accuracy: 0.9695
1543/1688 [==========================&gt;...] - ETA: 0s - loss: 0.2248 - accuracy: 0.9696
1566/1688 [==========================&gt;...] - ETA: 0s - loss: 0.2251 - accuracy: 0.9695
1588/1688 [===========================&gt;..] - ETA: 0s - loss: 0.2250 - accuracy: 0.9695
1610/1688 [===========================&gt;..] - ETA: 0s - loss: 0.2253 - accuracy: 0.9695
1633/1688 [============================&gt;.] - ETA: 0s - loss: 0.2278 - accuracy: 0.9693
1655/1688 [============================&gt;.] - ETA: 0s - loss: 0.2273 - accuracy: 0.9694
1678/1688 [============================&gt;.] - ETA: 0s - loss: 0.2266 - accuracy: 0.9696
1688/1688 [==============================] - 4s 2ms/step - loss: 0.2272 - accuracy: 0.9695 - val_loss: 0.3444 - val_accuracy: 0.9665
Epoch 3/5

   1/1688 [..............................] - ETA: 4s - loss: 0.1859 - accuracy: 0.9688
  24/1688 [..............................] - ETA: 3s - loss: 0.1979 - accuracy: 0.9818
  47/1688 [..............................] - ETA: 3s - loss: 0.2221 - accuracy: 0.9734
  70/1688 [&gt;.............................] - ETA: 3s - loss: 0.2086 - accuracy: 0.9723
  93/1688 [&gt;.............................] - ETA: 3s - loss: 0.2112 - accuracy: 0.9718
 116/1688 [=&gt;............................] - ETA: 3s - loss: 0.1987 - accuracy: 0.9725
 139/1688 [=&gt;............................] - ETA: 3s - loss: 0.2074 - accuracy: 0.9723
 162/1688 [=&gt;............................] - ETA: 3s - loss: 0.2030 - accuracy: 0.9730
 185/1688 [==&gt;...........................] - ETA: 3s - loss: 0.1924 - accuracy: 0.9740
 208/1688 [==&gt;...........................] - ETA: 3s - loss: 0.2002 - accuracy: 0.9740
 230/1688 [===&gt;..........................] - ETA: 3s - loss: 0.2075 - accuracy: 0.9735
 252/1688 [===&gt;..........................] - ETA: 3s - loss: 0.2008 - accuracy: 0.9737
 275/1688 [===&gt;..........................] - ETA: 3s - loss: 0.2145 - accuracy: 0.9732
 297/1688 [====&gt;.........................] - ETA: 3s - loss: 0.2147 - accuracy: 0.9733
 319/1688 [====&gt;.........................] - ETA: 3s - loss: 0.2105 - accuracy: 0.9736
 341/1688 [=====&gt;........................] - ETA: 3s - loss: 0.2095 - accuracy: 0.9740
 363/1688 [=====&gt;........................] - ETA: 2s - loss: 0.2149 - accuracy: 0.9735
 385/1688 [=====&gt;........................] - ETA: 2s - loss: 0.2153 - accuracy: 0.9736
 407/1688 [======&gt;.......................] - ETA: 2s - loss: 0.2150 - accuracy: 0.9736
 430/1688 [======&gt;.......................] - ETA: 2s - loss: 0.2133 - accuracy: 0.9737
 452/1688 [=======&gt;......................] - ETA: 2s - loss: 0.2120 - accuracy: 0.9741
 474/1688 [=======&gt;......................] - ETA: 2s - loss: 0.2112 - accuracy: 0.9740
 497/1688 [=======&gt;......................] - ETA: 2s - loss: 0.2091 - accuracy: 0.9741
 520/1688 [========&gt;.....................] - ETA: 2s - loss: 0.2076 - accuracy: 0.9740
 542/1688 [========&gt;.....................] - ETA: 2s - loss: 0.2062 - accuracy: 0.9742
 565/1688 [=========&gt;....................] - ETA: 2s - loss: 0.2074 - accuracy: 0.9744
 588/1688 [=========&gt;....................] - ETA: 2s - loss: 0.2114 - accuracy: 0.9739
 610/1688 [=========&gt;....................] - ETA: 2s - loss: 0.2124 - accuracy: 0.9738
 632/1688 [==========&gt;...................] - ETA: 2s - loss: 0.2117 - accuracy: 0.9738
 654/1688 [==========&gt;...................] - ETA: 2s - loss: 0.2111 - accuracy: 0.9740
 677/1688 [===========&gt;..................] - ETA: 2s - loss: 0.2193 - accuracy: 0.9738
 700/1688 [===========&gt;..................] - ETA: 2s - loss: 0.2171 - accuracy: 0.9738
 723/1688 [===========&gt;..................] - ETA: 2s - loss: 0.2167 - accuracy: 0.9736
 746/1688 [============&gt;.................] - ETA: 2s - loss: 0.2141 - accuracy: 0.9736
 769/1688 [============&gt;.................] - ETA: 2s - loss: 0.2156 - accuracy: 0.9734
 792/1688 [=============&gt;................] - ETA: 2s - loss: 0.2155 - accuracy: 0.9733
 814/1688 [=============&gt;................] - ETA: 1s - loss: 0.2154 - accuracy: 0.9735
 837/1688 [=============&gt;................] - ETA: 1s - loss: 0.2141 - accuracy: 0.9736
 860/1688 [==============&gt;...............] - ETA: 1s - loss: 0.2150 - accuracy: 0.9735
 882/1688 [==============&gt;...............] - ETA: 1s - loss: 0.2147 - accuracy: 0.9733
 905/1688 [===============&gt;..............] - ETA: 1s - loss: 0.2158 - accuracy: 0.9731
 928/1688 [===============&gt;..............] - ETA: 1s - loss: 0.2154 - accuracy: 0.9731
 950/1688 [===============&gt;..............] - ETA: 1s - loss: 0.2145 - accuracy: 0.9731
 972/1688 [================&gt;.............] - ETA: 1s - loss: 0.2140 - accuracy: 0.9732
 994/1688 [================&gt;.............] - ETA: 1s - loss: 0.2142 - accuracy: 0.9732
1017/1688 [=================&gt;............] - ETA: 1s - loss: 0.2165 - accuracy: 0.9730
1039/1688 [=================&gt;............] - ETA: 1s - loss: 0.2181 - accuracy: 0.9730
1062/1688 [=================&gt;............] - ETA: 1s - loss: 0.2185 - accuracy: 0.9731
1085/1688 [==================&gt;...........] - ETA: 1s - loss: 0.2200 - accuracy: 0.9731
1107/1688 [==================&gt;...........] - ETA: 1s - loss: 0.2201 - accuracy: 0.9731
1130/1688 [===================&gt;..........] - ETA: 1s - loss: 0.2235 - accuracy: 0.9730
1153/1688 [===================&gt;..........] - ETA: 1s - loss: 0.2236 - accuracy: 0.9729
1175/1688 [===================&gt;..........] - ETA: 1s - loss: 0.2242 - accuracy: 0.9728
1197/1688 [====================&gt;.........] - ETA: 1s - loss: 0.2238 - accuracy: 0.9728
1220/1688 [====================&gt;.........] - ETA: 1s - loss: 0.2235 - accuracy: 0.9728
1243/1688 [=====================&gt;........] - ETA: 1s - loss: 0.2238 - accuracy: 0.9729
1266/1688 [=====================&gt;........] - ETA: 0s - loss: 0.2236 - accuracy: 0.9729
1288/1688 [=====================&gt;........] - ETA: 0s - loss: 0.2249 - accuracy: 0.9729
1310/1688 [======================&gt;.......] - ETA: 0s - loss: 0.2274 - accuracy: 0.9728
1333/1688 [======================&gt;.......] - ETA: 0s - loss: 0.2266 - accuracy: 0.9728
1356/1688 [=======================&gt;......] - ETA: 0s - loss: 0.2246 - accuracy: 0.9730
1379/1688 [=======================&gt;......] - ETA: 0s - loss: 0.2260 - accuracy: 0.9730
1402/1688 [=======================&gt;......] - ETA: 0s - loss: 0.2256 - accuracy: 0.9730
1425/1688 [========================&gt;.....] - ETA: 0s - loss: 0.2258 - accuracy: 0.9731
1448/1688 [========================&gt;.....] - ETA: 0s - loss: 0.2258 - accuracy: 0.9731
1470/1688 [=========================&gt;....] - ETA: 0s - loss: 0.2258 - accuracy: 0.9730
1492/1688 [=========================&gt;....] - ETA: 0s - loss: 0.2241 - accuracy: 0.9731
1515/1688 [=========================&gt;....] - ETA: 0s - loss: 0.2243 - accuracy: 0.9732
1538/1688 [==========================&gt;...] - ETA: 0s - loss: 0.2261 - accuracy: 0.9731
1561/1688 [==========================&gt;...] - ETA: 0s - loss: 0.2261 - accuracy: 0.9732
1584/1688 [===========================&gt;..] - ETA: 0s - loss: 0.2258 - accuracy: 0.9731
1607/1688 [===========================&gt;..] - ETA: 0s - loss: 0.2250 - accuracy: 0.9731
1630/1688 [===========================&gt;..] - ETA: 0s - loss: 0.2257 - accuracy: 0.9730
1653/1688 [============================&gt;.] - ETA: 0s - loss: 0.2269 - accuracy: 0.9729
1676/1688 [============================&gt;.] - ETA: 0s - loss: 0.2271 - accuracy: 0.9730
1688/1688 [==============================] - 4s 2ms/step - loss: 0.2273 - accuracy: 0.9730 - val_loss: 0.2548 - val_accuracy: 0.9778
Epoch 4/5

   1/1688 [..............................] - ETA: 4s - loss: 1.8303e-04 - accuracy: 1.0000
  25/1688 [..............................] - ETA: 3s - loss: 0.1830 - accuracy: 0.9775    
  48/1688 [..............................] - ETA: 3s - loss: 0.1832 - accuracy: 0.9746
  70/1688 [&gt;.............................] - ETA: 3s - loss: 0.2022 - accuracy: 0.9741
  93/1688 [&gt;.............................] - ETA: 3s - loss: 0.1973 - accuracy: 0.9745
 116/1688 [=&gt;............................] - ETA: 3s - loss: 0.2021 - accuracy: 0.9747
 138/1688 [=&gt;............................] - ETA: 3s - loss: 0.2163 - accuracy: 0.9737
 161/1688 [=&gt;............................] - ETA: 3s - loss: 0.2094 - accuracy: 0.9752
 184/1688 [==&gt;...........................] - ETA: 3s - loss: 0.2098 - accuracy: 0.9749
 207/1688 [==&gt;...........................] - ETA: 3s - loss: 0.2006 - accuracy: 0.9757
 229/1688 [===&gt;..........................] - ETA: 3s - loss: 0.1915 - accuracy: 0.9769
 252/1688 [===&gt;..........................] - ETA: 3s - loss: 0.1963 - accuracy: 0.9767
 274/1688 [===&gt;..........................] - ETA: 3s - loss: 0.1995 - accuracy: 0.9765
 296/1688 [====&gt;.........................] - ETA: 3s - loss: 0.1957 - accuracy: 0.9766
 319/1688 [====&gt;.........................] - ETA: 3s - loss: 0.1937 - accuracy: 0.9769
 342/1688 [=====&gt;........................] - ETA: 3s - loss: 0.1875 - accuracy: 0.9773
 364/1688 [=====&gt;........................] - ETA: 2s - loss: 0.1880 - accuracy: 0.9771
 385/1688 [=====&gt;........................] - ETA: 2s - loss: 0.1861 - accuracy: 0.9772
 407/1688 [======&gt;.......................] - ETA: 2s - loss: 0.1892 - accuracy: 0.9771
 429/1688 [======&gt;.......................] - ETA: 2s - loss: 0.1920 - accuracy: 0.9768
 451/1688 [=======&gt;......................] - ETA: 2s - loss: 0.1957 - accuracy: 0.9761
 473/1688 [=======&gt;......................] - ETA: 2s - loss: 0.1954 - accuracy: 0.9760
 496/1688 [=======&gt;......................] - ETA: 2s - loss: 0.1935 - accuracy: 0.9760
 519/1688 [========&gt;.....................] - ETA: 2s - loss: 0.1961 - accuracy: 0.9758
 542/1688 [========&gt;.....................] - ETA: 2s - loss: 0.1935 - accuracy: 0.9757
 564/1688 [=========&gt;....................] - ETA: 2s - loss: 0.1941 - accuracy: 0.9756
 586/1688 [=========&gt;....................] - ETA: 2s - loss: 0.1960 - accuracy: 0.9754
 609/1688 [=========&gt;....................] - ETA: 2s - loss: 0.1967 - accuracy: 0.9753
 632/1688 [==========&gt;...................] - ETA: 2s - loss: 0.1984 - accuracy: 0.9751
 655/1688 [==========&gt;...................] - ETA: 2s - loss: 0.1996 - accuracy: 0.9752
 677/1688 [===========&gt;..................] - ETA: 2s - loss: 0.2008 - accuracy: 0.9754
 699/1688 [===========&gt;..................] - ETA: 2s - loss: 0.2019 - accuracy: 0.9756
 721/1688 [===========&gt;..................] - ETA: 2s - loss: 0.2008 - accuracy: 0.9756
 745/1688 [============&gt;.................] - ETA: 2s - loss: 0.2011 - accuracy: 0.9758
 768/1688 [============&gt;.................] - ETA: 2s - loss: 0.2037 - accuracy: 0.9757
 790/1688 [=============&gt;................] - ETA: 2s - loss: 0.2055 - accuracy: 0.9755
 813/1688 [=============&gt;................] - ETA: 1s - loss: 0.2042 - accuracy: 0.9756
 836/1688 [=============&gt;................] - ETA: 1s - loss: 0.2066 - accuracy: 0.9757
 859/1688 [==============&gt;...............] - ETA: 1s - loss: 0.2063 - accuracy: 0.9756
 882/1688 [==============&gt;...............] - ETA: 1s - loss: 0.2071 - accuracy: 0.9757
 905/1688 [===============&gt;..............] - ETA: 1s - loss: 0.2065 - accuracy: 0.9758
 928/1688 [===============&gt;..............] - ETA: 1s - loss: 0.2067 - accuracy: 0.9757
 950/1688 [===============&gt;..............] - ETA: 1s - loss: 0.2050 - accuracy: 0.9758
 973/1688 [================&gt;.............] - ETA: 1s - loss: 0.2032 - accuracy: 0.9757
 996/1688 [================&gt;.............] - ETA: 1s - loss: 0.2017 - accuracy: 0.9758
1018/1688 [=================&gt;............] - ETA: 1s - loss: 0.2011 - accuracy: 0.9760
1040/1688 [=================&gt;............] - ETA: 1s - loss: 0.2005 - accuracy: 0.9761
1063/1688 [=================&gt;............] - ETA: 1s - loss: 0.2018 - accuracy: 0.9761
1086/1688 [==================&gt;...........] - ETA: 1s - loss: 0.2016 - accuracy: 0.9761
1108/1688 [==================&gt;...........] - ETA: 1s - loss: 0.2003 - accuracy: 0.9761
1130/1688 [===================&gt;..........] - ETA: 1s - loss: 0.2002 - accuracy: 0.9761
1153/1688 [===================&gt;..........] - ETA: 1s - loss: 0.2022 - accuracy: 0.9761
1175/1688 [===================&gt;..........] - ETA: 1s - loss: 0.2011 - accuracy: 0.9760
1197/1688 [====================&gt;.........] - ETA: 1s - loss: 0.2010 - accuracy: 0.9759
1220/1688 [====================&gt;.........] - ETA: 1s - loss: 0.2027 - accuracy: 0.9759
1243/1688 [=====================&gt;........] - ETA: 1s - loss: 0.2017 - accuracy: 0.9759
1266/1688 [=====================&gt;........] - ETA: 0s - loss: 0.2010 - accuracy: 0.9760
1288/1688 [=====================&gt;........] - ETA: 0s - loss: 0.2004 - accuracy: 0.9761
1310/1688 [======================&gt;.......] - ETA: 0s - loss: 0.2011 - accuracy: 0.9761
1332/1688 [======================&gt;.......] - ETA: 0s - loss: 0.2007 - accuracy: 0.9762
1355/1688 [=======================&gt;......] - ETA: 0s - loss: 0.1999 - accuracy: 0.9762
1377/1688 [=======================&gt;......] - ETA: 0s - loss: 0.1994 - accuracy: 0.9762
1400/1688 [=======================&gt;......] - ETA: 0s - loss: 0.1997 - accuracy: 0.9762
1422/1688 [========================&gt;.....] - ETA: 0s - loss: 0.1980 - accuracy: 0.9764
1444/1688 [========================&gt;.....] - ETA: 0s - loss: 0.1991 - accuracy: 0.9764
1466/1688 [=========================&gt;....] - ETA: 0s - loss: 0.1986 - accuracy: 0.9765
1489/1688 [=========================&gt;....] - ETA: 0s - loss: 0.1978 - accuracy: 0.9765
1511/1688 [=========================&gt;....] - ETA: 0s - loss: 0.1974 - accuracy: 0.9765
1533/1688 [==========================&gt;...] - ETA: 0s - loss: 0.1956 - accuracy: 0.9766
1556/1688 [==========================&gt;...] - ETA: 0s - loss: 0.1969 - accuracy: 0.9766
1579/1688 [===========================&gt;..] - ETA: 0s - loss: 0.1967 - accuracy: 0.9766
1601/1688 [===========================&gt;..] - ETA: 0s - loss: 0.1971 - accuracy: 0.9767
1624/1688 [===========================&gt;..] - ETA: 0s - loss: 0.1976 - accuracy: 0.9768
1647/1688 [============================&gt;.] - ETA: 0s - loss: 0.1957 - accuracy: 0.9769
1670/1688 [============================&gt;.] - ETA: 0s - loss: 0.1961 - accuracy: 0.9769
1688/1688 [==============================] - 4s 2ms/step - loss: 0.1979 - accuracy: 0.9768 - val_loss: 0.3215 - val_accuracy: 0.9755
Epoch 5/5

   1/1688 [..............................] - ETA: 4s - loss: 0.8228 - accuracy: 0.9375
  24/1688 [..............................] - ETA: 3s - loss: 0.2273 - accuracy: 0.9766
  46/1688 [..............................] - ETA: 3s - loss: 0.1513 - accuracy: 0.9837
  69/1688 [&gt;.............................] - ETA: 3s - loss: 0.1487 - accuracy: 0.9823
  92/1688 [&gt;.............................] - ETA: 3s - loss: 0.1378 - accuracy: 0.9827
 114/1688 [=&gt;............................] - ETA: 3s - loss: 0.1498 - accuracy: 0.9814
 136/1688 [=&gt;............................] - ETA: 3s - loss: 0.1553 - accuracy: 0.9807
 159/1688 [=&gt;............................] - ETA: 3s - loss: 0.1568 - accuracy: 0.9803
 181/1688 [==&gt;...........................] - ETA: 3s - loss: 0.1586 - accuracy: 0.9796
 204/1688 [==&gt;...........................] - ETA: 3s - loss: 0.1588 - accuracy: 0.9802
 227/1688 [===&gt;..........................] - ETA: 3s - loss: 0.1669 - accuracy: 0.9798
 251/1688 [===&gt;..........................] - ETA: 3s - loss: 0.1608 - accuracy: 0.9797
 273/1688 [===&gt;..........................] - ETA: 3s - loss: 0.1540 - accuracy: 0.9804
 295/1688 [====&gt;.........................] - ETA: 3s - loss: 0.1497 - accuracy: 0.9807
 317/1688 [====&gt;.........................] - ETA: 3s - loss: 0.1494 - accuracy: 0.9806
 340/1688 [=====&gt;........................] - ETA: 3s - loss: 0.1493 - accuracy: 0.9806
 362/1688 [=====&gt;........................] - ETA: 3s - loss: 0.1573 - accuracy: 0.9806
 384/1688 [=====&gt;........................] - ETA: 2s - loss: 0.1594 - accuracy: 0.9805
 407/1688 [======&gt;.......................] - ETA: 2s - loss: 0.1589 - accuracy: 0.9803
 430/1688 [======&gt;.......................] - ETA: 2s - loss: 0.1629 - accuracy: 0.9798
 453/1688 [=======&gt;......................] - ETA: 2s - loss: 0.1647 - accuracy: 0.9796
 475/1688 [=======&gt;......................] - ETA: 2s - loss: 0.1686 - accuracy: 0.9792
 498/1688 [=======&gt;......................] - ETA: 2s - loss: 0.1692 - accuracy: 0.9790
 521/1688 [========&gt;.....................] - ETA: 2s - loss: 0.1686 - accuracy: 0.9792
 543/1688 [========&gt;.....................] - ETA: 2s - loss: 0.1691 - accuracy: 0.9789
 566/1688 [=========&gt;....................] - ETA: 2s - loss: 0.1718 - accuracy: 0.9781
 589/1688 [=========&gt;....................] - ETA: 2s - loss: 0.1722 - accuracy: 0.9781
 611/1688 [=========&gt;....................] - ETA: 2s - loss: 0.1701 - accuracy: 0.9784
 634/1688 [==========&gt;...................] - ETA: 2s - loss: 0.1726 - accuracy: 0.9786
 656/1688 [==========&gt;...................] - ETA: 2s - loss: 0.1712 - accuracy: 0.9787
 678/1688 [===========&gt;..................] - ETA: 2s - loss: 0.1715 - accuracy: 0.9786
 701/1688 [===========&gt;..................] - ETA: 2s - loss: 0.1690 - accuracy: 0.9787
 723/1688 [===========&gt;..................] - ETA: 2s - loss: 0.1691 - accuracy: 0.9786
 746/1688 [============&gt;.................] - ETA: 2s - loss: 0.1715 - accuracy: 0.9784
 768/1688 [============&gt;.................] - ETA: 2s - loss: 0.1739 - accuracy: 0.9781
 791/1688 [=============&gt;................] - ETA: 2s - loss: 0.1737 - accuracy: 0.9783
 814/1688 [=============&gt;................] - ETA: 1s - loss: 0.1728 - accuracy: 0.9782
 837/1688 [=============&gt;................] - ETA: 1s - loss: 0.1763 - accuracy: 0.9779
 860/1688 [==============&gt;...............] - ETA: 1s - loss: 0.1761 - accuracy: 0.9781
 882/1688 [==============&gt;...............] - ETA: 1s - loss: 0.1768 - accuracy: 0.9782
 905/1688 [===============&gt;..............] - ETA: 1s - loss: 0.1747 - accuracy: 0.9783
 927/1688 [===============&gt;..............] - ETA: 1s - loss: 0.1752 - accuracy: 0.9783
 950/1688 [===============&gt;..............] - ETA: 1s - loss: 0.1745 - accuracy: 0.9784
 973/1688 [================&gt;.............] - ETA: 1s - loss: 0.1760 - accuracy: 0.9782
 995/1688 [================&gt;.............] - ETA: 1s - loss: 0.1780 - accuracy: 0.9780
1018/1688 [=================&gt;............] - ETA: 1s - loss: 0.1766 - accuracy: 0.9781
1040/1688 [=================&gt;............] - ETA: 1s - loss: 0.1783 - accuracy: 0.9781
1063/1688 [=================&gt;............] - ETA: 1s - loss: 0.1788 - accuracy: 0.9781
1086/1688 [==================&gt;...........] - ETA: 1s - loss: 0.1811 - accuracy: 0.9780
1109/1688 [==================&gt;...........] - ETA: 1s - loss: 0.1820 - accuracy: 0.9781
1131/1688 [===================&gt;..........] - ETA: 1s - loss: 0.1815 - accuracy: 0.9781
1154/1688 [===================&gt;..........] - ETA: 1s - loss: 0.1810 - accuracy: 0.9781
1177/1688 [===================&gt;..........] - ETA: 1s - loss: 0.1816 - accuracy: 0.9780
1199/1688 [====================&gt;.........] - ETA: 1s - loss: 0.1805 - accuracy: 0.9781
1221/1688 [====================&gt;.........] - ETA: 1s - loss: 0.1803 - accuracy: 0.9781
1244/1688 [=====================&gt;........] - ETA: 1s - loss: 0.1792 - accuracy: 0.9781
1267/1688 [=====================&gt;........] - ETA: 0s - loss: 0.1811 - accuracy: 0.9781
1290/1688 [=====================&gt;........] - ETA: 0s - loss: 0.1825 - accuracy: 0.9781
1313/1688 [======================&gt;.......] - ETA: 0s - loss: 0.1810 - accuracy: 0.9784
1336/1688 [======================&gt;.......] - ETA: 0s - loss: 0.1807 - accuracy: 0.9785
1358/1688 [=======================&gt;......] - ETA: 0s - loss: 0.1811 - accuracy: 0.9785
1381/1688 [=======================&gt;......] - ETA: 0s - loss: 0.1800 - accuracy: 0.9786
1404/1688 [=======================&gt;......] - ETA: 0s - loss: 0.1815 - accuracy: 0.9786
1427/1688 [========================&gt;.....] - ETA: 0s - loss: 0.1809 - accuracy: 0.9786
1449/1688 [========================&gt;.....] - ETA: 0s - loss: 0.1800 - accuracy: 0.9786
1472/1688 [=========================&gt;....] - ETA: 0s - loss: 0.1797 - accuracy: 0.9787
1495/1688 [=========================&gt;....] - ETA: 0s - loss: 0.1781 - accuracy: 0.9788
1517/1688 [=========================&gt;....] - ETA: 0s - loss: 0.1807 - accuracy: 0.9786
1539/1688 [==========================&gt;...] - ETA: 0s - loss: 0.1802 - accuracy: 0.9786
1562/1688 [==========================&gt;...] - ETA: 0s - loss: 0.1791 - accuracy: 0.9787
1585/1688 [===========================&gt;..] - ETA: 0s - loss: 0.1805 - accuracy: 0.9786
1607/1688 [===========================&gt;..] - ETA: 0s - loss: 0.1798 - accuracy: 0.9787
1630/1688 [===========================&gt;..] - ETA: 0s - loss: 0.1807 - accuracy: 0.9786
1653/1688 [============================&gt;.] - ETA: 0s - loss: 0.1809 - accuracy: 0.9785
1676/1688 [============================&gt;.] - ETA: 0s - loss: 0.1817 - accuracy: 0.9784
1688/1688 [==============================] - 4s 2ms/step - loss: 0.1825 - accuracy: 0.9783 - val_loss: 0.2445 - val_accuracy: 0.9802
Test accuracy after fine tuning: 0.9761000275611877
</pre></div>
</div>
</section>
<section id="model-conversion">
<h2>6. Model conversion<a class="headerlink" href="#model-conversion" title="Permalink to this headline"></a></h2>
<p>After having obtained a quantized model with satisfactory performance, it can
be converted to a model suitable to be used in the Akida NSoC in inference
mode. The <a class="reference external" href="../../api_reference/cnn2snn_apis.html#convert">convert</a>
function returns a model in Akida format, ready for the Akida NSoC or the
Akida software simulator.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>One needs to supply the coefficients used to rescale the input
dataset before the training - here <code class="docutils literal notranslate"><span class="pre">input_scaling</span></code>.</p>
</div>
<p>As with Keras, the summary() method provides a textual representation of the
Akida model.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">cnn2snn</span> <span class="kn">import</span> <span class="n">convert</span>

<span class="n">model_akida</span> <span class="o">=</span> <span class="n">convert</span><span class="p">(</span><span class="n">model_quantized</span><span class="p">,</span> <span class="n">input_scaling</span><span class="o">=</span><span class="n">input_scaling</span><span class="p">)</span>
<span class="n">model_akida</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>

<span class="n">results</span> <span class="o">=</span> <span class="n">model_akida</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">raw_x_test</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="p">(</span><span class="n">raw_y_test</span> <span class="o">==</span> <span class="n">results</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Test accuracy after conversion:&#39;</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">)</span>

<span class="c1"># For non-regression purpose</span>
<span class="k">assert</span> <span class="n">accuracy</span> <span class="o">&gt;</span> <span class="mf">0.97</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>                Model Summary
______________________________________________
Input shape  Output shape  Sequences  Layers
==============================================
[28, 28, 1]  [1, 1, 10]    1          3
______________________________________________

                  SW/conv2d-dense (Software)
______________________________________________________________
Layer (type)                  Output shape  Kernel shape
==============================================================
conv2d (InputConv.)           [13, 13, 32]  (3, 3, 1, 32)
______________________________________________________________
separable_conv2d (Sep.Conv.)  [7, 7, 64]    (3, 3, 32, 1)
______________________________________________________________
                                            (1, 1, 32, 64)
______________________________________________________________
dense (Fully.)                [1, 1, 10]    (1, 1, 3136, 10)
______________________________________________________________

Test accuracy after conversion: 0.9764
</pre></div>
</div>
<p>Depending on the number of samples you run, you should find a
performance of around 98% (better results can be achieved using more
epochs for training).</p>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> ( 0 minutes  51.712 seconds)</p>
<div class="sphx-glr-footer class sphx-glr-footer-example docutils container" id="sphx-glr-download-examples-cnn2snn-plot-0-cnn-flow-py">
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/feacc23c72579af617bca35c381667de/plot_0_cnn_flow.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">plot_0_cnn_flow.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/303bd1ef847df461f1433347bc300bd7/plot_0_cnn_flow.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">plot_0_cnn_flow.ipynb</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../general/plot_5_voc_yolo_detection.html" class="btn btn-neutral float-left" title="YOLO/PASCAL-VOC detection tutorial" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="plot_1_advanced_cnn2snn.html" class="btn btn-neutral float-right" title="Advanced CNN2SNN tutorial" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, BrainChip Holdings Ltd. All Rights Reserved.</p>
  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>