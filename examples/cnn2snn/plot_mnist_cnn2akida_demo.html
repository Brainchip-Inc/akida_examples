

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>CNN conversion flow tutorial for MNIST &mdash; Akida Examples  documentation</title>
  

  
  
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
  
  
  

  
  <script type="text/javascript" src="../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/gallery.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Learning and inference on MNIST" href="../semisupervised/plot_mnist_main.html" />
    <link rel="prev" title="Inference on ImageNet with MobileNet" href="plot_mobilenet_imagenet.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #3f51b5" >
          

          
            <a href="../../index.html">
          

          
            
            <img src="../../_static/akida.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                1.7.3
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../index.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../installation.html">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../installation.html#requirements">Requirements</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../installation.html#quick-installation">Quick installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../installation.html#running-examples">Running examples</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../user_guide/user_guide.html">User guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../user_guide/getting_started.html">Getting started</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/getting_started.html#for-beginners">For beginners</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/getting_started.html#for-users-familiar-with-deep-learning">For users familiar with deep-learning</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../user_guide/aee.html">AEE user guide</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/aee.html#the-akida-execution-engine">The Akida Execution Engine</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/aee.html#id1">1. The Spiking Neural Network model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/aee.html#id2">2. Input data format</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/aee.html#id3">3. Determine training mode</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/aee.html#id4">4. Interpreting outputs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/aee.html#neural-network-model">Neural Network model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/aee.html#specifying-the-neural-network-model">Specifying the Neural Network model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/aee.html#saving-and-loading">Saving and loading</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/aee.html#input-layer-types">Input layer types</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/aee.html#data-processing-layer-types">Data-Processing layer types</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/aee.html#id5">Using Akida Unsupervised Learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/aee.html#learning-constraints">Learning constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/aee.html#compiling-a-layer">Compiling a layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/aee.html#id7">Learning parameters</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../user_guide/cnn2snn.html">CNN2SNN toolkit</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/cnn2snn.html#overview">Overview</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#conversion-workflow">Conversion Workflow</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#compatibility-constraints">Compatibility Constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#typical-training-scenario">Typical training scenario</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/cnn2snn.html#layers-considerations">Layers Considerations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#supported-layer-types">Supported layer types</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#quantization-aware-layers">Quantization-aware layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#training-only-layers">Training-Only Layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#first-layers">First Layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#id6">Final Layers</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/cnn2snn.html#layer-blocks">Layer Blocks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#id7">Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#conv-block"><code class="docutils literal notranslate"><span class="pre">conv_block</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#dense-block"><code class="docutils literal notranslate"><span class="pre">dense_block</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/cnn2snn.html#separable-conv-block"><code class="docutils literal notranslate"><span class="pre">separable_conv_block</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/cnn2snn.html#tips-and-tricks">Tips and Tricks</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../user_guide/hw_constraints.html">Hardware constraints</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/hw_constraints.html#input-layer">Input layer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../user_guide/hw_constraints.html#data-processing-layers">Data-Processing layers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/hw_constraints.html#convolutional-layer">Convolutional layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../user_guide/hw_constraints.html#fully-connected-layer">Fully connected layer</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../api_reference/api_reference.html">API reference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../api_reference/aee_apis.html">AEE</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#model">Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#layer">Layer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#layerstatistics">LayerStatistics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#observer">Observer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#inputdata">InputData</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#inputconvolutional">InputConvolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#inputbcspike">InputBCSpike</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#fullyconnected">FullyConnected</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#convolutional">Convolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#separableconvolutional">SeparableConvolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#tensor">Tensor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#sparse">Sparse</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#coords-to-sparse">coords_to_sparse</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#dense-to-sparse">dense_to_sparse</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#backendtype">BackendType</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#convolutionmode">ConvolutionMode</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#poolingtype">PoolingType</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/aee_apis.html#learningtype">LearningType</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html">CNN2SNN</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#convert">convert</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#check-model-compatibility">check_model_compatibility</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#weightquantizer">WeightQuantizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#weightfloat">WeightFloat</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#quantizedconv2d">QuantizedConv2D</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#quantizeddepthwiseconv2d">QuantizedDepthwiseConv2D</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#quantizeddense">QuantizedDense</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#quantizedseparableconv2d">QuantizedSeparableConv2D</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api_reference/cnn2snn_apis.html#activationdiscreterelu">ActivationDiscreteRelu</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Examples</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../index.html#cnn2snn">CNN2SNN</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="plot_mnist_cnn2akida_main.html">Inference on MNIST</a><ul>
<li class="toctree-l4"><a class="reference internal" href="plot_mnist_cnn2akida_main.html#loading-the-mnist-dataset">1. Loading the MNIST dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_mnist_cnn2akida_main.html#look-at-some-images-from-the-test-dataset">2. Look at some images from the test dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_mnist_cnn2akida_main.html#load-the-pre-trained-akida-model">3. Load the pre-trained Akida model</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_mnist_cnn2akida_main.html#classify-a-single-image">4. Classify a single image</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_mnist_cnn2akida_main.html#check-performance-across-a-number-of-samples">5. Check performance across a number of samples</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="plot_mobilenet_kws.html">Inference on KWS with MobileNet</a><ul>
<li class="toctree-l4"><a class="reference internal" href="plot_mobilenet_kws.html#load-cnn2snn-tool-dependencies">1. Load CNN2SNN tool dependencies</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_mobilenet_kws.html#load-the-preprocessed-dataset">2. Load the preprocessed dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_mobilenet_kws.html#create-a-keras-model-satisfying-akida-nsoc-requirements">3. Create a Keras model satisfying Akida NSoC requirements</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_mobilenet_kws.html#check-performance">4. Check performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_mobilenet_kws.html#conversion-to-akida">5. Conversion to Akida</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="plot_cifar10_cnn2akida_demo.html">Inference on CIFAR10 with VGG and MobileNet</a><ul>
<li class="toctree-l4"><a class="reference internal" href="plot_cifar10_cnn2akida_demo.html#load-cnn2snn-tool-dependencies">1. Load CNN2SNN tool dependencies</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_cifar10_cnn2akida_demo.html#load-and-reshape-cifar10-dataset">2. Load and reshape CIFAR10 dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_cifar10_cnn2akida_demo.html#create-a-quantized-keras-vgg-model">3. Create a quantized Keras VGG model</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_cifar10_cnn2akida_demo.html#conversion-to-akida">4. Conversion to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_cifar10_cnn2akida_demo.html#create-a-quantized-keras-mobilenet-model">5. Create a quantized Keras MobileNet model</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_cifar10_cnn2akida_demo.html#id2">6. Conversion to Akida</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="plot_cats_vs_dogs_cnn2akida_demo.html">Transfer learning with MobileNet for cats vs. dogs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="plot_cats_vs_dogs_cnn2akida_demo.html#transfer-learning-process">1. Transfer learning process</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_cats_vs_dogs_cnn2akida_demo.html#load-and-preprocess-data">2. Load and preprocess data</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_cats_vs_dogs_cnn2akida_demo.html#convert-a-quantized-keras-model-to-akida">3. Convert a quantized Keras model to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_cats_vs_dogs_cnn2akida_demo.html#classify-test-images">4. Classify test images</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="plot_mobilenet_imagenet.html">Inference on ImageNet with MobileNet</a><ul>
<li class="toctree-l4"><a class="reference internal" href="plot_mobilenet_imagenet.html#load-cnn2snn-tool-dependencies">1. Load CNN2SNN tool dependencies</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_mobilenet_imagenet.html#load-test-images-from-imagenet">2. Load test images from ImageNet</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_mobilenet_imagenet.html#create-a-quantized-keras-model">3. Create a quantized Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="plot_mobilenet_imagenet.html#convert-keras-model-for-akida-nsoc">4. Convert Keras model for Akida NSoC</a></li>
</ul>
</li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">CNN conversion flow tutorial for MNIST</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#system-configuration">1. System configuration</a></li>
<li class="toctree-l4"><a class="reference internal" href="#model-creation-and-performance-check">2. Model creation and performance check</a></li>
<li class="toctree-l4"><a class="reference internal" href="#model-akida-compatibility-check-and-changes">3. Model Akida-compatibility check and changes</a></li>
<li class="toctree-l4"><a class="reference internal" href="#model-quantization-and-training">4. Model quantization and training</a></li>
<li class="toctree-l4"><a class="reference internal" href="#convert-trained-model-for-akida-and-test">5. Convert trained model for Akida and test</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#semisupervised">Semisupervised</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../semisupervised/plot_mnist_main.html">Learning and inference on MNIST</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../semisupervised/plot_mnist_main.html#loading-the-mnist-dataset">1. Loading the MNIST dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../semisupervised/plot_mnist_main.html#look-at-some-images-from-the-dataset">2. Look at some images from the dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../semisupervised/plot_mnist_main.html#configuring-akida-model">3. Configuring Akida model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../semisupervised/plot_mnist_main.html#testing-performance">4. Testing performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../semisupervised/plot_mnist_main.html#learning-and-inference">5. Learning and inference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../semisupervised/plot_dvs_main.html">Learning and inference on Characters DVS</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../semisupervised/plot_dvs_main.html#loading-the-characters-dvs-dataset">1. Loading the Characters DVS dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../semisupervised/plot_dvs_main.html#look-at-some-events-from-the-dataset">2. Look at some events from the dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../semisupervised/plot_dvs_main.html#configuring-akida-model">3. Configuring Akida model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../semisupervised/plot_dvs_main.html#learning-and-inference">4. Learning and inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../semisupervised/plot_dvs_main.html#unsupervised-learning-with-supervised-classification">5. Unsupervised learning with supervised classification</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../semisupervised/plot_nslkdd_main.html">Learning and inference on NSL-KDD</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../semisupervised/plot_nslkdd_main.html#download-and-prepare-the-nsl-kdd-dataset">1. Download and prepare the NSL-KDD dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../semisupervised/plot_nslkdd_main.html#sneak-peek-of-the-input-tabular-data">2. Sneak peek of the input tabular data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../semisupervised/plot_nslkdd_main.html#convert-from-tabular-to-binary-data">3. Convert from tabular to binary data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../semisupervised/plot_nslkdd_main.html#oversampling-the-training-data-to-cope-with-imbalanced-dataset">4. Oversampling the training data to cope with imbalanced dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../semisupervised/plot_nslkdd_main.html#configuring-akida-model">5. Configuring Akida model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../semisupervised/plot_nslkdd_main.html#learning-and-inference">6. Learning and inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../semisupervised/plot_nslkdd_main.html#display-results">7. Display results</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#unsupervised">Unsupervised</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../unsupervised/plot_unsupervised_main.html">Native learning for pattern detection</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../unsupervised/plot_unsupervised_main.html#creating-the-dataset">1. Creating the dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../unsupervised/plot_unsupervised_main.html#creating-random-dot-images">2. Creating random dot images</a></li>
<li class="toctree-l4"><a class="reference internal" href="../unsupervised/plot_unsupervised_main.html#take-a-look-at-some-of-the-random-dots-images">3. Take a look at some of the random dots images</a></li>
<li class="toctree-l4"><a class="reference internal" href="../unsupervised/plot_unsupervised_main.html#configuring-the-akida-model">4. Configuring the Akida model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../unsupervised/plot_unsupervised_main.html#do-the-learning">5. Do the learning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../unsupervised/plot_unsupervised_main.html#test-the-performance">6. Test the performance</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference external" href="https://support.brainchip.com/portal/home">Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../license.html">License</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Akida Examples</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../index.html">Akida examples</a> &raquo;</li>
        
      <li>CNN conversion flow tutorial for MNIST</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p>Click <a class="reference internal" href="#sphx-glr-download-examples-cnn2snn-plot-mnist-cnn2akida-demo-py"><span class="std std-ref">here</span></a> to download the full example code</p>
</div>
<div class="sphx-glr-example-title section" id="cnn-conversion-flow-tutorial-for-mnist">
<span id="sphx-glr-examples-cnn2snn-plot-mnist-cnn2akida-demo-py"></span><h1>CNN conversion flow tutorial for MNIST<a class="headerlink" href="#cnn-conversion-flow-tutorial-for-mnist" title="Permalink to this headline">¶</a></h1>
<p>The CNN2SNN tool is based on Keras, TensorFlow high-level API for building and
training deep learning models.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Please refer to TensorFlow  <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/models">tf.keras.models</a>
module for model creation/import details and <a class="reference external" href="https://www.tensorflow.org/guide">TensorFlow
Guide</a> for details of how
TensorFlow works.</p>
</div>
<p><strong>CNN2SNN tool</strong> allows you to <strong>convert CNN networks to SNN networks</strong>
compatible with the <strong>Akida NSoC</strong> in a few steps.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>MNIST example below is light enough so you do not need a <a class="reference external" href="https://www.tensorflow.org/install/gpu">GPU</a> to run the CNN2SNN
tool.</p>
</div>
<img alt="../../_images/cnn2snn_flow_small.png" src="../../_images/cnn2snn_flow_small.png" />
<p><strong>In this tutorial you will:</strong></p>
<blockquote>
<div><ul class="simple">
<li><p>load and reshape MNIST dataset,</p></li>
<li><p>create a CNN model,</p></li>
<li><p>configure it for Akida NSoC,</p></li>
<li><p>quantize it,</p></li>
<li><p>train it,</p></li>
<li><p>convert to an Akida model,</p></li>
<li><p>check its performance using the Akida Execution Engine.</p></li>
</ul>
</div></blockquote>
<div class="section" id="system-configuration">
<h2>1. System configuration<a class="headerlink" href="#system-configuration" title="Permalink to this headline">¶</a></h2>
<div class="section" id="load-cnn2snn-tool-dependencies">
<h3>1.1 Load CNN2SNN tool dependencies<a class="headerlink" href="#load-cnn2snn-tool-dependencies" title="Permalink to this headline">¶</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># System imports</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="kn">from</span> <span class="nn">tempfile</span> <span class="kn">import</span> <span class="n">TemporaryDirectory</span>

<span class="c1"># TensorFlow imports</span>
<span class="kn">import</span> <span class="nn">tensorflow.keras.backend</span> <span class="k">as</span> <span class="nn">K</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.callbacks</span> <span class="kn">import</span> <span class="n">LearningRateScheduler</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.datasets</span> <span class="kn">import</span> <span class="n">mnist</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Conv2D</span><span class="p">,</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">BatchNormalization</span><span class="p">,</span> <span class="n">MaxPooling2D</span><span class="p">,</span> <span class="n">Activation</span><span class="p">,</span> <span class="n">ReLU</span><span class="p">,</span> <span class="n">Flatten</span><span class="p">,</span> <span class="n">Input</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span><span class="p">,</span> <span class="n">Model</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.optimizers</span> <span class="kn">import</span> <span class="n">Adam</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.utils</span> <span class="kn">import</span> <span class="n">to_categorical</span>
</pre></div>
</div>
</div>
<div class="section" id="load-and-reshape-mnist-dataset">
<h3>1.2 Load and reshape MNIST dataset<a class="headerlink" href="#load-and-reshape-mnist-dataset" title="Permalink to this headline">¶</a></h3>
<p>After loading, we make 3 transformations on the dataset:</p>
<ol class="arabic simple">
<li><p>Reshape the sample content data (x values) into a num_samples x width x
height x channels matrix.</p></li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>At this point, we’ll set aside the raw data for testing our
converted model in the Akida Execution Engine later</p>
</div>
<ol class="arabic simple" start="2">
<li><p>Rescale the 8-bit loaded data to the range 0-to-1 for training.</p></li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This shift makes almost no difference in the current example, but
for some datasets rescaling the absolute values (and also shifting
to zero-mean) can make a really major difference.</p>
<p>Also note that we store the scaling values <code class="docutils literal notranslate"><span class="pre">input_scaling</span></code> for
use when preparing the model for the Akida Execution Engine. The
implementation of the Akida neural network allows us to completely
skip the rescaling step (i.e. the Akida model should be fed with
the raw 8-bit values) but that does require information about what
scaling was applied prior to training -see below for more details-.</p>
</div>
<p>3. Transform the loaded labels from a scalar representation (single integer
value per sample) to a one-hot vector representation, appropriate for use
with the squared hinge loss function used in the current model.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Input data normalization is a common step dealing with CNN
(rationale is to keep data in a range that works with selected
optimizers, some interesting reading can be found
<a class="reference external" href="https://www.jeremyjordan.me/batch-normalization/">here</a>.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load MNIST dataset</span>
<span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>

<span class="c1"># Reshape x-data</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">60000</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Set aside raw test data for use with Akida Execution Engine later</span>
<span class="n">raw_x_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;uint8&#39;</span><span class="p">)</span>
<span class="n">raw_y_test</span> <span class="o">=</span> <span class="n">y_test</span>

<span class="c1"># Rescale x-data</span>
<span class="n">a</span> <span class="o">=</span> <span class="mi">255</span>
<span class="n">b</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">input_scaling</span> <span class="o">=</span> <span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_train</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span><span class="o">/</span><span class="n">a</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_test</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span><span class="o">/</span><span class="n">a</span>

<span class="c1"># Transform scalar labels to one-hot representation, scaled to +/- 1 appropriate for squared hinge loss function</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
<div class="section" id="set-training-parameters">
<h3>1.3 Set training parameters<a class="headerlink" href="#set-training-parameters" title="Permalink to this headline">¶</a></h3>
<p>Set some training parameters used across the different training sessions:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set dataset relative training parameters</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>

<span class="c1"># Set the learning rate parameters</span>
<span class="n">lr_start</span> <span class="o">=</span> <span class="mf">1e-3</span>
<span class="n">lr_end</span> <span class="o">=</span> <span class="mf">1e-4</span>
<span class="n">lr_decay</span> <span class="o">=</span> <span class="p">(</span><span class="n">lr_end</span> <span class="o">/</span> <span class="n">lr_start</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">/</span> <span class="n">epochs</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="model-creation-and-performance-check">
<h2>2. Model creation and performance check<a class="headerlink" href="#model-creation-and-performance-check" title="Permalink to this headline">¶</a></h2>
<div class="section" id="model-creation">
<h3>2.1 Model creation<a class="headerlink" href="#model-creation" title="Permalink to this headline">¶</a></h3>
<p>Note that at this stage, there is nothing specific to the Akida NSoC.
This start point is very much a completely standard CNN as defined
within <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras">Keras</a>.</p>
<p>An appropriate model for MNIST (inspired by <a class="reference external" href="https://arxiv.org/pdf/1705.09283.pdf">this
paper</a>) might look something
like the following:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">img_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
           <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
           <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span>
           <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
           <span class="n">data_format</span><span class="o">=</span><span class="s1">&#39;channels_last&#39;</span><span class="p">)(</span><span class="n">img_input</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">MaxPooling2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">ReLU</span><span class="p">(</span><span class="mf">6.</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
           <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
           <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span>
           <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">MaxPooling2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">ReLU</span><span class="p">(</span><span class="mf">6.</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">Flatten</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span>
          <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">ReLU</span><span class="p">(</span><span class="mf">6.</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span>
          <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

<span class="n">model_keras</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">img_input</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;mnistnet&#39;</span><span class="p">)</span>

<span class="n">opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">lr_start</span><span class="p">)</span>
<span class="n">model_keras</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;squared_hinge&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
<span class="n">model_keras</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Model: &quot;mnistnet&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_6 (InputLayer)         [(None, 28, 28, 1)]       0
_________________________________________________________________
conv2d (Conv2D)              (None, 28, 28, 32)        800
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0
_________________________________________________________________
batch_normalization (BatchNo (None, 14, 14, 32)        128
_________________________________________________________________
re_lu (ReLU)                 (None, 14, 14, 32)        0
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 14, 14, 64)        51200
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0
_________________________________________________________________
batch_normalization_1 (Batch (None, 7, 7, 64)          256
_________________________________________________________________
re_lu_1 (ReLU)               (None, 7, 7, 64)          0
_________________________________________________________________
flatten_1 (Flatten)          (None, 3136)              0
_________________________________________________________________
dense (Dense)                (None, 512)               1605632
_________________________________________________________________
batch_normalization_2 (Batch (None, 512)               2048
_________________________________________________________________
re_lu_2 (ReLU)               (None, 512)               0
_________________________________________________________________
dense_1 (Dense)              (None, 10)                5120
=================================================================
Total params: 1,665,184
Trainable params: 1,663,968
Non-trainable params: 1,216
_________________________________________________________________
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Adam optimizer is commonly used, more details can be found
<a class="reference external" href="https://arxiv.org/abs/1609.04747">here</a>.</p>
</div>
</div>
<div class="section" id="performance-check">
<h3>2.2 Performance check<a class="headerlink" href="#performance-check" title="Permalink to this headline">¶</a></h3>
<p>Before going any further, check the current model performance as a
benchmark for CNN2SNN conversion.
The created model should achieve a test accuracy a little over 99% after
5 epochs:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">callbacks</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">LearningRateScheduler</span><span class="p">(</span><span class="k">lambda</span> <span class="n">e</span><span class="p">:</span> <span class="n">lr_start</span> <span class="o">*</span> <span class="n">lr_decay</span> <span class="o">**</span> <span class="n">e</span><span class="p">)</span>
<span class="n">callbacks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lr_scheduler</span><span class="p">)</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model_keras</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
                    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span>
                    <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">),</span>
                    <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span>
<span class="n">score</span> <span class="o">=</span> <span class="n">model_keras</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Test score:&#39;</span><span class="p">,</span> <span class="n">score</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Test accuracy:&#39;</span><span class="p">,</span> <span class="n">score</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Train on 60000 samples, validate on 10000 samples
Epoch 1/5

  128/60000 [..............................] - ETA: 4:48 - loss: 1.2790 - accuracy: 0.1250
 1792/60000 [..............................] - ETA: 21s - loss: 0.3303 - accuracy: 0.7946 
 3712/60000 [&gt;.............................] - ETA: 10s - loss: 0.1919 - accuracy: 0.8672
 5632/60000 [=&gt;............................] - ETA: 7s - loss: 0.1413 - accuracy: 0.8961 
 7552/60000 [==&gt;...........................] - ETA: 5s - loss: 0.1123 - accuracy: 0.9146
 9472/60000 [===&gt;..........................] - ETA: 4s - loss: 0.0948 - accuracy: 0.9274
11392/60000 [====&gt;.........................] - ETA: 3s - loss: 0.0831 - accuracy: 0.9346
13440/60000 [=====&gt;........................] - ETA: 3s - loss: 0.0742 - accuracy: 0.9406
15616/60000 [======&gt;.......................] - ETA: 2s - loss: 0.0667 - accuracy: 0.9461
17664/60000 [=======&gt;......................] - ETA: 2s - loss: 0.0606 - accuracy: 0.9508
19584/60000 [========&gt;.....................] - ETA: 2s - loss: 0.0563 - accuracy: 0.9539
21632/60000 [=========&gt;....................] - ETA: 2s - loss: 0.0527 - accuracy: 0.9565
23680/60000 [==========&gt;...................] - ETA: 1s - loss: 0.0498 - accuracy: 0.9584
25728/60000 [===========&gt;..................] - ETA: 1s - loss: 0.0470 - accuracy: 0.9607
27776/60000 [============&gt;.................] - ETA: 1s - loss: 0.0449 - accuracy: 0.9624
29824/60000 [=============&gt;................] - ETA: 1s - loss: 0.0431 - accuracy: 0.9635
32000/60000 [===============&gt;..............] - ETA: 1s - loss: 0.0417 - accuracy: 0.9646
34176/60000 [================&gt;.............] - ETA: 1s - loss: 0.0399 - accuracy: 0.9659
36352/60000 [=================&gt;............] - ETA: 1s - loss: 0.0384 - accuracy: 0.9671
38400/60000 [==================&gt;...........] - ETA: 0s - loss: 0.0372 - accuracy: 0.9680
40576/60000 [===================&gt;..........] - ETA: 0s - loss: 0.0361 - accuracy: 0.9689
42752/60000 [====================&gt;.........] - ETA: 0s - loss: 0.0349 - accuracy: 0.9699
44928/60000 [=====================&gt;........] - ETA: 0s - loss: 0.0338 - accuracy: 0.9707
46976/60000 [======================&gt;.......] - ETA: 0s - loss: 0.0329 - accuracy: 0.9714
49024/60000 [=======================&gt;......] - ETA: 0s - loss: 0.0321 - accuracy: 0.9721
51072/60000 [========================&gt;.....] - ETA: 0s - loss: 0.0311 - accuracy: 0.9729
52992/60000 [=========================&gt;....] - ETA: 0s - loss: 0.0304 - accuracy: 0.9735
54912/60000 [==========================&gt;...] - ETA: 0s - loss: 0.0297 - accuracy: 0.9741
56832/60000 [===========================&gt;..] - ETA: 0s - loss: 0.0290 - accuracy: 0.9747
58880/60000 [============================&gt;.] - ETA: 0s - loss: 0.0284 - accuracy: 0.9753
60000/60000 [==============================] - 2s 40us/sample - loss: 0.0280 - accuracy: 0.9755 - val_loss: 0.0480 - val_accuracy: 0.9432
Epoch 2/5

  128/60000 [..............................] - ETA: 2s - loss: 0.0100 - accuracy: 0.9844
 2176/60000 [&gt;.............................] - ETA: 1s - loss: 0.0067 - accuracy: 0.9926
 4352/60000 [=&gt;............................] - ETA: 1s - loss: 0.0070 - accuracy: 0.9915
 6400/60000 [==&gt;...........................] - ETA: 1s - loss: 0.0066 - accuracy: 0.9922
 8576/60000 [===&gt;..........................] - ETA: 1s - loss: 0.0064 - accuracy: 0.9931
10752/60000 [====&gt;.........................] - ETA: 1s - loss: 0.0059 - accuracy: 0.9939
12928/60000 [=====&gt;........................] - ETA: 1s - loss: 0.0058 - accuracy: 0.9940
15104/60000 [======&gt;.......................] - ETA: 1s - loss: 0.0055 - accuracy: 0.9946
17408/60000 [=======&gt;......................] - ETA: 1s - loss: 0.0055 - accuracy: 0.9947
19584/60000 [========&gt;.....................] - ETA: 0s - loss: 0.0053 - accuracy: 0.9951
21760/60000 [=========&gt;....................] - ETA: 0s - loss: 0.0054 - accuracy: 0.9949
23936/60000 [==========&gt;...................] - ETA: 0s - loss: 0.0054 - accuracy: 0.9949
26112/60000 [============&gt;.................] - ETA: 0s - loss: 0.0055 - accuracy: 0.9948
28288/60000 [=============&gt;................] - ETA: 0s - loss: 0.0053 - accuracy: 0.9949
30208/60000 [==============&gt;...............] - ETA: 0s - loss: 0.0054 - accuracy: 0.9947
32256/60000 [===============&gt;..............] - ETA: 0s - loss: 0.0054 - accuracy: 0.9948
34176/60000 [================&gt;.............] - ETA: 0s - loss: 0.0054 - accuracy: 0.9947
36224/60000 [=================&gt;............] - ETA: 0s - loss: 0.0054 - accuracy: 0.9948
38272/60000 [==================&gt;...........] - ETA: 0s - loss: 0.0055 - accuracy: 0.9946
40320/60000 [===================&gt;..........] - ETA: 0s - loss: 0.0055 - accuracy: 0.9945
42368/60000 [====================&gt;.........] - ETA: 0s - loss: 0.0055 - accuracy: 0.9945
44416/60000 [=====================&gt;........] - ETA: 0s - loss: 0.0055 - accuracy: 0.9945
46208/60000 [======================&gt;.......] - ETA: 0s - loss: 0.0056 - accuracy: 0.9943
48256/60000 [=======================&gt;......] - ETA: 0s - loss: 0.0055 - accuracy: 0.9944
50304/60000 [========================&gt;.....] - ETA: 0s - loss: 0.0054 - accuracy: 0.9945
52352/60000 [=========================&gt;....] - ETA: 0s - loss: 0.0054 - accuracy: 0.9944
54400/60000 [==========================&gt;...] - ETA: 0s - loss: 0.0054 - accuracy: 0.9945
56448/60000 [===========================&gt;..] - ETA: 0s - loss: 0.0053 - accuracy: 0.9945
58624/60000 [============================&gt;.] - ETA: 0s - loss: 0.0053 - accuracy: 0.9946
60000/60000 [==============================] - 2s 27us/sample - loss: 0.0053 - accuracy: 0.9946 - val_loss: 0.0078 - val_accuracy: 0.9909
Epoch 3/5

  128/60000 [..............................] - ETA: 2s - loss: 0.0016 - accuracy: 1.0000
 2176/60000 [&gt;.............................] - ETA: 1s - loss: 0.0028 - accuracy: 0.9972
 4224/60000 [=&gt;............................] - ETA: 1s - loss: 0.0030 - accuracy: 0.9972
 6272/60000 [==&gt;...........................] - ETA: 1s - loss: 0.0027 - accuracy: 0.9974
 8192/60000 [===&gt;..........................] - ETA: 1s - loss: 0.0022 - accuracy: 0.9980
10240/60000 [====&gt;.........................] - ETA: 1s - loss: 0.0022 - accuracy: 0.9983
12288/60000 [=====&gt;........................] - ETA: 1s - loss: 0.0021 - accuracy: 0.9985
14464/60000 [======&gt;.......................] - ETA: 1s - loss: 0.0021 - accuracy: 0.9984
16512/60000 [=======&gt;......................] - ETA: 1s - loss: 0.0021 - accuracy: 0.9985
18560/60000 [========&gt;.....................] - ETA: 1s - loss: 0.0020 - accuracy: 0.9985
20608/60000 [=========&gt;....................] - ETA: 0s - loss: 0.0021 - accuracy: 0.9985
22656/60000 [==========&gt;...................] - ETA: 0s - loss: 0.0021 - accuracy: 0.9984
24704/60000 [===========&gt;..................] - ETA: 0s - loss: 0.0021 - accuracy: 0.9983
26752/60000 [============&gt;.................] - ETA: 0s - loss: 0.0022 - accuracy: 0.9982
28672/60000 [=============&gt;................] - ETA: 0s - loss: 0.0022 - accuracy: 0.9982
30592/60000 [==============&gt;...............] - ETA: 0s - loss: 0.0022 - accuracy: 0.9981
32512/60000 [===============&gt;..............] - ETA: 0s - loss: 0.0022 - accuracy: 0.9982
34560/60000 [================&gt;.............] - ETA: 0s - loss: 0.0022 - accuracy: 0.9981
36608/60000 [=================&gt;............] - ETA: 0s - loss: 0.0022 - accuracy: 0.9981
38656/60000 [==================&gt;...........] - ETA: 0s - loss: 0.0022 - accuracy: 0.9980
40704/60000 [===================&gt;..........] - ETA: 0s - loss: 0.0023 - accuracy: 0.9979
42752/60000 [====================&gt;.........] - ETA: 0s - loss: 0.0023 - accuracy: 0.9978
44800/60000 [=====================&gt;........] - ETA: 0s - loss: 0.0023 - accuracy: 0.9978
46848/60000 [======================&gt;.......] - ETA: 0s - loss: 0.0023 - accuracy: 0.9977
48896/60000 [=======================&gt;......] - ETA: 0s - loss: 0.0024 - accuracy: 0.9978
50816/60000 [========================&gt;.....] - ETA: 0s - loss: 0.0023 - accuracy: 0.9979
52864/60000 [=========================&gt;....] - ETA: 0s - loss: 0.0023 - accuracy: 0.9979
54912/60000 [==========================&gt;...] - ETA: 0s - loss: 0.0023 - accuracy: 0.9979
56960/60000 [===========================&gt;..] - ETA: 0s - loss: 0.0023 - accuracy: 0.9979
59008/60000 [============================&gt;.] - ETA: 0s - loss: 0.0023 - accuracy: 0.9979
60000/60000 [==============================] - 2s 28us/sample - loss: 0.0023 - accuracy: 0.9979 - val_loss: 0.0063 - val_accuracy: 0.9927
Epoch 4/5

  128/60000 [..............................] - ETA: 1s - loss: 3.9318e-04 - accuracy: 1.0000
 2176/60000 [&gt;.............................] - ETA: 1s - loss: 7.1773e-04 - accuracy: 1.0000
 4352/60000 [=&gt;............................] - ETA: 1s - loss: 0.0011 - accuracy: 0.9993    
 6528/60000 [==&gt;...........................] - ETA: 1s - loss: 0.0011 - accuracy: 0.9994
 8448/60000 [===&gt;..........................] - ETA: 1s - loss: 0.0010 - accuracy: 0.9995
10624/60000 [====&gt;.........................] - ETA: 1s - loss: 0.0010 - accuracy: 0.9995
12672/60000 [=====&gt;........................] - ETA: 1s - loss: 0.0010 - accuracy: 0.9995
14720/60000 [======&gt;.......................] - ETA: 1s - loss: 0.0010 - accuracy: 0.9995
16896/60000 [=======&gt;......................] - ETA: 1s - loss: 0.0010 - accuracy: 0.9994
18944/60000 [========&gt;.....................] - ETA: 1s - loss: 0.0010 - accuracy: 0.9994
20992/60000 [=========&gt;....................] - ETA: 0s - loss: 0.0011 - accuracy: 0.9993
23040/60000 [==========&gt;...................] - ETA: 0s - loss: 0.0011 - accuracy: 0.9994
25088/60000 [===========&gt;..................] - ETA: 0s - loss: 0.0012 - accuracy: 0.9994
27136/60000 [============&gt;.................] - ETA: 0s - loss: 0.0011 - accuracy: 0.9994
29056/60000 [=============&gt;................] - ETA: 0s - loss: 0.0012 - accuracy: 0.9993
31104/60000 [==============&gt;...............] - ETA: 0s - loss: 0.0011 - accuracy: 0.9993
33024/60000 [===============&gt;..............] - ETA: 0s - loss: 0.0011 - accuracy: 0.9993
35072/60000 [================&gt;.............] - ETA: 0s - loss: 0.0011 - accuracy: 0.9993
37248/60000 [=================&gt;............] - ETA: 0s - loss: 0.0011 - accuracy: 0.9993
39296/60000 [==================&gt;...........] - ETA: 0s - loss: 0.0011 - accuracy: 0.9993
41472/60000 [===================&gt;..........] - ETA: 0s - loss: 0.0011 - accuracy: 0.9993
43520/60000 [====================&gt;.........] - ETA: 0s - loss: 0.0011 - accuracy: 0.9993
45440/60000 [=====================&gt;........] - ETA: 0s - loss: 0.0011 - accuracy: 0.9993
47488/60000 [======================&gt;.......] - ETA: 0s - loss: 0.0011 - accuracy: 0.9993
49664/60000 [=======================&gt;......] - ETA: 0s - loss: 0.0011 - accuracy: 0.9993
51840/60000 [========================&gt;.....] - ETA: 0s - loss: 0.0011 - accuracy: 0.9993
53888/60000 [=========================&gt;....] - ETA: 0s - loss: 0.0011 - accuracy: 0.9994
55936/60000 [==========================&gt;...] - ETA: 0s - loss: 0.0011 - accuracy: 0.9994
58112/60000 [============================&gt;.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9994
60000/60000 [==============================] - 2s 27us/sample - loss: 0.0010 - accuracy: 0.9994 - val_loss: 0.0054 - val_accuracy: 0.9933
Epoch 5/5

  128/60000 [..............................] - ETA: 2s - loss: 2.2402e-06 - accuracy: 1.0000
 2176/60000 [&gt;.............................] - ETA: 1s - loss: 6.5210e-04 - accuracy: 0.9995
 4096/60000 [=&gt;............................] - ETA: 1s - loss: 5.7247e-04 - accuracy: 0.9998
 6144/60000 [==&gt;...........................] - ETA: 1s - loss: 5.1875e-04 - accuracy: 0.9998
 8192/60000 [===&gt;..........................] - ETA: 1s - loss: 5.4224e-04 - accuracy: 0.9998
10368/60000 [====&gt;.........................] - ETA: 1s - loss: 5.3163e-04 - accuracy: 0.9997
12416/60000 [=====&gt;........................] - ETA: 1s - loss: 5.3606e-04 - accuracy: 0.9998
14464/60000 [======&gt;.......................] - ETA: 1s - loss: 5.1939e-04 - accuracy: 0.9998
16512/60000 [=======&gt;......................] - ETA: 1s - loss: 5.0434e-04 - accuracy: 0.9998
18688/60000 [========&gt;.....................] - ETA: 1s - loss: 4.8109e-04 - accuracy: 0.9998
20864/60000 [=========&gt;....................] - ETA: 0s - loss: 4.6218e-04 - accuracy: 0.9999
22784/60000 [==========&gt;...................] - ETA: 0s - loss: 4.4222e-04 - accuracy: 0.9999
24832/60000 [===========&gt;..................] - ETA: 0s - loss: 4.2828e-04 - accuracy: 0.9999
26880/60000 [============&gt;.................] - ETA: 0s - loss: 4.1786e-04 - accuracy: 0.9999
28928/60000 [=============&gt;................] - ETA: 0s - loss: 4.1717e-04 - accuracy: 0.9999
31104/60000 [==============&gt;...............] - ETA: 0s - loss: 4.1401e-04 - accuracy: 0.9999
33280/60000 [===============&gt;..............] - ETA: 0s - loss: 4.2330e-04 - accuracy: 0.9999
35328/60000 [================&gt;.............] - ETA: 0s - loss: 4.3883e-04 - accuracy: 0.9999
37504/60000 [=================&gt;............] - ETA: 0s - loss: 4.5324e-04 - accuracy: 0.9999
39552/60000 [==================&gt;...........] - ETA: 0s - loss: 4.4808e-04 - accuracy: 0.9999
41600/60000 [===================&gt;..........] - ETA: 0s - loss: 4.4633e-04 - accuracy: 0.9999
43648/60000 [====================&gt;.........] - ETA: 0s - loss: 4.4258e-04 - accuracy: 0.9999
45696/60000 [=====================&gt;........] - ETA: 0s - loss: 4.4337e-04 - accuracy: 0.9999
47872/60000 [======================&gt;.......] - ETA: 0s - loss: 4.4371e-04 - accuracy: 0.9999
49920/60000 [=======================&gt;......] - ETA: 0s - loss: 4.4295e-04 - accuracy: 0.9999
52096/60000 [=========================&gt;....] - ETA: 0s - loss: 4.4066e-04 - accuracy: 0.9999
54144/60000 [==========================&gt;...] - ETA: 0s - loss: 4.3896e-04 - accuracy: 0.9999
56192/60000 [===========================&gt;..] - ETA: 0s - loss: 4.4721e-04 - accuracy: 0.9999
58240/60000 [============================&gt;.] - ETA: 0s - loss: 4.4224e-04 - accuracy: 0.9999
60000/60000 [==============================] - 2s 27us/sample - loss: 4.4135e-04 - accuracy: 0.9999 - val_loss: 0.0054 - val_accuracy: 0.9934
Test score: 0.005371444219406294
Test accuracy: 0.9934
</pre></div>
</div>
</div>
</div>
<div class="section" id="model-akida-compatibility-check-and-changes">
<h2>3. Model Akida-compatibility check and changes<a class="headerlink" href="#model-akida-compatibility-check-and-changes" title="Permalink to this headline">¶</a></h2>
<div class="section" id="compatibility-check">
<h3>3.1 Compatibility check<a class="headerlink" href="#compatibility-check" title="Permalink to this headline">¶</a></h3>
<p>The first step is to ensure that the model as defined doesn’t include
any layers or operations that aren’t Akida-compatible (please refer to
the <a class="reference external" href="../../user_guide/cnn2snn.html">CNN2SNN toolkit</a> documentation for full
details):</p>
<ul class="simple">
<li><p>Standard Conv2D and Dense layers are supported (Note that
there is currently no support for skip, recursive and parallel layers).</p></li>
<li><p>Each of these trainable core layers except for the last one must be followed
by an Activation layer.</p></li>
<li><p>All blocks can optionally include a BatchNormalization layer.</p></li>
<li><p>Convolutional blocks can optionally include a MaxPooling type layer.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This configuration of layers (Conv/Dense + BatchNormalization +
Activation) constitutes the basic building block of
Akida-compatible models and is widely used in deep learning.</p>
</div>
<p>If the model defined is not fully compatible with the Akida NSoC,
substitutes will be needed for the relevant layers/operations
(guidelines included in the documentation).</p>
</div>
<div class="section" id="model-adaptation">
<h3>3.2 Model adaptation<a class="headerlink" href="#model-adaptation" title="Permalink to this headline">¶</a></h3>
<p>As noted above, the basic building blocks of Akida compatible models
actually comprise a trio of layers: Conv/Dense + BatchNormalization +
Activation (with, optionally, pooling). The CNN2SNN tool provides a set
of functions that simplify using these building blocks, and subsequently
enable easy application of Brainchip’s custom quantization functions.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">akida_models.quantization_blocks</span> <span class="kn">import</span> <span class="n">conv_block</span><span class="p">,</span> <span class="n">dense_block</span>
</pre></div>
</div>
<p>The following code illustrates how to express the MNIST model defined
above using the functions provided by Brainchip. A couple of points to
avoid confusion when you look through it:</p>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">weight_quantization</span></code> in each block isn’t used here, but will be used
later to apply a quantization method to the model weights.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">block_id</span></code> is just used for naming the layers, and will be good
practice in enabling reloading of partially trained models in more advanced
training cases.</p></li>
<li><p>Note that in the final block, we set the nonlinearity
<code class="docutils literal notranslate"><span class="pre">activ_quantization</span></code> to <code class="docutils literal notranslate"><span class="pre">None</span></code>. In that case, the block has no
Activation layer, and the output is simply the output from the
BatchNormalization layer.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Removes all the nodes left over from the previous model and free memory</span>
<span class="n">K</span><span class="o">.</span><span class="n">clear_session</span><span class="p">()</span>

<span class="c1"># Define the model.</span>
<span class="c1"># The commented code shows the sets of layers in the original definition</span>
<span class="c1"># that are being replaced by the provided conv_block and dense_blocks here</span>

<span class="n">img_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="c1"># x = Conv2D(filters=32,</span>
<span class="c1">#            kernel_size=(5, 5),</span>
<span class="c1">#            padding=&#39;same&#39;,</span>
<span class="c1">#            use_bias=False,</span>
<span class="c1">#            data_format=&#39;channels_last&#39;)(img_input)</span>
<span class="c1"># x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=&#39;same&#39;)(x)</span>
<span class="c1"># x = BatchNormalization()(x)</span>
<span class="c1"># x = ReLU(6.)(x)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">conv_block</span><span class="p">(</span><span class="n">img_input</span><span class="p">,</span> <span class="n">filters</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
               <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
               <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span>
               <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
               <span class="n">name</span><span class="o">=</span><span class="s1">&#39;conv_0&#39;</span><span class="p">,</span>
               <span class="n">pooling</span><span class="o">=</span><span class="s1">&#39;max&#39;</span><span class="p">,</span>
               <span class="n">add_batchnorm</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># x = Conv2D(filters=64,</span>
<span class="c1">#            kernel_size=(5, 5),</span>
<span class="c1">#            padding=&#39;same&#39;,</span>
<span class="c1">#            use_bias=False)(x)</span>
<span class="c1"># x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=&#39;same&#39;)(x)</span>
<span class="c1"># x = BatchNormalization()(x)</span>
<span class="c1"># x = ReLU(6.)(x)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">conv_block</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">filters</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
               <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
               <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span>
               <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
               <span class="n">name</span><span class="o">=</span><span class="s1">&#39;conv_1&#39;</span><span class="p">,</span>
               <span class="n">pooling</span><span class="o">=</span><span class="s1">&#39;max&#39;</span><span class="p">,</span>
               <span class="n">add_batchnorm</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">Flatten</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># x = Dense(512,</span>
<span class="c1">#           use_bias=False)(x)</span>
<span class="c1"># x = BatchNormalization()(x)</span>
<span class="c1"># x = ReLU(6.)(x)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">dense_block</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">units</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
                <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">name</span><span class="o">=</span><span class="s1">&#39;dense_2&#39;</span><span class="p">,</span>
                <span class="n">add_batchnorm</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># x = Dense(10,</span>
<span class="c1">#           use_bias=False)(x)</span>
<span class="c1"># x = BatchNormalization()(x)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">dense_block</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">units</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">name</span><span class="o">=</span><span class="s1">&#39;dense_3&#39;</span><span class="p">,</span>
                <span class="n">activ_quantization</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

<span class="n">model_keras</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">img_input</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;mnistnet&#39;</span><span class="p">)</span>

<span class="n">opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">lr_start</span><span class="p">)</span>
<span class="n">model_keras</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;squared_hinge&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
<span class="n">model_keras</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Model: &quot;mnistnet&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         [(None, 28, 28, 1)]       0
_________________________________________________________________
conv_0 (QuantizedConv2D)     (None, 28, 28, 32)        800
_________________________________________________________________
conv_0_maxpool (MaxPooling2D (None, 14, 14, 32)        0
_________________________________________________________________
conv_0_BN (BatchNormalizatio (None, 14, 14, 32)        128
_________________________________________________________________
conv_0_relu (ReLU)           (None, 14, 14, 32)        0
_________________________________________________________________
conv_1 (QuantizedConv2D)     (None, 14, 14, 64)        51200
_________________________________________________________________
conv_1_maxpool (MaxPooling2D (None, 7, 7, 64)          0
_________________________________________________________________
conv_1_BN (BatchNormalizatio (None, 7, 7, 64)          256
_________________________________________________________________
conv_1_relu (ReLU)           (None, 7, 7, 64)          0
_________________________________________________________________
flatten (Flatten)            (None, 3136)              0
_________________________________________________________________
dense_2 (QuantizedDense)     (None, 512)               1605632
_________________________________________________________________
dense_2_BN (BatchNormalizati (None, 512)               2048
_________________________________________________________________
dense_2_relu (ReLU)          (None, 512)               0
_________________________________________________________________
dense_3 (QuantizedDense)     (None, 10)                5120
=================================================================
Total params: 1,665,184
Trainable params: 1,663,968
Non-trainable params: 1,216
_________________________________________________________________
</pre></div>
</div>
</div>
<div class="section" id="id1">
<h3>3.3 Performance check<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<p>Check modifed model performance:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">callbacks</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">LearningRateScheduler</span><span class="p">(</span><span class="k">lambda</span> <span class="n">e</span><span class="p">:</span> <span class="n">lr_start</span> <span class="o">*</span> <span class="n">lr_decay</span> <span class="o">**</span> <span class="n">e</span><span class="p">)</span>
<span class="n">callbacks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lr_scheduler</span><span class="p">)</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model_keras</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
                    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span>
                    <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">),</span>
                    <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span>
<span class="n">score</span> <span class="o">=</span> <span class="n">model_keras</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Test score:&#39;</span><span class="p">,</span> <span class="n">score</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Test accuracy:&#39;</span><span class="p">,</span> <span class="n">score</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Train on 60000 samples, validate on 10000 samples
Epoch 1/5

  128/60000 [..............................] - ETA: 3:48 - loss: 1.6553 - accuracy: 0.1094
 2176/60000 [&gt;.............................] - ETA: 14s - loss: 0.3325 - accuracy: 0.8185 
 4352/60000 [=&gt;............................] - ETA: 7s - loss: 0.1899 - accuracy: 0.8826 
 6400/60000 [==&gt;...........................] - ETA: 5s - loss: 0.1407 - accuracy: 0.9083
 8576/60000 [===&gt;..........................] - ETA: 4s - loss: 0.1111 - accuracy: 0.9256
10624/60000 [====&gt;.........................] - ETA: 3s - loss: 0.0949 - accuracy: 0.9346
12672/60000 [=====&gt;........................] - ETA: 2s - loss: 0.0827 - accuracy: 0.9418
14848/60000 [======&gt;.......................] - ETA: 2s - loss: 0.0743 - accuracy: 0.9468
16768/60000 [=======&gt;......................] - ETA: 2s - loss: 0.0683 - accuracy: 0.9505
18816/60000 [========&gt;.....................] - ETA: 2s - loss: 0.0631 - accuracy: 0.9536
20992/60000 [=========&gt;....................] - ETA: 1s - loss: 0.0588 - accuracy: 0.9560
23040/60000 [==========&gt;...................] - ETA: 1s - loss: 0.0550 - accuracy: 0.9582
25088/60000 [===========&gt;..................] - ETA: 1s - loss: 0.0524 - accuracy: 0.9597
27264/60000 [============&gt;.................] - ETA: 1s - loss: 0.0492 - accuracy: 0.9617
29440/60000 [=============&gt;................] - ETA: 1s - loss: 0.0468 - accuracy: 0.9634
31616/60000 [==============&gt;...............] - ETA: 1s - loss: 0.0450 - accuracy: 0.9644
33664/60000 [===============&gt;..............] - ETA: 1s - loss: 0.0432 - accuracy: 0.9655
35712/60000 [================&gt;.............] - ETA: 0s - loss: 0.0416 - accuracy: 0.9666
37760/60000 [=================&gt;............] - ETA: 0s - loss: 0.0400 - accuracy: 0.9678
39936/60000 [==================&gt;...........] - ETA: 0s - loss: 0.0386 - accuracy: 0.9685
41984/60000 [===================&gt;..........] - ETA: 0s - loss: 0.0374 - accuracy: 0.9695
44032/60000 [=====================&gt;........] - ETA: 0s - loss: 0.0362 - accuracy: 0.9703
46208/60000 [======================&gt;.......] - ETA: 0s - loss: 0.0353 - accuracy: 0.9710
48256/60000 [=======================&gt;......] - ETA: 0s - loss: 0.0343 - accuracy: 0.9716
50304/60000 [========================&gt;.....] - ETA: 0s - loss: 0.0333 - accuracy: 0.9724
52352/60000 [=========================&gt;....] - ETA: 0s - loss: 0.0324 - accuracy: 0.9730
54400/60000 [==========================&gt;...] - ETA: 0s - loss: 0.0316 - accuracy: 0.9737
56576/60000 [===========================&gt;..] - ETA: 0s - loss: 0.0309 - accuracy: 0.9742
58624/60000 [============================&gt;.] - ETA: 0s - loss: 0.0302 - accuracy: 0.9745
60000/60000 [==============================] - 2s 37us/sample - loss: 0.0298 - accuracy: 0.9748 - val_loss: 0.1663 - val_accuracy: 0.7651
Epoch 2/5

  128/60000 [..............................] - ETA: 1s - loss: 0.0033 - accuracy: 1.0000
 2048/60000 [&gt;.............................] - ETA: 1s - loss: 0.0063 - accuracy: 0.9927
 3840/60000 [&gt;.............................] - ETA: 1s - loss: 0.0071 - accuracy: 0.9917
 5888/60000 [=&gt;............................] - ETA: 1s - loss: 0.0068 - accuracy: 0.9913
 7936/60000 [==&gt;...........................] - ETA: 1s - loss: 0.0060 - accuracy: 0.9928
 9984/60000 [===&gt;..........................] - ETA: 1s - loss: 0.0059 - accuracy: 0.9932
12032/60000 [=====&gt;........................] - ETA: 1s - loss: 0.0056 - accuracy: 0.9934
14080/60000 [======&gt;.......................] - ETA: 1s - loss: 0.0058 - accuracy: 0.9935
16128/60000 [=======&gt;......................] - ETA: 1s - loss: 0.0056 - accuracy: 0.9937
18048/60000 [========&gt;.....................] - ETA: 1s - loss: 0.0057 - accuracy: 0.9935
20096/60000 [=========&gt;....................] - ETA: 1s - loss: 0.0057 - accuracy: 0.9937
22144/60000 [==========&gt;...................] - ETA: 0s - loss: 0.0056 - accuracy: 0.9939
24320/60000 [===========&gt;..................] - ETA: 0s - loss: 0.0055 - accuracy: 0.9942
26368/60000 [============&gt;.................] - ETA: 0s - loss: 0.0055 - accuracy: 0.9942
28288/60000 [=============&gt;................] - ETA: 0s - loss: 0.0054 - accuracy: 0.9942
30336/60000 [==============&gt;...............] - ETA: 0s - loss: 0.0053 - accuracy: 0.9943
32384/60000 [===============&gt;..............] - ETA: 0s - loss: 0.0051 - accuracy: 0.9946
34304/60000 [================&gt;.............] - ETA: 0s - loss: 0.0051 - accuracy: 0.9946
36480/60000 [=================&gt;............] - ETA: 0s - loss: 0.0050 - accuracy: 0.9947
38656/60000 [==================&gt;...........] - ETA: 0s - loss: 0.0050 - accuracy: 0.9947
40704/60000 [===================&gt;..........] - ETA: 0s - loss: 0.0050 - accuracy: 0.9948
42624/60000 [====================&gt;.........] - ETA: 0s - loss: 0.0050 - accuracy: 0.9947
44672/60000 [=====================&gt;........] - ETA: 0s - loss: 0.0050 - accuracy: 0.9948
46848/60000 [======================&gt;.......] - ETA: 0s - loss: 0.0050 - accuracy: 0.9947
48896/60000 [=======================&gt;......] - ETA: 0s - loss: 0.0051 - accuracy: 0.9945
51072/60000 [========================&gt;.....] - ETA: 0s - loss: 0.0051 - accuracy: 0.9946
52992/60000 [=========================&gt;....] - ETA: 0s - loss: 0.0051 - accuracy: 0.9946
54912/60000 [==========================&gt;...] - ETA: 0s - loss: 0.0051 - accuracy: 0.9946
56960/60000 [===========================&gt;..] - ETA: 0s - loss: 0.0050 - accuracy: 0.9946
59136/60000 [============================&gt;.] - ETA: 0s - loss: 0.0050 - accuracy: 0.9946
60000/60000 [==============================] - 2s 28us/sample - loss: 0.0050 - accuracy: 0.9947 - val_loss: 0.0074 - val_accuracy: 0.9923
Epoch 3/5

  128/60000 [..............................] - ETA: 2s - loss: 0.0043 - accuracy: 1.0000
 2048/60000 [&gt;.............................] - ETA: 1s - loss: 0.0028 - accuracy: 0.9980
 4096/60000 [=&gt;............................] - ETA: 1s - loss: 0.0025 - accuracy: 0.9976
 6272/60000 [==&gt;...........................] - ETA: 1s - loss: 0.0024 - accuracy: 0.9973
 8320/60000 [===&gt;..........................] - ETA: 1s - loss: 0.0022 - accuracy: 0.9978
10368/60000 [====&gt;.........................] - ETA: 1s - loss: 0.0021 - accuracy: 0.9981
12416/60000 [=====&gt;........................] - ETA: 1s - loss: 0.0020 - accuracy: 0.9982
14464/60000 [======&gt;.......................] - ETA: 1s - loss: 0.0020 - accuracy: 0.9981
16512/60000 [=======&gt;......................] - ETA: 1s - loss: 0.0020 - accuracy: 0.9982
18560/60000 [========&gt;.....................] - ETA: 1s - loss: 0.0019 - accuracy: 0.9984
20608/60000 [=========&gt;....................] - ETA: 0s - loss: 0.0018 - accuracy: 0.9984
22784/60000 [==========&gt;...................] - ETA: 0s - loss: 0.0018 - accuracy: 0.9984
24960/60000 [===========&gt;..................] - ETA: 0s - loss: 0.0019 - accuracy: 0.9984
27008/60000 [============&gt;.................] - ETA: 0s - loss: 0.0019 - accuracy: 0.9984
29184/60000 [=============&gt;................] - ETA: 0s - loss: 0.0020 - accuracy: 0.9984
31360/60000 [==============&gt;...............] - ETA: 0s - loss: 0.0020 - accuracy: 0.9983
33536/60000 [===============&gt;..............] - ETA: 0s - loss: 0.0020 - accuracy: 0.9983
35584/60000 [================&gt;.............] - ETA: 0s - loss: 0.0020 - accuracy: 0.9983
37632/60000 [=================&gt;............] - ETA: 0s - loss: 0.0020 - accuracy: 0.9983
39808/60000 [==================&gt;...........] - ETA: 0s - loss: 0.0020 - accuracy: 0.9982
41728/60000 [===================&gt;..........] - ETA: 0s - loss: 0.0020 - accuracy: 0.9982
43776/60000 [====================&gt;.........] - ETA: 0s - loss: 0.0020 - accuracy: 0.9982
45824/60000 [=====================&gt;........] - ETA: 0s - loss: 0.0020 - accuracy: 0.9983
47872/60000 [======================&gt;.......] - ETA: 0s - loss: 0.0020 - accuracy: 0.9983
49920/60000 [=======================&gt;......] - ETA: 0s - loss: 0.0020 - accuracy: 0.9982
52096/60000 [=========================&gt;....] - ETA: 0s - loss: 0.0020 - accuracy: 0.9983
54144/60000 [==========================&gt;...] - ETA: 0s - loss: 0.0020 - accuracy: 0.9983
56192/60000 [===========================&gt;..] - ETA: 0s - loss: 0.0020 - accuracy: 0.9983
58240/60000 [============================&gt;.] - ETA: 0s - loss: 0.0020 - accuracy: 0.9983
60000/60000 [==============================] - 2s 27us/sample - loss: 0.0021 - accuracy: 0.9983 - val_loss: 0.0074 - val_accuracy: 0.9911
Epoch 4/5

  128/60000 [..............................] - ETA: 1s - loss: 0.0027 - accuracy: 1.0000
 2304/60000 [&gt;.............................] - ETA: 1s - loss: 0.0013 - accuracy: 1.0000
 4352/60000 [=&gt;............................] - ETA: 1s - loss: 0.0012 - accuracy: 0.9993
 6272/60000 [==&gt;...........................] - ETA: 1s - loss: 0.0011 - accuracy: 0.9995
 8320/60000 [===&gt;..........................] - ETA: 1s - loss: 9.7621e-04 - accuracy: 0.9996
10368/60000 [====&gt;.........................] - ETA: 1s - loss: 9.3703e-04 - accuracy: 0.9996
12416/60000 [=====&gt;........................] - ETA: 1s - loss: 9.1714e-04 - accuracy: 0.9996
14464/60000 [======&gt;.......................] - ETA: 1s - loss: 8.9260e-04 - accuracy: 0.9996
16512/60000 [=======&gt;......................] - ETA: 1s - loss: 8.7825e-04 - accuracy: 0.9996
18688/60000 [========&gt;.....................] - ETA: 1s - loss: 8.3775e-04 - accuracy: 0.9997
20736/60000 [=========&gt;....................] - ETA: 0s - loss: 8.2952e-04 - accuracy: 0.9996
22784/60000 [==========&gt;...................] - ETA: 0s - loss: 8.0086e-04 - accuracy: 0.9996
24960/60000 [===========&gt;..................] - ETA: 0s - loss: 8.0600e-04 - accuracy: 0.9996
26880/60000 [============&gt;.................] - ETA: 0s - loss: 8.3150e-04 - accuracy: 0.9996
28928/60000 [=============&gt;................] - ETA: 0s - loss: 8.5211e-04 - accuracy: 0.9996
30976/60000 [==============&gt;...............] - ETA: 0s - loss: 8.4533e-04 - accuracy: 0.9995
32896/60000 [===============&gt;..............] - ETA: 0s - loss: 8.1628e-04 - accuracy: 0.9996
35072/60000 [================&gt;.............] - ETA: 0s - loss: 8.0454e-04 - accuracy: 0.9996
37120/60000 [=================&gt;............] - ETA: 0s - loss: 7.9165e-04 - accuracy: 0.9996
39296/60000 [==================&gt;...........] - ETA: 0s - loss: 7.9914e-04 - accuracy: 0.9996
41472/60000 [===================&gt;..........] - ETA: 0s - loss: 7.9670e-04 - accuracy: 0.9996
43520/60000 [====================&gt;.........] - ETA: 0s - loss: 8.0881e-04 - accuracy: 0.9995
45696/60000 [=====================&gt;........] - ETA: 0s - loss: 8.2307e-04 - accuracy: 0.9995
47872/60000 [======================&gt;.......] - ETA: 0s - loss: 8.2228e-04 - accuracy: 0.9995
49920/60000 [=======================&gt;......] - ETA: 0s - loss: 8.0428e-04 - accuracy: 0.9995
52096/60000 [=========================&gt;....] - ETA: 0s - loss: 8.1813e-04 - accuracy: 0.9995
54272/60000 [==========================&gt;...] - ETA: 0s - loss: 8.2783e-04 - accuracy: 0.9995
56320/60000 [===========================&gt;..] - ETA: 0s - loss: 8.1636e-04 - accuracy: 0.9995
58240/60000 [============================&gt;.] - ETA: 0s - loss: 8.2847e-04 - accuracy: 0.9995
60000/60000 [==============================] - 2s 27us/sample - loss: 8.2257e-04 - accuracy: 0.9995 - val_loss: 0.0057 - val_accuracy: 0.9933
Epoch 5/5

  128/60000 [..............................] - ETA: 1s - loss: 2.2527e-04 - accuracy: 1.0000
 2304/60000 [&gt;.............................] - ETA: 1s - loss: 5.4721e-04 - accuracy: 1.0000
 4480/60000 [=&gt;............................] - ETA: 1s - loss: 4.3859e-04 - accuracy: 1.0000
 6528/60000 [==&gt;...........................] - ETA: 1s - loss: 4.1073e-04 - accuracy: 1.0000
 8576/60000 [===&gt;..........................] - ETA: 1s - loss: 3.9290e-04 - accuracy: 1.0000
10624/60000 [====&gt;.........................] - ETA: 1s - loss: 3.9796e-04 - accuracy: 1.0000
12672/60000 [=====&gt;........................] - ETA: 1s - loss: 4.4045e-04 - accuracy: 0.9999
14720/60000 [======&gt;.......................] - ETA: 1s - loss: 4.2530e-04 - accuracy: 0.9999
16768/60000 [=======&gt;......................] - ETA: 1s - loss: 4.0967e-04 - accuracy: 0.9999
18816/60000 [========&gt;.....................] - ETA: 1s - loss: 4.1147e-04 - accuracy: 0.9999
20992/60000 [=========&gt;....................] - ETA: 0s - loss: 4.1721e-04 - accuracy: 0.9999
23040/60000 [==========&gt;...................] - ETA: 0s - loss: 4.0634e-04 - accuracy: 0.9999
25088/60000 [===========&gt;..................] - ETA: 0s - loss: 3.9424e-04 - accuracy: 0.9999
27136/60000 [============&gt;.................] - ETA: 0s - loss: 4.0398e-04 - accuracy: 0.9999
29056/60000 [=============&gt;................] - ETA: 0s - loss: 4.0395e-04 - accuracy: 0.9999
31104/60000 [==============&gt;...............] - ETA: 0s - loss: 4.1123e-04 - accuracy: 0.9999
33280/60000 [===============&gt;..............] - ETA: 0s - loss: 4.0932e-04 - accuracy: 0.9999
35200/60000 [================&gt;.............] - ETA: 0s - loss: 4.0344e-04 - accuracy: 0.9999
37120/60000 [=================&gt;............] - ETA: 0s - loss: 4.0299e-04 - accuracy: 0.9999
39040/60000 [==================&gt;...........] - ETA: 0s - loss: 4.0331e-04 - accuracy: 0.9999
40832/60000 [===================&gt;..........] - ETA: 0s - loss: 4.0020e-04 - accuracy: 0.9999
42752/60000 [====================&gt;.........] - ETA: 0s - loss: 3.9302e-04 - accuracy: 0.9999
44800/60000 [=====================&gt;........] - ETA: 0s - loss: 3.9665e-04 - accuracy: 0.9999
46848/60000 [======================&gt;.......] - ETA: 0s - loss: 3.9528e-04 - accuracy: 0.9999
48768/60000 [=======================&gt;......] - ETA: 0s - loss: 3.9396e-04 - accuracy: 0.9999
50688/60000 [========================&gt;.....] - ETA: 0s - loss: 4.0515e-04 - accuracy: 0.9999
52736/60000 [=========================&gt;....] - ETA: 0s - loss: 4.0188e-04 - accuracy: 0.9999
54784/60000 [==========================&gt;...] - ETA: 0s - loss: 4.0619e-04 - accuracy: 0.9999
56832/60000 [===========================&gt;..] - ETA: 0s - loss: 4.0395e-04 - accuracy: 0.9999
58752/60000 [============================&gt;.] - ETA: 0s - loss: 3.9979e-04 - accuracy: 0.9999
60000/60000 [==============================] - 2s 28us/sample - loss: 3.9917e-04 - accuracy: 0.9999 - val_loss: 0.0054 - val_accuracy: 0.9938
Test score: 0.0053676083627567
Test accuracy: 0.9938
</pre></div>
</div>
<p>Saving the model weights as <code class="docutils literal notranslate"><span class="pre">mnistnet_act_fp_wgt_fp.hdf5</span></code> to reload
them as init weights for the quantization step:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This is not mandatory but helps with training speed.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">temp_dir</span> <span class="o">=</span> <span class="n">TemporaryDirectory</span><span class="p">()</span>
<span class="n">model_keras</span><span class="o">.</span><span class="n">save_weights</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">temp_dir</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="s1">&#39;mnistnet_act_fp_wgt_fp.hdf5&#39;</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="model-quantization-and-training">
<h2>4. Model quantization and training<a class="headerlink" href="#model-quantization-and-training" title="Permalink to this headline">¶</a></h2>
<div class="section" id="quantize-the-model">
<h3>4.1 Quantize the model<a class="headerlink" href="#quantize-the-model" title="Permalink to this headline">¶</a></h3>
<p>We can now turn to training a discretized version of the model, where
the weights and activations are quantized so as to be suitable for
implementation in the Akida NSoC.</p>
<p>For this, we just have to change very slightly the definition of the
model used above, changing just the values of <code class="docutils literal notranslate"><span class="pre">weight_quantization</span></code>
and <code class="docutils literal notranslate"><span class="pre">activ_quantization</span></code> used for the blocks (but still with no output
nonlinearity for the final block). Additionally, we’ll initialise the
model using the set of pre-trained weights that we just saved (not so
important here, but for more complex datasets can make a huge difference
both to the accuracy level ultimately achieved and to the speed of
convergence).</p>
<p>Note that, for more challenging datasets, it may also be useful to make
stepwise changes towards a fully quantized model - e.g. by first
training with only the activations quantized, re-saving, and then adding
the quantized weights. Additionally, the toolkit documentation describes
how one can go further, optimizing the degree of sparsity in the model
to reduce computational cost while maintaining accuracy. In this first
example however, we’ll stick to a one step conversion (mainly because
the MNIST dataset simply isn’t complex enough to see the benefit of the
advanced techniques).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Removes all the nodes left over from the previous model and free memory</span>
<span class="n">K</span><span class="o">.</span><span class="n">clear_session</span><span class="p">()</span>

<span class="n">img_input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">conv_block</span><span class="p">(</span><span class="n">img_input</span><span class="p">,</span> <span class="n">filters</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
               <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
               <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span>
               <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
               <span class="n">name</span><span class="o">=</span><span class="s1">&#39;conv_0&#39;</span><span class="p">,</span>
               <span class="n">weight_quantization</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
               <span class="n">activ_quantization</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
               <span class="n">pooling</span><span class="o">=</span><span class="s1">&#39;max&#39;</span><span class="p">,</span>
               <span class="n">add_batchnorm</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">conv_block</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">filters</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
               <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
               <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span>
               <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
               <span class="n">name</span><span class="o">=</span><span class="s1">&#39;conv_1&#39;</span><span class="p">,</span>
               <span class="n">weight_quantization</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
               <span class="n">activ_quantization</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
               <span class="n">pooling</span><span class="o">=</span><span class="s1">&#39;max&#39;</span><span class="p">,</span>
               <span class="n">add_batchnorm</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Flatten</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">dense_block</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">units</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
                <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">name</span><span class="o">=</span><span class="s1">&#39;dense_2&#39;</span><span class="p">,</span>
                <span class="n">weight_quantization</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                <span class="n">activ_quantization</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">add_batchnorm</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">dense_block</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">units</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">name</span><span class="o">=</span><span class="s1">&#39;dense_3&#39;</span><span class="p">,</span>
                <span class="n">weight_quantization</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                <span class="n">activ_quantization</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

<span class="n">model_keras</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">img_input</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;mnistnet_quantized&#39;</span><span class="p">)</span>
<span class="n">lr_start</span> <span class="o">=</span> <span class="mf">1e-3</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">lr_start</span><span class="p">)</span>
<span class="n">model_keras</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;squared_hinge&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
<span class="n">model_keras</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>

<span class="c1"># Reload previously computed weights as init weights for the quantization step</span>
<span class="n">load_status</span> <span class="o">=</span> <span class="n">model_keras</span><span class="o">.</span><span class="n">load_weights</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">temp_dir</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="s1">&#39;mnistnet_act_fp_wgt_fp.hdf5&#39;</span><span class="p">))</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Model: &quot;mnistnet_quantized&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         [(None, 28, 28, 1)]       0
_________________________________________________________________
conv_0 (QuantizedConv2D)     (None, 28, 28, 32)        800
_________________________________________________________________
conv_0_maxpool (MaxPooling2D (None, 14, 14, 32)        0
_________________________________________________________________
conv_0_BN (BatchNormalizatio (None, 14, 14, 32)        128
_________________________________________________________________
conv_0_relu (ActivationDiscr (None, 14, 14, 32)        0
_________________________________________________________________
conv_1 (QuantizedConv2D)     (None, 14, 14, 64)        51200
_________________________________________________________________
conv_1_maxpool (MaxPooling2D (None, 7, 7, 64)          0
_________________________________________________________________
conv_1_BN (BatchNormalizatio (None, 7, 7, 64)          256
_________________________________________________________________
conv_1_relu (ActivationDiscr (None, 7, 7, 64)          0
_________________________________________________________________
flatten (Flatten)            (None, 3136)              0
_________________________________________________________________
dense_2 (QuantizedDense)     (None, 512)               1605632
_________________________________________________________________
dense_2_BN (BatchNormalizati (None, 512)               2048
_________________________________________________________________
dense_2_relu (ActivationDisc (None, 512)               0
_________________________________________________________________
dense_3 (QuantizedDense)     (None, 10)                5120
=================================================================
Total params: 1,665,184
Trainable params: 1,663,968
Non-trainable params: 1,216
_________________________________________________________________
</pre></div>
</div>
</div>
<div class="section" id="id2">
<h3>4.2 Performance check<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<p>Re-train and save the quantized model:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">callbacks</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">LearningRateScheduler</span><span class="p">(</span><span class="k">lambda</span> <span class="n">e</span><span class="p">:</span> <span class="n">lr_start</span> <span class="o">*</span> <span class="n">lr_decay</span> <span class="o">**</span> <span class="n">e</span><span class="p">)</span>
<span class="n">callbacks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lr_scheduler</span><span class="p">)</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model_keras</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
                    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span>
                    <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">),</span>
                    <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span>
<span class="n">score</span> <span class="o">=</span> <span class="n">model_keras</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Test score:&#39;</span><span class="p">,</span> <span class="n">score</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Test accuracy:&#39;</span><span class="p">,</span> <span class="n">score</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Train on 60000 samples, validate on 10000 samples
Epoch 1/5

  128/60000 [..............................] - ETA: 8:26 - loss: 0.1404 - accuracy: 0.7734
 1280/60000 [..............................] - ETA: 52s - loss: 0.0594 - accuracy: 0.9273 
 2432/60000 [&gt;.............................] - ETA: 28s - loss: 0.0443 - accuracy: 0.9486
 3584/60000 [&gt;.............................] - ETA: 19s - loss: 0.0373 - accuracy: 0.9579
 4736/60000 [=&gt;............................] - ETA: 15s - loss: 0.0338 - accuracy: 0.9609
 6016/60000 [==&gt;...........................] - ETA: 12s - loss: 0.0305 - accuracy: 0.9653
 7296/60000 [==&gt;...........................] - ETA: 10s - loss: 0.0282 - accuracy: 0.9679
 8448/60000 [===&gt;..........................] - ETA: 8s - loss: 0.0267 - accuracy: 0.9695 
 9600/60000 [===&gt;..........................] - ETA: 7s - loss: 0.0256 - accuracy: 0.9708
10880/60000 [====&gt;.........................] - ETA: 7s - loss: 0.0249 - accuracy: 0.9722
12160/60000 [=====&gt;........................] - ETA: 6s - loss: 0.0242 - accuracy: 0.9729
13440/60000 [=====&gt;........................] - ETA: 5s - loss: 0.0238 - accuracy: 0.9729
14592/60000 [======&gt;.......................] - ETA: 5s - loss: 0.0233 - accuracy: 0.9736
15744/60000 [======&gt;.......................] - ETA: 4s - loss: 0.0229 - accuracy: 0.9742
17024/60000 [=======&gt;......................] - ETA: 4s - loss: 0.0221 - accuracy: 0.9753
18304/60000 [========&gt;.....................] - ETA: 4s - loss: 0.0216 - accuracy: 0.9760
19584/60000 [========&gt;.....................] - ETA: 3s - loss: 0.0211 - accuracy: 0.9767
20864/60000 [=========&gt;....................] - ETA: 3s - loss: 0.0208 - accuracy: 0.9769
22144/60000 [==========&gt;...................] - ETA: 3s - loss: 0.0203 - accuracy: 0.9771
23424/60000 [==========&gt;...................] - ETA: 3s - loss: 0.0203 - accuracy: 0.9770
24704/60000 [===========&gt;..................] - ETA: 3s - loss: 0.0199 - accuracy: 0.9775
25984/60000 [===========&gt;..................] - ETA: 2s - loss: 0.0196 - accuracy: 0.9779
27392/60000 [============&gt;.................] - ETA: 2s - loss: 0.0193 - accuracy: 0.9782
28672/60000 [=============&gt;................] - ETA: 2s - loss: 0.0191 - accuracy: 0.9784
29952/60000 [=============&gt;................] - ETA: 2s - loss: 0.0188 - accuracy: 0.9789
31232/60000 [==============&gt;...............] - ETA: 2s - loss: 0.0185 - accuracy: 0.9791
32512/60000 [===============&gt;..............] - ETA: 2s - loss: 0.0182 - accuracy: 0.9795
33792/60000 [===============&gt;..............] - ETA: 1s - loss: 0.0180 - accuracy: 0.9799
35072/60000 [================&gt;.............] - ETA: 1s - loss: 0.0179 - accuracy: 0.9800
36352/60000 [=================&gt;............] - ETA: 1s - loss: 0.0176 - accuracy: 0.9803
37632/60000 [=================&gt;............] - ETA: 1s - loss: 0.0174 - accuracy: 0.9805
39040/60000 [==================&gt;...........] - ETA: 1s - loss: 0.0173 - accuracy: 0.9807
40320/60000 [===================&gt;..........] - ETA: 1s - loss: 0.0172 - accuracy: 0.9809
41600/60000 [===================&gt;..........] - ETA: 1s - loss: 0.0171 - accuracy: 0.9809
42880/60000 [====================&gt;.........] - ETA: 1s - loss: 0.0171 - accuracy: 0.9808
44032/60000 [=====================&gt;........] - ETA: 1s - loss: 0.0172 - accuracy: 0.9807
45312/60000 [=====================&gt;........] - ETA: 0s - loss: 0.0170 - accuracy: 0.9809
46592/60000 [======================&gt;.......] - ETA: 0s - loss: 0.0170 - accuracy: 0.9808
47872/60000 [======================&gt;.......] - ETA: 0s - loss: 0.0169 - accuracy: 0.9808
49152/60000 [=======================&gt;......] - ETA: 0s - loss: 0.0167 - accuracy: 0.9810
50432/60000 [========================&gt;.....] - ETA: 0s - loss: 0.0167 - accuracy: 0.9810
51712/60000 [========================&gt;.....] - ETA: 0s - loss: 0.0165 - accuracy: 0.9811
52992/60000 [=========================&gt;....] - ETA: 0s - loss: 0.0165 - accuracy: 0.9812
54272/60000 [==========================&gt;...] - ETA: 0s - loss: 0.0163 - accuracy: 0.9814
55552/60000 [==========================&gt;...] - ETA: 0s - loss: 0.0162 - accuracy: 0.9816
56832/60000 [===========================&gt;..] - ETA: 0s - loss: 0.0160 - accuracy: 0.9819
58240/60000 [============================&gt;.] - ETA: 0s - loss: 0.0159 - accuracy: 0.9820
59520/60000 [============================&gt;.] - ETA: 0s - loss: 0.0158 - accuracy: 0.9821
60000/60000 [==============================] - 4s 65us/sample - loss: 0.0158 - accuracy: 0.9821 - val_loss: 0.0155 - val_accuracy: 0.9822
Epoch 2/5

  128/60000 [..............................] - ETA: 2s - loss: 0.0146 - accuracy: 0.9844
 1408/60000 [..............................] - ETA: 2s - loss: 0.0084 - accuracy: 0.9901
 2688/60000 [&gt;.............................] - ETA: 2s - loss: 0.0095 - accuracy: 0.9888
 4096/60000 [=&gt;............................] - ETA: 2s - loss: 0.0087 - accuracy: 0.9905
 5376/60000 [=&gt;............................] - ETA: 2s - loss: 0.0088 - accuracy: 0.9901
 6784/60000 [==&gt;...........................] - ETA: 2s - loss: 0.0082 - accuracy: 0.9906
 8064/60000 [===&gt;..........................] - ETA: 2s - loss: 0.0081 - accuracy: 0.9905
 9344/60000 [===&gt;..........................] - ETA: 2s - loss: 0.0086 - accuracy: 0.9900
10624/60000 [====&gt;.........................] - ETA: 1s - loss: 0.0080 - accuracy: 0.9910
11904/60000 [====&gt;.........................] - ETA: 1s - loss: 0.0077 - accuracy: 0.9915
13056/60000 [=====&gt;........................] - ETA: 1s - loss: 0.0076 - accuracy: 0.9917
14336/60000 [======&gt;.......................] - ETA: 1s - loss: 0.0075 - accuracy: 0.9918
15488/60000 [======&gt;.......................] - ETA: 1s - loss: 0.0074 - accuracy: 0.9920
16768/60000 [=======&gt;......................] - ETA: 1s - loss: 0.0072 - accuracy: 0.9923
18048/60000 [========&gt;.....................] - ETA: 1s - loss: 0.0072 - accuracy: 0.9924
19328/60000 [========&gt;.....................] - ETA: 1s - loss: 0.0071 - accuracy: 0.9925
20736/60000 [=========&gt;....................] - ETA: 1s - loss: 0.0073 - accuracy: 0.9924
22016/60000 [==========&gt;...................] - ETA: 1s - loss: 0.0072 - accuracy: 0.9927
23296/60000 [==========&gt;...................] - ETA: 1s - loss: 0.0071 - accuracy: 0.9927
24576/60000 [===========&gt;..................] - ETA: 1s - loss: 0.0070 - accuracy: 0.9928
25728/60000 [===========&gt;..................] - ETA: 1s - loss: 0.0072 - accuracy: 0.9926
27008/60000 [============&gt;.................] - ETA: 1s - loss: 0.0072 - accuracy: 0.9926
28288/60000 [=============&gt;................] - ETA: 1s - loss: 0.0071 - accuracy: 0.9925
29568/60000 [=============&gt;................] - ETA: 1s - loss: 0.0072 - accuracy: 0.9923
30592/60000 [==============&gt;...............] - ETA: 1s - loss: 0.0073 - accuracy: 0.9922
31872/60000 [==============&gt;...............] - ETA: 1s - loss: 0.0072 - accuracy: 0.9923
33152/60000 [===============&gt;..............] - ETA: 1s - loss: 0.0072 - accuracy: 0.9922
34432/60000 [================&gt;.............] - ETA: 1s - loss: 0.0071 - accuracy: 0.9924
35840/60000 [================&gt;.............] - ETA: 0s - loss: 0.0071 - accuracy: 0.9925
37120/60000 [=================&gt;............] - ETA: 0s - loss: 0.0071 - accuracy: 0.9924
38400/60000 [==================&gt;...........] - ETA: 0s - loss: 0.0071 - accuracy: 0.9923
39680/60000 [==================&gt;...........] - ETA: 0s - loss: 0.0072 - accuracy: 0.9923
41088/60000 [===================&gt;..........] - ETA: 0s - loss: 0.0071 - accuracy: 0.9924
42240/60000 [====================&gt;.........] - ETA: 0s - loss: 0.0071 - accuracy: 0.9924
43392/60000 [====================&gt;.........] - ETA: 0s - loss: 0.0072 - accuracy: 0.9923
44672/60000 [=====================&gt;........] - ETA: 0s - loss: 0.0073 - accuracy: 0.9922
45952/60000 [=====================&gt;........] - ETA: 0s - loss: 0.0073 - accuracy: 0.9922
47360/60000 [======================&gt;.......] - ETA: 0s - loss: 0.0074 - accuracy: 0.9920
48640/60000 [=======================&gt;......] - ETA: 0s - loss: 0.0074 - accuracy: 0.9920
49920/60000 [=======================&gt;......] - ETA: 0s - loss: 0.0074 - accuracy: 0.9919
51200/60000 [========================&gt;.....] - ETA: 0s - loss: 0.0074 - accuracy: 0.9920
52480/60000 [=========================&gt;....] - ETA: 0s - loss: 0.0074 - accuracy: 0.9920
53760/60000 [=========================&gt;....] - ETA: 0s - loss: 0.0074 - accuracy: 0.9920
55040/60000 [==========================&gt;...] - ETA: 0s - loss: 0.0074 - accuracy: 0.9920
56320/60000 [===========================&gt;..] - ETA: 0s - loss: 0.0073 - accuracy: 0.9920
57600/60000 [===========================&gt;..] - ETA: 0s - loss: 0.0073 - accuracy: 0.9919
58880/60000 [============================&gt;.] - ETA: 0s - loss: 0.0074 - accuracy: 0.9919
60000/60000 [==============================] - 3s 44us/sample - loss: 0.0074 - accuracy: 0.9918 - val_loss: 0.0095 - val_accuracy: 0.9888
Epoch 3/5

  128/60000 [..............................] - ETA: 2s - loss: 0.0040 - accuracy: 0.9922
 1408/60000 [..............................] - ETA: 2s - loss: 0.0046 - accuracy: 0.9957
 2560/60000 [&gt;.............................] - ETA: 2s - loss: 0.0060 - accuracy: 0.9926
 3968/60000 [&gt;.............................] - ETA: 2s - loss: 0.0055 - accuracy: 0.9942
 5120/60000 [=&gt;............................] - ETA: 2s - loss: 0.0060 - accuracy: 0.9930
 6272/60000 [==&gt;...........................] - ETA: 2s - loss: 0.0055 - accuracy: 0.9938
 7552/60000 [==&gt;...........................] - ETA: 2s - loss: 0.0053 - accuracy: 0.9939
 8960/60000 [===&gt;..........................] - ETA: 2s - loss: 0.0055 - accuracy: 0.9941
10368/60000 [====&gt;.........................] - ETA: 2s - loss: 0.0055 - accuracy: 0.9941
11648/60000 [====&gt;.........................] - ETA: 1s - loss: 0.0054 - accuracy: 0.9944
12928/60000 [=====&gt;........................] - ETA: 1s - loss: 0.0052 - accuracy: 0.9947
14208/60000 [======&gt;.......................] - ETA: 1s - loss: 0.0051 - accuracy: 0.9947
15488/60000 [======&gt;.......................] - ETA: 1s - loss: 0.0050 - accuracy: 0.9950
16640/60000 [=======&gt;......................] - ETA: 1s - loss: 0.0050 - accuracy: 0.9950
17920/60000 [=======&gt;......................] - ETA: 1s - loss: 0.0049 - accuracy: 0.9950
19200/60000 [========&gt;.....................] - ETA: 1s - loss: 0.0048 - accuracy: 0.9952
20480/60000 [=========&gt;....................] - ETA: 1s - loss: 0.0047 - accuracy: 0.9951
21760/60000 [=========&gt;....................] - ETA: 1s - loss: 0.0046 - accuracy: 0.9952
23040/60000 [==========&gt;...................] - ETA: 1s - loss: 0.0046 - accuracy: 0.9953
24320/60000 [===========&gt;..................] - ETA: 1s - loss: 0.0047 - accuracy: 0.9951
25728/60000 [===========&gt;..................] - ETA: 1s - loss: 0.0047 - accuracy: 0.9951
27008/60000 [============&gt;.................] - ETA: 1s - loss: 0.0046 - accuracy: 0.9951
28160/60000 [=============&gt;................] - ETA: 1s - loss: 0.0046 - accuracy: 0.9952
29440/60000 [=============&gt;................] - ETA: 1s - loss: 0.0045 - accuracy: 0.9953
30720/60000 [==============&gt;...............] - ETA: 1s - loss: 0.0045 - accuracy: 0.9953
32128/60000 [===============&gt;..............] - ETA: 1s - loss: 0.0045 - accuracy: 0.9952
33408/60000 [===============&gt;..............] - ETA: 1s - loss: 0.0046 - accuracy: 0.9952
34560/60000 [================&gt;.............] - ETA: 1s - loss: 0.0045 - accuracy: 0.9953
35712/60000 [================&gt;.............] - ETA: 1s - loss: 0.0045 - accuracy: 0.9952
36992/60000 [=================&gt;............] - ETA: 0s - loss: 0.0045 - accuracy: 0.9953
38272/60000 [==================&gt;...........] - ETA: 0s - loss: 0.0046 - accuracy: 0.9952
39424/60000 [==================&gt;...........] - ETA: 0s - loss: 0.0046 - accuracy: 0.9952
40704/60000 [===================&gt;..........] - ETA: 0s - loss: 0.0046 - accuracy: 0.9952
41856/60000 [===================&gt;..........] - ETA: 0s - loss: 0.0046 - accuracy: 0.9951
43008/60000 [====================&gt;.........] - ETA: 0s - loss: 0.0046 - accuracy: 0.9952
44288/60000 [=====================&gt;........] - ETA: 0s - loss: 0.0046 - accuracy: 0.9951
45696/60000 [=====================&gt;........] - ETA: 0s - loss: 0.0046 - accuracy: 0.9951
46976/60000 [======================&gt;.......] - ETA: 0s - loss: 0.0046 - accuracy: 0.9951
48384/60000 [=======================&gt;......] - ETA: 0s - loss: 0.0045 - accuracy: 0.9952
49664/60000 [=======================&gt;......] - ETA: 0s - loss: 0.0046 - accuracy: 0.9951
50944/60000 [========================&gt;.....] - ETA: 0s - loss: 0.0046 - accuracy: 0.9951
52096/60000 [=========================&gt;....] - ETA: 0s - loss: 0.0046 - accuracy: 0.9950
53376/60000 [=========================&gt;....] - ETA: 0s - loss: 0.0047 - accuracy: 0.9949
54656/60000 [==========================&gt;...] - ETA: 0s - loss: 0.0047 - accuracy: 0.9949
55936/60000 [==========================&gt;...] - ETA: 0s - loss: 0.0047 - accuracy: 0.9950
57088/60000 [===========================&gt;..] - ETA: 0s - loss: 0.0047 - accuracy: 0.9950
58368/60000 [============================&gt;.] - ETA: 0s - loss: 0.0047 - accuracy: 0.9949
59648/60000 [============================&gt;.] - ETA: 0s - loss: 0.0047 - accuracy: 0.9949
60000/60000 [==============================] - 3s 44us/sample - loss: 0.0047 - accuracy: 0.9949 - val_loss: 0.0080 - val_accuracy: 0.9908
Epoch 4/5

  128/60000 [..............................] - ETA: 2s - loss: 0.0011 - accuracy: 1.0000
 1408/60000 [..............................] - ETA: 2s - loss: 0.0029 - accuracy: 0.9972
 2688/60000 [&gt;.............................] - ETA: 2s - loss: 0.0029 - accuracy: 0.9978
 3968/60000 [&gt;.............................] - ETA: 2s - loss: 0.0036 - accuracy: 0.9970
 5120/60000 [=&gt;............................] - ETA: 2s - loss: 0.0038 - accuracy: 0.9963
 6400/60000 [==&gt;...........................] - ETA: 2s - loss: 0.0038 - accuracy: 0.9966
 7680/60000 [==&gt;...........................] - ETA: 2s - loss: 0.0040 - accuracy: 0.9961
 9088/60000 [===&gt;..........................] - ETA: 2s - loss: 0.0038 - accuracy: 0.9963
10368/60000 [====&gt;.........................] - ETA: 2s - loss: 0.0036 - accuracy: 0.9964
11648/60000 [====&gt;.........................] - ETA: 1s - loss: 0.0036 - accuracy: 0.9965
12928/60000 [=====&gt;........................] - ETA: 1s - loss: 0.0034 - accuracy: 0.9968
14080/60000 [======&gt;.......................] - ETA: 1s - loss: 0.0033 - accuracy: 0.9969
15360/60000 [======&gt;.......................] - ETA: 1s - loss: 0.0032 - accuracy: 0.9970
16640/60000 [=======&gt;......................] - ETA: 1s - loss: 0.0032 - accuracy: 0.9969
17920/60000 [=======&gt;......................] - ETA: 1s - loss: 0.0032 - accuracy: 0.9970
19200/60000 [========&gt;.....................] - ETA: 1s - loss: 0.0032 - accuracy: 0.9970
20480/60000 [=========&gt;....................] - ETA: 1s - loss: 0.0032 - accuracy: 0.9970
21888/60000 [=========&gt;....................] - ETA: 1s - loss: 0.0033 - accuracy: 0.9968
23168/60000 [==========&gt;...................] - ETA: 1s - loss: 0.0032 - accuracy: 0.9968
24448/60000 [===========&gt;..................] - ETA: 1s - loss: 0.0033 - accuracy: 0.9969
25600/60000 [===========&gt;..................] - ETA: 1s - loss: 0.0032 - accuracy: 0.9969
26752/60000 [============&gt;.................] - ETA: 1s - loss: 0.0032 - accuracy: 0.9969
28032/60000 [=============&gt;................] - ETA: 1s - loss: 0.0032 - accuracy: 0.9969
29312/60000 [=============&gt;................] - ETA: 1s - loss: 0.0032 - accuracy: 0.9969
30592/60000 [==============&gt;...............] - ETA: 1s - loss: 0.0033 - accuracy: 0.9968
31872/60000 [==============&gt;...............] - ETA: 1s - loss: 0.0032 - accuracy: 0.9969
33152/60000 [===============&gt;..............] - ETA: 1s - loss: 0.0032 - accuracy: 0.9969
34432/60000 [================&gt;.............] - ETA: 1s - loss: 0.0033 - accuracy: 0.9969
35712/60000 [================&gt;.............] - ETA: 1s - loss: 0.0033 - accuracy: 0.9968
36992/60000 [=================&gt;............] - ETA: 0s - loss: 0.0033 - accuracy: 0.9969
38144/60000 [==================&gt;...........] - ETA: 0s - loss: 0.0033 - accuracy: 0.9969
39296/60000 [==================&gt;...........] - ETA: 0s - loss: 0.0033 - accuracy: 0.9969
40448/60000 [===================&gt;..........] - ETA: 0s - loss: 0.0033 - accuracy: 0.9969
41728/60000 [===================&gt;..........] - ETA: 0s - loss: 0.0033 - accuracy: 0.9969
43008/60000 [====================&gt;.........] - ETA: 0s - loss: 0.0033 - accuracy: 0.9969
44288/60000 [=====================&gt;........] - ETA: 0s - loss: 0.0034 - accuracy: 0.9968
45568/60000 [=====================&gt;........] - ETA: 0s - loss: 0.0034 - accuracy: 0.9967
46848/60000 [======================&gt;.......] - ETA: 0s - loss: 0.0034 - accuracy: 0.9967
48128/60000 [=======================&gt;......] - ETA: 0s - loss: 0.0034 - accuracy: 0.9967
49408/60000 [=======================&gt;......] - ETA: 0s - loss: 0.0034 - accuracy: 0.9967
50688/60000 [========================&gt;.....] - ETA: 0s - loss: 0.0034 - accuracy: 0.9967
51968/60000 [========================&gt;.....] - ETA: 0s - loss: 0.0034 - accuracy: 0.9967
53120/60000 [=========================&gt;....] - ETA: 0s - loss: 0.0034 - accuracy: 0.9967
54400/60000 [==========================&gt;...] - ETA: 0s - loss: 0.0034 - accuracy: 0.9967
55680/60000 [==========================&gt;...] - ETA: 0s - loss: 0.0034 - accuracy: 0.9967
57088/60000 [===========================&gt;..] - ETA: 0s - loss: 0.0035 - accuracy: 0.9966
58368/60000 [============================&gt;.] - ETA: 0s - loss: 0.0035 - accuracy: 0.9966
59648/60000 [============================&gt;.] - ETA: 0s - loss: 0.0035 - accuracy: 0.9966
60000/60000 [==============================] - 3s 44us/sample - loss: 0.0035 - accuracy: 0.9965 - val_loss: 0.0085 - val_accuracy: 0.9896
Epoch 5/5

  128/60000 [..............................] - ETA: 2s - loss: 2.4402e-04 - accuracy: 1.0000
 1408/60000 [..............................] - ETA: 2s - loss: 0.0028 - accuracy: 0.9972    
 2688/60000 [&gt;.............................] - ETA: 2s - loss: 0.0031 - accuracy: 0.9970
 3968/60000 [&gt;.............................] - ETA: 2s - loss: 0.0030 - accuracy: 0.9972
 5248/60000 [=&gt;............................] - ETA: 2s - loss: 0.0030 - accuracy: 0.9973
 6528/60000 [==&gt;...........................] - ETA: 2s - loss: 0.0028 - accuracy: 0.9979
 7680/60000 [==&gt;...........................] - ETA: 2s - loss: 0.0027 - accuracy: 0.9979
 8960/60000 [===&gt;..........................] - ETA: 2s - loss: 0.0027 - accuracy: 0.9979
10240/60000 [====&gt;.........................] - ETA: 2s - loss: 0.0028 - accuracy: 0.9979
11520/60000 [====&gt;.........................] - ETA: 1s - loss: 0.0027 - accuracy: 0.9978
12800/60000 [=====&gt;........................] - ETA: 1s - loss: 0.0027 - accuracy: 0.9979
14080/60000 [======&gt;.......................] - ETA: 1s - loss: 0.0026 - accuracy: 0.9980
15360/60000 [======&gt;.......................] - ETA: 1s - loss: 0.0026 - accuracy: 0.9979
16640/60000 [=======&gt;......................] - ETA: 1s - loss: 0.0025 - accuracy: 0.9980
17920/60000 [=======&gt;......................] - ETA: 1s - loss: 0.0026 - accuracy: 0.9979
19328/60000 [========&gt;.....................] - ETA: 1s - loss: 0.0027 - accuracy: 0.9979
20480/60000 [=========&gt;....................] - ETA: 1s - loss: 0.0027 - accuracy: 0.9978
21760/60000 [=========&gt;....................] - ETA: 1s - loss: 0.0027 - accuracy: 0.9978
23040/60000 [==========&gt;...................] - ETA: 1s - loss: 0.0026 - accuracy: 0.9979
24320/60000 [===========&gt;..................] - ETA: 1s - loss: 0.0026 - accuracy: 0.9979
25600/60000 [===========&gt;..................] - ETA: 1s - loss: 0.0026 - accuracy: 0.9978
27008/60000 [============&gt;.................] - ETA: 1s - loss: 0.0026 - accuracy: 0.9978
28288/60000 [=============&gt;................] - ETA: 1s - loss: 0.0026 - accuracy: 0.9978
29568/60000 [=============&gt;................] - ETA: 1s - loss: 0.0026 - accuracy: 0.9979
30848/60000 [==============&gt;...............] - ETA: 1s - loss: 0.0026 - accuracy: 0.9978
32256/60000 [===============&gt;..............] - ETA: 1s - loss: 0.0026 - accuracy: 0.9979
33536/60000 [===============&gt;..............] - ETA: 1s - loss: 0.0025 - accuracy: 0.9979
34816/60000 [================&gt;.............] - ETA: 1s - loss: 0.0026 - accuracy: 0.9979
35968/60000 [================&gt;.............] - ETA: 0s - loss: 0.0026 - accuracy: 0.9979
37248/60000 [=================&gt;............] - ETA: 0s - loss: 0.0025 - accuracy: 0.9979
38400/60000 [==================&gt;...........] - ETA: 0s - loss: 0.0026 - accuracy: 0.9978
39680/60000 [==================&gt;...........] - ETA: 0s - loss: 0.0026 - accuracy: 0.9978
40960/60000 [===================&gt;..........] - ETA: 0s - loss: 0.0026 - accuracy: 0.9978
42240/60000 [====================&gt;.........] - ETA: 0s - loss: 0.0026 - accuracy: 0.9978
43520/60000 [====================&gt;.........] - ETA: 0s - loss: 0.0026 - accuracy: 0.9977
44800/60000 [=====================&gt;........] - ETA: 0s - loss: 0.0026 - accuracy: 0.9978
46080/60000 [======================&gt;.......] - ETA: 0s - loss: 0.0026 - accuracy: 0.9979
47232/60000 [======================&gt;.......] - ETA: 0s - loss: 0.0026 - accuracy: 0.9979
48512/60000 [=======================&gt;......] - ETA: 0s - loss: 0.0026 - accuracy: 0.9978
49792/60000 [=======================&gt;......] - ETA: 0s - loss: 0.0026 - accuracy: 0.9978
51072/60000 [========================&gt;.....] - ETA: 0s - loss: 0.0026 - accuracy: 0.9978
52352/60000 [=========================&gt;....] - ETA: 0s - loss: 0.0026 - accuracy: 0.9978
53632/60000 [=========================&gt;....] - ETA: 0s - loss: 0.0026 - accuracy: 0.9978
54912/60000 [==========================&gt;...] - ETA: 0s - loss: 0.0026 - accuracy: 0.9978
56192/60000 [===========================&gt;..] - ETA: 0s - loss: 0.0026 - accuracy: 0.9977
57472/60000 [===========================&gt;..] - ETA: 0s - loss: 0.0026 - accuracy: 0.9977
58752/60000 [============================&gt;.] - ETA: 0s - loss: 0.0026 - accuracy: 0.9977
60000/60000 [==============================] - 3s 44us/sample - loss: 0.0026 - accuracy: 0.9977 - val_loss: 0.0086 - val_accuracy: 0.9900
Test score: 0.008608629779826878
Test accuracy: 0.99
</pre></div>
</div>
</div>
</div>
<div class="section" id="convert-trained-model-for-akida-and-test">
<h2>5. Convert trained model for Akida and test<a class="headerlink" href="#convert-trained-model-for-akida-and-test" title="Permalink to this headline">¶</a></h2>
<div class="section" id="final-conversion">
<h3>5.1 Final conversion<a class="headerlink" href="#final-conversion" title="Permalink to this headline">¶</a></h3>
<p>Convert the quantized model to a version suitable to be used in the Akida NSoC
in inference mode:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>One needs to supply the coefficients used to rescale the input
dataset before the training - here <code class="docutils literal notranslate"><span class="pre">input_scaling</span></code> -</p>
</div>
<p>As with Keras, the summary() method provides a textual representation of the
model.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">cnn2snn</span> <span class="kn">import</span> <span class="n">convert</span>

<span class="n">model_akida</span> <span class="o">=</span> <span class="n">convert</span><span class="p">(</span><span class="n">model_keras</span><span class="p">,</span> <span class="n">input_scaling</span><span class="o">=</span><span class="n">input_scaling</span><span class="p">)</span>
<span class="n">model_akida</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>-------------------------------------------------------------------------------------------------------------------------
Layer (type)           HW  Input shape   Output shape  Kernel shape  Learning (#classes)       #InConn/#Weights/ThFire
=========================================================================================================================
conv_0 (InputConvoluti yes [28, 28, 1]   [14, 14, 32]  (5 x 5 x 1)   N/A                       25 / 17 / 0
-------------------------------------------------------------------------------------------------------------------------
conv_1 (Convolutional) yes [14, 14, 32]  [7, 7, 64]    (5 x 5 x 32)  N/A                       800 / 478 / 0
-------------------------------------------------------------------------------------------------------------------------
dense_2 (FullyConnecte yes [7, 7, 64]    [1, 1, 512]   N/A           N/A                       3136 / 1115 / 0
-------------------------------------------------------------------------------------------------------------------------
dense_3 (FullyConnecte yes [1, 1, 512]   [1, 1, 10]    N/A           N/A                       512 / 326 / 0
-------------------------------------------------------------------------------------------------------------------------
</pre></div>
</div>
</div>
<div class="section" id="performances-check-with-the-aee">
<h3>5.2 Performances check with the AEE<a class="headerlink" href="#performances-check-with-the-aee" title="Permalink to this headline">¶</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">num_samples</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="n">results</span> <span class="o">=</span> <span class="n">model_akida</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">raw_x_test</span><span class="p">[:</span><span class="n">num_samples</span><span class="p">])</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">raw_y_test</span><span class="p">[:</span><span class="n">num_samples</span><span class="p">],</span> <span class="n">results</span><span class="p">[:</span><span class="n">num_samples</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy: &quot;</span><span class="o">+</span><span class="s2">&quot;</span><span class="si">{0:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="n">accuracy</span><span class="p">)</span><span class="o">+</span><span class="s2">&quot;%&quot;</span><span class="p">)</span>

<span class="c1"># For non-regression purpose</span>
<span class="k">assert</span> <span class="n">accuracy</span> <span class="o">&gt;</span> <span class="mf">0.95</span>

<span class="c1"># Print model statistics</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model statistics&quot;</span><span class="p">)</span>
<span class="n">stats</span> <span class="o">=</span> <span class="n">model_akida</span><span class="o">.</span><span class="n">get_statistics</span><span class="p">()</span>
<span class="n">model_akida</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">raw_x_test</span><span class="p">[:</span><span class="mi">20</span><span class="p">])</span>
<span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">stat</span> <span class="ow">in</span> <span class="n">stats</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">stat</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Accuracy: 98.50%
Model statistics
Layer (type)                  output sparsity
conv_0 (InputConvolutional)   0.86
Layer (type)                  input sparsity      output sparsity     ops
conv_1 (Convolutional)        0.86                0.77                1389200
Layer (type)                  input sparsity      output sparsity     ops
dense_2 (FullyConnected)      0.77                0.69                366438
Layer (type)                  input sparsity      output sparsity     ops
dense_3 (FullyConnected)      0.69                0.00                1580
</pre></div>
</div>
<p>Depending on the number of samples you run, you should find a
performance of around 99% (better results can be achieved using more
epochs for training).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Akida-compatible model first layer type is <code class="docutils literal notranslate"><span class="pre">InputConvolutional</span></code>
and holds underlying data to spike conversion (please refer to
<a class="reference external" href="../../user_guide/aee.html">Akida Execution Engine documentation</a> for more details).</p>
</div>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> ( 0 minutes  35.228 seconds)</p>
<div class="sphx-glr-footer class sphx-glr-footer-example docutils container" id="sphx-glr-download-examples-cnn2snn-plot-mnist-cnn2akida-demo-py">
<div class="sphx-glr-download docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/ddea16649fca6eb1b57d5ca0944eff85/plot_mnist_cnn2akida_demo.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">plot_mnist_cnn2akida_demo.py</span></code></a></p>
</div>
<div class="sphx-glr-download docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/f5c5d9b48b0e5385bd3c5c428478a864/plot_mnist_cnn2akida_demo.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">plot_mnist_cnn2akida_demo.ipynb</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../semisupervised/plot_mnist_main.html" class="btn btn-neutral float-right" title="Learning and inference on MNIST" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="plot_mobilenet_imagenet.html" class="btn btn-neutral float-left" title="Inference on ImageNet with MobileNet" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright Copyright 2020, BrainChip Holdings Ltd. All Rights Reserved.

    </p>
  </div> 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>