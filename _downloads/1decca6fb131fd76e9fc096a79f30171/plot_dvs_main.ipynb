{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nLearning and inference on Characters DVS\n========================================\n\nThe Characters_DVS dataset comprises a set of recordings made using a\nDynamic Vision Sensor (DVS). This is a type of event-based camera, where\neach event indicates the lightening or darkening of a given pixel at a\nspecific time. The stimuli are the 36 (latin) alphanumeric characters,\nprinted on paper and affixed to a rotating drum so that they drift\nacross the camera's field of view. The dataset includes 2 samples for\neach character. For a full description of the dataset, see `Orchard et\nal, (2015), doi:10.1109/TPAMI.2015.2392947\n<https://www.researchgate.net/publication/273308877_HFirst_A_Temporal_Approach_to_Object_Recognition>`__.\n\nFor this simple demonstration, we use the ExtractedStabilized version of\nthe dataset, where, rather than use the full 128 x 128 pixel scene,\nactivity related to individual characters has been extracted and\ncentered in a 32 x 32 pixel scene.\n\nEach DVS event is characterized by 3 values: x- and y-coordinates, and\nthe polarity of the luminance change (increment or decrement). Note that\nin the current demo, we've discarded the polarity information for\nsimplicity (and because it's not useful in a task where we're only\ninterested in object shape and not direction of movement).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Loading the Characters DVS dataset\n-------------------------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Various imports needed for the tutorial\nimport os\n\nimport numpy as np\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport warnings\nfrom tensorflow.keras.utils import get_file\nimport csv\n\n# Filter warnings\nwarnings.filterwarnings(\"ignore\", module=\"matplotlib\")\n\n# Akida specific imports\nfrom akida import Model, Sparse, InputData, FullyConnected, LearningType, coords_to_sparse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Retrieve Characters DVS data set\nfile_path = get_file(\"CharDVS.tar.gz\", \"http://data.brainchip.com/dataset-mirror/charDVS/CharDVS.tar.gz\", cache_subdir='datasets/charDVS', extract=True)\nworking_dir = os.path.dirname(file_path)\n\ndatafilenames = []\ndvs_labels = []\nlbl_filepath = os.path.join(working_dir, \"CharDVS_data\", \"CharDVS_labels.csv\")\nif os.path.exists(lbl_filepath):\n    with open(lbl_filepath, 'r') as csvfile:\n        csvreader = csv.reader(csvfile, delimiter=',')\n        for row in csvreader:\n            datafilenames.append(row[0])\n            dvs_labels.append(row[1])\nelse:\n    print(\"Failed to find labels file \" + lbl_filepath)\n\ndvs_events = []\nfor fn in datafilenames[:]:\n    fname = os.path.join(working_dir, \"CharDVS_data\", fn)\n    if os.path.exists(fname):\n        dvs_events.append(np.genfromtxt(fname, dtype=np.int32, delimiter=','))\n    else:\n        print(\"Failed to find data file \" + fname)\n\n# Using 32 x 32 images\ndvs_sz = (32, 32)\ndvs_shape = (*dvs_sz, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Look at some events from the dataset\n---------------------------------------\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As described above, each DVS event is characterized by 3 values.\nAlthough we've discarded the polarity information, we've kept a third\nchannel for each event (always set to zero), because that's the input\nevent format expected by the Akida Execution Engine.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Cherry-pick an abitrary event\ntest_events = dvs_events[27]\nprint(test_events[1,:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Akida Execution Engine processes groups of events, which we'll refer\nto as 'packets'. But how many events should be in a packet?\n\nIn the current case, it's helpful to visualize the input data: we're\ngoing to want to group together enough events to generate recognizable\nfeatures, but without allowing too many duplicate events (multiple\nevents occurring at the same input location). Try varying the\npacket_size in the following:\n\nNote that this reconstruction of the events into an image is purely for\nvisualization here - this is not at all what happens inside the Akida\nExecution Engine.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Displaying a 'packet' of events as an image\npacket_size = 150\n\ntest_img = np.zeros(dvs_sz, dtype=np.int32)\ntest_events = dvs_events[27]\nxx = test_events[:packet_size,0]\nyy = test_events[:packet_size,1]\nfor i in range(packet_size):\n    test_img[yy[i],xx[i]] += 1\n\nplt.imshow(test_img, cmap=cm.Greys_r)\nplt.title('Displaying a packet of %i events' % packet_size)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ultimately, this is a variable that can be optimized according to the\ntask.\n\nHere, we'll go forward with a packet_size of 150. You can try other\nvalues.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Set packet size to 150 from now on\npacket_size = 150"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Configuring Akida model\n--------------------------\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A neural network model can be sequentially defined. Check the `Akida\nExecution Engine documentation <../../api_reference/aee_apis.html>`__ for a\nfull description of the parameters and layer types available.\n\nNote that we've defined the expected packet size to be 150 events. A\nrelated value is num_weights, here also set to 150. Typically, those two\nvalues will be similar - there are specific cases where more or fewer\nweights will yield improved performance but setting them equal to the\npacket size is a reasonable starting point.\n\nWith the neural network model in place, it's a simple matter to launch\nthe Akida Execution Engine:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "#Create a model\nmodel = Model()\nmodel.add(InputData(\"input\", input_width=32, input_height=32, input_features=1))\nfully = FullyConnected(\"fully\", num_neurons=32, threshold_fire=40)\nmodel.add(fully)\n# Configure fully connected layer for training\nfully.compile(num_weights=150)\nmodel.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. Learning and inference\n-------------------------\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A key feature of the Akida Execution Engine and the Akida NSoC is its\nunsupervised learning algorithm, emulating the plasticity found between\nbiological neurons. As a result, we can send unlabeled data to the model\nand it will learn to recognize patterns in the data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Define a simple function that iterates over a set of events\ndef evaluate_events(events, num_packets, packet_size, learn):\n    for pk in range(num_packets):\n        pk_start = pk*packet_size\n        event_packet = coords_to_sparse(coords=events[pk_start:(pk_start+packet_size), :], shape=dvs_shape)\n        # This is where we call akida\n        if learn:\n            out_spikes = model.fit(event_packet)\n        else:\n            out_spikes = model.forward(event_packet)\n        print(\"Packet \" + str(pk))\n        if out_spikes.nnz > 0:\n            print(\"Output events:\")\n            print(out_spikes.coords)\n        else:\n            print('Zero output spikes generated')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, send the neural network a few packets of data from the letter 'A'\nsample, and let it learn.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# learning 'A' samples\nevents_A = dvs_events[27]\nnum_packets = 5\n\nstats = model.get_statistics()\nevaluate_events(events_A, num_packets, packet_size, learn=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Print model statistics\nprint(\"Model statistics\")\nfor _, stat in stats.items():\n    print(stat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The output events generated by the Akida Execution Engine are similar to\nthe input events we looked at above, in that each event comprises a\nn-coordinate, then an x-coordinate, then a y-coordinate, then a feature\nindex. For output events, the x- and y-coordinates are only meaningful\nfor Convolutional layer types, so here, with a FullyConnected layer,\nthey'll always be zero. The fourth value, the feature index, is the\nimportant one: in this case, it tells us which neuron in the model\ngenerated the event. You can see that over the course of the packets\nsent to the model, the same neurons kept responding: those are the\nneurons that learned to recognize the presented input (here, the letter\n'A').\n\nNow, try sending the model some events from a stimulus that it hasn't\nlearned yet, say, the letter 'B' (and note that here, we've kept\nlearning turned off for now):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Inference with 'B' samples\nevents_B = dvs_events[29]\nnum_packets = 5\n\nevaluate_events(events_B, num_packets, packet_size, learn=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In most cases, no neurons will have responded. If any have (e.g. if\nyou've increased the packet size without adjusting the firing threshold\nin the configuration file), it should be apparent that they are much\nless activated (the 4th value in each output event) than they were for\nthe 'A' inputs.\n\nNow send those same events again, but this time with learning enabled:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# learning 'B' samples\nevaluate_events(events_B, num_packets, packet_size, learn=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Some neurons should have started to respond to the 'B'. Importantly,\nnote that these are different neurons from those that learned the 'A'.\nThat means that, if we send some unknown events, depending on which\nneurons respond, we should be able to infer whether the stimulus was an\n'A' or a 'B'. Try it with some new packets of events, first whith letter\n'A':\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Inference with 'A' samples\njump_events = 5000\n\nevaluate_events(events_A[jump_events:,:], num_packets, packet_size, learn=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "and now letter 'B':\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Inference with 'B' samples - forward(xxx, False, xxx)\njump_events = 5000\n\nevaluate_events(events_B[jump_events:,:], num_packets, packet_size, learn=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5. Unsupervised learning with supervised classification\n-------------------------------------------------------\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Up to now, we've been learning in a purely unsupervised manner. That's\nfine, but recognizing these inputs is a fundamentally supervised task:\nwe can look at the outputs and see that different neurons respond to\ndifferent inputs, but, by definition since it's unsupervised, we can't\nattach any meaning to its activity. It would be relatively simple for us\nto go back, look at which inputs drove which outputs and add labels\nourselves.\n\nHowever, with a small change in the way the model is trained, we can\nautomate that process: we simply have to tell Akida how many different\nclasses to expect (in the neural network model file), and then send a\nlabel with each training sample.\n\nReplace the Akida Execution Engine instance we've been using up to now\nby a new one with a slightly different neural network model:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Create a different model\nmodel = Model()\nmodel.add(InputData(\"input\", input_width=32, input_height=32, input_features=1))\n# Add a fully connected layer to the model, without activations so that we can\n# evaluate potentials directly\nfully = FullyConnected(\"fully\", num_neurons=288, activations_enabled=False)\nmodel.add(fully)\n# Configure the fully connected layer for semi-supervised training by specifying\n# a number of classes\nfully.compile(num_weights=150, num_classes=36)\nmodel.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's train over a few hundred events from each input sample\n(actually, only the first repeat of each character, so that we can come\nback and use the second repeats for testing).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Learn with the input label as an argument\nfor inchar in range(36):\n    events = dvs_events[inchar]\n    label = inchar\n    num_packets = 5\n\n    for pk in range(num_packets):\n        pk_start = pk*packet_size\n        event_packet = coords_to_sparse(events[pk_start:(pk_start+packet_size), :], dvs_shape)\n        model.fit(event_packet, input_labels=label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now use a different Akida API to retrieve the most active label among\nthe spiking neurons:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Check updated output with a few samples\nevents_B = dvs_events[29]\nnum_packets = 5\n\nstats = model.get_statistics()\nfor pk in range(num_packets):\n    pk_start = pk*packet_size\n    event_packet = coords_to_sparse(events_B[pk_start:(pk_start+packet_size), :], dvs_shape)\n    out_label = model.predict(event_packet, num_classes=36)\n    if out_label is not None:\n        print(\"Output label:\")\n        print(out_label)\n    else:\n        print('Zero output spikes generated')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Print model statistics\nprint(\"Model statistics\")\nfor _, stat in stats.items():\n    print(stat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It should be apparent that the predicted label corresponds to the\n'label' of the input events that we sent (29).\n\nNow let's run a full test on events that we didn't train on:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Final check\ninLabels = []\noutLabels = []\n\nfor inchar in range(36):\n    events = dvs_events[inchar+36]\n    num_packets = 5\n\n    for pk in range(num_packets):\n        print(\"Sample \" + dvs_labels[inchar+36] + \" In (number \" + str(pk+1)  + \"), Out\", end=\"\")\n        inLabels.append(dvs_labels[inchar+36])\n        pk_start = pk*packet_size\n        event_packet = coords_to_sparse(events[pk_start:(pk_start+packet_size), :], dvs_shape)\n        out_label = model.predict(event_packet, 36)[0]\n        if out_label != -1:\n            print(\" \" + dvs_labels[out_label], end=\"\\n\")\n        else:\n            print(\" ?\", end=\"\\n\")\n\n    print()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}