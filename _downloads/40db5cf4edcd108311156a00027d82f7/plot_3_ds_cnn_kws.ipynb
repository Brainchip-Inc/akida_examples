{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# DS-CNN/KWS inference\n\nThis tutorial illustrates how to build a basic speech recognition\nAkida network that recognizes thirty-two different words.\n\nThe model will be first defined as a CNN and trained in Keras, then\nconverted using the `CNN2SNN toolkit <../../user_guide/cnn2snn.html>`__.\n\nThis example uses a Keyword Spotting Dataset prepared using\n**TensorFlow** `audio recognition\nexample <https://www.tensorflow.org/tutorials/audio/simple_audio>`__ utils.\n\nThe words to recognize are first converted to `spectrogram\nimages <https://github.com/tensorflow/docs/blob/master/site/en/r1/tutorials/sequences/audio_recognition.md#how-does-this-model-work>`__\nthat allows us to use a model architecture that is typically used for\nimage recognition tasks.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load the preprocessed dataset\n\nThe TensorFlow `speech_commands <http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz>`__\ndataset is used for training and validation. All keywords except \"backward\",\n\"follow\" and \"forward\", are retrieved. These three words are kept to\nillustrate the edge learning in this\n`edge example <../edge/plot_1_edge_learning_kws.html>`__.\nThe data are not directly used for training. They are preprocessed,\ntransforming the audio files into MFCC features, well-suited for CNN networks.\nA pickle file containing the preprocessed data is available on our data\nserver.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pickle\n\nfrom tensorflow.keras.utils import get_file\n\n# Fetch pre-processed data for 32 keywords\nfname = get_file(\n    fname='kws_preprocessed_all_words_except_backward_follow_forward.pkl',\n    origin=\n    \"http://data.brainchip.com/dataset-mirror/kws/kws_preprocessed_all_words_except_backward_follow_forward.pkl\",\n    cache_subdir='datasets/kws')\nwith open(fname, 'rb') as f:\n    [_, _, x_valid, y_valid, _, _, word_to_index, _] = pickle.load(f)\n\n# Preprocessed dataset parameters\nnum_classes = len(word_to_index)\n\nprint(\"Wanted words and labels:\\n\", word_to_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load a pre-trained native Keras model\n\nThe model consists of:\n\n* a first convolutional layer accepting dense inputs (images),\n* several separable convolutional layers preserving spatial dimensions,\n* a global pooling reducing the spatial dimensions to a single pixel,\n* a last separable convolutional to reduce the number of outputs\n* a final fully connected layer to classify words\n\nAll layers are followed by a batch normalization and a ReLU activation.\n\nThis model was obtained with unconstrained float weights and activations after\n16 epochs of training.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n\n# Retrieve the model file from the BrainChip data server\nmodel_file = get_file(\"ds_cnn_kws.h5\",\n                      \"http://data.brainchip.com/models/ds_cnn/ds_cnn_kws.h5\",\n                      cache_subdir='models')\n\n# Load the native Keras pre-trained model\nmodel_keras = load_model(model_file)\nmodel_keras.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nfrom sklearn.metrics import accuracy_score\n\n# Check Keras Model performance\npotentials_keras = model_keras.predict(x_valid)\npreds_keras = np.squeeze(np.argmax(potentials_keras, 1))\n\naccuracy = accuracy_score(y_valid, preds_keras)\nprint(\"Accuracy: \" + \"{0:.2f}\".format(100 * accuracy) + \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load a pre-trained quantized Keras model satisfying Akida NSoC requirements\n\nThe above native Keras model is quantized and fine-tuned to get a quantized\nKeras model satisfying the `Akida NSoC requirements\n<../../user_guide/hw_constraints.html>`__.\nThe first convolutional layer uses 8 bits weights, but other layers use\n4 bits weights.\n\nAll activations are 4 bits except for the final Separable Convolutional that\nuses binary activations.\n\nPre-trained weights were obtained after a few training episodes:\n\n* we train the model with quantized activations only, with weights initialized\n  from those trained in the previous episode (native Keras model),\n* then, we train the model with quantized weights, with both weights and\n  activations initialized from those trained in the previous episode,\n* finally, we train the model with quantized weights and activations and by\n  gradually increasing quantization in the last layer.\n\nThe table below summarizes the results obtained when preparing the\nweights stored under `<http://data.brainchip.com/models/ds_cnn/>`__ :\n\n+---------+----------------+----------------------------+----------+--------+\n| Episode | Weights Quant. | Activ. Quant. / last layer | Accuracy | Epochs |\n+=========+================+============================+==========+========+\n| 1       | N/A            | N/A                        | 93.06 %  | 16     |\n+---------+----------------+----------------------------+----------+--------+\n| 2       | N/A            | 4 bits / 4 bits            | 92.30 %  | 16     |\n+---------+----------------+----------------------------+----------+--------+\n| 3       | 8/4 bits       | 4 bits / 4 bits            | 92.11 %  | 16     |\n+---------+----------------+----------------------------+----------+--------+\n| 4       | 8/4 bits       | 4 bits / 3 bits            | 92.38 %  | 16     |\n+---------+----------------+----------------------------+----------+--------+\n| 5       | 8/4 bits       | 4 bits / 2 bits            | 92.23 %  | 16     |\n+---------+----------------+----------------------------+----------+--------+\n| 6       | 8/4 bits       | 4 bits / 1 bit             | 92.22 %  | 16     |\n+---------+----------------+----------------------------+----------+--------+\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from akida_models import ds_cnn_kws_pretrained\n\n# Load the pre-trained quantized model\nmodel_keras_quantized = ds_cnn_kws_pretrained()\nmodel_keras_quantized.summary()\n\n# Check Model performance\npotentials_keras_q = model_keras_quantized.predict(x_valid)\npreds_keras_q = np.squeeze(np.argmax(potentials_keras_q, 1))\n\naccuracy_q = accuracy_score(y_valid, preds_keras_q)\nprint(\"Accuracy: \" + \"{0:.2f}\".format(100 * accuracy_q) + \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Conversion to Akida\n\nWe convert the model to Akida and then evaluate the performances on the\ndataset.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from cnn2snn import convert\n\n# Convert the model\nmodel_akida = convert(model_keras_quantized)\nmodel_akida.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Check Akida model performance\npreds_akida = model_akida.predict(x_valid, num_classes=num_classes)\n\naccuracy = accuracy_score(y_valid, preds_akida)\nprint(\"Accuracy: \" + \"{0:.2f}\".format(100 * accuracy) + \"%\")\n\n# For non-regression purpose\nassert accuracy > 0.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Print model statistics\nprint(\"Model statistics\")\nprint(model_akida.statistics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Confusion matrix\n\nThe confusion matrix provides a good summary of what mistakes the\nnetwork is making.\n\nPer scikit-learn convention it displays the true class in each row (ie\non each row you can see what the network predicted for the corresponding\nword).\n\nPlease refer to the Tensorflow `audio\nrecognition <https://github.com/tensorflow/docs/blob/master/site/en/r1/tutorials/sequences/audio_recognition.md#confusion-matrix>`__\nexample for a detailed explanation of the confusion matrix.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import itertools\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import confusion_matrix\n\n# Create confusion matrix\ncm = confusion_matrix(y_valid, preds_akida, list(word_to_index.values()))\n\n# Normalize\ncm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n# Display confusion matrix\nplt.rcParams[\"figure.figsize\"] = (16, 16)\nplt.figure()\n\ntitle = 'Confusion matrix'\ncmap = plt.cm.Blues\n\nplt.imshow(cm, interpolation='nearest', cmap=cmap)\nplt.title(title)\nplt.colorbar()\ntick_marks = np.arange(len(word_to_index))\nplt.xticks(tick_marks, word_to_index, rotation=45)\nplt.yticks(tick_marks, word_to_index)\n\nthresh = cm.max() / 2.\nfor i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n    plt.text(j,\n             i,\n             format(cm[i, j], '.2f'),\n             horizontalalignment=\"center\",\n             color=\"white\" if cm[i, j] > thresh else \"black\")\n\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.autoscale()\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}