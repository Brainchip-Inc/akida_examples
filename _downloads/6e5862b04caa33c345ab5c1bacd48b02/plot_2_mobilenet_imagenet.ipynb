{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# MobileNet/ImageNet inference\n\nThis CNN2SNN tutorial presents how to convert a MobileNet pre-trained\nmodel into Akida.\n\nAs ImageNet images are not publicly available, performances\nare assessed using a set of 10 copyright free images that were found on Google\nusing ImageNet class names.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Dataset preparation\n\nTest images all have at least 256 pixels in the smallest dimension. They must\nbe preprocessed to fit in the model. The\n``imagenet.preprocessing.resize_and_crop`` function decodes, crops and\nextracts a square 224x224x3 patch from an input image.\n\n.. Note:: Input size is here set to 224x224x3 as this is what is used by the\n          model presented in the next section.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport numpy as np\n\nfrom tensorflow.io import read_file\nfrom tensorflow.keras.utils import get_file\n\nfrom akida_models.imagenet import preprocessing\n\n# Model specification and hyperparameters\nNUM_CHANNELS = 3\nIMAGE_SIZE = 224\nNUM_CLASSES = 1000\n\nnum_images = 10\n\n# Retrieve dataset file from Brainchip data server\nfile_path = get_file(\n    \"imagenet_like.zip\",\n    \"http://data.brainchip.com/dataset-mirror/imagenet_like/imagenet_like.zip\",\n    cache_subdir='datasets/imagenet_like',\n    extract=True)\ndata_folder = os.path.dirname(file_path)\n\n# Load images for test set\nx_test_files = []\nx_test = np.zeros((num_images, 224, 224, 3)).astype('uint8')\nfor id in range(num_images):\n    test_file = 'image_' + str(id + 1).zfill(2) + '.jpg'\n    x_test_files.append(test_file)\n    img_path = os.path.join(data_folder, test_file)\n    base_image = read_file(img_path)\n    image = preprocessing.resize_and_crop(image_buffer=base_image,\n                                          output_width=IMAGE_SIZE,\n                                          output_height=IMAGE_SIZE,\n                                          num_channels=NUM_CHANNELS)\n    x_test[id, :, :, :] = np.expand_dims(image, axis=0)\n\nprint(f'{num_images} images loaded and preprocessed.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. Note:: Akida runtime is configured to take 8-bit inputs without\n          rescaling. For conversion, rescaling values used for\n          training the Keras model are needed.\n\n\nLabels for test images are stored in the akida_models package. The matching\nbetween names (*string*) and labels (*integer*) is given through the\n``imagenet.preprocessing.index_to_label`` method.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import csv\n\n# Parse labels file\nfname = os.path.join(data_folder, 'labels_validation.txt')\nvalidation_labels = dict()\nwith open(fname, newline='') as csvfile:\n    reader = csv.reader(csvfile, delimiter=' ')\n    for row in reader:\n        validation_labels[row[0]] = row[1]\n\n# Get labels for the test set by index\nlabels_test = np.zeros(num_images)\nfor i in range(num_images):\n    labels_test[i] = int(validation_labels[x_test_files[i]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Create a Keras MobileNet model\n\nThe MobileNet architecture is available in the Akida model zoo as\n`mobilenet_imagenet <../../api_reference/akida_models_apis.html#akida_models.mobilenet_imagenet>`_.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n\n# Retrieve the float model with pretrained weights and load it\nmodel_file = get_file(\n    \"mobilenet_imagenet.h5\",\n    \"http://data.brainchip.com/models/mobilenet/mobilenet_imagenet_224.h5\",\n    cache_subdir='models/mobilenet_imagenet')\nmodel_keras = load_model(model_file)\nmodel_keras.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Top-1 accuracy on the actual ImageNet is 71.89%, the perfomance given below\nuses the 10 images subset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from timeit import default_timer as timer\n\n\n# Check model performance\ndef check_model_performances(model, x_test=x_test, labels_test=labels_test):\n    num_images = len(x_test)\n\n    start = timer()\n    potentials_keras = model.predict(x_test, batch_size=100)\n    end = timer()\n    print(f'Keras inference on {num_images} images took {end-start:.2f} s.\\n')\n\n    preds_keras = np.squeeze(np.argmax(potentials_keras, 1))\n    accuracy_keras = np.sum(np.equal(preds_keras, labels_test)) / num_images\n\n    print(f\"Keras accuracy: {accuracy_keras*100:.2f} %\")\n\n\ncheck_model_performances(model_keras)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Quantized model\n\nQuantizing a model is done using `cnn2snn.quantize\n<../../api_reference/cnn2snn_apis.html#quantize>`_.\n\nThe quantized model satisfies the Akida NSoC requirements:\n\n * the first layer has 8-bit weights,\n * all other convolutional layers have 4-bit weights,\n * all convolutional layers have 4-bit activations.\n\nHowever, this model will suffer from a drop in accuracy due to quantization\nas shown in the table below for ImageNet and in the next cell for the 10\nimages set.\n\n+----------------+--------------------+\n| Float accuracy | Quantized accuracy |\n+================+====================+\n|     71.89 %    |        2.70 %      |\n+----------------+--------------------+\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from cnn2snn import quantize\n\n# Quantize the model to 4-bit weights and activations, 8-bit weights for the\n# first convolutional layer\nmodel_keras_quantized = quantize(model_keras, 4, 4, 8)\n\n# Check Model performance\ncheck_model_performances(model_keras_quantized)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Pretrained quantized model\n\nThe Akida models zoo also contains a `pretrained quantized helper\n<../../api_reference/akida_models_apis.html#akida_models.mobilenet_imagenet_pretrained>`_\nthat was obtained after fine tuning the model for 30 epochs.\n\nTuning the model, that is training with a lowered learning rate, allows to\nrecover performances up to the initial floating point accuracy.\n\nPerformances on the full ImageNet dataset are:\n\n+----------------+--------------------+--------------------+\n| Float accuracy | Quantized accuracy |     After tuning   |\n+================+====================+====================+\n|     71.89 %    |       2.70 %       |       69.62 %      |\n+----------------+--------------------+--------------------+\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from akida_models import mobilenet_imagenet_pretrained\n\n# Use a quantized model with pretrained quantized weights\nmodel_keras_quantized_pretrained = mobilenet_imagenet_pretrained()\nmodel_keras_quantized_pretrained.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Check model performance\ncheck_model_performances(model_keras_quantized_pretrained)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Conversion to Akida\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 Convert to Akida model\n\nHere, the Keras quantized model is converted into a suitable version for\nthe Akida NSoC. The `cnn2snn.convert <../../api_reference/cnn2snn_apis.html#convert>`__\nfunction needs as arguments the Keras model and the input scaling parameters.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from cnn2snn import convert\n\nmodel_akida = convert(model_keras_quantized_pretrained)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Check hardware compliancy\n\nThe `Model.summary <../../api_reference/akida_apis.html#akida.Model.summary>`__\nmethod provides a detailed description of the Model layers.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_akida.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3 Check performance\n\nWhile we compute accuracy for the 10 images set in the next cell, the\nfollowing table summarizes results obtained on ImageNet.\n\n+----------------+----------------+\n| Keras accuracy | Akida accuracy |\n+================+================+\n|     69.62 %    |     69.53 %    |\n+----------------+----------------+\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Check Model performance\nstart = timer()\npreds_akida = model_akida.predict(x_test)\nend = timer()\nprint(f'Inference on {num_images} images took {end-start:.2f} s.\\n')\n\naccuracy_akida = np.sum(np.equal(preds_akida, labels_test)) / num_images\n\nprint(f\"Accuracy: {accuracy_akida*100:.2f} %\")\n\n# For non-regression purpose\nassert accuracy_akida >= 0.9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.4 Show predictions for a random image\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport matplotlib.lines as lines\n\n\n# Functions used to display the top5 results\ndef get_top5(potentials, true_label):\n    \"\"\"\n    Returns the top 5 classes from the output potentials\n    \"\"\"\n    tmp_pots = potentials.copy()\n    top5 = []\n    min_val = np.min(tmp_pots)\n    for ii in range(5):\n        best = np.argmax(tmp_pots)\n        top5.append(best)\n        tmp_pots[best] = min_val\n\n    vals = np.zeros((6,))\n    vals[:5] = potentials[top5]\n    if true_label not in top5:\n        vals[5] = potentials[true_label]\n    else:\n        vals[5] = 0\n    vals /= np.max(vals)\n\n    class_name = []\n    for ii in range(5):\n        class_name.append(preprocessing.index_to_label(top5[ii]).split(',')[0])\n    if true_label in top5:\n        class_name.append('')\n    else:\n        class_name.append(\n            preprocessing.index_to_label(true_label).split(',')[0])\n\n    return top5, vals, class_name\n\n\ndef adjust_spines(ax, spines):\n    for loc, spine in ax.spines.items():\n        if loc in spines:\n            spine.set_position(('outward', 10))  # outward by 10 points\n        else:\n            spine.set_color('none')  # don't draw spine\n    # turn off ticks where there is no spine\n    if 'left' in spines:\n        ax.yaxis.set_ticks_position('left')\n    else:\n        # no yaxis ticks\n        ax.yaxis.set_ticks([])\n    if 'bottom' in spines:\n        ax.xaxis.set_ticks_position('bottom')\n    else:\n        # no xaxis ticks\n        ax.xaxis.set_ticks([])\n\n\ndef prepare_plots():\n    fig = plt.figure(figsize=(8, 4))\n    # Image subplot\n    ax0 = plt.subplot(1, 3, 1)\n    imgobj = ax0.imshow(np.zeros((224, 224, 3), dtype=np.uint8))\n    ax0.set_axis_off()\n    # Top 5 results subplot\n    ax1 = plt.subplot(1, 2, 2)\n    bar_positions = (0, 1, 2, 3, 4, 6)\n    rects = ax1.barh(bar_positions, np.zeros((6,)), align='center', height=0.5)\n    plt.xlim(-0.2, 1.01)\n    ax1.set(xlim=(-0.2, 1.15), ylim=(-1.5, 12))\n    ax1.set_yticks(bar_positions)\n    ax1.invert_yaxis()\n    ax1.yaxis.set_ticks_position('left')\n    ax1.xaxis.set_ticks([])\n    adjust_spines(ax1, 'left')\n    ax1.add_line(lines.Line2D((0, 0), (-0.5, 6.5), color=(0.0, 0.0, 0.0)))\n    txt_axlbl = ax1.text(-1, -1, 'Top 5 Predictions:', size=12)\n    # Adjust Plot Positions\n    ax0.set_position([0.05, 0.055, 0.3, 0.9])\n    l1, b1, w1, h1 = ax1.get_position().bounds\n    ax1.set_position([l1 * 1.05, b1 + 0.09 * h1, w1, 0.8 * h1])\n    # Add title box\n    plt.figtext(0.5,\n                0.9,\n                \"Imagenet Classification by Akida\",\n                size=20,\n                ha=\"center\",\n                va=\"center\",\n                bbox=dict(boxstyle=\"round\",\n                          ec=(0.5, 0.5, 0.5),\n                          fc=(0.9, 0.9, 1.0)))\n\n    return fig, imgobj, ax1, rects\n\n\ndef update_bars_chart(rects, vals, true_label):\n    counter = 0\n    for rect, h in zip(rects, yvals):\n        rect.set_width(h)\n        if counter < 5:\n            if top5[counter] == true_label:\n                if counter == 0:\n                    rect.set_facecolor((0.0, 1.0, 0.0))\n                else:\n                    rect.set_facecolor((0.0, 0.5, 0.0))\n            else:\n                rect.set_facecolor('gray')\n        elif counter == 5:\n            rect.set_facecolor('red')\n        counter += 1\n\n\n# Prepare plots\nfig, imgobj, ax1, rects = prepare_plots()\n\n# Get a random image\nimg = np.random.randint(num_images)\n\n# Predict image class\npotentials_akida = model_akida.evaluate(np.expand_dims(x_test[img],\n                                                       axis=0)).squeeze()\n\n# Get top 5 prediction labels and associated names\ntrue_label = int(validation_labels[x_test_files[img]])\ntop5, yvals, class_name = get_top5(potentials_akida, true_label)\n\n# Draw Plots\nimgobj.set_data(x_test[img])\nax1.set_yticklabels(class_name, rotation='horizontal', size=9)\nupdate_bars_chart(rects, yvals, true_label)\nfig.canvas.draw()\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}