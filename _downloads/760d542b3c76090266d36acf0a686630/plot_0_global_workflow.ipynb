{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Global Akida workflow tutorial\n\nThis tutorial illustrates how to use the QuantizeML and CNN2SNN toolkits to produce a model that can\nbe used with Akida accelerator. You can refer to our [Akida user guide](../../user_guide/akida.html)_ for further explanation.\n\n.. Note:: Please refer to TensorFlow  [tf.keras.models](https://www.tensorflow.org/api_docs/python/tf/keras/models)_\n          module for model creation/import details and [TensorFlow Guide](https://www.tensorflow.org/guide)_ for details of how TensorFlow works.\n\n          MNIST example below is light enough so you do not need a [GPU](https://www.tensorflow.org/install/gpu)_ to run the training steps.\n\n\n.. figure:: ../../img/overall_flow.png\n   :target: ../../_images/overall_flow.png\n   :alt: Overall flow\n   :scale: 25 %\n   :align: center\n\n   Akida workflow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Create and train\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1. Load and reshape MNIST dataset\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\n\nfrom keras.datasets import mnist\n\n# Load MNIST dataset\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n# Add a channels dimension to the image sets as Akida expects 4-D inputs (corresponding to\n# (num_samples, width, height, channels). Note: MNIST is unusual in this respect - most image data\n# already includes a channel dimension, and this step will not be necessary.\nx_train = np.expand_dims(x_train, -1)\nx_test = np.expand_dims(x_test, -1)\n\n# Display a few images from the test set\nf, axarr = plt.subplots(1, 4)\nfor i in range(0, 4):\n    axarr[i].imshow(x_test[i].reshape((28, 28)), cmap=cm.Greys_r)\n    axarr[i].set_title('Class %d' % y_test[i])\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2. Model definition\n\nNote that at this stage, there is nothing specific to the Akida IP.\nThis start point is very much a completely standard CNN as defined\nwithin [Keras](https://www.tensorflow.org/api_docs/python/tf/keras)_.\n\nAn appropriate model for MNIST (inspired by [this example](https://www.tensorflow.org/model_optimization/guide/quantization/training_example#train_a_model_for_mnist_without_quantization_aware_training)_)\nmight look something like the following:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import keras\n\nmodel_keras = keras.models.Sequential([\n    keras.layers.Rescaling(1. / 255, input_shape=(28, 28, 1)),\n    keras.layers.Conv2D(filters=32, kernel_size=3, strides=2),\n    keras.layers.BatchNormalization(),\n    keras.layers.ReLU(),\n    # Separable layer\n    keras.layers.DepthwiseConv2D(kernel_size=3, padding='same', strides=2),\n    keras.layers.Conv2D(filters=64, kernel_size=1, padding='same'),\n    keras.layers.BatchNormalization(),\n    keras.layers.ReLU(),\n    keras.layers.Flatten(),\n    keras.layers.Dense(10)\n], 'mnistnet')\n\nmodel_keras.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3. Model training\n\nBefore going any further, train the model and get its performance. The created model should\nachieve a test accuracy over 98% after 10 epochs.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from keras.optimizers import Adam\n\nmodel_keras.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=Adam(learning_rate=1e-3),\n    metrics=['accuracy'])\n\n_ = model_keras.fit(x_train, y_train, epochs=10, validation_split=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "score = model_keras.evaluate(x_test, y_test, verbose=0)\nprint('Test accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Quantize\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1. 8-bit quantization\n\nWe can now turn to quantization to get a discretized version of the model, where the weights and\nactivations are quantized so as to be suitable for conversion towards an Akida accelerator.\n\nFor this, we just have to quantize the Keras model using the QuantizeML\n[quantize](../../api_reference/quantizeml_apis.html#quantizeml.models.quantize)_\nfunction. The selected quantization scheme is 8/8/8 which stands for 8-bit weights in the first\nlayer, 8-bit weights in other layers and 8-bit activations respectively.\n\nThe quantized model is a Keras model where the layers are replaced with custom [QuantizeML\nquantized layers](../../api_reference/quantizeml_apis.html#layers)_. All Keras API functions\ncan be applied on this new model: ``summary()``, ``compile()``, ``fit()``. etc.\n\n.. Note:: The ``quantize`` function applies [several transformations](../../api_reference/quantizeml_apis.html#transforms)_ to\n          the original model. For example, it folds the batch normalization layers into the\n          corresponding neural layers. The new weights are computed according to this folding\n          operation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from quantizeml.models import quantize\nfrom quantizeml.layers import QuantizationParams\n\nqparams = QuantizationParams(input_weight_bits=8, weight_bits=8, activation_bits=8)\nmodel_quantized = quantize(model_keras, qparams=qparams)\nmodel_quantized.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check the quantized model accuracy.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def compile_evaluate(model):\n    \"\"\" Compiles and evaluates the model, then return accuracy score. \"\"\"\n    model.compile(metrics=['accuracy'])\n    return model.evaluate(x_test, y_test, verbose=0)[1]\n\n\nprint('Test accuracy after 8-bit quantization:', compile_evaluate(model_quantized))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2. Effect of calibration\n\nThe previous call to ``quantize`` was made with random samples for calibration (default\nparameters). While the observed accuracy drop is minimal, that is around 1%, it can be higher on\nmore complex models and it is advised to use a set of real samples from the training set. Note\nthat this remains a calibration rather than a training step: any relevant data could be used, and\ncrucially, no labels are required.\nThe recommended configuration for calibration that is widely used to obtain\n[model zoo performance](../../model_zoo_performance.html#akida-2-0-models)_ is:\n\n- 1024 samples\n- a batch size of 100\n- 2 epochs\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_quantized = quantize(model_keras, qparams=qparams,\n                           samples=x_train, num_samples=1024, batch_size=100, epochs=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check the quantized and calibrated with real samples accuracy.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print('Test accuracy after calibration:', compile_evaluate(model_quantized))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calibrating with real samples on this model allows to recover the initial float accuracy.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3. 4-bit quantization\n\nThe accuracy of the 8-bit quantized model is equivalent to the one of the base model. In this\nsection, a lower bitwidth quantization scheme that is still compatible with Akida accelerator is\nadopted.\nThe accuracy of the 8-bit quantized model is equal to that of the base model. That quantized model\nis already compatible with the Akida accelerator (following \"conversion\", see below), and for most\nusers, no further quantization is required. In a few cases, it may be attractive to bring the\nmodel down to an even lower bitwidth quantization scheme, and here we show how to do that.\n\nThe model will now be quantized to 8/4/4, that is 8-bit weights in the first layer and 4-bit\nweights and activations everywhere else. Such a quantization scheme will usually introduce a\nperformance drop.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "qparams = QuantizationParams(input_weight_bits=8, weight_bits=4, activation_bits=4)\nmodel_quantized = quantize(model_keras, qparams=qparams,\n                           samples=x_train, num_samples=1024, batch_size=100, epochs=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check the 4-bit quantized accuracy.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print('Test accuracy after 4-bit quantization:', compile_evaluate(model_quantized))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4. Model fine tuning (quantization-aware training)\n\nWhen a model suffers from a large accuracy drop after quantization, fine tuning or \"quantization\naware training\" (QAT) allows to recover some or all performance.\n\nNote that since this is a fine tuning step, both the number of epochs and learning rate are\nexpected to be lower than during the initial float training.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_quantized.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=Adam(learning_rate=1e-4),\n    metrics=['accuracy'])\n\nmodel_quantized.fit(x_train, y_train, epochs=5, validation_split=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "score = model_quantized.evaluate(x_test, y_test, verbose=0)[1]\nprint('Test accuracy after fine tuning:', score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Convert\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Convert to Akida model\n\nWhen the quantized model achieves satisfactory performance, it can be converted to an Akida\naccelerator suitable format. The\n[convert](../../api_reference/cnn2snn_apis.html#cnn2snn.convert)_ function returns a model in\nAkida format ready for inference.\n\nAs with Keras, the summary() method provides a textual representation of the Akida model.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from cnn2snn import convert\n\nmodel_akida = convert(model_quantized)\nmodel_akida.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2. Check performance\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "accuracy = model_akida.evaluate(x_test, y_test)\nprint('Test accuracy after conversion:', accuracy)\n\n# For non-regression purposes\nassert accuracy > 0.96"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Show predictions for a single image\n\nNow try processing a single image, say, the first image in the dataset\nthat we looked at above:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Test a single example\nsample_image = 0\nimage = x_test[sample_image]\noutputs = model_akida.predict(image.reshape(1, 28, 28, 1))\nprint('Input Label: %i' % y_test[sample_image])\n\nf, axarr = plt.subplots(1, 2)\naxarr[0].imshow(x_test[sample_image].reshape((28, 28)), cmap=cm.Greys_r)\naxarr[0].set_title('Class %d' % y_test[sample_image])\naxarr[1].bar(range(10), outputs.squeeze())\naxarr[1].set_xticks(range(10))\nplt.show()\n\nprint(outputs.squeeze())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Consider the output from the model, printed above. As is typical in backprop trained models, the\nfinal layer here comprises a Dense layer, with one neuron per class in the dataset (here, 10). The\ngoal of training is to maximize the response of the neuron corresponding to the label of each\ntraining sample, while minimizing the responses of the other neurons.\n\nIn the bar chart above, you can see the outputs from all 10 neurons. It is easy to see that neuron\n7 responds much more strongly than the others. The first sample is indeed a number 7.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. GXNOR/MNIST\n\nA more robust model called GXNOR/MNIST is provided in [the model zoo](../../api_reference/akida_models_apis.html#akida_models.gxnor_mnist)_ It is inspired from the\n[GXNOR-Net paper](https://arxiv.org/pdf/1705.09283.pdf)_. It comes with its\n[pretrained 2/2/1 helper](../../api_reference/akida_models_apis.html#akida_models.gxnor_mnist_pretrained)_ for which the\nfloat training was done for 20 epochs, then the model was then gradually quantized following:\n4/4/4 --> 4/4/2 --> 2/2/2 --> 2/2/1 with a 15 epochs QAT step at each quantization stage.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}