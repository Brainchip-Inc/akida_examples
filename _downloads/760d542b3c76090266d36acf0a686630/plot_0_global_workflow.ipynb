{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Global Akida workflow\n\nUsing the MNIST dataset, this example shows the definition and training of a keras\nfloating point model, its quantization to 8-bit with the help of calibration,\nits quantization to 4-bit using QAT and its conversion to Akida.\nNotice that the performance of the original keras floating point model is maintained\nthroughout the Akida flow.\nPlease refer to the [Akida user guide](../../user_guide/akida.html)_ for further information.\n\n.. Note:: Please refer to the TensorFlow  [tf.keras.models](https://www.tensorflow.org/api_docs/python/tf/keras/models)_\n          module for model creation/import details and the [TensorFlow Guide](https://www.tensorflow.org/guide)_ for TensorFlow usage.\n\n          The MNIST example below is light enough so that a [GPU](https://www.tensorflow.org/install/gpu)_ is not needed for training.\n\n\n.. figure:: ../../img/overall_flow.png\n   :target: ../../_images/overall_flow.png\n   :alt: Overall flow\n   :scale: 60 %\n   :align: center\n\n   Global Akida workflow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Create and train\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1. Load and reshape MNIST dataset\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\n\nfrom keras.datasets import mnist\n\n# Load MNIST dataset\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n# Add a channels dimension to the image sets as Akida expects 4-D inputs (corresponding to\n# (num_samples, width, height, channels). Note: MNIST is a grayscale dataset and is unusual\n# in this respect - most image data already includes a channel dimension, and this step will\n# not be necessary.\nx_train = np.expand_dims(x_train, -1)\nx_test = np.expand_dims(x_test, -1)\n\n# Display a few images from the test set\nf, axarr = plt.subplots(1, 4)\nfor i in range(0, 4):\n    axarr[i].imshow(x_test[i].reshape((28, 28)), cmap=cm.Greys_r)\n    axarr[i].set_title('Class %d' % y_test[i])\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2. Model definition\n\nNote that at this stage, there is nothing specific to the Akida IP.\nThe model constructed below, as inspired by [this example](https://www.tensorflow.org/model_optimization/guide/quantization/training_example#train_a_model_for_mnist_without_quantization_aware_training)_,\nis a completely standard [Keras](https://www.tensorflow.org/api_docs/python/tf/keras)_ CNN model.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import keras\n\nmodel_keras = keras.models.Sequential([\n    keras.layers.Rescaling(1. / 255, input_shape=(28, 28, 1)),\n    keras.layers.Conv2D(filters=32, kernel_size=3, strides=2),\n    keras.layers.BatchNormalization(),\n    keras.layers.ReLU(),\n    # Separable layer\n    keras.layers.DepthwiseConv2D(kernel_size=3, padding='same', strides=2),\n    keras.layers.Conv2D(filters=64, kernel_size=1, padding='same'),\n    keras.layers.BatchNormalization(),\n    keras.layers.ReLU(),\n    keras.layers.Flatten(),\n    keras.layers.Dense(10)\n], 'mnistnet')\n\nmodel_keras.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3. Model training\n\nGiven the model created above, train the model and check its accuracy. The model should achieve\na test accuracy over 98% after 10 epochs.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from keras.optimizers import Adam\n\nmodel_keras.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=Adam(learning_rate=1e-3),\n    metrics=['accuracy'])\n\n_ = model_keras.fit(x_train, y_train, epochs=10, validation_split=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "score = model_keras.evaluate(x_test, y_test, verbose=0)\nprint('Test accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Quantize\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1. 8-bit quantization\n\nAn Akida accelerator processes 8, 4, 2, or 1 bit integer activations and weights. Therefore,\nthe floating point Keras model must be quantized in preparation to run on an Akida accelerator.\n\nThe QuantizeML [quantize](../../api_reference/quantizeml_apis.html#quantizeml.models.quantize)_\nfunction can be used to quantize a Keras model for Akida. For this step in this example, an\n\u201c8/8/8\u201d quantization scheme will be applied to the floating point Keras model to produce 8-bit\nweights in the first layer, 8-bit weights in all other layers, and 8-bit activations.\n\nThe quantization process results in a Keras model with custom [QuantizeML quantized layers](../../api_reference/quantizeml_apis.html#layers)_ substituted for the original Keras layers.\nAll Keras API functions can be applied on this new model: ``summary()``, ``compile()``, ``fit()``. etc.\n\n.. Note:: The ``quantize`` function applies [several transformations](../../api_reference/quantizeml_apis.html#transforms)_ to\n          the original model. For example, it folds the batch normalization layers into the\n          corresponding neural layers. The new weights are computed according to this folding\n          operation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from quantizeml.models import quantize\nfrom quantizeml.layers import QuantizationParams\n\nqparams = QuantizationParams(input_weight_bits=8, weight_bits=8, activation_bits=8)\nmodel_quantized = quantize(model_keras, qparams=qparams)\nmodel_quantized.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. Note:: Note that the number of parameters for the floating and quantized models differs,\n          a consequence of the BatchNormalization folding and the additional parameters\n          added for quantization. For further details, please refer to their respective summary.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check the quantized model accuracy.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def compile_evaluate(model):\n    \"\"\" Compiles and evaluates the model, then return accuracy score. \"\"\"\n    model.compile(metrics=['accuracy'])\n    return model.evaluate(x_test, y_test, verbose=0)[1]\n\n\nprint('Test accuracy after 8-bit quantization:', compile_evaluate(model_quantized))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2. Effect of calibration\n\nThe previous call to ``quantize`` was made with random samples for calibration\n(default parameters). While the observed drop in accuracy is minimal, that is\naround 1%, it can be worse on more complex models. Therefore, it is advised to\nuse a set of real samples from the training set for calibration during a call\nto ``quantize``.\nNote that this remains a calibration step rather than a training step in that\nno output labels are required. Furthermore, any relevant data could be used for\ncalibration. The recommended settings for calibration that are widely used to\nobtain the [zoo performance](../../model_zoo_performance.html#akida-2-0-models)_ are:\n\n- 1024 samples\n- a batch size of 100\n- 2 epochs\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_quantized = quantize(model_keras, qparams=qparams,\n                           samples=x_train, num_samples=1024, batch_size=100, epochs=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check the accuracy for the quantized and calibrated model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print('Test accuracy after calibration:', compile_evaluate(model_quantized))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calibrating with real samples on this model recovers the initial float accuracy.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3. 4-bit quantization\n\nThe accuracy of the 8/8/8 quantized model is equal to that of the Keras floating point\nmodel. In some cases, a smaller memory size for the model is required. This can be\naccomplished through quantization of the model to smaller bitwidths.\n\nThe model will now be quantized to 8/4/4, that is 8-bit weights in the first layer with\n4-bit weights and activations in all other layers. Such a quantization scheme will usually\nintroduce a performance drop.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "qparams = QuantizationParams(input_weight_bits=8, weight_bits=4, activation_bits=4)\nmodel_quantized = quantize(model_keras, qparams=qparams,\n                           samples=x_train, num_samples=1024, batch_size=100, epochs=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check the 4-bit quantized accuracy.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print('Test accuracy after 4-bit quantization:', compile_evaluate(model_quantized))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4. Model fine tuning (Quantization Aware Training)\n\nWhen a model suffers from an accuracy drop after quantization, fine tuning or Quantization\nAware Training (QAT) may recover some or all of the original performance.\n\nNote that since this is a fine tuning step, both the number of epochs and learning rate are\nexpected to be lower than during the initial float training.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_quantized.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=Adam(learning_rate=1e-4),\n    metrics=['accuracy'])\n\nmodel_quantized.fit(x_train, y_train, epochs=5, validation_split=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "score = model_quantized.evaluate(x_test, y_test, verbose=0)[1]\nprint('Test accuracy after fine tuning:', score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Convert\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Convert to Akida model\n\nWhen the quantized model produces satisfactory performance, it can be converted to the native\nAkida format. The [convert](../../api_reference/cnn2snn_apis.html#cnn2snn.convert)_ function\nreturns a model in Akida format ready for inference.\n\nAs with Keras, the summary() method provides a textual representation of the Akida model.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from cnn2snn import convert\n\nmodel_akida = convert(model_quantized)\nmodel_akida.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2. Check performance\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "accuracy = model_akida.evaluate(x_test, y_test)\nprint('Test accuracy after conversion:', accuracy)\n\n# For non-regression purposes\nassert accuracy > 0.96"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Show predictions for a single image\n\nDisplay one of the test images, such as the first image in the dataset from above, to visualize\nthe output of the model.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Test a single example\nsample_image = 0\nimage = x_test[sample_image]\noutputs = model_akida.predict(image.reshape(1, 28, 28, 1))\nprint('Input Label: %i' % y_test[sample_image])\n\nf, axarr = plt.subplots(1, 2)\naxarr[0].imshow(x_test[sample_image].reshape((28, 28)), cmap=cm.Greys_r)\naxarr[0].set_title('Class %d' % y_test[sample_image])\naxarr[1].bar(range(10), outputs.squeeze())\naxarr[1].set_xticks(range(10))\nplt.show()\n\nprint(outputs.squeeze())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Consider the output from the model above. As is typical in backprop-trained models, the final\nlayer is a Dense layer with one neuron for each of the 10 classes in the dataset. The goal of\ntraining is to maximize the response of the neuron corresponding to the label of each training\nsample while minimizing the responses of the other neurons.\n\nIn the bar chart above, you can see the outputs from all 10 neurons. It is easy to see that neuron\n7 responds much more strongly than the others. The first sample is indeed a number 7.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. GXNOR/MNIST\n\nA more robust model called GXNOR/MNIST is provided in [the model zoo](../../api_reference/akida_models_apis.html#akida_models.gxnor_mnist)_ It is inspired from the\n[GXNOR-Net paper](https://arxiv.org/pdf/1705.09283.pdf)_. It comes with its\n[pretrained 2/2/1 helper](../../api_reference/akida_models_apis.html#akida_models.gxnor_mnist_pretrained)_ for which the\nfloat training was done for 20 epochs, then the model was then gradually quantized following:\n4/4/4 --> 4/4/2 --> 2/2/2 --> 2/2/1 with a 15 epochs QAT step at each quantization stage.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}