{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# GXNOR/MNIST inference\n\nThe MNIST dataset is a handwritten digits database. It has a training\nset of 60,000 samples, and a test set of 10,000 samples. Each sample\ncomprises a 28x28 pixel image and an associated label.\n\nThis tutorial illustrates how to use a pre-trained model to process the MNIST\ndataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Dataset preparation\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\n\nfrom tensorflow.keras.datasets import mnist\n\n# Retrieve MNIST dataset\n_, (test_set, test_label) = mnist.load_data()\n\n# Add a dimension to images sets as akida expects 4 dimensions inputs\ntest_set = np.expand_dims(test_set, -1)\n\n# Display a few images from the test set\nf, axarr = plt.subplots(1, 4)\nfor i in range(0, 4):\n    axarr[i].imshow(test_set[i].reshape((28, 28)), cmap=cm.Greys_r)\n    axarr[i].set_title('Class %d' % test_label[i])\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Create a Keras GXNOR model\n\nThe GXNOR architecture is available in the `Akida models zoo\n<../../api_reference/akida_models_apis.html#akida_models.gxnor_mnist>`_ along\nwith pretrained weights.\n\n .. Note:: The pre-trained weights were obtained with knowledge distillation\n           training, using the EfficientNet model from `this repository\n           <https://github.com/EscVM/Efficient-CapsNet>`_ and the `Distiller`\n           class from the `knowledge distillation toolkit\n           <../../api_reference/akida_models_apis.html#knowledge-distillation>`_.\n\n           The float training was done for 30 epochs, the model is then\n           gradually quantized following:\n           8-4-4 --> 4-4-4 --> 4-4-2 --> 2-2-2 --> 2-2-1\n           by tuning the model at each step with the same distillation\n           training method for 5 epochs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from akida_models import gxnor_mnist_pretrained\n\nmodel_keras = gxnor_mnist_pretrained()\nmodel_keras.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Check Model performances\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\n\nmodel_keras.compile(optimizer='adam',\n                    loss=SparseCategoricalCrossentropy(from_logits=True),\n                    metrics=['accuracy'])\nkeras_accuracy = model_keras.evaluate(test_set, test_label, verbose=0)[1]\nprint(f\"Keras accuracy : {keras_accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Conversion to Akida\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Convert to Akida model\n\nWhen converting to an Akida model, we just need to pass the Keras model\nto `cnn2snn.convert <../../api_reference/cnn2snn_apis.html#convert>`_.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from cnn2snn import convert\n\nmodel_akida = convert(model_keras)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Check hardware compliancy\n\nThe `Model.summary <../../api_reference/akida_apis.html#akida.Model.summary>`__\nmethod provides a detailed description of the Model layers.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_akida.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3. Check performance\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n\n# Check performance against num_samples samples\nnum_samples = 10000\n\nresults = model_akida.predict(test_set[:num_samples])\naccuracy = accuracy_score(test_label[:num_samples], results[:num_samples])\n\n# For non-regression purpose\nassert accuracy > 0.99\n\n# Display results\nprint(\"Accuracy: \" + \"{0:.2f}\".format(100 * accuracy) + \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Depending on the number of samples you run, you should find a\nperformance of around 99% (99.20% if you run all 10000 samples).\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.4 Show predictions for a single image\n\nNow try processing a single image, say, the first image in the dataset\nthat we looked at above:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Test a single example\nsample_image = 0\nimage = test_set[sample_image]\noutputs = model_akida.evaluate(image.reshape(1, 28, 28, 1))\nprint('Input Label: %i' % test_label[sample_image])\n\nf, axarr = plt.subplots(1, 2)\naxarr[0].imshow(test_set[sample_image].reshape((28, 28)), cmap=cm.Greys_r)\naxarr[0].set_title('Class %d' % test_label[sample_image])\naxarr[1].bar(range(10), outputs.squeeze())\naxarr[1].set_xticks(range(10))\nplt.show()\n\nprint(outputs.squeeze())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Consider the output from the model, printed above. As is typical in\nbackprop trained models, the final layer here comprises a\n'fully-connected or 'dense' layer, with one neuron per class in the\ndata (here, 10). The goal of training is to maximize the response of the\nneuron corresponding to the label of each training sample, while\nminimizing the responses of the other neurons.\n\nIn the bar chart above, you can see the outputs from all 10 neurons. It\nis easy to see that neuron 7 responds much more strongly than the\nothers. The first sample is indeed a number 7.\n\nCheck this for some of the other samples by editing the value of\nsample_image in the script above (anything from 0 to 9999).\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}