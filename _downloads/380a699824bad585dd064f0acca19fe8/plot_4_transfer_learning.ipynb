{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Transfer learning with AkidaNet for PlantVillage\n\nThis tutorial presents how to perform transfer learning for quantized models\ntargetting Akida NSoC.\n\nThe transfer learning example is derived from the [Tensorflow tutorial](https://www.tensorflow.org/tutorials/images/transfer_learning)_ where the\nbase  model is an AkidaNet 0.5 quantized model trained on ImageNet and the\ntarget dataset is [PlantVillage](https://www.tensorflow.org/datasets/catalog/plant_village)_.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transfer learning process\n\nTransfer learning consists in customizing a pretrained model or feature\nextractor to fit another task.\n\n**Base model**\n\nThe base model is a quantized version of AkidaNet 0.5 that was trained on the\nImageNet dataset. Please refer to the [dedicated example](plot_1_akidanet_imagenet.html)_ for more information on the model\narchitecture and performances.\n\nLayers of this model have 4-bit weights (except for the first layer having\n8-bit weights) and activations are quantized to 4 bits.\n\n**Classification head**\n\nCustomization of the model happens by adding layers on top of the base model,\nwhich in AkidaNet case ends with a global average operation.\n\nThe classification head is typically composed of two dense layers as follows:\n\n  - the first dense layer number of units is configurable and depends on the\n    task but is generally 512 or below,\n  - a BatchNormalization operation and ReLU activation follow the first layer,\n  - a dropout layer is placed between the two dense layers to prevent\n    overfitting,\n  - the second dense layer is the prediction layer and should have its units\n    value set to the number of classes to predict,\n  - a softmax activation ends the model.\n\n**Training process**\n\nThe standard training process for transfer learning for AkidaNet is:\n\n  1. Get a trained AkidaNet base model\n  2. Add a float classification head to the model\n  3. Freeze the base model\n  4. Train for a few epochs\n  5. Quantize the classification head\n\nWhile this process will apply to most of the tasks, there might be cases where\nvariants are needed:\n\n  - for some target datasets, freezing the base model will not produce the\n    best accuracy. In such a case, the base model with stay trainable and the\n    learning rate when tuning the model should be small enough to preserve\n    features learned by the features extractor.\n  - quantization in the 5th step might lead to drop in accuracy. In such a\n    case, an additional step of fine tuning is needed and consists in training\n    for a few additional epochs with a lower learning rate (e.g 10 to 100\n    times lower than the initial rate) and with the base model unfrozen.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Dataset preparation\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\nimport tensorflow_datasets as tfds\n\n# Define task specific variables\nIMG_SIZE = 224\nBATCH_SIZE = 32\nCLASSES = 38\n\n# Load the tensorflow dataset\n(train_ds, validation_ds, test_ds), ds_info = tfds.load(\n    'plant_village',\n    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n    with_info=True,\n    as_supervised=True)\n\n# Visualize some data\n_ = tfds.show_examples(test_ds, ds_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Format test data\ndef format_example(image, label):\n    image = tf.cast(image, tf.float32)\n    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n    return image, label\n\n\ntest_batches = test_ds.map(format_example).batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Get a trained AkidaNet base model\n\nThe AkidaNet architecture is available in the Akida model zoo as\n[akidanet_imagenet](../../api_reference/akida_models_apis.html#akida_models.akidanet_imagenet).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from akida_models import akidanet_imagenet\nfrom keras.utils.data_utils import get_file\n\n# Create a quantized base model without top layers\nbase_model = akidanet_imagenet(input_shape=(IMG_SIZE, IMG_SIZE, 3),\n                               classes=CLASSES,\n                               alpha=0.5,\n                               include_top=False,\n                               pooling='avg',\n                               weight_quantization=4,\n                               activ_quantization=4,\n                               input_weight_quantization=8)\n\n# Get pretrained quantized weights and load them into the base model\npretrained_weights = get_file(\n    \"akidanet_imagenet_224_alpha_50_iq8_wq4_aq4.h5\",\n    \"http://data.brainchip.com/models/akidanet/akidanet_imagenet_224_alpha_50_iq8_wq4_aq4.h5\",\n    cache_subdir='models')\n\nbase_model.load_weights(pretrained_weights, by_name=True)\nbase_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Add a float classification head to the model\n\nAs explained in [section 1](#transfer-learning-process)_, the classification\nhead is defined as a dense layer with batch normalization and activation,\nwhich correspond to a [dense_block](../../api_reference/akida_models_apis.html#dense-block)_, followed by a\ndropout layer and a second dense layer.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from keras import Model\nfrom keras.layers import Activation, Dropout, Reshape, Flatten\nfrom akida_models.layer_blocks import dense_block\n\nx = base_model.output\nx = Flatten(name='flatten')(x)\nx = dense_block(x,\n                units=512,\n                name='fc1',\n                add_batchnorm=True,\n                add_activation=True)\nx = Dropout(0.5, name='dropout_1')(x)\nx = dense_block(x,\n                units=CLASSES,\n                name='predictions',\n                add_batchnorm=False,\n                add_activation=False)\nx = Activation('softmax', name='act_softmax')(x)\nx = Reshape((CLASSES,), name='reshape')(x)\n\n# Build the model\nmodel_keras = Model(base_model.input, x, name='akidanet_plantvillage')\n\nmodel_keras.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Freeze the base model\n\nFreezing can be done by setting the `trainable` attribute of a layer to False.\nFor convenience, a [freeze_model_before](../../api_reference/akida_models_apis.html#akida_models.training.freeze_model_before)_\nAPI is provided in akida_models. It allows to freeze all layers before the\nclassification head.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from akida_models.training import freeze_model_before\n\nfreeze_model_before(model_keras, 'flatten')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Train for a few epochs\n\nOnly giving textual information for training in this tutorial:\n\n  - the model is compiled with an Adam optimizer and the sparse categorical\n    crossentropy loss is used,\n  - the initial learning rate is set to 1e-2 and ends at 1e-4 with a linear\n    decay,\n  - the training lasts for 10 epochs.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Quantize the classification head\n\nQuantization is done using [cnn2snn.quantize](../../api_reference/cnn2snn_apis.html#quantize)_.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from cnn2snn import quantize\n\n# Quantize weights and activation to 4 bits, first layer weights to 8 bits\nmodel_quantized = quantize(model_keras, 4, 4, 8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After this step the model is trained and ready for use after for conversion to\nAkida (no extra fine tuning step needed for this task).\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Compute accuracy\n\nBecause training is not included in this tutorial, the pretrained Keras model\nis retrieved from the zoo.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from akida_models import akidanet_plantvillage_pretrained\nfrom akida_models.training import evaluate_model\n\nmodel = akidanet_plantvillage_pretrained()\n\n# Evaluate Keras accuracy\nmodel.compile(metrics=['accuracy'])\nevaluate_model(model, test_batches)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Convert the model and evaluate the Akida model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom cnn2snn import convert\nfrom akida_models.training import evaluate_akida_model\n\nmodel_akida = convert(model)\n\npreds, labels = evaluate_akida_model(model_akida, test_batches, 'softmax')\naccuracy = (np.squeeze(np.argmax(preds, 1)) == labels).mean()\n\nprint(f\"Akida accuracy: {accuracy}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}