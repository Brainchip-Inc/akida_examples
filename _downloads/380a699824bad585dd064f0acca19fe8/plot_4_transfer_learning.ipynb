{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Transfer learning with AkidaNet for PlantVillage\n\nThis tutorial presents how to perform transfer learning for quantized models targeting an Akida\naccelerator.\n\nThe transfer learning example is derived from the [Tensorflow tutorial](https://www.tensorflow.org/tutorials/images/transfer_learning)_ where the\nbase model is an AkidaNet 0.5 quantized model trained on ImageNet and the\ntarget dataset is [PlantVillage](https://www.tensorflow.org/datasets/catalog/plant_village)_.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transfer learning process\n\nTransfer learning consists in customizing a pretrained model or feature\nextractor to fit another task.\n\n**Base model**\n\nThe base model is an AkidaNet 0.5 that was trained on the\nImageNet dataset. Please refer to the [dedicated example](plot_1_akidanet_imagenet.html)_ for more information on the model\narchitecture and performance.\n\n**Classification head**\n\nCustomization of the model happens by adding layers on top of the base model,\nwhich in AkidaNet case ends with a global average operation.\n\nThe classification head is typically composed of two dense layers as follows:\n\n  - the first dense layer number of units is configurable and depends on the\n    task but is generally 512 or below,\n  - a BatchNormalization operation and ReLU activation follow the first layer,\n  - a dropout layer is placed between the two dense layers to prevent\n    overfitting,\n  - the second dense layer is the prediction layer and should have its units\n    value set to the number of classes to predict,\n  - a softmax activation ends the model.\n\n**Training process**\n\nThe standard training process for transfer learning for AkidaNet is:\n\n  1. Get a trained float AkidaNet base model\n  2. Add a classification head to the model\n  3. Tune the whole model for a few epochs\n  4. Quantize the whole model\n  5. Optionally perform QAT for a few epochs to recover accuracy\n\nThe transfer learning process operates in float precision,\nensuring seamless integration with users' existing familiarity in setting\nhyperparameters, adding a new head, and deciding on layer freezing.\n\nWhile this process will apply to most of the tasks, there might be cases where\nvariants are needed:\n\n  - Quantization in the 4th step might lead to drop in accuracy (especially for 4\n    bits quantization). In such a case, an additional step of fine tuning is needed\n    and consists in training for a few additional epochs with a lower learning rate\n    (e.g 10 to 100 times lower than the initial rate).\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Dataset preparation\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\nimport tensorflow_datasets as tfds\n\n# Define task specific variables\nIMG_SIZE = 224\nBATCH_SIZE = 32\nCLASSES = 38\n\n# Load the tensorflow dataset\n(train_ds, validation_ds, test_ds), ds_info = tfds.load(\n    'plant_village',\n    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n    with_info=True,\n    as_supervised=True)\n\n# Visualize some data\n_ = tfds.show_examples(test_ds, ds_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Format test data\ndef format_example(image, label):\n    image = tf.cast(image, tf.float32)\n    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n    return image, label\n\n\ntest_batches = test_ds.map(format_example).batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Get a trained AkidaNet base model\n\nThe AkidaNet architecture is available in the Akida model zoo as\n[akidanet_imagenet](../../api_reference/akida_models_apis.html#akida_models.akidanet_imagenet).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from akida_models import fetch_file, akidanet_imagenet\n\n# Create a base model without top layers\nbase_model = akidanet_imagenet(input_shape=(IMG_SIZE, IMG_SIZE, 3),\n                               classes=CLASSES,\n                               alpha=0.5,\n                               include_top=False,\n                               pooling='avg')\n\n# Get pretrained quantized weights and load them into the base model\npretrained_weights = fetch_file(\n    origin=\"https://data.brainchip.com/models/AkidaV2/akidanet/akidanet_imagenet_224_alpha_0.5.h5\",\n    fname=\"akidanet_imagenet_224_alpha_0.5.h5\",\n    cache_subdir='models')\n\nbase_model.load_weights(pretrained_weights, by_name=True)\nbase_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Add a classification head to the model\n\nAs explained in [section 1](#transfer-learning-process)_, the classification\nhead is defined as a dense layer with batch normalization and activation,\nwhich correspond to a [dense_block](../../api_reference/akida_models_apis.html#akida_models.layer_blocks.dense_block)_, followed by\na dropout layer and a second dense layer.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from keras import Model\nfrom keras.layers import Activation, Dropout, Reshape\nfrom akida_models.layer_blocks import dense_block\n\nx = base_model.output\nx = dense_block(x,\n                units=512,\n                name='fc1',\n                add_batchnorm=True,\n                relu_activation='ReLU7.5')\nx = Dropout(0.5, name='dropout_1')(x)\nx = dense_block(x,\n                units=CLASSES,\n                name='predictions',\n                add_batchnorm=False,\n                relu_activation=False)\nx = Activation('softmax', name='act_softmax')(x)\nx = Reshape((CLASSES,), name='reshape')(x)\n\n# Build the model\nmodel_keras = Model(base_model.input, x, name='akidanet_plantvillage')\n\nmodel_keras.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train for a few epochs\n\nOnly giving textual information for training in this tutorial:\n\n  - the model is compiled with an Adam optimizer and the sparse categorical\n    crossentropy loss is used,\n  - the initial learning rate is set to 1e-3 and ends at 1e-5 with an exponential decay,\n  - the training lasts for 10 epochs.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Quantize the model\n\nQuantization is done using QuantizeML [quantize](../../api_reference/quantizeml_apis.html#quantizeml.models.quantize)_.\n\nIn order to get the best possible model, calibration samples should be provided to the model.\nUsing here samples from the train set.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from quantizeml.models import quantize\nfrom quantizeml.layers import QuantizationParams\n\ntrain_batches = train_ds.map(format_example).batch(BATCH_SIZE)\n\n# Prepare a quantization scheme: first layer weights to 8-bit, other weights and activation to 4-bit\nqparams = QuantizationParams(input_weight_bits=8, weight_bits=4, activation_bits=4)\n\n# Quantize the model, using the 1024 calibration samples from the train set and calibrate over 2\n# epochs with a batch_size of 100.\nmodel_quantized = quantize(model_keras, qparams=qparams,\n                           samples=train_batches, epochs=2, batch_size=BATCH_SIZE, num_samples=1024)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To recover the loss of accuracy introduced with 4-bit quantization, an extra QAT step with a lower\nlearning rate (training rate divided by 10) is required. Note that you could also aim for 8-bit\nquantization and not require this extra QAT step.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Compute accuracy\n\nBecause training is not included in this tutorial, the pretrained Keras model\nis retrieved from the zoo.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from akida_models import akidanet_plantvillage_pretrained\n\nmodel = akidanet_plantvillage_pretrained()\n\n# Evaluate Keras accuracy\nmodel.compile(metrics=['accuracy'])\nhistory = model.evaluate(test_batches, verbose=0)\nprint('Keras accuracy: ', history[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Convert the model and evaluate the Akida model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom cnn2snn import convert\n\nmodel_akida = convert(model)\n\n# Manual evaluation loop to retrieve activations and labels\nlabels, logits = None, None\nfor batch, label_batch in test_batches:\n    logits_batch = model_akida.predict(batch.numpy().astype('uint8'))\n\n    if labels is None:\n        labels = label_batch\n        logits = logits_batch.squeeze(axis=(1, 2))\n    else:\n        labels = np.concatenate((labels, label_batch))\n        logits = np.concatenate((logits, logits_batch.squeeze(axis=(1, 2))))\npreds = Activation(\"softmax\")(logits)\naccuracy = (np.argmax(preds, 1) == labels).mean()\n\nprint(f\"Akida accuracy: {accuracy}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}