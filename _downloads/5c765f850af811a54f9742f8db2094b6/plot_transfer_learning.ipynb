{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nTransfer learning with MobileNet for cats vs. dogs\n==================================================\n\nThis tutorial presents a demonstration of transfer learning and the\nconversion to an Akida model of a quantized Keras network.\n\nThe transfer learning example is derived from the `Tensorflow\ntutorial <https://www.tensorflow.org/tutorials/images/transfer_learning>`__:\n\n    * Our base model is an Akida-compatible version of **MobileNet v1**,\n      trained on ImageNet.\n    * The new dataset for transfer learning is **cats vs. dogs**\n      (`link <https://www.tensorflow.org/datasets/catalog/cats_vs_dogs>`__).\n    * We use transfer learning to customize the model to the new task of\n      classifying cats and dogs.\n\n.. Note:: This tutorial only shows the inference of the trained Keras\n          model and its conversion to an Akida network. A textual explanation\n          of the training is given below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Transfer learning process\n----------------------------\n.. figure:: https://s2.qwant.com/thumbr/0x380/7/0/7b7386531ea24ab1294fdf9b8698b008a51e38a3c57e81427fbef626ff226c/1*6ACbDsBMeDZcLg9W8CFT_Q.png?u=https%3A%2F%2Fcdn-images-1.medium.com%2Fmax%2F1600%2F1%2A6ACbDsBMeDZcLg9W8CFT_Q.png&q=0&b=1&p=0&a=1\n   :alt: transfer_learning_image\n   :target: https://s2.qwant.com/thumbr/0x380/7/0/7b7386531ea24ab1294fdf9b8698b008a51e38a3c57e81427fbef626ff226c/1*6ACbDsBMeDZcLg9W8CFT_Q.png?u=https%3A%2F%2Fcdn-images-1.medium.com%2Fmax%2F1600%2F1%2A6ACbDsBMeDZcLg9W8CFT_Q.png&q=0&b=1&p=0&a=1\n   :align: center\n\nTransfer learning allows to classify on a specific task by using a\npre-trained base model. For an introduction to transfer learning, please\nrefer to the `Tensorflow\ntutorial <https://www.tensorflow.org/tutorials/images/transfer_learning>`__\nbefore exploring this tutorial. Here, we focus on how to quantize the\nKeras model in order to convert it to an Akida one.\n\nThe model is composed of:\n\n  * a base quantized MobileNet model used to extract image features\n  * a top layer to classify cats and dogs\n  * a sigmoid activation function to interpret model outputs as a probability\n\n**Base model**\n\nThe base model is an Akida-compatible version of MobileNet v1. This\nmodel was trained and quantized using the ImageNet dataset. Please refer\nto the corresponding `example <plot_mobilenet_imagenet.html>`__ for\nmore information. The layers have 4-bit weights (except for the first\nlayer having 8-bit weights) and the activations are quantized to 4 bits.\nThis base model ends with a global average pooling whose output is (1,\n1, 1024).\n\nIn our transfer learning process, the base model is frozen, i.e., the\nweights are not updated during training. Pre-trained weights for the\nquantized model are provided on\n`<http://data.brainchip.com/models/mobilenet/>`__. These are\nloaded in our frozen base model.\n\n**Top layer**\n\nWhile the Tensorflow tutorial uses a fully-connected top layer with one\noutput neuron, the only Akida layer supporting 4-bit weights is a separable\nconvolutional layer (see `hardware compatibility\n<../user_guide/hw_constraints.html>`__).\n\nWe thus decided to use a separable convolutional layer with one output\nneuron for the top layer of our model.\n\n**Final activation**\n\nReLU6 is the only activation function that can be converted into an Akida SNN\nequivalent. The converted Akida model doesn't therefore include the 'sigmoid'\nactivation, and we must instead apply it explicitly on the raw values returned\nby the model Top layer.\n\n**Training steps**\n\nThe transfer learning process consists in two training phases:\n\n  1. **Float top layer training**: The base model is quantized using 4-bit\n     weights and activations. Pre-trained 4-bit weights of MobileNet/ImageNet\n     are loaded. Then a top layer is added with float weights. The base model\n     is frozen and the training is only applied on the top layer. After 10\n     epochs, the weights are saved. Note that the weights of the layers of\n     the frozen base model haven't changed; only those of the top layer are\n     updated.\n  2. **4-bit top layer training**: The base model is still\n     quantized using 4-bit weights and activations. The added top layer is\n     now quantized (4-bit weights). The weights saved at step 1 are used as\n     initialization. The base model is frozen and the training is only\n     applied on the top layer. After 10 epochs, the new quantized weights are\n     saved. This final weights are those used in the inference below.\n\n+----------+-------------------+------------+---------+------------+----+\n| Training | Frozen base model | Init.      | Top     | Init.      | E  |\n| step     |                   | weights    | layer   | weights    | p  |\n|          |                   | base model |         | top layer  | o  |\n|          |                   |            |         |            | c  |\n|          |                   |            |         |            | h  |\n|          |                   |            |         |            | s  |\n+==========+===================+============+=========+============+====+\n| step 1   | 4-bit weights /   | pre-trained| float   | random     | 10 |\n|          | activations       | 4-bit      | weights |            |    |\n|          |                   |            |         |            |    |\n+----------+-------------------+------------+---------+------------+----+\n| step 2   | 4-bit weights /   | pre-trained| 4-bit   | saved from | 10 |\n|          | activations       | 4-bit      | weights | step 1     |    |\n|          |                   |            |         |            |    |\n+----------+-------------------+------------+---------+------------+----+\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport matplotlib.pyplot as plt\n\nfrom akida_models import mobilenet_imagenet\nfrom cnn2snn import convert\nfrom akida_models.quantization_blocks import separable_conv_block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Load and preprocess data\n---------------------------\n\nIn this section, we will load the 'cats_vs_dogs' dataset preprocess\nthe data to match the required model's inputs:\n\n  * **2.A - Load and split data**: we only keep the test set which represents\n    10% of the dataset.\n  * **2.B - Preprocess the test set** by resizing and rescaling the images.\n  * **2.C - Get labels**\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2.A - Load and split data\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe ``cats_vs_dogs``\n`dataset <https://www.tensorflow.org/datasets/catalog/cats_vs_dogs>`__\nis loaded and split into train, validation and test sets. The train and\nvalidation sets were used for the transfer learning process. Here only\nthe test set is used. We use here ``tf.Dataset`` objects to load and\npreprocess batches of data (one can look at the TensorFlow guide\n`here <https://www.tensorflow.org/guide/data>`__ for more information).\n\n.. Note:: The ``cats_vs_dogs`` dataset version used here is 2.0.1.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "SPLIT_WEIGHTS = (8, 1, 1)\nsplits = tfds.Split.TRAIN.subsplit(weighted=SPLIT_WEIGHTS)\n\ntfds.disable_progress_bar()\n(raw_train, raw_validation,\n raw_test), metadata = tfds.load('cats_vs_dogs:2.0.1',\n                                 split=list(splits),\n                                 with_info=True,\n                                 as_supervised=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2.B - Preprocess the test set\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nWe must apply the same preprocessing as for training: rescaling and\nresizing. Since Akida models directly accept integer-valued images, we\nalso define a preprocessing function for Akida:\n\n  - for Keras: images are rescaled between 0 and 1, and resized to 160x160\n  - for Akida: images are only resized to 160x160 (uint8 values).\n\nKeras and Akida models require 4-dimensional (N,H,W,C) arrays as inputs.\nWe must then create batches of images to feed the model. For inference,\nthe batch size is not relevant; you can set it such that the batch of\nimages can be loaded in memory depending on your CPU/GPU.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "IMG_SIZE = 160\ninput_scaling = (127.5, 127.5)\n\n\ndef format_example_keras(image, label):\n    image = tf.cast(image, tf.float32)\n    image = (image - input_scaling[1]) / input_scaling[0]\n    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n    return image, label\n\n\ndef format_example_akida(image, label):\n    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n    image = tf.cast(image, tf.uint8)\n    return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\ntest_batches_keras = raw_test.map(format_example_keras).batch(BATCH_SIZE)\ntest_batches_akida = raw_test.map(format_example_akida).batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2.C - Get labels\n~~~~~~~~~~~~~~~~\n\nLabels are contained in the test set as '0' for cats and '1' for dogs.\nWe read through the batches to extract the labels.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "labels = np.array([])\nfor _, label_batch in test_batches_keras:\n    labels = np.concatenate((labels, label_batch))\n\nget_label_name = metadata.features['label'].int2str\nnum_images = labels.shape[0]\n\nprint(f\"Test set composed of {num_images} images: \"\n      f\"{np.count_nonzero(labels==0)} cats and \"\n      f\"{np.count_nonzero(labels==1)} dogs.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Convert a quantized Keras model to Akida\n-------------------------------------------\n\nIn this section, we will instantiate a quantized Keras model based on\nMobileNet and modify the last layers to specify the classification for\n``cats_vs_dogs``. After loading the pre-trained weights, we will convert\nthe Keras model to Akida.\n\nThis section goes as follows:\n\n  * **3.A - Instantiate a Keras base model**\n  * **3.B - Modify the network and load pre-trained weights**\n  * **3.C - Convert to Akida**\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3.A - Instantiate a Keras base model\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nHere, we instantiate a quantized Keras model based on a MobileNet model.\nThis base model was previously trained using the 1000 classes of the\nImageNet dataset. For more information, please see the `ImageNet\ntutorial <plot_mobilenet_imagenet.html>`__.\n\nThe quantized MobileNet model satisfies the Akida NSoC requirements:\n\n  * The model relies on a convolutional layer (first layer) and separable\n    convolutional layers, all being Akida-compatible.\n  * All the separable convolutional layers have 4-bit weights, the first\n    convolutional layer has 8-bit weights.\n  * The activations are quantized with 4 bits.\n\nUsing the provided quantized MobileNet model, we create an instance\nwithout the top classification layer ('include_top=False').\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "base_model_keras = mobilenet_imagenet(input_shape=(IMG_SIZE, IMG_SIZE, 3),\n                                      include_top=False,\n                                      pooling='avg',\n                                      weight_quantization=4,\n                                      activ_quantization=4,\n                                      input_weight_quantization=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3.B - Modify the network and load pre-trained weights\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nAs explained in `section 1 <plot_cats_vs_dogs_cnn2akida_demo.html#transfer-learning-process>`__,\nwe add a separable convolutional layer as top layer with one output neuron.\nThe new model is now appropriate for the ``cats_vs_dogs`` dataset and is\nAkida-compatible. Note that a sigmoid activation is added at the end of\nthe model: the output neuron returns a probability between 0 and 1 that\nthe input image is a dog.\n\nThe transfer learning process has been run internally and the weights have\nbeen saved. In this tutorial, the pre-trained weights are loaded for inference\nand conversion.\n\n.. Note:: The pre-trained weights which are loaded corresponds to the\n          quantization parameters described as above. If you want to modify\n          these parameters, you must re-train the model and save weights.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Add a top layer for classification\nx = base_model_keras.output\nx = tf.keras.layers.Reshape((1, 1, 1024), name='reshape_1')(x)\nx = separable_conv_block(x,\n                         filters=1,\n                         kernel_size=(3, 3),\n                         padding='same',\n                         use_bias=False,\n                         name='top_layer_separable',\n                         weight_quantization=4,\n                         activ_quantization=None)\nx = tf.keras.layers.Activation('sigmoid')(x)\npreds = tf.keras.layers.Reshape((1,), name='reshape_2')(x)\nmodel_keras = tf.keras.Model(inputs=base_model_keras.input,\n                             outputs=preds,\n                             name=\"model_cats_vs_dogs\")\n\nmodel_keras.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Load pre-trained weights\npretrained_weights = tf.keras.utils.get_file(\n    \"mobilenet_cats_vs_dogs_wq4_aq4.h5\",\n    \"http://data.brainchip.com/models/mobilenet/mobilenet_cats_vs_dogs_wq4_aq4.h5\",\n    cache_subdir='models/mobilenet')\nmodel_keras.load_weights(pretrained_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3.C - Convert to Akida\n~~~~~~~~~~~~~~~~~~~~~~\n\nThe new Keras model with pre-trained weights is now converted to an\nAkida model. It only requires the quantized Keras model and the inputs\nscaling used during training.\nNote: the 'sigmoid' activation has no SNN equivalent and will be simply\nignored during the conversion.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_akida = convert(model_keras, input_scaling=input_scaling)\n\nmodel_akida.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. Classify test images\n-----------------------\n\nThis section gives a comparison of the results between the quantized\nKeras and the Akida models. It goes as follows:\n\n  * **4.A - Classify test images** with the quantized Keras and the Akida\n    models\n  * **4.B - Compare results**\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.A Classify test images\n~~~~~~~~~~~~~~~~~~~~~~~~\n\nHere, we will predict the classes of the test images using the quantized\nKeras model and the converted Akida model. Remember that:\n\n  * Input images in Keras and Akida are not scaled in the same range, be\n    careful to use the correct inputs: uint8 images for Akida and float\n    rescaled images for Keras.\n  * The ``predict`` function of tf.keras can take a ``tf.data.Dataset``\n    object as argument. However, the Akida `evaluate <../api_reference/aee_apis.html#akida.Model.evaluate>`__\n    function takes a NumPy array containing the images. Though the Akida\n    `predict <../api_reference/aee_apis.html#akida.Model.predict>`__\n    function exists, it outputs a class label and not the raw predictions.\n  * The Keras ``predict`` function returns the probability to be a dog:\n    if the output is greater than 0.5, the model predicts a 'dog'. However,\n    the Akida `evaluate <../api_reference/aee_apis.html#akida.Model.evaluate>`__\n    function directly returns the potential before the 'sigmoid' activation, which has\n    no SNN equivalent. We must therefore apply it explicitly on the model outputs to obtain\n    the Akida probabilities.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Classify test images with the quantized Keras model\nfrom timeit import default_timer as timer\n\nstart = timer()\npots_keras = model_keras.predict(test_batches_keras)\nend = timer()\n\npreds_keras = pots_keras.squeeze() > 0.5\nprint(f\"Keras inference on {num_images} images took {end-start:.2f} s.\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Classify test images with the Akida model\nfrom progressbar import ProgressBar\nn_batches = num_images // BATCH_SIZE + 1\npbar = ProgressBar(maxval=n_batches)\ni = 1\npbar.start()\nstart = timer()\npots_akida = np.array([], dtype=np.float32)\nfor batch, _ in test_batches_akida:\n    pots_batch_akida = model_akida.evaluate(batch.numpy())\n    pots_akida = np.concatenate((pots_akida, pots_batch_akida.squeeze()))\n    pbar.update(i)\n    i = i + 1\npbar.finish()\nend = timer()\n\npreds_akida = tf.keras.layers.Activation('sigmoid')(pots_akida) > 0.5\nprint(f\"Akida inference on {num_images} images took {end-start:.2f} s.\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Print model statistics\nprint(\"Model statistics\")\nstats = model_akida.get_statistics()\nbatch, _ = iter(test_batches_akida).get_next()\nmodel_akida.evaluate(batch[:20].numpy())\nfor _, stat in stats.items():\n    print(stat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.B Compare results\n~~~~~~~~~~~~~~~~~~~\n\nThe Keras and Akida accuracies are compared and the Akida confusion\nmatrix is given (the quantized Keras confusion matrix is almost\nidentical to the Akida one). Note that there is no exact equivalence\nbetween the quantized Keras and the Akida models. However, the\naccuracies are highly similar.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Compute accuracies\nn_good_preds_keras = np.sum(np.equal(preds_keras, labels))\nn_good_preds_akida = np.sum(np.equal(preds_akida, labels))\n\nkeras_accuracy = n_good_preds_keras / num_images\nakida_accuracy = n_good_preds_akida / num_images\n\nprint(f\"Quantized Keras accuracy: {keras_accuracy*100:.2f} %  \"\n      f\"({n_good_preds_keras} / {num_images} images)\")\nprint(f\"Akida accuracy:           {akida_accuracy*100:.2f} %  \"\n      f\"({n_good_preds_akida} / {num_images} images)\")\n\n# For non-regression purpose\nassert akida_accuracy > 0.97"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def confusion_matrix_2classes(labels, predictions):\n    tp = np.count_nonzero(labels + predictions == 2)\n    tn = np.count_nonzero(labels + predictions == 0)\n    fp = np.count_nonzero(predictions - labels == 1)\n    fn = np.count_nonzero(labels - predictions == 1)\n\n    return np.array([[tp, fn], [fp, tn]])\n\n\ndef plot_confusion_matrix_2classes(cm, classes):\n    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.xticks([0, 1], classes)\n    plt.yticks([0, 1], classes)\n\n    for i, j in zip([0, 0, 1, 1], [0, 1, 0, 1]):\n        plt.text(j,\n                 i,\n                 f\"{cm[i, j]:.2f}\",\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > cm.max() / 2. else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.autoscale()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Plot confusion matrix for Akida\ncm_akida = confusion_matrix_2classes(labels, preds_akida.numpy())\nprint(\"Confusion matrix quantized Akida:\")\nplot_confusion_matrix_2classes(cm_akida, ['dog', 'cat'])\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}