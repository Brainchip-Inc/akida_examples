{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Off-the-shelf models quantization\n\n.. Warning::\n   QuantizeML ONNX quantization is an **evolving feature**. Some models may not be compatible.\n\n| The [Global Akida workflow](../general/plot_0_global_workflow.html)_ and the\n  [PyTorch to Akida workflow](../general/plot_8_global_pytorch_workflow.html)_ guides\n  describe all the steps required to create, train, quantize and convert a model for Akida,\n  respectively using TensorFlow/Keras and PyTorch frameworks.\n| Here we will illustrate off-the-shelf/pretrained CNN models quantization for Akida using\n  [MobileNet V2](https://huggingface.co/docs/transformers/model_doc/mobilenet_v2)_ from\n  the [Hugging Face Hub](https://huggingface.co/docs/hub/index)_.\n\n.. Note::\n   | Off-the-shelf CNN models refer to already trained floating point models.\n   | Their training recipe and framework have no importance as long as they can be exported\n     to [ONNX](https://onnx.ai)_.\n   | Note however that this pathway offers slightly less flexibility than our default,\n     TensorFlow-based pathway - specifically, fine tuning of the quantized model is\n     not possible.\n   | In most cases, that won't matter, there should be almost no performance drop when\n     quantizing to 8-bit anyway.\n\n.. Note::\n   | This tutorial leverages the [Optimum toolkit](https://huggingface.co/docs/optimum/main/en/exporters/onnx/usage_guides/export_a_model)_,\n     an external tool, based on [PyTorch](https://pytorch.org/)_, that allows models direct\n     download and export to  ONNX.\n\n```\npip install optimum[exporters]\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Workflow overview\n\n.. figure:: ../../img/off_the_shelf_flow.png\n   :target: ../../_images/off_the_shelf_flow.png\n   :alt: Off-the-shelf models quantization flow\n   :scale: 60 %\n   :align: center\n\n   Off-the-shelf CNN models Akida workflow\n\nAs shown in the figure above, the [QuantizeML toolkit](../../api_reference/quantizeml_apis.html)_ allows the Post Training Quantization of ONNX\nmodels.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data preparation\n\nGiven that the reference model was trained on [ImageNet](https://www.image-net.org/)_ dataset\n(which is not publicly available), this tutorial used a subset of 10 copyright free images.\nA helper function ``imagenet.preprocessing.get_preprocessed_samples`` loads\nand preprocesses (decodes, crops and extracts a square 224x224x3 patch from an input image)\nthese images.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom akida_models.imagenet import get_preprocessed_samples\n\n# Model specification and hyperparameters\nNUM_CHANNELS = 3\nIMAGE_SIZE = 224\n\n# Load the preprocessed images and their corresponding labels for the test set\nx_test_raw, labels_test = get_preprocessed_samples(IMAGE_SIZE, NUM_CHANNELS)\nnum_images = x_test_raw.shape[0]\n\n# Get labels for the test set by index\n# Note: Hugging Face models reserve the first index to null predictions\n# (labeled as 'background' id). That is why we increase in '1' the original label id.\nlabels_test = labels_test + 1\n\nprint(f'{num_images} images and their labels are loaded and preprocessed.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As illustrated in `1. Workflow overview`_, the model's source is at the user's\ndiscretion. Here, we know a priori that MobileNet V2 was trained with\nimages normalized within [-1, 1] interval. Also, ONNX models are usually\nsaved with a `channels-first` format, input images are expected to be passed\nwith the channels dimension on `axis = 1`.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Project images in the range [-1, 1]\nx_test = (x_test_raw / 127.5 - 1).astype('float32')\n\n# Transpose the channels to the first axis\nx_test = np.transpose(x_test, (0, 3, 1, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Download and export\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1. Download ONNX MobileNet V2\n\nThere are many repositories with models saved in ONNX format. In this example the\n[Optimum API](https://huggingface.co/docs/optimum/main/en/exporters/onnx/usage_guides/export_a_model)_\nis used for downloading and exporting models to ONNX.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from optimum.exporters.onnx import main_export\n\n# Download and convert MobiletNet V2 to ONNX\nmain_export(model_name_or_path=\"google/mobilenet_v2_1.0_224\",\n            task=\"image-classification\",\n            output=\"./\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import onnx\n\n# Load the model in memory\nmodel_onnx = onnx.load_model(\"./model.onnx\")\nprint(onnx.helper.printable_graph(model_onnx.graph))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2. Evaluate model performances\n\nThe [ONNXRuntime](https://onnxruntime.ai)_ package is a cross-platform\naccelerator capable of loading and running models described in ONNX format.\nWe take advantage of this framework to run the ONNX models and evaluate\ntheir performances.\n\n.. Note:: We only compute accuracy on 10 images.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from onnxruntime import InferenceSession\n\n\ndef evaluate_onnx_model(model):\n    sess = InferenceSession(model.SerializeToString())\n    # Calculate outputs by running images through the session\n    outputs = sess.run(None, {model.graph.input[0].name: x_test})\n    # The class with the highest score is what we choose as prediction\n    predicted = np.squeeze(np.argmax(outputs[0], 1))\n    # Compute the number of valid predictions\n    return int((predicted == labels_test).sum())\n\n\ncorrectly_classified_floating = evaluate_onnx_model(model_onnx)\nprint(f'Floating point model accuracy: {correctly_classified_floating}/{num_images}.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Quantize\n\n| Akida processes integer activations and weights. Therefore, the floating point model\n  must be quantized in preparation to run on an Akida accelerator.\n| [QuantizeML quantize()](../../api_reference/quantizeml_apis.html#quantizeml.models.quantize)_\n  function recognizes [ModelProto](https://onnx.ai/onnx/api/classes.html#modelproto)_ objects\n  and quantizes them for Akida. The result is another ``ModelProto``, compatible with the\n  [CNN2SNN Toolkit](../../user_guide/cnn2snn.html)_.\n| The table below summarizes the obtained accuracy at the various stages using the full\n  ImageNet dataset.\n\n+-------------------------------+----------------+--------------------+----------------+\n| Calibration parameters        | Float accuracy | Quantized accuracy | Akida accuracy |\n+===============================+================+====================+================+\n| Random samples / per-tensor   | 71.790         | 70.550             | 70.588         |\n+-------------------------------+----------------+--------------------+----------------+\n| Imagenet samples / per-tensor | 71.790         | 70.472             | 70.628         |\n+-------------------------------+----------------+--------------------+----------------+\n\n.. Note::\n   Please refer to the [QuantizeML toolkit user guide](../../user_guide/quantizeml.html)_\n   and the [Advanced QuantizeML tutorial](plot_0_advanced_quantizeml.html)_ for details\n   about quantization parameters.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from quantizeml.models import quantize, QuantizationParams\n\n# Quantize with activations quantized per tensor\nqparams = QuantizationParams(per_tensor_activations=True)\nmodel_quantized = quantize(model_onnx, qparams=qparams, num_samples=5)\n\n# Evaluate the quantized model performance\ncorrectly_classified = evaluate_onnx_model(model_quantized)\nprint(f'Quantized model accuracy: {correctly_classified_floating}/{num_images}.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. Note:: Once the model is quantized, the [convert](../../api_reference/cnn2snn_apis.html#cnn2snn.convert)_\n   function must be used to retrieve a model in Akida format ready for inference. Please\n   refer to the [PyTorch to Akida workflow](../general/plot_8_global_pytorch_workflow.html)_ for further details.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}