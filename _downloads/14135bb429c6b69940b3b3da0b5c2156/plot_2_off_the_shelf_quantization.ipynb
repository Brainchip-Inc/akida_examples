{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Off-the-shelf models quantization\n\n.. Warning::\n   QuantizeML ONNX quantization is an **evolving feature**. Some models may not be compatible.\n\n| The [Global Akida workflow](../general/plot_0_global_workflow.html)_ and the\n  [PyTorch to Akida workflow](../general/plot_8_global_pytorch_workflow.html)_ guides\n  describe all the steps required to create, train, quantize and convert a model for Akida,\n  respectively using TensorFlow/Keras and PyTorch frameworks.\n| Here we will illustrate off-the-shelf/pretrained CNN models quantization for Akida using\n  [MobileNet V2](https://huggingface.co/docs/transformers/model_doc/mobilenet_v2)_ from\n  the [Hugging Face Hub](https://huggingface.co/docs/hub/index)_.\n\n.. Note::\n   | Off-the-shelf CNN models refer to already trained floating point models.\n   | Their training recipe and framework have no importance as long as they can be exported\n     to [ONNX](https://onnx.ai)_.\n   | Note however that this pathway offers slightly less flexibility than our default,\n     TensorFlow-based pathway - specifically, fine tuning of the quantized model is\n     not possible.\n   | In most cases, that won't matter, there should be almost no performance drop when\n     quantizing to 8-bit anyway.\n\n.. Note::\n   | This tutorial leverages the [Optimum toolkit](https://huggingface.co/docs/optimum/main/en/exporters/onnx/usage_guides/export_a_model)_,\n     an external tool, based on [PyTorch](https://pytorch.org/)_, that allows models direct\n     download and export to  ONNX.\n\n```\npip install optimum[exporters]\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Workflow overview\n\n.. figure:: ../../img/off_the_shelf_flow.png\n   :target: ../../_images/off_the_shelf_flow.png\n   :alt: Off-the-shelf models quantization flow\n   :scale: 60 %\n   :align: center\n\n   Off-the-shelf CNN models Akida workflow\n\nAs shown in the figure above, the [QuantizeML toolkit](../../api_reference/quantizeml_apis.html)_ allows the Post Training Quantization of ONNX\nmodels.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data preparation\n\nGiven that the reference model was trained on [ImageNet](https://www.image-net.org/)_ dataset\n(which is not publicly available), this tutorial used a subset of 10 copyright free images.\nSee [data preparation](../general/plot_1_akidanet_imagenet.html#dataset-preparation)_\nfor more details.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport csv\nimport numpy as np\n\nfrom tensorflow.io import read_file\nfrom tensorflow.image import decode_jpeg\nfrom tensorflow.keras.utils import get_file\n\nfrom akida_models.imagenet import preprocessing\n\n# Model specification and hyperparameters\nNUM_CHANNELS = 3\nIMAGE_SIZE = 224\n\nnum_images = 10\n\n# Retrieve dataset file from Brainchip data server\nfile_path = get_file(\n    \"imagenet_like.zip\",\n    \"https://data.brainchip.com/dataset-mirror/imagenet_like/imagenet_like.zip\",\n    cache_subdir='datasets/imagenet_like',\n    extract=True)\ndata_folder = os.path.dirname(file_path)\n\n# Load images for test set\nx_test_files = []\nx_test_raw = np.zeros((num_images, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)).astype('uint8')\nfor id in range(num_images):\n    test_file = 'image_' + str(id + 1).zfill(2) + '.jpg'\n    x_test_files.append(test_file)\n    img_path = os.path.join(data_folder, test_file)\n    base_image = read_file(img_path)\n    image = decode_jpeg(base_image, channels=NUM_CHANNELS)\n    image = preprocessing.preprocess_image(image, (IMAGE_SIZE, IMAGE_SIZE))\n    x_test_raw[id, :, :, :] = np.expand_dims(image, axis=0)\n\n# Parse labels file\nfname = os.path.join(data_folder, 'labels_validation.txt')\nvalidation_labels = dict()\nwith open(fname, newline='') as csvfile:\n    reader = csv.reader(csvfile, delimiter=' ')\n    for row in reader:\n        validation_labels[row[0]] = row[1]\n\n# Get labels for the test set by index\n# Note: Hugging Face models reserve the first index to null predictions\n# (labeled as 'background' id). That is why we increase in '1' the original label id.\nlabels_test = np.zeros(num_images)\nfor i in range(num_images):\n    labels_test[i] = int(validation_labels[x_test_files[i]]) + 1\n\nprint(f'{num_images} images loaded and preprocessed.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As illustrated in `1. Workflow overview`_, the model's source is at the user's\ndiscretion. Here, we know a priori that MobileNet V2 was trained with\nimages normalized within [-1, 1] interval. Also, ONNX models are usually\nsaved with a `channels-first` format, input images are expected to be passed\nwith the channels dimension on `axis = 1`.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Project images in the range [-1, 1]\nx_test = (x_test_raw / 127.5 - 1).astype('float32')\n\n# Transpose the channels to the first axis\nx_test = np.transpose(x_test, (0, 3, 1, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Download and export\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1. Download ONNX MobileNet V2\n\nThere are many repositories with models saved in ONNX format. In this example the\n[Optimum API](https://huggingface.co/docs/optimum/main/en/exporters/onnx/usage_guides/export_a_model)_\nis used for downloading and exporting models to ONNX.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from optimum.exporters.onnx import main_export\n\n# Download and convert MobiletNet V2 to ONNX\nmain_export(model_name_or_path=\"google/mobilenet_v2_1.0_224\",\n            task=\"image-classification\",\n            output=\"./\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import onnx\n\n# Load the model in memory\nmodel_onnx = onnx.load_model(\"./model.onnx\")\nprint(onnx.helper.printable_graph(model_onnx.graph))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2. Evaluate model performances\n\nThe [ONNXRuntime](https://onnxruntime.ai)_ package is a cross-platform\naccelerator capable of loading and running models described in ONNX format.\nWe take advantage of this framework to run the ONNX models and evaluate\ntheir performances.\n\n.. Note:: We only compute accuracy on 10 images.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from onnxruntime import InferenceSession\n\n\ndef evaluate_onnx_model(model):\n    sess = InferenceSession(model.SerializeToString())\n    # Calculate outputs by running images through the session\n    outputs = sess.run(None, {model.graph.input[0].name: x_test})\n    # The class with the highest score is what we choose as prediction\n    predicted = np.squeeze(np.argmax(outputs[0], 1))\n    # Compute the model accuracy\n    accuracy = (predicted == labels_test).sum() / num_images\n    return accuracy\n\n\naccuracy_floating = evaluate_onnx_model(model_onnx)\nprint(f'Floating point model accuracy: {100 * accuracy_floating:.2f} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Quantize\n\n| Akida processes integer activations and weights. Therefore, the floating point model\n  must be quantized in preparation to run on an Akida accelerator.\n| [QuantizeML quantize()](../../api_reference/quantizeml_apis.html#quantizeml.models.quantize)_\n  function recognizes [ModelProto](https://onnx.ai/onnx/api/classes.html#modelproto)_ objects\n  and quantizes them for Akida. The result is another ``ModelProto``, compatible with the\n  [CNN2SNN Toolkit](../../user_guide/cnn2snn.html)_.\n| The table below summarizes the obtained accuracy at the various stages using the full\n  ImageNet dataset.\n\n+-------------------------------+----------------+--------------------+----------------+\n| Calibration parameters        | Float accuracy | Quantized accuracy | Akida accuracy |\n+===============================+================+====================+================+\n| Random samples / per-tensor   | 71.790         | 70.550             | 70.588         |\n+-------------------------------+----------------+--------------------+----------------+\n| Imagenet samples / per-tensor | 71.790         | 70.472             | 70.628         |\n+-------------------------------+----------------+--------------------+----------------+\n\n.. Note::\n   Please refer to the [QuantizeML toolkit user guide](../../user_guide/quantizeml.html)_\n   and the [Advanced QuantizeML tutorial](plot_0_advanced_quantizeml.html)_ for details\n   about quantization parameters.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from quantizeml.layers import QuantizationParams\nfrom quantizeml.models import quantize\n\n# Quantize with activations quantized per tensor\nqparams = QuantizationParams(per_tensor_activations=True)\nmodel_quantized = quantize(model_onnx, qparams=qparams, num_samples=5)\n\n# Evaluate the quantized model performance\naccuracy = evaluate_onnx_model(model_quantized)\nprint(f'Quantized model accuracy: {100 * accuracy:.2f} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. Note:: Once the model is quantized, the [convert](../../api_reference/cnn2snn_apis.html#cnn2snn.convert)_\n   function must be used to retrieve a model in Akida format ready for inference. Please\n   refer to the [PyTorch to Akida workflow](../general/plot_8_global_pytorch_workflow.html)_ for further details.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}