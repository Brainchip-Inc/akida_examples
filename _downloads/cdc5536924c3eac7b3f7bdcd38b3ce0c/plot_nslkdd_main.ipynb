{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nLearning and inference on NSL-KDD\n=================================\n\nThe NSL-KDD dataset is an improvement over the KDD'99 provided by the Canadian\nInstitute for Cybersecurity.\n\n\nFrom their `webpage <https://www.unb.ca/cic/datasets/nsl.html>`__:\n\n    *NSL-KDD is a data set suggested to solve some of\n    the inherent problems of the KDD'99 data set which are mentioned in [1].\n    Although, this new version of the KDD data set still suffers from some\n    of the problems discussed by McHugh and may not be a perfect\n    representative of existing real networks, because of the lack of public\n    data sets for network-based IDSs, we believe it still can be applied as\n    an effective benchmark data set to help researchers compare different\n    intrusion detection methods.*\n\n    *Furthermore, the number of records in the\n    NSL-KDD train and test sets are reasonable. This advantage makes it\n    affordable to run the experiments on the complete set without the need\n    to randomly select a small portion. Consequently, evaluation results of\n    different research work will be consistent and comparable.*\n\n    *[1] M. Tavallaee, E. Bagheri, W. Lu, and A. Ghorbani, \"A Detailed Analysis\n    of the KDD CUP 99 Data Set,\" Submitted to Second IEEE Symposium on\n    Computational Intelligence for Security and Defense Applications\n    (CISDA), 2009.*\n\nThis tutorial is a demonstration of how to use the Akida Execution\nEngine to classify connexions into 5 classes: normal activity and 4\ndifferent groups of attacks (DoS, U2R, R2L and probe).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Download and prepare the NSL-KDD dataset\n-------------------------------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Various imports needed for the tutorial\nimport os, sys, time\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pickle\nimport itertools\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.utils import get_file\nfrom sklearn import preprocessing\nfrom sklearn.utils import shuffle\n\n# Akida specific imports\nfrom akida import Model, InputData, FullyConnected, LearningType, dense_to_sparse\n\n# Cybersecurity specific imports\nfrom progressbar import ProgressBar\nfrom sklearn.metrics import confusion_matrix, f1_score, precision_score, \\\n    recall_score, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Retrieve NSLKDD dataset\nfile_path = get_file('NSL-KDD.zip', 'https://iscxdownloads.cs.unb.ca/iscxdownloads/NSL-KDD/NSL-KDD.zip', cache_subdir='datasets/NSL-KDD', extract=True)\nworking_dir = os.path.dirname(file_path)\nprint ('Using NSL-KDD dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Define data set column names\ncol_names = np.array([\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\n    \"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n    \"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n    \"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n    \"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n    \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n    \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n    \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n    \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n    \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"label\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Funtion used to prepare the NSL-KDD dataset\ndef data_prep_NSLKDD(data):\n    \"\"\"\n    Rename group original attack names in 4 categories: DoS, Probe, R2L, U2R\n    Perform LabelEncoding() on the label column (convert strings to numbers)\n    Remove features/columns with a cardinality of 1 (= all cells are identical)\n\n    Args:\n        data: NSLKDD dataset\n    \"\"\"\n    # Dictionary that contains mapping of various attacks to the four main\n    # categories\n    attack_dict = {\n        'normal': 'normal',\n\n        'back': 'DoS',\n        'land': 'DoS',\n        'neptune': 'DoS',\n        'pod': 'DoS',\n        'smurf': 'DoS',\n        'teardrop': 'DoS',\n        'mailbomb': 'DoS',\n        'apache2': 'DoS',\n        'processtable': 'DoS',\n        'udpstorm': 'DoS',\n\n        'ipsweep': 'Probe',\n        'nmap': 'Probe',\n        'portsweep': 'Probe',\n        'satan': 'Probe',\n        'mscan': 'Probe',\n        'saint': 'Probe',\n\n        'ftp_write': 'R2L',\n        'guess_passwd': 'R2L',\n        'imap': 'R2L',\n        'multihop': 'R2L',\n        'phf': 'R2L',\n        'spy': 'R2L',\n        'warezclient': 'R2L',\n        'warezmaster': 'R2L',\n        'sendmail': 'R2L',\n        'named': 'R2L',\n        'snmpgetattack': 'R2L',\n        'snmpguess': 'R2L',\n        'xlock': 'R2L',\n        'xsnoop': 'R2L',\n        'worm': 'R2L',\n\n        'buffer_overflow': 'U2R',\n        'loadmodule': 'U2R',\n        'perl': 'U2R',\n        'rootkit': 'U2R',\n        'httptunnel': 'U2R',\n        'ps': 'U2R',\n        'sqlattack': 'U2R',\n        'xterm': 'U2R'\n    }\n    data[\"label\"].replace(attack_dict, inplace=True)\n\n    # Label encoding\n    le = preprocessing.LabelEncoder()\n    data['label'] = le.fit_transform(data['label'])\n    Label_mapping = dict(zip(le.classes_, range(len(le.classes_))))\n\n    # Drop columns which are all zeros in the training set\n    cardinalities = data.apply(pd.Series.nunique)\n    good_columns = cardinalities>1\n    data = data.loc[:, good_columns]\n\n    return(data, Label_mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Load the dataset\ntrain_data = pd.read_csv(os.path.join(working_dir, 'KDDTrain+.txt'), sep=',', header=None, names=col_names, index_col=False)\ntrain_data['split'] = 'train'\ntest_data = pd.read_csv(os.path.join(working_dir, 'KDDTest+.txt'), sep=',', header=None, names=col_names, index_col=False)\ntest_data['split'] = 'test'\n\n# Prepare the dataset\ndata = pd.concat([train_data, test_data])\ndata, Label_mapping = data_prep_NSLKDD(data)\n\n# Resplit train and test sets\ny_train_df = data.label[data.split=='train']\nX_train_df = data[data.split=='train'].drop(['label','split'],axis=1)\n\ny_test_df = data.label[data.split=='test']\nX_test_df = data[data.split=='test'].drop(['label','split'],axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Sneak peek of the input tabular data\n---------------------------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Display train set shape and data sneak peek\nprint ('Train set shape: %s' % (X_train_df.shape,))\nprint(X_train_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Convert from tabular to binary data\n--------------------------------------\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ``onehotencode_df()`` function will transform these\ntabular data to \"binary\" data using a one-hot encoding scheme for all\nvariables. This technique is usually the way to go only for categorical\nvariables. Here, since we want to learn from binary data, we will use\nthis technique also for the other variable types in ``X_train``:\n\n  * For continuous numeric variables, we will use binning + one-hot-encoding.\n    This process is akin to a Gaussian Receptive Field *data to spike*\n    conversion in our case. The only difference between the classical GRF\n    and the current implementation is that we did not implement overlap\n    between bins. This may cancel the *encoding precision for free* you can\n    get with overlap. But on the other hand this allows to know exactly the\n    number of positive activations. Thus, the balance is probably positive\n    here.\n  * For boolean variables, we used 2 neurons for each variable. Only\n    one may have been used. But having two again allows to know exactly the\n    number of positive activations.\n\nAs already said, one nice feat of this scheme is that the number of\npositive activations is fixed: it is exactly one per original variable.\nThis is convenient for Akida native learning.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def compute_df_mappings(df, n_bins=20, method='quantized'):\n    \"\"\" Compute mappings to tranform columns of a Pandas dataframe to be Akida-ready.\n\n    The mappings type depends on the column dtype:\n    - numerical: bin edges for bucketizing\n    - categorical: list of modalities for one-hot encoding\n    - boolean: same as categorical\n\n    :param df: raw tabular data, standard X in machine learning for\n        structured data. Should be a Pandas Dataframe (which is\n        standard now): one line per instance, one column per feature.\n    :param n_bins: number of bins used for numerical columns.\n        Default is 20.\n    :param method: method used to make the binning. If 'quantized' then\n        it makes the bin probability uniform on the train set. If 'hist'\n        it keep the original distribution intact.\n        Default is 'quantized'\n\n    :return: A tuple containing the mappings, the recognized type\n        of each column, and the new column names for the \"extended\" X.\n    \"\"\"\n\n    # Define quantiles\n    myquantiles = np.linspace(0,1,n_bins+1)\n\n    mappings = dict()\n    used_type = dict()\n    n_col = len(df.columns)\n\n    col_names = []\n    for col in df.columns:\n        kind = df[col].dtype.kind\n        cardinality = df[col].nunique()\n        print(\"Compute mappings for column *{}* of kind {} with cardinality {}\".format(col,kind,cardinality))\n        if cardinality<=n_bins: kind = 'O' # do as if it was an object for low cardinality col\n        if kind == 'O':\n            X, mapping_index = pd.Series(df[col]).factorize()\n            mappings[col] = dict(zip(mapping_index, range(len(mapping_index))))\n            col_names = col_names + [str(col) + '_' + str(s) for s in mappings[col]]\n            used_type[col] = 'cat'\n        elif kind == 'b':\n            mappings[col] = {False:0, True:1} # always the same, no need to compute\n            col_names = col_names + [str(col) + '_' + s for s in ['0','1']]\n            used_type[col] = 'boo'\n        else: # if continuous/numerical\n            if method=='quantized':\n                bin_edges = np.nanquantile(df[col], myquantiles) # make the distribution uniform\n                bin_edges = np.delete(bin_edges,[0,len(myquantiles)-1])\n            elif method=='hist':\n                _,bin_edges = np.histogram(df[col], bins=n_bins) # keep distribution as it is\n                bin_edges = np.delete(bin_edges,[0,len(myquantiles)-1])\n            mappings[col] = np.unique(bin_edges) # Merge identical bin_edges\n            col_names = col_names + [str(col) + '_bin_' + str(x) for x in range(len(mappings[col])+1)]\n            used_type[col] = 'num'\n    print(\"Done.\", end='\\n')\n    print(\"Information for Akida yml configuration: inputWidth = {} and numWeights = {}.\".format(len(col_names), len(df.columns)))\n    return mappings, used_type, col_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def onehotencode_df(df, mappings=None, n_bins=20, verbose=0):\n    \"\"\" Convert tabular/structured data (=dataframe) to spike for Akida.\n\n    This function transforms a Pandas dataframe to be Akida-ready using\n    one-hot encoding on virtually all kind of input features (numerical,\n    boolean, categorical).\n\n    For numerical features, since it bucketizes the data based on quantiles,\n    it thus also \"normalizes\" the data distribution: there is no need for\n    scaling or procedure to take care of outliers).\n\n    The input data should be NaN-free.\n\n    NOTES:\n    - This function may be rewritten to work as a layer.\n    - No overlap in binning (different from GRF). Is it a problem? Apparently not.\n\n    :param df: raw tabular data, standard X in machine learning for\n        structured data. Should be a Pandas Dataframe (which is standard\n        now): one line per instance, one column per feature.\n    :param mappings: a tuple with the parameters of the transformations\n        for each column. See compute_df_mappings() on how to build it,\n        defaults to None (compute_df_mappings() is then performed first\n        here).\n    :param n_bins: number of bins used for numerical columns. Default is 20.\n        Only used if mappings==None.\n    :param verbose: set verbosity. Default is 0 (= no output).\n\n    :return: A tuple containing the binarized df (as a boolean numpy array,\n        same number of than the input df) and a tuple with the parameters of\n        the transformations for each column.\n    \"\"\"\n\n    if mappings is None:\n        mappings = compute_df_mappings(df, n_bins=20)\n\n    X = np.array([], dtype=np.bool_).reshape(len(df),0) # init w/ good length\n    if verbose>0: print(\"Convert column: \", end=\"\")\n    for col in df.columns:\n        if verbose>0: print(\"{}\".format(col), end=', ')\n        if mappings[1][col] == 'cat':\n            X = np.hstack((X, onehotencode_categorical_column(df[col], mapping_index=mappings[0][col])))\n        elif mappings[1][col] == 'boo':\n            X = np.hstack((X, onehotencode_boolean_column(df[col])))\n        elif mappings[1][col] == 'num':\n            X = np.hstack((X, onehotencode_numerical_column(df[col], bin_edges=mappings[0][col])))\n        else:\n            print('Warning: unknown column format. Try rerunning compute_df_mappings().')\n    if verbose>0: print(\"Done. X is now {}\".format(X.shape))\n    return X, mappings\n\n\ndef onehotencode_numerical_column(X, bin_edges):\n    \"\"\"\n    Transform a numerical vector to Akida-ready data using one-hot encoding\n\n    :param X: A vector of values (typically a Pandas column/series from a dataframe).\n    :param bin_edges: precomputed bin edges for the bucketizing.\n\n    :return: A boolean numpy matrix corresponding to a one-hot encoded input X.\n    \"\"\"\n    X = np.digitize(X, bin_edges) # ~label encoding\n    X = np.eye(len(bin_edges)+1)[X] # make it one-hot\n    return X.astype(np.bool_)\n\ndef onehotencode_boolean_column(X):\n    \"\"\"\n    Transform a boolean-like vector to Akida-ready data using one-hot encoding\n\n    :param X: A vector of values that are boolean-like (= contains only\n        zeros and ones, they do not need to have a dtype==bool_).\n\n    :return: A boolean numpy matrix (same number of lines than input X, 2 columns)\n        corresponding to a one-hot encoded input X.\n    \"\"\"\n    X = np.eye(2)[np.asarray(X).astype(np.uint8)]\n    return X.astype(np.bool_)\n\ndef onehotencode_categorical_column(X, mapping_index):\n    \"\"\"\n    Transform a categorical vector to Akida-ready data using one-hot encoding\n\n    :param X: A vector of values (typically a Pandas column/series from a dataframe).\n    :param mapping_index: precomputed bin edges for the bucketizing.\n\n    :return: A boolean numpy matrix corresponding to a one-hot encoded input X.\n    \"\"\"\n    remove_last_column = False\n    X.iloc[:].loc[~X.isin(mapping_index.keys())] = -1 # values not in mapping are replaced with -1\n    X = X.replace(mapping_index)\n    n_col = len(mapping_index)\n    if np.any(X==-1):\n        remove_last_column = True\n        n_col = n_col+1\n\n    X = np.eye(n_col)[X.astype(int)]\n    if remove_last_column:\n        X = np.delete(X, -1, axis=1) # remove the -1 column (=values not seen during training)\n\n    return X.astype(np.bool_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Convert from tabular to binary data\nmappings = compute_df_mappings(X_train_df, n_bins=20)\nX_train, mappings = onehotencode_df(X_train_df, mappings=mappings)\nX_test, mappings = onehotencode_df(X_test_df, mappings=mappings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. Note:: The output printed from the cell above should be used to define\n          the architecture and the parameters of the Akida model.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. Oversampling the training data to cope with imbalanced dataset\n-----------------------------------------------------------------\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since there is much more exemplars of normal activity than any other\nattacks, we used over-sampling. This technique consists in duplicating\nexemplars of the classes less represented in the training set so that\nall classes are equally represented.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Re-sampling specific import\nfrom imblearn.over_sampling import RandomOverSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Check original distribution\nprint('Classes and their frequencies in the dataset:')\nprint(y_train_df.value_counts(normalize=True, sort=False))\n\n# Convert label series to numpy arrays\ny_train = y_train_df.to_numpy()\ny_test = y_test_df.to_numpy()\n\n# Oversampling\nros = RandomOverSampler(random_state=0)\nX_train, y_train = ros.fit_resample(X_train, y_train)\nX_train, y_train = shuffle(X_train, y_train)\n\nprint('Classes and their frequencies in the dataset (after oversampling):')\nprint(pd.Series(y_train).value_counts(normalize=True, sort=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5. Configuring Akida model\n--------------------------\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To modify Akida architecture and parameters, edit model below.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "#Create a model\nmodel = Model()\nmodel.add(InputData(\"input\", input_width=1, input_height=1, input_features=312))\nfully = FullyConnected(\"fully\", num_neurons=10240, activations_enabled=False)\nmodel.add(fully)\n# Configure the last layer for semi-supervised training\nfully.compile(num_weights=40, num_classes=5)\n\nmodel.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6. Learning and inference\n-------------------------\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Depending on your setup, training the Akida model will take some time\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def convert_dataset_to_spikes(X):\n    X_spikes = []\n    for i in ProgressBar()(range(X.shape[0])):\n        sample = X[i].reshape((1, 1, X[i].shape[0]))\n        X_spikes.append(dense_to_sparse(sample))\n    return X_spikes\n\nprint(\"Convert the train set to spikes\")\nX_train_spikes = convert_dataset_to_spikes(X_train)\n\nprint(\"Perform training one sample at a time\")\nfor i in ProgressBar()(range(len(X_train_spikes))):\n    model.fit(X_train_spikes[i], input_labels=y_train[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "7. Display results\n------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Define a method to compute performances\ndef CS_performance_measures(y_true, y_pred, labels=None, normal_class=0):\n    \"\"\" Cybersecurity / anomaly detection custom performance measures.\n\n    These specific performance measures are designed to assess precisely\n    the performance in the case of a multiclass classification task in\n    which one class is \"special\". For cybersecurity for example the 'normal'\n    activity is common and a special class in comparison to different\n    type of attacks. This class is here indexed as the normal_class variable.\n\n    What happens here is that performance measures are computed at two nested\n    levels:\n    1) All attacks are merged together and measures are computed\n    (i.e. 0 vs. [1,2,3,4] as 1)\n    2) Measured are computed on attacks only (i.e. 1 vs. 2 vs. 3 vs. 4)\n\n    Relies on sklearn.metrics.\n\n    labels is a list of labels ids. If labels and their original names are saved in\n    a dictionnary with the class names as keys and the ids as index\n    (e.g. labels = {'DoS': 0, 'Probe': 1, 'R2L': 2, 'U2R': 3, 'normal': 4}),\n    then labels = list(labels.values()).\n\n    normal_class is the index of the class considered as normal (vs. e.g. attack or\n    failure depending on the use case)\n\n    Note that in binary classification, recall of the positive class is also\n    known as sensitivity; recall of the negative class is specificity.\n    \"\"\"\n\n    # Make the new (binary classif) vectors corresponding to attack vs normal\n    attack_true = np.full(len(y_true), np.nan)\n    attack_true[y_true == normal_class] = 0\n    attack_true[y_true != normal_class] = 1\n    attack_pred = np.full(len(y_pred), np.nan)\n    attack_pred[y_pred == normal_class] = 0\n    attack_pred[y_pred != normal_class] = 1\n\n    normal_lines = np.logical_or(y_true==0, y_pred==0)\n    attack_lines = ~normal_lines\n\n    results = {\"accuracy\": accuracy_score(attack_true, attack_pred),\n               \"f1\": f1_score(attack_true, attack_pred, average='weighted'),\n               \"recall-sensitivity\": recall_score(attack_true, attack_pred, pos_label=1),\n               \"recall-specificity\": recall_score(attack_true, attack_pred, pos_label=0),\n               \"precision\": precision_score(attack_true, attack_pred, average='weighted'),\n\n               \"accuracy_among_attacks\": accuracy_score(y_true[attack_lines], y_pred[attack_lines]),\n               \"f1_among_attacks\": f1_score(y_true[attack_lines], y_pred[attack_lines], average='weighted'),\n               \"recall_among_attacks\": recall_score(y_true[attack_lines], y_pred[attack_lines], average='weighted'),\n               \"precision_among_attacks\": precision_score(y_true[attack_lines], y_pred[attack_lines], average='weighted'),\n\n               \"cm\": confusion_matrix(y_true, y_pred, labels)}\n    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Check performances against the test set\nres = pd.DataFrame()\n\nprint(\"Convert the test set to spikes\")\nX_test_spikes = convert_dataset_to_spikes(X_test)\n\nprint(\"Classify test samples\")\ny_pred = np.empty((0, 1), dtype=int)\nfor i in ProgressBar()(range(len(X_test_spikes))):\n    outputs = model.predict(X_test_spikes[i], 5)\n    y_pred = np.append(y_pred, outputs)\nresults = CS_performance_measures(y_test, y_pred, list(Label_mapping.values()), normal_class=4)\n\n# Display results\nres = res.append(results, ignore_index=True)\nprint(\"Accuracy: \"+\"{0:.2f}\".format(100*results[\"accuracy\"])+\"% / \"+\"F1 score: \"+\"{0:.2f}\".format(results[\"f1\"]))\n\n# For non-regression purpose\nassert results[\"accuracy\"] > 0.84\n\n# Get model statistics on a few samples\nstats = model.get_statistics()\nfor i in range(20):\n    model.forward(X_test_spikes[i])\n\n# Print model statistics\nprint(\"Model statistics\")\nfor _, stat in stats.items():\n    print(stat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n\n    Taken from:\n    https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.autoscale()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Display normalized confusion matrix\nplt.rcParams['figure.figsize'] = [8, 4]\nplt.figure()\nplot_confusion_matrix(res['cm'][len(res)-1], classes=Label_mapping, normalize=True,\n                      title='Normalized confusion matrix')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Display confusion matrix, raw numbers\nplt.figure()\nplot_confusion_matrix(res['cm'][len(res)-1], classes=Label_mapping,\n                      title='Confusion matrix, raw numbers')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}