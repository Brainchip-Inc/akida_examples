{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# DS-CNN CIFAR10 inference\n\nThis tutorial uses the CIFAR-10 dataset (60k training images distributed in 10\nobject classes) for a classic object classification task with a network built\naround the Depthwise Separable Convolutional Neural Network (DS-CNN) which is\noriginated from `Zhang et al (2018) <https://arxiv.org/pdf/1711.07128.pdf>`_.\n\nThe goal of the tutorial is to provide users with an example of a complex model\nthat can be converted to an Akida model and that can be run on Akida NSoC\nwith an accuracy similar to a standard Keras floating point model.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Dataset preparation\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import cifar10\n\n# Load CIFAR10 dataset\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n\n# Reshape x-data\nx_train = x_train.reshape(50000, 32, 32, 3)\nx_test = x_test.reshape(10000, 32, 32, 3)\ninput_shape = (32, 32, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Create a Keras DS-CNN model\n\nThe DS-CNN architecture is available in the `Akida models zoo\n<../../api_reference/akida_models_apis.html#cifar-10>`_ along with pretrained\nweights.\n\n .. Note:: The pre-trained weights were obtained after training the model with\n           unconstrained float weights and activations for 1000 epochs\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import get_file\nfrom tensorflow.keras.models import load_model\n\n# Retrieve the float model with pretrained weights and load it\nmodel_file = get_file(\n    \"ds_cnn_cifar10.h5\",\n    \"http://data.brainchip.com/models/ds_cnn/ds_cnn_cifar10.h5\",\n    cache_subdir='models/ds_cnn_cifar10')\nmodel_keras = load_model(model_file)\nmodel_keras.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Keras model accuracy is checked against the first *n* images of the test set.\n\nThe table below summarizes the expected results:\n\n+---------+----------+\n| #Images | Accuracy |\n+=========+==========+\n| 100     |  96.00 % |\n+---------+----------+\n| 1000    |  94.30 % |\n+---------+----------+\n| 10000   |  93.60 % |\n+---------+----------+\n\n.. Note:: Depending on your hardware setup, the processing time may vary.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nfrom sklearn.metrics import accuracy_score\nfrom timeit import default_timer as timer\n\n\n# Check Model performance\ndef check_model_performances(model, x_test, num_images=1000):\n    start = timer()\n    potentials_keras = model.predict(x_test[:num_images])\n    preds_keras = np.squeeze(np.argmax(potentials_keras, 1))\n\n    accuracy = accuracy_score(y_test[:num_images], preds_keras)\n    print(\"Accuracy: \" + \"{0:.2f}\".format(100 * accuracy) + \"%\")\n    end = timer()\n    print(f'Keras inference on {num_images} images took {end-start:.2f} s.\\n')\n\n\ncheck_model_performances(model_keras, x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Quantized model\n\nQuantizing a model is done using `cnn2snn.quantize\n<../../api_reference/cnn2snn_apis.html#quantize>`_. After the call, all the\nlayers will have 4-bit weights and 4-bit activations.\n\nThis model will therefore satisfy the Akida NSoC requirements but will suffer\nfrom a drop in accuracy due to quantization as shown in the table below:\n\n+---------+----------------+--------------------+\n| #Images | Float accuracy | Quantized accuracy |\n+=========+================+====================+\n| 100     |     96.00 %    |       96.00 %      |\n+---------+----------------+--------------------+\n| 1000    |     94.30 %    |       92.60 %      |\n+---------+----------------+--------------------+\n| 10000   |     93.66 %    |       92.58 %      |\n+---------+----------------+--------------------+\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from cnn2snn import quantize\n\n# Quantize the model to 4-bit weights and activations\nmodel_keras_quantized = quantize(model_keras, 4, 4)\n\n# Check Model performance\ncheck_model_performances(model_keras_quantized, x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Pretrained quantized model\n\nThe Akida models zoo also contains a `pretrained quantized helper\n<../../api_reference/akida_models_apis.html#akida_models.ds_cnn_cifar10_pretrained>`_\nthat was obtained using the `tune <../../user_guide/akida_models.html#cifar10-training-and-tuning>`_\naction of ``akida_models`` CLI on the quantized model for 100 epochs.\n\nTuning the model, that is training with a lowered learning rate, allows to\nrecover performances up to the initial floating point accuracy.\n\n+---------+----------------+--------------------+--------------+\n| #Images | Float accuracy | Quantized accuracy | After tuning |\n+=========+================+====================+==============+\n| 100     |     96.00 %    |       96.00 %      |    97.00 %   |\n+---------+----------------+--------------------+--------------+\n| 1000    |     94.30 %    |       92.60 %      |    94.20 %   |\n+---------+----------------+--------------------+--------------+\n| 10000   |     93.66 %    |       92.58 %      |    93.08 %   |\n+---------+----------------+--------------------+--------------+\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from akida_models import ds_cnn_cifar10_pretrained\n\n# Use a quantized model with pretrained quantized weights\nmodel_keras_quantized_pretrained = ds_cnn_cifar10_pretrained()\n\n# Check Model performance\ncheck_model_performances(model_keras_quantized_pretrained, x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Conversion to Akida\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 Convert to Akida model\n\nWhen converting to an Akida model, we just need to pass the Keras model\nand the input scaling that was used during training to `cnn2snn.convert\n<../../api_reference/cnn2snn_apis.html#convert>`_.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from cnn2snn import convert\n\nmodel_akida = convert(model_keras_quantized_pretrained)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Check hardware compliancy\n\nThe `Model.summary <../../api_reference/aee_apis.html#akida.Model.summary>`__\nmethod provides a detailed description of the Model layers.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_akida.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3 Check performance\n\nWe check the Akida model accuracy on the first *n* images of the test\nset.\n\nThe table below summarizes the expected results:\n\n+---------+----------------+----------------+\n| #Images | Keras accuracy | Akida accuracy |\n+=========+================+================+\n| 100     |     96.00 %    |     97.00 %    |\n+---------+----------------+----------------+\n| 1000    |     94.30 %    |     94.00 %    |\n+---------+----------------+----------------+\n| 10000   |     93.66 %    |     93.04 %    |\n+---------+----------------+----------------+\n\nDue to the conversion process, the predictions may be slightly different\nbetween the original Keras model and Akida on some specific images.\n\nThis explains why when testing on a limited number of images the\naccuracy numbers between Keras and Akida may be quite different. On the\nfull test set however, the two models accuracies are very close.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_images = 1000\n\n# Check Model performance\nstart = timer()\nresults = model_akida.predict(x_test[:num_images])\naccuracy = accuracy_score(y_test[:num_images], results)\n\nprint(\"Accuracy: \" + \"{0:.2f}\".format(100 * accuracy) + \"%\")\nend = timer()\nprint(f'Akida inference on {num_images} images took {end-start:.2f} s.\\n')\n\n# For non-regression purpose\nif num_images == 1000:\n    assert accuracy == 0.94"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Activations sparsity has a great impact on akida inference time. One can have\na look at the average input and output sparsity of each layer using\n`Model.statistics <../../api_reference/aee_apis.html#akida.Model.statistics>`_\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Print model statistics\nprint(\"Model statistics\")\nprint(model_akida.statistics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.4 Show predictions for a random image\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport matplotlib.lines as lines\nimport matplotlib.patches as patches\n\nlabel_names = [\n    'airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse',\n    'ship', 'truck'\n]\n\n# prepare plot\nbarWidth = 0.75\npause_time = 1\n\nfig = plt.figure(num='CIFAR10 Classification by Akida Execution Engine',\n                 figsize=(8, 4))\nax0 = plt.subplot(1, 3, 1)\nimgobj = ax0.imshow(np.zeros((32, 32, 3), dtype=np.uint8))\nax0.set_axis_off()\n# Results subplots\nax1 = plt.subplot(1, 2, 2)\nax1.xaxis.set_visible(False)\nax0.text(0, 34, 'Actual class:')\nactual_class = ax0.text(16, 34, 'None')\nax0.text(0, 37, 'Predicted class:')\npredicted_class = ax0.text(20, 37, 'None')\n\n# Take a random test image\ni = np.random.randint(y_test.shape[0])\n\ntrue_idx = int(y_test[i])\npot = model_akida.evaluate(np.expand_dims(x_test[i], axis=0)).squeeze()\n\nrpot = np.arange(len(pot))\nax1.barh(rpot, pot, height=barWidth)\nax1.set_yticks(rpot - 0.07 * barWidth)\nax1.set_yticklabels(label_names)\npredicted_idx = pot.argmax()\nimgobj.set_data(x_test[i])\nif predicted_idx == true_idx:\n    ax1.get_children()[predicted_idx].set_color('g')\nelse:\n    ax1.get_children()[predicted_idx].set_color('r')\nactual_class.set_text(label_names[true_idx])\npredicted_class.set_text(label_names[predicted_idx])\nax1.set_title('Akida\\'s predictions')\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}