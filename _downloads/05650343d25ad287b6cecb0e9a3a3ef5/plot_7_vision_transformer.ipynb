{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Build Vision Transformers for Akida\n\nThe Vision Transformer, or ViT, is a model for image classification that employs a Transformer-like\narchitecture over patches of the image. An image is split into fixed-size patches, each of them are\nthen linearly embedded, position embeddings are added, and the resulting sequence of vectors are\nfed to a standard Transformer encoder. Please refer to https://arxiv.org/abs/2010.11929 for further\ndetails.\n\nAkida 2.0 now supports patch and position embeddings, and the encoder block in hardware. This\ntutorial explains how to build an optimized ViT using Akida models python API for Akida 2.0 hardware.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Model selection\nThere are many variants of ViT. The choice of the model is typically influenced by the tradeoff\namong architecture size, accuracy, inference speed, and training capabilities.\n\nThe following table shows few variants of commonly used ViT:\n\n+--------------+-------------------+---------+-------------------+\n| Architecture | Original accuracy | #Params | Architecture      |\n+==============+===================+=========+===================+\n| ViT Base     |  79.90%           |  86M    |  12 heads,        |\n|              |                   |         |  12 blocks,       |\n|              |                   |         |  hidden size 768  |\n+--------------+-------------------+---------+-------------------+\n| ViT Tiny     |  75.48%           |  5.8M   |  3 heads,         |\n|              |                   |         |  12 blocks,       |\n|              |                   |         |  hidden size 192  |\n+--------------+-------------------+---------+-------------------+\n| DeiT-dist    |  74.17%           |  5.8M   |  3 heads,         |\n| Tiny         |                   |         |  12 blocks,       |\n|              |                   |         |  hidden size 192  |\n+--------------+-------------------+---------+-------------------+\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>The Vision Transformers support has been introduced in Akida 2.0.</p></div>\n\nThe Akida model zoo provides tiny  ViT architectures that are optimized to run on Akida\nhardware:\n\n - [ViT (tiny)](../../api_reference/akida_models_apis.html#akida_models.bc_vit_ti16)_,\n - [DeiT-dist (tiny)](../../api_reference/akida_models_apis.html#akida_models.bc_deit_ti16)_.\n\nBoth architectures have been modified so that their layers can be quantized to integer only\noperations.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Model optimization for Akida hardware\n\nViT has many encoder blocks that perform self-attention to process visual data. Each encoder\nblock consists of many different layers. To optimally run ViT at the edge using Akida requires\ntransforming this encoder block in the following way:\n\n  - replace [LayerNormalization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LayerNormalization)_ with\n    [LayerMadNormalization](../../api_reference/quantizeml_apis.html#quantizeml.layers.LayerMadNormalization)_,\n  - replace the last [LayerNormalization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LayerNormalization)_ previous\n    to the classification head with a [BatchNormalization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization)_,\n  - replace [GeLU](https://www.tensorflow.org/addons/api_docs/python/tfa/layers/GELU)_\n    with [ReLU8](https://www.tensorflow.org/api_docs/python/tf/keras/layers/ReLU)_ activations,\n  - replace [Softmax](https://www.tensorflow.org/api_docs/python/tf/keras/activations/softmax)_ operation in\n    [Attention](../../api_reference/quantizeml_apis.html#quantizeml.layers.Attention)_ with a\n    [shiftmax](../../api_reference/quantizeml_apis.html#quantizeml.layers.shiftmax)_ operation.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Sections below show different ways to train a ViT for Akida which uses the above\n          transformations.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model Training\nAkida accelerates ViT model that has the transformation mentioned in Section 2. Training a ViT\nthat optimally runs on Akida can be made possible in the following two ways:\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Option 1: Training a ViT (original) model first and then transforming each layer incrementally\nFirst, train a ViT (original) model on a custom dataset until satisfactory accuracy. It is then\npossible to transform this model into an Akida optimized one as per Section 2. The layers mentioned\nin Section 2 are functionally equivalent to each of the layers present in the original model.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>To overcome the accuracy drop from the original when transforming the model as per\n          Section 2, it is recommended to replace the original layers all at once and to fine-tune\n          afterwards.</p></div>\n\nThe example below shows the transformation of ViT (tiny) into an optimized model that can run on\nthe Akida hardware.\n\nThe [akida_models](https://pypi.org/project/akida-models)_ python package provides a Command Line\nInterface (CLI) to transform [vit_ti16](../../_modules/akida_models/transformers/model_vit.html#vit_ti16)_\nand [deit_ti16](../../_modules/akida_models/transformers/model_deit.html#deit_ti16)_ model architectures\nand fine-tune them respectively.\n\n```bash\n$ akida_models create vit_ti16 -h\nusage: akida_models create vit_ti16 [-h] [-c CLASSES] [-bw BASE_WEIGHTS] [--norm {LN,GN1,BN,LMN}]\n                                    [--last_norm {LN,BN}] [--softmax {softmax,softmax2}]\n                                    [--act {GeLU,ReLU8,swish}] [-i {224,384}]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -c CLASSES, --classes CLASSES\n                        The number of classes, by default 1000.\n  -bw BASE_WEIGHTS, --base_weights BASE_WEIGHTS\n                        Optional keras weights to load in the model, by default None.\n  --norm {LN,GN1,BN,LMN}\n                        Replace normalization in model with a custom function, by default LN\n  --last_norm {LN,BN}   Replace last normalization in model with a custom function, by default LN\n  --softmax {softmax,softmax2}\n                        Replace softmax operation in model with custom function, by default softmax\n  --act {GeLU,ReLU8,swish}\n                        Replace activation function in model with custom function, by default GeLU\n  -i {224,384}, --image_size {224,384}\n                        The square input image size\n```\nThe following shows the transformation of a vit_ti16 model architecture which was trained on ImageNet. The\nsame methods can be applied for other datasets.\n\n```bash\n# download the pre-trained weights\nwget https://data.brainchip.com/models/AkidaV2/vit/vit_ti16_224.h5\n\n# transformations: replace layer normalization with mad norm layer, last layer normalization\n# with batch normalization, GeLU layer with ReLU and softmax with shiftmax layer\nakida_models create -s vit_ti16_transformed.h5 vit_ti16 --norm LMN --last_norm BN --act ReLU8 \\\n                                                --softmax softmax2 -bw vit_ti16_224.h5\n# fine-tuning\nimagenet_train tune -m vit_ti16_transformed.h5 -e 30 --optim Adam --lr_policy cosine_decay \\\n                    -lr 6e-5 -s vit_ti16_transformed.h5\n```\nThe above transformation generates a ViT model that is optimized to run efficiently on Akida hardware.\nSimilar steps can also be applied to deit_ti16. The table below highlights the accuracy of the original\nand transformed models.\n\n+--------------+-------------------+----------------------+\n| Architecture | Original accuracy | Transformed accuracy |\n+==============+===================+======================+\n| ViT          |  75.48%           | 74.25%               |\n+--------------+-------------------+----------------------+\n| DeiT-dist    |  74.17%           | 75.03%               |\n+--------------+-------------------+----------------------+\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>The models obtained above have floating point weights and are ready to be quantized.\n          See Section 4.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Option 2: Transfer Learning using Pre-trained transformed model\nThe [Akida models python package](../../api_reference/akida_models_apis.html)_ has  [APIs for ViTs](../../api_reference/akida_models_apis.html#layer-blocks)_ which provides pre-trained models for\n[vit_ti16](../../_modules/akida_models/transformers/model_vit.html#vit_ti16)_ and [deit_ti16](../../_modules/akida_models/transformers/model_deit.html#deit_ti16)_. These models can be used\nfor Transfer Learning on a custom dataset. Since the above models are already transformed, no\nfurther transformation is required.\n\nVisit our [Transfer Learning Example](plot_4_transfer_learning.html)_ to learn more about Transfer\nLearning using the [Akida models python package](../../api_reference/akida_models_apis.html)_. The\nfollowing code snippet downloads a pre-trained model that can be used for Transfer Learning.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# The following is the API download the vit_t16 model trained on ImageNet dataset\nfrom akida_models import fetch_file\nfrom akida_models.model_io import load_model\n\n# Retrieve the float model with pretrained weights and load it\nmodel_file = fetch_file(\n    fname=\"bc_vit_ti16_224.h5\",\n    origin=\"https://data.brainchip.com/models/AkidaV2/vit/bc_vit_ti16_224.h5\",\n    cache_subdir='models/akidanet_imagenet')\nmodel_keras = load_model(model_file)\nmodel_keras.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>The models in Section 3 have floating point weights. Once the desired accuracy is obtained,\n          these models should go through quantization before converting to Akida.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model quantization\nAkida 2.0 hardware adds efficient processing of 8-bit weights and activations for Vision Transformer\nmodels. This requires models in Section 3 to be quantized to 8-bit integer numbers. This means both\nweights and activation outputs become 8-bit integer numbers. This results in a smaller  model with\nminimal to no drop in accuracy and achieves improvements in latency and power when running on Akida\nhardware.\n\nQuantization of ViT models can be done using [QuantizeML python package](../../user_guide/quantizeml.html)_\nusing either Post Training Quantization (PTQ) or Quantization Aware Training (QAT) methods. The following\nsection shows quantization an example, quantization of [vit_ti16](../../_modules/akida_models/transformers/model_vit.html#vit_ti16)_ trained on ImageNet dataset.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Post-Training Quantization\nUsing [QuantizeML python package](../../user_guide/quantizeml.html)_, ViT model can be quantized to\n8-bit integer numbers (both weights and activation outputs). PTQ requires calibration (ideally using\nreference data) which helps to determine optimal quantization ranges. To learn more about PTQ, refer\nto [Advanced QuantizeML tutorial](../quantization/plot_0_advanced_quantizeml.html)_.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Using QuantizeML to perform quantization\nfrom quantizeml.models import quantize\nfrom quantizeml.layers import QuantizationParams\n\n# Define the quantization parameters.\nqparams = QuantizationParams(weight_bits=8, activation_bits=8)\n\n# Quantize the model defined in Section 3.2\nmodel_quantized = quantize(model_keras, qparams=qparams)\nmodel_quantized.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The [bc_vit_ti16_imagenet_pretrained helper](../../api_reference/akida_models_apis.html#akida_models.bc_vit_ti16_imagenet_pretrained)_\nwas obtained with the same 8-bit quantization scheme but with an additional QAT step to further\nimprove accuracy.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Quantization Aware Training (Optional)\nIn Section 4.1, we performed PTQ and converted the weights and activation outputs to 8-bit integer numbers.\nIn most cases, there is no accuracy drop observed after quantization, however in cases where an accurary\ndrop is observed, it is possible to further fine-tune this model using QAT.\n\nThe model that is obtained through [QuantizeML python package](../../user_guide/quantizeml.html)_ is an\ninstance of Keras. This allows the model to be fine-tuned using the original dataset to regain accuracy.\n\n[Akida models python package](../../api_reference/akida_models_apis.html)_  provides pre-trained models\nfor vit_ti16 and deit_ti16 that have been trained using QAT method. It can be used in the following way:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from akida_models import bc_vit_ti16_imagenet_pretrained\n\n# Load the pre-trained quantized model\nmodel_quantized = bc_vit_ti16_imagenet_pretrained()\nmodel_quantized.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Conversion to Akida\n\nA model quantized through [QuantizeML python package](../../user_guide/quantizeml.html)_ is ready to be\nconverted to Akida. Once the quantized model has the desired accuracy [CNN2SNN toolkit](../../user_guide/cnn2snn.html)_\nis used for conversion to Akida. There is no further optimization required and equivalent accuracy is\nobserved upon converting the model to Akida.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from cnn2snn import convert\n\n# Convert the model\nmodel_akida = convert(model_quantized)\nmodel_akida.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Displaying results Attention Maps\n\nInstead of showing predictions, here we propose to show attention maps on an image. This is\nderived from [Abnar et al. attention rollout](https://arxiv.org/abs/2005.00928)_ as shown in the\nfollowing [Keras tutorial](https://keras.io/examples/vision/probing_vits/#method-ii-attention-rollout)_. This aims to\nhighlight the model abilities to focus on relevant parts in the input image.\n\nJust like for the [AkidaNet example](plot_1_akidanet_imagenet.html#sphx-glr-examples-general-plot-1-akidanet-imagenet-py)_, ImageNet\nimages are not publicly available, this example uses a set of 10 copyright free images that were\nfound on Google using ImageNet class names.\n\nGet sample images and preprocess them:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport numpy as np\n\nfrom tensorflow.io import read_file\nfrom tensorflow.image import decode_jpeg\n\nfrom akida_models.imagenet import preprocessing\n\n# Model specification and hyperparameters\nNUM_CHANNELS = 3\nIMAGE_SIZE = 224\n\nNUM_IMAGES = 10\n\n# Retrieve dataset file from Brainchip data server\nfile_path = fetch_file(\n    fname=\"imagenet_like.zip\",\n    origin=\"https://data.brainchip.com/dataset-mirror/imagenet_like/imagenet_like.zip\",\n    cache_subdir='datasets/imagenet_like',\n    extract=True)\ndata_folder = os.path.dirname(file_path)\n\n# Load images for test set\nx_test_files = []\nx_test = np.zeros((NUM_IMAGES, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)).astype('uint8')\nfor id in range(NUM_IMAGES):\n    test_file = 'image_' + str(id + 1).zfill(2) + '.jpg'\n    x_test_files.append(test_file)\n    img_path = os.path.join(data_folder, test_file)\n    base_image = read_file(img_path)\n    image = decode_jpeg(base_image, channels=NUM_CHANNELS)\n    image = preprocessing.preprocess_image(image, (IMAGE_SIZE, IMAGE_SIZE))\n    x_test[id, :, :, :] = np.expand_dims(image, axis=0)\n\nprint(f'{NUM_IMAGES} images loaded and preprocessed.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Build and display the attention map for one selected sample:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import cv2\nimport matplotlib.pyplot as plt\n\nfrom keras import Model\nfrom quantizeml.layers import ClassToken, Attention\nfrom quantizeml.tensors import FixedPoint\nfrom quantizeml.models.transforms.transforms_utils import get_layers_by_type\n\n\ndef build_attention_map(model, image):\n    # Get the Attention layers list\n    attentions = get_layers_by_type(model, Attention)\n\n    # Calculate the number of tokens and deduce the grid size\n    num_tokens = sum(isinstance(ly, ClassToken) for ly in model.layers)\n    grid_size = int(np.sqrt(attentions[0].output_shape[0][-2] - num_tokens))\n\n    # Get the attention weights from each transformer\n    outputs = [la.output[1] for la in attentions]\n    weights = Model(inputs=model.inputs, outputs=outputs).predict(np.expand_dims(image, 0))\n\n    # Converts to float if needed\n    weights = [w.to_float() if isinstance(w, FixedPoint) else w for w in weights]\n    weights = np.array(weights)\n\n    # Heads number\n    num_heads = weights.shape[2]\n    num_layers = weights.shape[0]\n    reshaped = weights.reshape((num_layers, num_heads, grid_size**2 + 1, grid_size**2 + 1))\n\n    # Average the attention weights across all heads\n    reshaped = reshaped.mean(axis=1)\n\n    # To account for residual connections, we add an identity matrix to the attention matrix and\n    # re-normalize the weights.\n    reshaped = reshaped + np.eye(reshaped.shape[1])\n    reshaped = reshaped / reshaped.sum(axis=(1, 2))[:, np.newaxis, np.newaxis]\n\n    # Recursively multiply the weight matrices\n    v = reshaped[-1]\n    for n in range(1, len(reshaped)):\n        v = np.matmul(v, reshaped[-1 - n])\n\n    # Attention from the output token to the input space\n    mask = v[0, 1:].reshape(grid_size, grid_size)\n    mask = cv2.resize(mask / mask.max(), (image.shape[1], image.shape[0]))[..., np.newaxis]\n    return (mask * image).astype(\"uint8\")\n\n\n# Using a specific image for which attention map is easier to observe\nimage = x_test[8]\n\n# Compute the attention map\nattention_float = build_attention_map(model_keras, image)\nattention_quantized = build_attention_map(model_quantized, image)\n\n# Display the attention map\nfig, (ax1, ax2, ax3) = plt.subplots(ncols=3)\nax1.axis('off')\nax1.set_title('Original')\nax1.imshow(image)\n\nax2.axis('off')\nax2.set_title('Float')\nax2.imshow(attention_float)\n\nax3.axis('off')\nax3.set_title('Quantized')\nax3.imshow(attention_quantized)\nfig.suptitle('Attention masks', fontsize=10)\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}