{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Vision transformers\n\nThis tutorial demonstrates that Vision Transformers can be adapted and converted to Akida to perform\nimage classification.\n\nJust like for the [AkidaNet example](plot_1_akidanet_imagenet.html#sphx-glr-examples-general-plot-1-akidanet-imagenet-py)_, ImageNet\nimages are not publicly available, performance is assessed using a set of 10 copyright free images\nthat were found on Google using ImageNet class names.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Dataset preparation\n\nSee [AkidaNet example](plot_1_akidanet_imagenet.html#sphx-glr-examples-general-plot-1-akidanet-imagenet-py)_ for\ndetails on dataset preparation.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport csv\nimport numpy as np\n\nfrom tensorflow.io import read_file\nfrom tensorflow.image import decode_jpeg\nfrom tensorflow.keras.utils import get_file\n\nfrom akida_models.imagenet import preprocessing\n\n# Model specification and hyperparameters\nNUM_CHANNELS = 3\nIMAGE_SIZE = 224\n\nNUM_IMAGES = 10\n\n# Retrieve dataset file from Brainchip data server\nfile_path = get_file(\n    \"imagenet_like.zip\",\n    \"https://data.brainchip.com/dataset-mirror/imagenet_like/imagenet_like.zip\",\n    cache_subdir='datasets/imagenet_like',\n    extract=True)\ndata_folder = os.path.dirname(file_path)\n\n# Load images for test set\nx_test_files = []\nx_test = np.zeros((NUM_IMAGES, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)).astype('uint8')\nfor id in range(NUM_IMAGES):\n    test_file = 'image_' + str(id + 1).zfill(2) + '.jpg'\n    x_test_files.append(test_file)\n    img_path = os.path.join(data_folder, test_file)\n    base_image = read_file(img_path)\n    image = decode_jpeg(base_image, channels=NUM_CHANNELS)\n    image = preprocessing.preprocess_image(image, IMAGE_SIZE)\n    x_test[id, :, :, :] = np.expand_dims(image, axis=0)\n\nprint(f'{NUM_IMAGES} images loaded and preprocessed.')\n\n# Parse labels file\nfname = os.path.join(data_folder, 'labels_validation.txt')\nvalidation_labels = dict()\nwith open(fname, newline='') as csvfile:\n    reader = csv.reader(csvfile, delimiter=' ')\n    for row in reader:\n        validation_labels[row[0]] = row[1]\n\n# Get labels for the test set by index\nlabels_test = np.zeros(NUM_IMAGES)\nfor i in range(NUM_IMAGES):\n    labels_test[i] = int(validation_labels[x_test_files[i]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Create a transformer model\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1. Selecting an architecture\n\nVision Transformers is a hot-topic in AI and new architectures are being introduced regularly.\nWhen selecting an appropriate achitecture for Akida, some size, speed and training capabilities\nmust be considered.\n\nThe following table briefly shows what led to chose the ViT Tiny and DeiT-dist architectures:\n\n+--------------+-------------------+---------+-------------------+----------------------+\n| Architecture | Original accuracy | #Params | Architecture      | Commment             |\n+==============+===================+=========+===================+======================+\n| ViT Base     |  79.90%           |  86M    |  12 heads,        | base model but huge  |\n|              |                   |         |  12 blocks,       | amount of parameters |\n|              |                   |         |  hidden size 768  |                      |\n+--------------+-------------------+---------+-------------------+----------------------+\n| ViT Tiny     |  75.48%           |  5.8M   |  3 heads,         | edge compatible      |\n|              |                   |         |  12 blocks,       |                      |\n|              |                   |         |  hidden size 192  |                      |\n+--------------+-------------------+---------+-------------------+----------------------+\n| DeiT-dist    |  74.17%           |  5.8M   |  3 heads,         | easy to retrain      |\n| Tiny         |                   |         |  12 blocks,       | thanks to the        |\n|              |                   |         |  hidden size 192  | distilled token      |\n+--------------+-------------------+---------+-------------------+----------------------+\n\nThe model zoo then comes with two vision transformers architectures:\n\n - [BC ViT Ti16](../../api_reference/akida_models_apis.html#akida_models.bc_vit_ti16)_, which\n   is a modified version of [ViT TI16](../../api_reference/akida_models_apis.html#akida_models.vit_ti16)_ described in [the\n   original ViT paper](https://arxiv.org/abs/2010.11929)_,\n - [BC DeiT-dist Ti16](../../api_reference/akida_models_apis.html#akida_models.bc_deit_ti16)_,\n   which is a modified version of the original [DeiT TI16](../../api_reference/akida_models_apis.html#akida_models.deit_ti16)_ described in [the\n   original DeiT-dist paper](https://arxiv.org/abs/2012.12877)_.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>The Vision Transformers support has been introduced in Akida 2.0.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2. Model transformations\n\nBoth architectures have been modified so that their layers can be quantized to integer only\noperations. The detailed list of changes is:\n\n  - replace [LayerNormalization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LayerNormalization)_ with\n    [LayerMadNormalization](../../api_reference/quantizeml_apis.html#quantizeml.layers.LayerMadNormalization)_ and\n    replace the last normalization previous to the classification head with a [BatchNormalization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization)_,\n  - replace [GeLU](https://www.tensorflow.org/addons/api_docs/python/tfa/layers/GELU)_\n    activations with [ReLU8](https://www.tensorflow.org/api_docs/python/tf/keras/layers/ReLU)_,\n  - replace the [softmax](https://www.tensorflow.org/api_docs/python/tf/keras/activations/softmax)_ operation in\n    [Attention](../../api_reference/quantizeml_apis.html#quantizeml.layers.Attention)_ with a\n    [shiftmax](../../api_reference/quantizeml_apis.html#quantizeml.layers.shiftmax)_ operation.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Details on the custom layers and operations are given in the API docstrings.</p></div>\n\nLayer replacement is made possible through the ``akida_models create`` CLI that comes with\ndedicated options for the ``vit_ti16`` and ``deit_ti16`` architectures. See for example the helper\nfor ViT:\n\n```bash\n$ akida_models create vit_ti16 -h\nusage: akida_models create vit_ti16 [-h] [-c CLASSES] [-bw BASE_WEIGHTS] [--norm {LN,GN1,BN,LMN}]\n                                    [--last_norm {LN,BN}] [--softmax {softmax,softmax2}]\n                                    [--act {GeLU,ReLU8,swish}] [-i {224,384}]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -c CLASSES, --classes CLASSES\n                        The number of classes, by default 1000.\n  -bw BASE_WEIGHTS, --base_weights BASE_WEIGHTS\n                        Optional keras weights to load in the model, by default None.\n  --norm {LN,GN1,BN,LMN}\n                        Replace normalization in model with a custom function, by default LN\n  --last_norm {LN,BN}   Replace last normalization in model with a custom function, by default LN\n  --softmax {softmax,softmax2}\n                        Replace softmax operation in model with custom function, by default softmax\n  --act {GeLU,ReLU8,swish}\n                        Replace activation function in model with custom function, by default GeLU\n  -i {224,384}, --image_size {224,384}\n                        The square input image size\n```\nThe replacement layers are functionaly equivalent to the base layers but an accuracy loss is\nintroduced at each step. This is compensated by a tuning step after each change.\n\nFor example, replacing activations layers can be done with:\n\n```bash\nwget https://data.brainchip.com/models/AkidaV2/vit/vit_ti16_224.h5\nakida_models create -s vit_ti16_relu.h5 vit_ti16 -bw vit_ti16_224.h5 --act ReLU8\nimagenet_train tune -m vit_ti16_relu.h5 -e 15 --optim Adam --lr_policy cosine_decay \\\n                    -lr 6e-5 -s vit_ti16_relu_tuned.h5\n```\nAfter all changes, the model accuracy is close to (or better than) the original model and the\n\"BC\" transformer model is ready for quantization.\n\n+--------------+-------------------+---------------+\n| Architecture | Original accuracy | \"BC\" accuracy |\n+==============+===================+===============+\n| ViT          |  75.48%           | 74.25%        |\n+--------------+-------------------+---------------+\n| DeiT-dist    |  74.17%           | 75.03%        |\n+--------------+-------------------+---------------+\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>In the following sections, the ViT model will be used but the very same steps apply to\n          DeiT-dist.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3. Load a pre-trained native Keras model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from akida_models.model_io import load_model\n\n# Retrieve the float model with pretrained weights and load it\nmodel_file = get_file(\n    \"bc_vit_ti16_224.h5\",\n    \"https://data.brainchip.com/models/AkidaV2/vit/bc_vit_ti16_224.h5\",\n    cache_subdir='models/akidanet_imagenet')\nmodel_keras = load_model(model_file)\nmodel_keras.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The perfomance given below uses the 10 ImageNet like images subset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Check model performance\ndef check_model_performance(model, x_test=x_test, labels_test=labels_test):\n    outputs_keras = model.predict(x_test, batch_size=NUM_IMAGES)\n    outputs_keras = np.squeeze(np.argmax(outputs_keras, 1))\n    accuracy_keras = np.sum(np.equal(outputs_keras, labels_test)) / NUM_IMAGES\n    print(f\"Keras accuracy: {accuracy_keras*100:.2f} %\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check_model_performance(model_keras)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Quantization\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1. 8-bit PTQ\nThe above native Keras model is quantized to 8-bit (all weights and activations) and we compute\nthe post-training quantization (PTQ) accuracy.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from akida_models import fetch_file\n\n# Retrieve calibration samples\nsamples = fetch_file(\"https://data.brainchip.com/dataset-mirror/samples/imagenet/imagenet_batch1024_224.npz\",\n                     fname=\"imagenet_batch1024_224.npz\")\nsamples = np.load(samples)\nsamples = np.concatenate([samples[item] for item in samples.files])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from quantizeml.models import quantize\nfrom quantizeml.layers import QuantizationParams\n\n# Quantize the model to 8-bit and calibrate using 1024 samples with a batch size of 100 over 2\n# epochs.\nmodel_quantized = quantize(model_keras,\n                           qparams=QuantizationParams(weight_bits=8, activation_bits=8),\n                           num_samples=1024, batch_size=100, epochs=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check_model_performance(model_quantized)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2. Load a pre-trained quantized Keras model\n\nThe [bc_vit_ti16_imagenet_pretrained helper](../../api_reference/akida_models_apis.html#akida_models.bc_vit_ti16_imagenet_pretrained)_ was\nobtained with the same 8-bit quantization scheme but with an additional QAT step to further\nimprove accuracy.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from akida_models import bc_vit_ti16_imagenet_pretrained\n\n# Load the pre-trained quantized model\nmodel_quantized = bc_vit_ti16_imagenet_pretrained()\nmodel_quantized.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "check_model_performance(model_quantized)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Conversion to Akida\n\nThe quantized Keras model is now converted into an Akida model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from cnn2snn import convert\n\n# Convert the model\nmodel_akida = convert(model_quantized)\nmodel_akida.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "accuracy_akida = model_akida.evaluate(x_test, labels_test)\nprint(f\"Accuracy: {accuracy_akida*100:.2f} %\")\n\n# For non-regression purposes\nassert accuracy_akida == 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.3 Attention maps\n\nInstead of showing predictions, here we propose to show attention maps on an image. This is\nderived from [Abnar et al. attention rollout](https://arxiv.org/abs/2005.00928)_ as shown in the\nfollowing [Keras tutorial](https://keras.io/examples/vision/probing_vits/#method-ii-attention-rollout)_. This aims to\nhighlight the model abilities to focus on relevant parts in the input image.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import cv2\nimport matplotlib.pyplot as plt\n\nfrom keras import Model\nfrom quantizeml.layers import ClassToken, Attention\nfrom quantizeml.tensors import FixedPoint\nfrom quantizeml.models.transforms.transforms_utils import get_layers_by_type\n\n\ndef build_attention_map(model, image):\n    # Get the Attention layers list\n    attentions = get_layers_by_type(model, Attention)\n\n    # Calculate the number of tokens and deduce the grid size\n    num_tokens = sum(isinstance(ly, ClassToken) for ly in model.layers)\n    grid_size = int(np.sqrt(attentions[0].output_shape[0][-2] - num_tokens))\n\n    # Get the attention weights from each transformer\n    outputs = [la.output[1] for la in attentions]\n    weights = Model(inputs=model.inputs, outputs=outputs).predict(np.expand_dims(image, 0))\n\n    # Converts to float if needed\n    weights = [w.to_float() if isinstance(w, FixedPoint) else w for w in weights]\n    weights = np.array(weights)\n\n    # Heads number\n    num_heads = weights.shape[2]\n    num_layers = weights.shape[0]\n    reshaped = weights.reshape((num_layers, num_heads, grid_size**2 + 1, grid_size**2 + 1))\n\n    # Average the attention weights across all heads\n    reshaped = reshaped.mean(axis=1)\n\n    # To account for residual connections, we add an identity matrix to the attention matrix and\n    # re-normalize the weights.\n    reshaped = reshaped + np.eye(reshaped.shape[1])\n    reshaped = reshaped / reshaped.sum(axis=(1, 2))[:, np.newaxis, np.newaxis]\n\n    # Recursively multiply the weight matrices\n    v = reshaped[-1]\n    for n in range(1, len(reshaped)):\n        v = np.matmul(v, reshaped[-1 - n])\n\n    # Attention from the output token to the input space\n    mask = v[0, 1:].reshape(grid_size, grid_size)\n    mask = cv2.resize(mask / mask.max(), (image.shape[1], image.shape[0]))[..., np.newaxis]\n    return (mask * image).astype(\"uint8\")\n\n\n# Using a specific image for which attention map is easier to observe\nimage = x_test[8]\n\n# Compute the attention map\nattention_float = build_attention_map(model_keras, image)\nattention_quantized = build_attention_map(model_quantized, image)\n\n# Display the attention map\nfig, (ax1, ax2, ax3) = plt.subplots(ncols=3)\nax1.axis('off')\nax1.set_title('Original')\nax1.imshow(image)\n\nax2.axis('off')\nax2.set_title('Float')\nax2.imshow(attention_float)\n\nax3.axis('off')\nax3.set_title('Quantized')\nax3.imshow(attention_quantized)\nfig.suptitle('Attention masks', fontsize=10)\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}