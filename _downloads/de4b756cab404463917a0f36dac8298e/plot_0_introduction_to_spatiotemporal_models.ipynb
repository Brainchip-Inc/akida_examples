{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Gesture recognition with spatiotemporal models\n\nA tutorial on designing efficient models for streaming video tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introduction: why spatiotemporal models?\n\nRecognizing gestures from video is a challenging task that requires understanding not just\nindividual frames but how those frames evolve over time. Traditional 2D convolutional neural\nnetworks (CNNs) are limited here \u2014 they analyze only spatial features and discard temporal\ncontinuity. 3D CNNs, while well suited to the task, are on the other hand computationally heavy.\n\nTo tackle this, we turn to lightweight spatiotemporal models, specifically designed to process\npatterns in both space (image structure) and time (motion, rhythm). These models are essential\nfor tasks like:\n\n* Gesture classification\n* Online eye-tracking\n* Real-time activity detection in video streams\n\nAt the heart of these models lies a simple idea: decoupling spatial and temporal analysis,\nenables efficient, real-time detection \u2014 even on resource-constrained devices.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Spatiotemporal blocks: the core concept\n\nRather than using full, computationally expensive 3D convolutions, our spatiotemporal blocks break\nthe operation into two parts, a:\n\n1. Temporal convolution, which focuses on changes over time for each spatial pixel (e.g. motion).\n2. Spatial convolution, which looks at image structure in each frame (e.g. shape, position).\n\nThe figures below highlights the difference between a full 3D convolution kernel versus our\nspatiotemporal convolution (a.k.a. TENN in the figure below).\n\n.. figure:: ../../img/example3Dconv.png\n   :target: ../../_images/example3Dconv.png\n   :alt: 3D convolutions\n   :scale: 80 %\n   :align: center\n\n   3D convolutions example\n\n.. figure:: ../../img/exampleT2Sconv.png\n   :target: ../../_images/exampleT2Sconv.png\n   :alt: TENN convolutions\n   :scale: 80 %\n   :align: center\n\n   Spatiotemporal convolutions example\n\nThis factorized approach reduces compute requirements. In fact, this design proved effective in\nvery different domains: it was applied to gesture videos as well as event-based eye tracking\n(see tutorial).\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1. Making it efficient using depthwise separable convolutions\n\nTo further reduce the computational load of the blocks, we can make them separable, just like\ndepthwise separable convolutions replace full convolutions, reducing computation with minimal\naccuracy loss, our decomposed temporal-spatial convolutions can also be made separable using\nan approach inspired by the [MobileNet paper](https://arxiv.org/abs/1704.04861)_. Each layer\nfrom the spatiotemporal block is decomposed into 2: the temporal convolution is\ntransformed into a depthwise temporal convolutional layer followed by a pointwise convolutional\nlayer (see figure above), the same is done for the spatial convolution.\n\n.. Note::\n  The design of these spatiotemporal blocks is similar to R(2+1)D blocks, except we place the\n  temporal layer first. Doing this preserves the temporal richness of the raw input \u2014 a critical\n  decision that avoids \"smearing\" out important movement cues. Moreover, notice that our temporal\n  layers do not have a stride (compared to R(2+1)D layers).\n\n.. figure:: ../../img/comparing_3D_conv_block_designs.png\n   :target: ../../_images/comparing_3D_conv_block_designs.png\n   :alt: various types of 3D convolutions\n   :scale: 60 %\n   :align: center\n\n   Kernel dimensions and strides for various types of 3D convolutions. Dotted lines show depthwise\n   convolutions. Full lines show full convolutions. Orange outlines are for spatial 3D convs and\n   purple ones for temporal convolutions.\n\nA spatiotemporal block can be easily built using the predefined spatiotemporal\nblocks from Akida models available through the [akida_models.layer_blocks.spatiotemporal_block](../../api_reference/akida_models_apis.html#akida_models.layer_blocks.spatiotemporal_block)_\nAPI.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Building the model: from blocks to network\n\nOur gesture recognition model stacks 5 spatiotemporal blocks, forming a shallow yet expressive\nnetwork. This depth allows the model to:\n\n- Gradually capture complex temporal patterns (e.g. \"swipe up\", \"rotate clockwise\")\n- Downsample spatially to control compute load\n- Preserve fine-grained timing via non-strided temporal layers\n- Easily train without skip connections\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "input_shape = (100, 100, 3)\nsampling_frequency = 16\ninput_scaling = (127.5, -1.0)\nn_classes = 27\n\nfrom akida_models.tenn_spatiotemporal import tenn_spatiotemporal_jester\nmodel = tenn_spatiotemporal_jester(input_shape=(sampling_frequency,) + input_shape,\n                                   input_scaling=input_scaling, n_classes=n_classes)\nmodel.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Preserving temporal information\nAs you can see from the summary, the model ends with an 3D average pooling applied only\non the spatial dimensions. This ensures that the model can make predictions after the\nfirst input frame, preserving fine-grained temporal dynamics and bufferized inference\n(see section 6.)\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Gesture classification in videos\n\nIn this tutorial, we use the [Jester dataset](https://www.qualcomm.com/developer/software/jester-dataset)_,\na gesture recognition dataset specifically designed to include movements targeted at human/machine\ninteractions. To do well on the task, information needs to be aggregated across time to accurately\nseparate complex gestures such as clockwise or counterclowise hand turning.\n\nThe data is available to download in the form of zip files from the\n[qualcomm website](https://www.qualcomm.com/developer/software/jester-dataset)_ along with\n[download instructions](https://www.qualcomm.com/content/dam/qcomm-martech/dm-assets/documents/qualcomm-ai-research-jester-download-instructions-v2.pdf)_.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Dataset description\nIn the jester dataset, each sample is a short video clip (about 3 seconds) recorded through a\nwebcam with fixed resolution of 100 pixels in height and a frame rate of 12 FPS. There are in\ntotal 148,092 videos of 27 different complex gestures covering examples such as \"Zooming Out With\n2 fingers\", \"Rolling Hand Forward\", \"Shaking Hand\", \"Stop Sign\", \"Swiping Left\", etc..., also\nincluding a \"no gesture\" and a \"other movements\" classes.\n\nIt is a rich and varied dataset with over 1300 different actors performing the gestures.\nThe dataset has determined splits for training, validation and testing with the ratio of\n80%/10%/10%.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Data preprocessing\nTo train the model effectively, we apply minimal preprocessing:\n\n- Extract a fixed number of frames (here 16 frames) per sample\n- Use strided sampling (stride=2) to reduce redundancy and speed up training\n- Resize the input to a fixed input size (100, 100)\n- Normalize inputs (between -1 and 1)\n- Optionally apply an affine transform for training data (ie. randomly and independently apply\n  translation, scaling, shearing and rotation to each video).\n\nThe dataset is too large to load completely in a tutorial. If you download the dataset\nat the links mentioned above, you can load and preprocess it using the get_data API\navailable under akida_models.tenn_spatiotemporal.jester_train.\n\nAlternatively, the first few validation samples have been set aside and\ncan be loaded here to demonstration purposes.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batch_size = 8\n\n# Download and load validation subset from Brainchip data server\nimport os\nfrom akida_models import fetch_file\nfrom akida_models.tenn_spatiotemporal.jester_train import get_data\n\ndata_path = fetch_file(\n    fname=\"jester_subset.tar.gz\",\n    origin=\"https://data.brainchip.com/dataset-mirror/jester/jester_subset.tar.gz\",\n    cache_subdir=os.path.join(\"datasets\", \"jester\"), extract=True)\ndata_dir = os.path.join(os.path.dirname(data_path), \"jester_subset\")\nval_dataset, val_steps = get_data(\"val\", data_dir, sampling_frequency, input_shape[:2], batch_size)\n\n# Decode numeric labels into human readable ones: contains all string names for classes\n# available in the dataset\nimport csv\nwith open(os.path.join(data_dir, \"jester-v1-labels.csv\")) as csvfile:\n    class_names = [row[0] for row in csv.reader(csvfile)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"classes available are : {class_names}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training and evaluating the model\n\nThe model is trained using standard techniques: Adam optimizer, cosine LR scheduler and\nCategorical Cross-Entropy. We modify the categorical crossentropy slightly to make it \"temporal\":\nthe target class (y-label) is replicated at each time point, thus forcing the model to correctly\nclassify each video frame.\n\nSince the training requires a few GPU hours to complete, we will load a pre-trained model for\ninference. Pre-trained models are available either in floating point or quantized version.\nFirst, we'll look at the floating point model, available using the following apis. The evaluation\ntool is also available to rapidly test the performance on the validation dataset.\n\n.. Note: the accuracy here is low because it is computed weighing each time point equally, i.e.\n         the first frame when the event has not started contributes as much to the predicted label\n         as a frame with an actual movement in it. The validation accuracy will dramatically\n         improve once we allow the model to weigh its output in time (see section below).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from akida_models.model_io import get_model_path, load_model\nfrom akida_models.utils import fetch_file\nfrom akida_models.tenn_spatiotemporal.jester_train import compile_model\n\nmodel_name_v2 = \"tenn_spatiotemporal_jester.h5\"\nfile_hash_v2 = \"fca52a23152f7c56be1f0db59844a5babb443aaf55babed7669df35b516b8204\"\nmodel_path, model_name, file_hash = get_model_path(\"tenn_spatiotemporal\",\n                                                   model_name_v2=model_name_v2,\n                                                   file_hash_v2=file_hash_v2)\nmodel_path = fetch_file(model_path,\n                        fname=model_name,\n                        file_hash=file_hash,\n                        cache_subdir='models')\n\nmodel = load_model(model_path)\ncompile_model(model, 3e-4, val_steps, 1, sampling_frequency)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "hist = model.evaluate(val_dataset)\nprint(hist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Streaming inference: making real-time predictions\n\nOnce trained, these models can be deployed in online inference mode, making predictions\nframe-by-frame. This works thanks to:\n\n- **causal convolutions**, which ensure that predictions at time *t* use only past and current\n  frames, not future ones by adding (left-sided) zero-padding. This is critical for streaming\n  inference where latency matters: we want to be able to make predictions immediately. Our\n  causal temporal layers don't rely on future frames and start making predictions after the\n  first frame is received.\n- **not using a temporal stride**: our model purposefully preserves time information and thus\n  is able to make a classification guess after each incoming frame.\n\nThese choices also allow us to configure the spatio-temporal layer in a efficient way using\nFIFO buffers during inference.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 FIFO buffering\n\nDuring inference, each temporal layer is replaced with a bufferized 2D convolution: i.e. a\nConv2D with an input buffer the size of its kernel (initialized with zeros), handling the\nstreaming input features. Spatial convolutions that have a temporal kernel size of 1 can be\nseamlessly transformed into 2D convolutions too.\n\n.. figure:: ../../img/fifo_buffer.png\n   :target: ../../_images/fifo_buffer.png\n   :alt: fifo_buffer\n   :scale: 80 %\n   :align: center\n\nAt its core, a convolution (whether 2D or 3D) involves sliding a small filter (also called\na kernel) over the input data and computing a dot product between the filter and a small\nsegment (or window) of the input at each step.\n\nTo make this process more efficient, we can use a FIFO (First In, First Out) buffer to\nautomatically manage the sliding window. Here's how it works:\n\n- The input buffer holds the most recent values from the input signal (top row on the figure\n  above).\n- The size of this buffer is equal to the size of the temporal kernel.\n- After each new incoming values, we perform a dot product between the buffer contents and the\n  kernel to produce one output value.\n- Every time a new input value arrives, it's added to the buffer, and the oldest value is\n  removed.\n\nThis works seamlessly in causal convolutional networks, where the output at any time step only\ndepends on the current and past input values\u2014not future ones. Because of this causality, the\nbuffer never needs to \"wait\" for future input: it can compute the output as soon as the first\nframe comes in.\n\n**The result?**: Real-time gesture classification, running continuously, with predictions\nready after every frame.\n\n**How to?**: Quantization will automatically transform compatible spatiotemporal blocks into\ntheir equivalent bufferized version during the *sanitizing* step.\n\n.. Note::\n\n  - After conversion, the 3D Convolution layers are transformed into custom\n    [BufferTempConv](../../api_reference/quantizeml_apis.html#quantizeml.layers.BufferTempConv)_\n    layers.\n  - As opposed to training where the whole 16 frames samples is passed to the model, the inference\n    model requires samples to be passed one by one.\n  - For a better understanding of the buffering process, the sections below will explicitly use\n    [quantizeml.models.transforms.sanitize](../../api_reference/quantizeml_apis.html#quantizeml.models.transforms.sanitize)_ to convert\n    the model to its bufferized version. This is not necessary in practice, as the conversion is\n    done automatically during quantization (see section 8 below).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from quantizeml.models.transforms import sanitize\nmodel_buffer = sanitize(model)\nmodel_buffer.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The models then can be evaluated on the data using the helper available that passes\ndata frame by frame to the model, accumulating the model's responses\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from akida_models.tenn_spatiotemporal.jester_train import evaluate_bufferized_model\nevaluate_bufferized_model(model_buffer, val_dataset, val_steps // batch_size, in_akida=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Weighing information\n\nThe performance of the buffered model is improved because we use a smoothing mecanism on the\nmodel's output:\n\n- at time *t*, the model's outputs is softmaxed\n- the softmaxed values from time *t-1* are decayed (using a decay_factor of 0.8)\n- the two are added\n\nThis is done across all frames available in the video.\nThe predicted class is only computed once all the frames have been seen by the model for the\nbenchmark, but it is possible for the model to predict the video's class after each new frame.\nSection 7 below shows an example of this.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Visualizing the predictions of the model in real time\n\nBecause of this buffering and how the model was trained to output a prediction after each time\nstep, we can effectively visualize the response of the model in time.\nThis part of the tutorial is heavily inspired from the tensorflow tutorial on streaming\nrecognition of gestures based on the [movinet models](https://www.tensorflow.org/hub/tutorials/movinet)_.\n\nWe pass the data through the trained model frame by frame and collect the predicted classes,\napplying a softmax on the output of the model.\nTo make the prediction more robust, at each time step we decay the old predictions by a\ndecay_factor so that they contribute less and less to the final predicted class.\nThe decay_factor is an hyperparameter that you can play with. In practice, it slightly improves\nperformance by smoothing the prediction in time and reducing the impact of earlier frames to\nthe final prediction.\n\nThe video below shows one sample along with the probabilities of the top 5 predictions from\nour bufferized spatiotemporal model at each time point.\n\n.. video:: ../../img/streaming_preds.mp4\n   :nocontrols:\n   :autoplay:\n   :playsinline:\n   :muted:\n   :loop:\n   :width: 50%\n   :align: center\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Quantizing the model and convertion to akida\nThe model can be easily quantized with no cost in accuracy. It can then be easily deployed on\nhardware for online gesture recognition using the convert method from the cnn2snn package.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n# Get the calibration data for accurate quantization: these are a subset from the training data.\nsamples = fetch_file(\n    fname=\"jester_video_bs100.npz\",\n    origin=\"https://data.brainchip.com/dataset-mirror/samples/jester_video/jester_video_bs100.npz\",\n    cache_subdir=os.path.join(\"datasets\", \"jester\"), extract=False)\nsamples = os.path.join(os.path.dirname(data_path), \"jester_video_bs100.npz\")\ndata = np.load(samples)\nsamples_arr = np.concatenate([data[item] for item in data.files])\nnum_samples = len(samples_arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from quantizeml.layers import QuantizationParams, reset_buffers\nfrom quantizeml.models import quantize\n\n# Define the quantization parameters and quantize the model\nqparams = QuantizationParams(activation_bits=8,\n                             per_tensor_activations=True,\n                             weight_bits=8,\n                             input_weight_bits=8,\n                             input_dtype=\"uint8\")\nmodel_quantized = quantize(model, qparams=qparams, samples=samples_arr,\n                           num_samples=num_samples, batch_size=100, epochs=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Evaluate the quantized model\nevaluate_bufferized_model(model_quantized, val_dataset, val_steps // batch_size, in_akida=False)\nreset_buffers(model_quantized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Convert to akida\nfrom cnn2snn import convert\nakida_model = convert(model_quantized)\nakida_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Final thoughts: generalizing the approach\n\nSpatiotemporal networks are powerful, lightweight, and flexible. Whether you're building\ngesture-controlled interfaces or real-time eye-tracking systems, the same design principles\napply:\n\n- Prioritize temporal modeling early in the network\n- Use factorized spatiotemporal convolutions for efficiency\n- Train with augmentation that preserves causality\n- is seamlessly deployed using streaming inference using FIFO buffers\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}