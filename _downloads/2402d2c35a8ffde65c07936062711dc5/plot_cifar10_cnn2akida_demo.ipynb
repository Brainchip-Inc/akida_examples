{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nInference on CIFAR10 with VGG and MobileNet\n===========================================\n\nThe CIFAR-10 dataset consists of 60000 32x32 color images in 10 classes,\nwith 6000 images per class. There are 50000 training images and 10000\ntest images.\n\nThis tutorial uses this dataset for a classic object classification task\n(airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck).\n\nWe start from state-of-the-art CNN models, illustrating how these models\ncan be quantized, then converted to an Akida model with an equivalent\naccuracy.\n\nThe neural networks used in this tutorial are inspired from the\n`VGG <https://arxiv.org/abs/1409.1556>`__ and\n`MobileNets <https://arxiv.org/abs/1704.04861>`__ architecture\nrespectively.\n\nThe VGG architecture uses Convolutional and Dense layers: these layers\nmust therefore be quantized with at most 2 bits of precision to be\ncompatible with the Akida NSoC. This causes a 2 % drop in accuracy.\n\nThe MobileNet architecture uses Separable Convolutional layers that can\nbe quantized using 4 bits of precision, allowing to preserve the base\nKeras model accuracy.\n\n+---------------------------+-------------+\n| Model                     | Accuracy    |\n+===========================+=============+\n| VGG Keras                 | 93.15 %     |\n+---------------------------+-------------+\n| VGG Keras quantized       | 91.30 %     |\n+---------------------------+-------------+\n| VGG Akida                 | **91.59 %** |\n+---------------------------+-------------+\n| MobileNet Keras           | 93.49 %     |\n+---------------------------+-------------+\n| MobileNet Keras quantized | 93.07 %     |\n+---------------------------+-------------+\n| MobileNet Akida           | **93.22 %** |\n+---------------------------+-------------+\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Load CNN2SNN tool dependencies\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# System imports\nimport os\nimport sys\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\nfrom timeit import default_timer as timer\n\n# TensorFlow imports\nfrom tensorflow.keras.datasets import cifar10\n\n# Akida models imports\nfrom akida_models import mobilenet_cifar10, vgg_cifar10\n\n# CNN2SNN\nfrom cnn2snn import convert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Load and reshape CIFAR10 dataset\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Load CIFAR10 dataset\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n\n# Reshape x-data\nx_train = x_train.reshape(50000, 32, 32, 3)\nx_test = x_test.reshape(10000, 32, 32, 3)\ninput_shape = (32, 32, 3)\n\n# Set aside raw test data for use with Akida Execution Engine later\nraw_x_test = x_test.astype('uint8')\n\n# Rescale x-data\na = 255\nb = 0\ninput_scaling = (a, b)\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train = (x_train - b)/a\nx_test = (x_test - b)/a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Create a quantized Keras VGG model\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nA Keras model based on the `VGG <https://arxiv.org/abs/1409.1556>`__\narchitecture is instantiated with quantized weights and activations.\n\nThis model relies only on FullyConnected and Convolutional layers:\n\n  * all the layers have 2-bit weights,\n  * all the layers have 2-bit activations.\n\nThis model therefore satisfies the Akida NSoC requirements.\n\nThis section goes as follows:\n\n  * **3.A - Instantiate a quantized Keras VGG model** according to above\n    specifications and load pre-trained weights** that performs 91 % accuracy\n    on the test dataset.\n  * **3.B - Check performance** on the test set.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3.A Instantiate Keras model\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe ``vgg_cifar10`` function returns a VGG Keras model with custom\nquantized layers (see ``quantization_layers.py`` in the CNN2SNN module).\n\n.. Note:: The pre-trained weights which are loaded in the section 3.B\n          corresponds to the quantization parameters in the next cell. If you\n          want to modify some of these parameters, you must re-train the model\n          and save the weights.\n\nPre-trained weights were obtained after a series of training episodes,\nstarting from unconstrained float weights and activations and ending\nwith quantized 2-bits weights and activations.\n\nFor the first training episode, we train the model with unconstrained\nfloat weights and activations for 1000 epochs.\n\nFor the subsequent training episodes, we start from the weights trained\nin the previous episode, progressively reducing the bitwidth of\nactivations, then weights. We also stop the episode when the training\nloss has stopped decreasing for 20 epochs.\n\nThe table below summarizes the results obtained when preparing the\nweights stored under ``http://data.brainchip.com/models/vgg/``:\n\n+---------+----------------+---------------+----------+--------+\n| Episode | Weights Quant. | Activ. Quant. | Accuracy | Epochs |\n+=========+================+===============+==========+========+\n| 1       | N/A            | N/A           | 93.15 %  | 1000   |\n+---------+----------------+---------------+----------+--------+\n| 2       | 4 bits         | 4 bits        | 93.24 %  | 30     |\n+---------+----------------+---------------+----------+--------+\n| 3       | 3 bits         | 4 bits        | 92.91 %  | 50     |\n+---------+----------------+---------------+----------+--------+\n| 4       | 3 bits         | 3 bits        | 92.38 %  | 64     |\n+---------+----------------+---------------+----------+--------+\n| 5       | 2 bits         | 3 bits        | 91.48 %  | 82     |\n+---------+----------------+---------------+----------+--------+\n| 6       | 2 bits         | 2 bits        | 91.31 %  | 74     |\n+---------+----------------+---------------+----------+--------+\n\nPlease refer to `mnist_cnn2akida_demo example <mnist_cnn2akida_demo.html>`__\nand/or the `CNN2SNN toolkit <../../api_reference/cnn2snn_apis.html>`__\ndocumentation for flow and training steps details.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Instantiate the quantized model\nmodel_keras = vgg_cifar10(input_shape,\n                          weights='cifar10',\n                          weights_quantization=2,\n                          activ_quantization=2,\n                          input_weights_quantization=2)\nmodel_keras.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3.B Check performance\n^^^^^^^^^^^^^^^^^^^^^\n\nWe check the Keras model accuracy on the first *n* images of the test\nset.\n\nThe table below summarizes the expected results:\n\n+---------+----------+\n| #Images | Accuracy |\n+=========+==========+\n| 100     | 94.00 %  |\n+---------+----------+\n| 1000    | 90.80 %  |\n+---------+----------+\n| 10000   | 91.30 %  |\n+---------+----------+\n\n.. Note:: Depending on your hardware setup, the processing time may vary\n          greatly.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_images = 1000\n\n# Check Model performance\nstart = timer()\npotentials_keras = model_keras.predict(x_test[:num_images])\npreds_keras = np.squeeze(np.argmax(potentials_keras, 1))\n\naccuracy = accuracy_score(y_test[:num_images], preds_keras)\nprint(\"Accuracy: \"+\"{0:.2f}\".format(100*accuracy)+\"%\")\nend = timer()\nprint(f'Keras inference on {num_images} images took {end-start:.2f} s.\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. Conversion to Akida\n~~~~~~~~~~~~~~~~~~~~~~\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.A Convert to Akida model\n^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nWhen converting to an Akida model, we just need to pass the Keras model\nand the input scaling that was used during training.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Convert the model\nmodel_akida = convert(model_keras, input_scaling=input_scaling)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.B Check hardware compliancy\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe `Model.summary() <../../api_reference/aee_apis.html#akida.Model.summary>`__\nmethod provides a detailed description of the Model layers.\n\nIt also indicates it they are hardware-compatible (see the ``HW`` third\ncolumn).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_akida.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.C Check performance\n^^^^^^^^^^^^^^^^^^^^^\n\nWe check the Akida model accuracy on the first *n* images of the test\nset.\n\nThe table below summarizes the expected results:\n\n+---------+----------+\n| #Images | Accuracy |\n+=========+==========+\n| 100     | 95.00 %  |\n+---------+----------+\n| 1000    | 91.90 %  |\n+---------+----------+\n| 10000   | 91.59 %  |\n+---------+----------+\n\nDue to the conversion process, the predictions may be slightly different\nbetween the original Keras model and Akida on some specific images.\n\nThis explains why when testing on a limited number of images the\naccuracy numbers between Keras and Akida may be quite different. On the\nfull test set however, the two models accuracies are almost identical.\n\n .. Note:: Depending on your hardware setup, the processing time may vary\n           greatly.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_images = 1000\n\n# Check Model performance\nstart = timer()\nresults = model_akida.predict(raw_x_test[:num_images])\naccuracy = accuracy_score(y_test[:num_images], results)\n\nprint(\"Accuracy: \"+\"{0:.2f}\".format(100*accuracy)+\"%\")\nend = timer()\nprint(f'Akida inference on {num_images} images took {end-start:.2f} s.\\n')\n\n# For non-regression purpose\nif num_images == 1000:\n    assert accuracy == 0.919"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Print model statistics\nprint(\"Model statistics\")\nstats = model_akida.get_statistics()\nmodel_akida.predict(raw_x_test[:20])\nfor _, stat in stats.items():\n    print(stat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5. Create a quantized Keras MobileNet model\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nA Keras model based on the\n`MobileNets <https://arxiv.org/abs/1704.04861>`__ architecture is\ninstantiated with quantized weights and activations.\n\nThis model relies on a first Convolutional layer followed by several\nSeparable Convolutional layers:\n\n  * all the layers have 4-bit weights,\n  * all the layers have 4-bit activations.\n\nThis model therefore satisfies the Akida NSoC requirements.\n\nThis section goes as follows:\n\n  * **5.A - Instantiate a quantized Keras model** according to above\n    specifications\n  * **5.B - Check performance** on the test set.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5.A Instantiate Keras MobileNet model\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe ``mobilenet_cifar10`` function returns a MobileNet Keras model with\ncustom quantized layers (see ``quantization_layers.py`` in the CNN2SNN\nmodule).\n\n .. Note:: The pre-trained weights which are loaded in the section 3.B\n           corresponds to the quantization parameters in the next cell. If you\n           want to modify some of these parameters, you must re-train the\n           model and save the weights.\n\nPre-trained weights were obtained after two training episodes:\n\n  * first, we train the model with unconstrained float weights and\n    activations for 1000 epochs,\n  * then, we tune the model with quantized weights initialized from those\n    trained in the previous episode.\n\nWe stop the second training episode when the training loss has stopped\ndecreasing for 20 epochs.\n\nThe table below summarizes the results obtained when preparing the\nweights stored under ``http://data.brainchip.com/models/mobilenet/``:\n\n+---------+----------------+---------------+----------+--------+\n| Episode | Weights Quant. | Activ. Quant. | Accuracy | Epochs |\n+=========+================+===============+==========+========+\n| 1       | N/A            | N/A           | 93.49 %  | 1000   |\n+---------+----------------+---------------+----------+--------+\n| 2       | 4 bits         | 4 bits        | 93.07 %  | 44     |\n+---------+----------------+---------------+----------+--------+\n\nPlease refer to `mnist_cnn2akida_demo example <mnist_cnn2akida_demo.html>`__\nand/or the `CNN2SNN toolkit <../../api_reference/cnn2snn_apis.html>`__\ndocumentation for flow and training steps details.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Use a quantized model with pretrained quantized weights (93.07% accuracy)\nmodel_keras = mobilenet_cifar10(input_shape,\n                                weights='cifar10',\n                                weights_quantization=4,\n                                activ_quantization=4,\n                                input_weights_quantization=8)\nmodel_keras.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5.B Check performance\n^^^^^^^^^^^^^^^^^^^^^\n\nWe check the Keras model accuracy on the first *n* images of the test\nset.\n\nThe table below summarizes the expected results:\n\n+---------+----------+\n| #Images | Accuracy |\n+=========+==========+\n| 100     | 95.00 %  |\n+---------+----------+\n| 1000    | 93.10 %  |\n+---------+----------+\n| 10000   | 93.07 %  |\n+---------+----------+\n\n.. Note:: Depending on your hardware setup, the processing time may vary\n          greatly.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_images = 1000\n\n# Check Model performance\nstart = timer()\npotentials_keras = model_keras.predict(x_test[:num_images])\npreds_keras = np.squeeze(np.argmax(potentials_keras, 1))\n\naccuracy = accuracy_score(y_test[:num_images], preds_keras)\nprint(\"Accuracy: \"+\"{0:.2f}\".format(100*accuracy)+\"%\")\nend = timer()\nprint(f'Keras inference on {num_images} images took {end-start:.2f} s.\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6. Conversion to Akida\n~~~~~~~~~~~~~~~~~~~~~~\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6.A Convert to Akida model\n^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nWhen converting to an Akida model, we just need to pass the Keras model\nand the input scaling that was used during training.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_akida = convert(model_keras, input_scaling=input_scaling)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6.B Check hardware compliancy\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe `Model.summary() <../../api_reference/aee_apis.html#akida.Model.summary>`__\nmethod provides a detailed description of the Model layers.\n\nIt also indicates it they are hardware-compatible (see the ``HW`` third\ncolumn).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_akida.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6.C Check performance\n^^^^^^^^^^^^^^^^^^^^^\n\nWe check the Akida model accuracy on the first *n* images of the test\nset.\n\nThe table below summarizes the expected results:\n\n+---------+----------+\n| #Images | Accuracy |\n+=========+==========+\n| 100     | 95.00 %  |\n+---------+----------+\n| 1000    | 93.10 %  |\n+---------+----------+\n| 10000   | 93.22 %  |\n+---------+----------+\n\nDue to the conversion process, the predictions may be slightly different\nbetween the original Keras model and Akida on some specific images.\n\nThis explains why when testing on a limited number of images the\naccuracy numbers between Keras and Akida may be quite different. On the\nfull test set however, the two models accuracies are almost identical.\n\n .. Note:: Depending on your hardware setup, the processing time may vary\n           greatly.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_images = 1000\n\n# Check Model performance\nstart = timer()\nresults = model_akida.predict(raw_x_test[:num_images])\naccuracy = accuracy_score(y_test[:num_images], results)\n\nprint(\"Accuracy: \"+\"{0:.2f}\".format(100*accuracy)+\"%\")\nend = timer()\nprint(f'Akida inference on {num_images} images took {end-start:.2f} s.\\n')\n\n# For non-regression purpose\nif num_images == 1000:\n    assert accuracy == 0.931"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Print model statistics\nprint(\"Model statistics\")\nstats = model_akida.get_statistics()\nmodel_akida.predict(raw_x_test[:20])\nfor _, stat in stats.items():\n    print(stat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6D. Show predictions for a random image\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport matplotlib.lines as lines\nimport matplotlib.patches as patches\n\nlabel_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n\n# prepare plot\nbarWidth = 0.75\npause_time = 1\n\nfig = plt.figure(num='CIFAR10 Classification by Akida Execution Engine', figsize=(8, 4))\nax0 = plt.subplot(1, 3, 1)\nimgobj = ax0.imshow(np.zeros((32, 32, 3), dtype=np.uint8))\nax0.set_axis_off()\n# Results subplots\nax1 = plt.subplot(1, 2, 2)\nax1.xaxis.set_visible(False)\nax0.text(0, 34, 'Actual class:')\nactual_class = ax0.text(16, 34, 'None')\nax0.text(0, 37, 'Predicted class:')\npredicted_class = ax0.text(20, 37, 'None')\n\n# Take a random test image\ni = np.random.randint(y_test.shape[0])\n\ntrue_idx = int(y_test[i])\npot =  model_akida.evaluate(np.expand_dims(raw_x_test[i], axis=0)).squeeze()\n\nrpot = np.arange(len(pot))\nax1.barh(rpot, pot, height=barWidth)\nax1.set_yticks(rpot - 0.07*barWidth)\nax1.set_yticklabels(label_names)\npredicted_idx = pot.argmax()\nimgobj.set_data(raw_x_test[i])\nif predicted_idx == true_idx:\n    ax1.get_children()[predicted_idx].set_color('g')\nelse:\n    ax1.get_children()[predicted_idx].set_color('r')\nactual_class.set_text(label_names[true_idx])\npredicted_class.set_text(label_names[predicted_idx])\nax1.set_title('Akida\\'s predictions')\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}