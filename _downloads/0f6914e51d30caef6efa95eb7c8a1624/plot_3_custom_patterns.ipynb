{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Advanced ONNX models quantization\n\nAkida, like any specialized hardware accelerator, sacrifices very generalized computational\nability in favor of highly optimized implementations of a subset of key operations. While\nwe strive to make sure that Akida directly supports the most important models, it isn't\nfeasible to support all possibilities. You may thus occasionally find yourself with a\nmodel which is very nearly compatible with Akida, but which fails to convert due to just\na few incompatibilities. In this example, we will look at some simple workarounds and how\nto implement them. The goal is to successfully convert the model to Akida without having\nto retrain.\n\nPreparing a model for Akida requires two steps: quantization, followed by conversion\nfor a specific target hardware device. We try to catch as many incompatibilities as\npossible at the quantization step. However, some constraints depend on the specific\ntarget device, and can only be caught at the conversion step. To illustrate, we will\nsimply walk through the process of preparing [ResNet50](https://github.com/onnx/models/tree/main/archive/vision/classification/resnet#model)_ for\nacceleration on Akida - we'll run into several incompatibilities at different points\nin that process, and see how to resolve them.\n\nThis example assumes a moderate level of experience with deep learning, and good familiarity\nwith the operations typically encountered in these types of models. For example, here we'll\nuse the following workarounds:\n\n* to avoid some incompatible sequences of operations we'll insert layers with \"identity\"\n  convolution kernels,\n* in order to avoid an unusual kernel-size 1/stride 2 convolution, we'll substitute those\n  kernels with equivalent size 3 kernels.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Get model and data\nBefore diving into the model incompatibilities and how to resolve them, we'll need to acquire\nsome sample data to test on, plus the pretrained model.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Data\n\nGiven that the reference model was trained on [ImageNet](https://www.image-net.org/)_ dataset\n(which is not publicly available), this tutorial uses a set of 10 copyright free images.\nSee [data preparation](../general/plot_1_akidanet_imagenet.html#dataset-preparation)_\nfor more details.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport csv\nimport numpy as np\n\nfrom tensorflow.io import read_file\nfrom tensorflow.image import decode_jpeg\nfrom tensorflow.keras.utils import get_file\n\nfrom akida_models.imagenet import preprocessing\nfrom akida_models.imagenet.imagenet_utils import IMAGENET_MEAN, IMAGENET_STD\n\n# Model specification and hyperparameters\nNUM_CHANNELS = 3\nIMAGE_SIZE = 224\n\nnum_images = 10\n\n# Retrieve dataset file from Brainchip data server\nfile_path = get_file(\n    \"imagenet_like.zip\",\n    \"https://data.brainchip.com/dataset-mirror/imagenet_like/imagenet_like.zip\",\n    cache_subdir='datasets/imagenet_like',\n    extract=True)\ndata_folder = os.path.dirname(file_path)\n\n# Load images for test set\nx_test_files = []\nx_test_raw = np.zeros((num_images, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)).astype('uint8')\nfor id in range(num_images):\n    test_file = 'image_' + str(id + 1).zfill(2) + '.jpg'\n    x_test_files.append(test_file)\n    img_path = os.path.join(data_folder, test_file)\n    base_image = read_file(img_path)\n    image = decode_jpeg(base_image, channels=NUM_CHANNELS)\n    image = preprocessing.preprocess_image(image, (IMAGE_SIZE, IMAGE_SIZE))\n    x_test_raw[id, :, :, :] = np.expand_dims(image, axis=0)\n\n# Parse labels file\nfname = os.path.join(data_folder, 'labels_validation.txt')\nvalidation_labels = dict()\nwith open(fname, newline='') as csvfile:\n    reader = csv.reader(csvfile, delimiter=' ')\n    for row in reader:\n        validation_labels[row[0]] = row[1]\n\n# Get labels for the test set by index\nlabels_test = np.zeros(num_images)\nfor i in range(num_images):\n    labels_test[i] = int(validation_labels[x_test_files[i]])\n\n# Normalize images as models expects\nimagenet_mean_255 = np.array(IMAGENET_MEAN, dtype=\"float32\") * 255.0\nimagenet_std_255 = np.array(IMAGENET_STD, dtype=\"float32\") * 255.0\nx_test = ((x_test_raw - imagenet_mean_255) / imagenet_std_255)\n\n# Transpose the channels to the first axis as per the default for ONNX models\nx_test = np.transpose(x_test, (0, 3, 1, 2))\n\nprint(f'{num_images} images loaded and preprocessed.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Download the model\n\nWe download ResNet50 from the [ONNX ZOO](https://github.com/onnx/models/tree/main/archive/vision/classification),\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import onnx\nimport onnx.hub\nfrom onnxruntime import InferenceSession\n\nonnx_model = onnx.hub.load(\"ResNet50\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 Evaluate model performance\n\nThe [ONNXRuntime](https://onnxruntime.ai)_ package is a cross-platform\naccelerator capable of loading and running models described in ONNX format.\nWe use this framework to evaluate the performance of the loaded ResNet50\nmodel.\n\n.. Note:: For example purposes, we only compute accuracy on 10 images.\n   Accuracy on the full ImageNet validation set is reported at the end.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def evaluate_onnx_model(model):\n    sess = InferenceSession(model.SerializeToString())\n    # Calculate outputs by running images through the session\n    outputs = sess.run(None, {model.graph.input[0].name: x_test})\n    # The class with the highest score is what we choose as prediction\n    predicted = np.squeeze(np.argmax(outputs[0], 1))\n    # Compute the model accuracy\n    accuracy = (predicted == labels_test).sum() / num_images\n    return accuracy\n\n\n# Evaluate over test dataset\naccuracy_floating = evaluate_onnx_model(onnx_model)\nprint(f'Floating point model accuracy: {100 * accuracy_floating:.2f} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Quantize\n\nAkida processes integer inputs, activations and weights. Therefore, the first step in\npreparing a floating point model to run on Akida is to quantize it using [QuantizeML quantize()](../../api_reference/quantizeml_apis.html#quantizeml.models.quantize)_.\n\n.. Note::\n  Please refer to the [QuantizeML toolkit user guide](../../user_guide/quantizeml.html)_\n  and the [Advanced QuantizeML tutorial](plot_0_advanced_quantizeml.html)_ for further details.\n  In particular here, for simplicity, we pass only the small number of samples we already prepared\n  for calibration. Typically, you will want to use many more samples for calibration, say 1000 if\n  you have them available; and not drawn from your test data. The akida_models package provides a\n  helper function, [extract_samples()](../../api_reference/akida_models_apis.html#extract-samples)_\n  which may be helpful in preparing those.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from quantizeml.models import quantize\n\nmodel_quantized = quantize(onnx_model, samples=x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that the model is not fully quantized, stopping at the first unrecognized\npattern (node ``resnetv17_stage1_activation0 (Relu)``). We know that Akida can definitely\nhandle ReLU activation functions, so we have to look more closely to understand the\nproblem. Analyzing the model, the ReLU immediately follows an ``Add`` operator. It is\nthis sequence of operations which is not supported by Akida.\n\n.. figure:: ../../img/unsupported_activation.png\n   :target: ../../_images/unsupported_activation.png\n   :alt: Unsupported activation\n   :scale: 70 %\n   :align: center\n\n   Unsupported pattern: [``Add``, ``Relu``].\n\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 About Patterns\n\nFor efficiency, Akida hardware actually groups certain commonly occuring\noperations together. For example, ReLU activation functions, where present,\nare almost always applied on the outputs of the hard-working computational\nlayers (Convolutions, Depthwise Convolutions, Dense layers etc.). So the ReLU\non Akida is tied to those operations. While efficient, this does mean that\nsome sequences of operations will not by default be considered Akida-compatible,\neven though the individual operations are known to be handled. That's the\ncause of the problem encountered here.\n\n\nTo properly see what's going on, and to resolve the problem, we'll need to\nunderstand the concept of \"patterns\". These are the objects that QuantizeML\nuses to map ONNX models to their Akida equivalents. A pattern is a sequence of\ncontinuous [ONNX operators](https://onnx.ai/onnx/operators/) in a graph that\n**can be converted** to an\n[Akida V2 layer](../../api_reference/akida_apis.html#akida-v2-layers).\nFor example, the following model would be converted to an [akida.InputConv2D](../../api_reference/akida_apis.html#akida.InputConv2D) layer:\n\n.. figure:: ../../img/onnx_input_conv2d.png\n   :target: ../../_images/onnx_input_conv2d.png\n   :alt: InputConv2D example model\n   :scale: 80 %\n   :align: center\n\n   One ONNX configuration that would map to an [InputConv2D](../../api_reference/akida_apis.html#akida.InputConv2D).\n\n\nThe sequence of operators [``Conv``, ``Clip``, ``MaxPool``] **is one valid pattern**\nfor conversion towards [InputConv2D](../../api_reference/akida_apis.html#akida.InputConv2D).\n\n\nCrucially, we can check the list of the currently supported patterns:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from quantizeml.onnx_support.quantization.register_patterns import PATTERNS_MAP\n\nprint(*PATTERNS_MAP, sep='\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looking at that list, it should be apparent that a ``ReLU`` operation on its own or\nfollowing an ``Add`` is not considered a compatible pattern.\n\n.. Note::\n  Before the conversion the following changes are automatically done to allow the\n  QuantizeML toolkit to see an ONNX graph suitable for quantization:\n\n      1. transforms the following operators for general purposes:\n\n         * ``Conv`` -> ``DepthwiseConv`` when kernel size is 1 x Kx x Ky and ``group`` is required\n         * ``Clip`` > ``Relu`` (if ``min = 0.0``)\n\n      2. uses [Graph Optimizations in ONNX Runtime](https://onnxruntime.ai/docs/performance/model-optimizations/graph-optimizations.html)\n         to optimize the graph (e.g. fuse BatchNorm into convolutions).\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2. Custom quantization patterns\n\nThe existing patterns won't allow us to map an isolated ReLU operation. But, for example,\nthe ReLU operation can be mapped when following a Conv layer, and we can easily implement\na Conv layer that performs an identity operation on its inputs, just by setting the kernel\nweights appropriately. We can implement this workaround by using custom quantization\npatterns to extend ``PATTERNS_MAP``.\n\nEvery pattern includes an ONNX layer that stores the ONNX graph information for the matching\nsequence of nodes. QuantizeML also allows for a function to create a compatible layer from\nan initially incompatible pattern. This pattern function has two input parameters: the graph\nand the pattern-matched sequence of nodes extracted from it.\n\nOnce a pattern function is defined for an unsupported pattern, both can be appended\nin the quantization context through the ``custom_pattern_scope`` function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from quantizeml.onnx_support import layers\nfrom quantizeml.onnx_support.quantization import custom_pattern_scope\n\n\nclass IdentityQuantizedConv2D(layers.QuantizedConv2D):\n    def __build__(self, input_ts, downscale=True):\n        # Produces a kernel such that the convolution does not modify the input.\n        identity_kernel = np.identity(input_ts.shape[1], \"float32\")[..., None, None]\n        self.set_weight(\"kernel\", identity_kernel)\n        return super().__build__(input_ts, downscale)\n\n\ndef relu_pattern_fn(block_nodes, graph):\n    \"\"\"Convert the incompatible patterns ['Relu'] and ['Relu', 'GlobalAveragePool'] into\n    an IdentityQuantizedConv2D.\n    \"\"\"\n    # Note that as 'quantization_pattern_map' is written, this function expects to receive\n    # only the isolated ('Relu') that matches in the graph.\n    block_ops = [x.op_type for x in block_nodes]\n    if block_ops == ['Relu']:\n        return IdentityQuantizedConv2D(activation=True)\n    else:\n        raise Exception(f\"Unrecognized pattern: {block_ops}\")\n\n\n# Define a custom patterns map, as a new pattern and associated replacement function.\nrelu_pattern_map = {\n    \"Relu\": relu_pattern_fn,\n}\n\n# Include relu_pattern_map in the quantization context\nwith custom_pattern_scope(relu_pattern_map):\n    model_quantized = quantize(onnx_model, samples=x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the isolated ReLU fixed, we managed to quantize much more of the model, but\nwe hit a new problem, node ``resnetv17_pool1_fwd (GlobalAveragePool)``. Looking back\nat the list of compatible patterns, we can see that, like the ReLU, a GlobalAveragePooling\n(GAP) operation cannot be handled in isolation, but is compatible when it follows\nConv or Conv + ReLU operations. The second of those will suit us better here,\nthat way we can combine it with our solution for the ReLU operation (because\nthe GAP here does indeed follow one of the isolated ReLU ops).\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def activation_pattern_fn(block_nodes, graph):\n    \"\"\"Convert the incompatible patterns ['Relu'] and ['Relu', 'GlobalAveragePool'] into\n    an IdentityQuantizedConv2D.\n    \"\"\"\n    # Note that as 'quantization_pattern_map' is written, this function expects to receive\n    # only the sequences ('Relu') or ('Relu', 'GlobalAveragePool').\n    block_ops = [x.op_type for x in block_nodes]\n    if block_ops == ['Relu']:\n        return IdentityQuantizedConv2D(activation=True)\n    elif block_ops == ['Relu', 'GlobalAveragePool']:\n        return IdentityQuantizedConv2D(activation=True, pool_type=\"gap\")\n    else:\n        raise Exception(f\"Unrecognized pattern: {block_ops}\")\n\n\n# Define quantization custom patterns map, as a set of patterns and associated replacement function.\n# activation_pattern_fn was designed to handle two similar incompatibilities present in ResNet50.\nquantization_pattern_map = {\n    (\"Relu\", \"GlobalAveragePool\"): activation_pattern_fn,\n    \"Relu\": activation_pattern_fn,\n}\n\n# Include quantization_pattern_map in the quantization context\nwith custom_pattern_scope(quantization_pattern_map):\n    model_quantized = quantize(onnx_model, samples=x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The full model is now quantized successfully.\nAt this point we can re-check its accuracy:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "accuracy = evaluate_onnx_model(model_quantized)\nprint(f'Quantized model accuracy: {100 * accuracy:.2f} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Conversion\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1. Incompatibility at Conversion\n\nAs indicated above, while most imcompatibilities will be picked up at the\nquantization step, some constraints are specific to the target hardware\ndevice, and can only be applied at the conversion step. We can detect these\neither with the [check_model_compatibility](../../api_reference/cnn2snn_apis.html#cnn2snn.check_model_compatibility)_tool,\nor by trying to [convert the model into Akida](../../api_reference/cnn2snn_apis.html#cnn2snn.convert)_.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from cnn2snn import convert\n\ntry:\n    akida_model = convert(model_quantized)\nexcept Exception as e:\n    print(f\"ResNet50 is not fully accelerated by Akida. Reason: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This error is raised because the ResNet50 padding scheme is very specific and differs\nfrom the Keras/Akida standard.\n\nIdeally, we should aim to swap incompatible operations with mathematically\nequivalent replacements. For issues of convolution kernel size or padding, we can\noften achieve that by putting the kernel weights within a larger kernel, placed\neccentrically to compensate for any padding issues etc. More on that below - but\nwe can't use that strategy here, because the kernel size for this layer (7x7) is\nalready the maximum supported by the Akida input layer. In this case, we'll have to\ntry simply modifying the padding to be Akida-compatible. Because this is the input\nlayer, we could actually negate that change by padding the input image along two\nedges before passing to Akida. However, precisely because this is the very start of\nthe network, and the consequence is only a single pixel of spatial offset, we might\nexpect that the impact on model performance will be negligible, and that's precisely\nwhat we find on testing. So let's keep things simple in this case: simply replace the\nincompatible values with compatible ones.\n\nTo achieve this, we'll again customize the pattern functions to modify the model before\nquantization. Rather than try to provide a general solution, we'll hard code this for\nthe problem layer:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from quantizeml.onnx_support import graph_tools\n\n\ndef align_input_conv_with_akida(block_nodes, graph):\n    \"\"\"Pattern function that handles convolutions incompatible with Akida\n    \"\"\"\n    # Recover initial ONNXLayer from block nodes and graph\n    qconv = layers.get_qconv(block_nodes, graph)\n\n    # Force the pads in first convolution to Akida compatible values\n    if qconv.name == 'resnetv17_conv0_fwd':\n        print(\"Setting Akida pads in first convolution...\")\n        # Note: pads in convolution include spatial dimension\n        qconv.set_weight(\"pads\", np.array([0, 0, 2, 2, 0, 0, 3, 3]))\n        graph_tools.replace_field(qconv, \"pool_pads\", [0, 0, 1, 1])\n    return qconv\n\n\n# Infer intermediate shape: This is required for some custom pattern functions\nonnx_model_temp = onnx.shape_inference.infer_shapes(onnx_model)\n\n# Quantize model with custom patterns\nquantization_pattern_map = {\n    (\"Conv\", \"Relu\", \"MaxPool\"): align_input_conv_with_akida,\n    (\"Conv\", \"Relu\"): align_input_conv_with_akida,\n    (\"Conv\",): align_input_conv_with_akida,\n    (\"Relu\", \"GlobalAveragePool\"): activation_pattern_fn,\n    \"Relu\": activation_pattern_fn,\n}\nwith custom_pattern_scope(quantization_pattern_map):\n    model_quantized = quantize(onnx_model_temp, samples=x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's try to convert again:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "try:\n    akida_model = convert(model_quantized)\nexcept Exception as e:\n    print(f\"ResNet50 is not fully accelerated by Akida. Reason: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The error message now indicates that there is a problem with a stride 2 operation,\nbecause the required kernel size is not supported. Looking at the ResNet50 definition,\nwe can see that there's a very unusual kernel-size 1 / stride 2 conv operation\napplied within the stride-2 blocks at the beginning of each stage.\nThat's a good candidate for the workaround mentioned earlier:\nwe can simply swap in a compatible kernel-size 3 / stride 2 convolution, placing\nthe weights from the original size 1 kernel within (otherwise zero-valued) size\n3 kernels. In Akida (and Keras), the kernel-size 3/ stride 2 conv operation has padding\n[0, 0, 1, 1], so to make the replacement operation equivalent we need to\nplace the smaller kernel weights eccentrically within the larger kernels.\n\nWe'll combine that with the previous padding fix within a single function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def align_conv_with_akida(block_nodes, graph):\n    \"\"\"Pattern function that handles convolutions incompatible with Akida\n    \"\"\"\n    # Recover initial ONNXLayer from block nodes and graph\n    qconv = layers.get_qconv(block_nodes, graph)\n\n    # Force the pads in first convolution to Akida compatible values\n    if qconv.name == 'resnetv17_conv0_fwd':\n        print(\"Setting Akida pads in first convolution...\")\n        # Note: pads in convolution include spatial dimension\n        qconv.set_weight(\"pads\", np.array([0, 0, 2, 2, 0, 0, 3, 3]))\n        graph_tools.replace_field(qconv, \"pool_pads\", [0, 0, 1, 1])\n\n    # Replace 1x1 kernel with strides 2x2 by a padded 3x3 one\n    kernel = qconv.weights[\"kernel\"]\n    strides = graph_tools.get_field(qconv, \"strides\")\n    if kernel.shape[-2:] == (1, 1) and strides == [2, 2]:\n        # Only spatial dimensions are padded\n        # The original weights are placed eccentrically in the new kernel\n        new_kernel = np.pad(kernel, ((0, 0), (0, 0), (0, 2), (0, 2)))\n        # Set new kernel in the layer\n        # This operation requires a specific padding pattern for Akida compatibility.\n        qconv.set_weight(\"pads\", np.array([0, 0, 0, 0, 0, 0, 1, 1]))\n        qconv.set_weight(\"kernel\", new_kernel)\n        print(f\"Kernel updated in {qconv.name} from 1x1 to 3x3.\")\n    return qconv\n\n\n# Infer intermediate shape: This is required for some custom pattern functions\nonnx_model = onnx.shape_inference.infer_shapes(onnx_model)\n\n# Quantize model with custom patterns\nquantization_pattern_map = {\n    (\"Conv\", \"Relu\", \"MaxPool\"): align_conv_with_akida,\n    (\"Conv\", \"Relu\"): align_conv_with_akida,\n    (\"Conv\",): align_conv_with_akida,\n    (\"Relu\", \"GlobalAveragePool\"): activation_pattern_fn,\n    \"Relu\": activation_pattern_fn,\n}\nwith custom_pattern_scope(quantization_pattern_map):\n    model_quantized = quantize(onnx_model, samples=x_test)\n\n# Evaluate quantized model performance\naccuracy = evaluate_onnx_model(model_quantized)\nprint(f'Quantized model accuracy: {100 * accuracy:.2f} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2. Successful Conversion\nTime to check conversion again\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "akida_model = convert(model_quantized)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Great - the model is now both quantized successfully, and can be\nentirely converted for acceleration on Akida. To check its\nperformance, we need to bear in mind that\n\n1. images must be numpy-raw, with an 8-bit unsigned integer data type and\n2. the channel dimension must be in the last dimension.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Evaluate performance\nakida_accuracy = akida_model.evaluate(x_test_raw, labels_test)\nprint(f'Akida model accuracy: {100 * akida_accuracy:.2f} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3. Performance on the full ImageNet validation set\nThe table below summarizes the obtained accuracy at the various stages using the full\nImageNet dataset. Note that forcing pads on the first layer decreases the performance\nof the model by 0.445% - as noted, that change could be rendered lossless by padding\nthe input image prior to sending instead.\n\n+------------------------------------------+----------------+--------------------+----------------+\n| Float accuracy (before Akida adaptation) | Float accuracy | Quantized accuracy | Akida accuracy |\n+==========================================+================+====================+================+\n| 74.368                                   | 73.918         | 73.590             | 73.620         |\n+------------------------------------------+----------------+--------------------+----------------+\n\n.. Note::\n   The images shown in this tutorial are produced through [Netron](https://netron.app/).\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}