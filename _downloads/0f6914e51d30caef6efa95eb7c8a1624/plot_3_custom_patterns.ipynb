{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Advanced ONNX models quantization\n\nAkida, like any specialized hardware accelerator, sacrifices very generalized computational\nability in favor of highly optimized implementations of a subset of key operations. While\nwe strive to make sure that Akida directly supports the most important models, it isn't\nfeasible to support all possibilities. You may thus occasionally find yourself with a\nmodel which is very nearly compatible with Akida, but which fails to convert due to just\na few incompatibilities. In this example, we will look at some simple workarounds and how\nto implement them. The goal is to successfully convert the model to Akida without having\nto retrain.\n\nPreparing a model for Akida requires two steps: quantization, followed by conversion\nfor a specific target hardware device. We try to catch as many incompatibilities as\npossible at the quantization step. However, some constraints depend on the specific\ntarget device, and can only be caught at the conversion step. To illustrate, we will\nsimply walk through the process of preparing [MobileNetV4](https://arxiv.org/abs/2404.10518)_ for\nacceleration on Akida - we'll run into several incompatibilities at different points\nin that process, and see how to resolve them.\n\nThis example assumes a moderate level of experience with deep learning, and good familiarity\nwith the operations typically encountered in these types of models. For example, here we'll\nuse the following workarounds:\n\n* to avoid an incompatible padding scheme in convolution, we will overwrite paddings values when\n  not aligned with Akida expectations,\n* in order to handle an unsupported kernel-size 5/stride 2 depthwise convolution, we'll split that\n  into two equivalent operations: a kernel-size 5 depthwise convolution using the same weights, but\n  with stride 1; followed by a kernel-size 3/stride 2 depthwise convolution with identity weights.\n\n.. Note::\n   | This tutorial leverages the [Optimum toolkit](https://huggingface.co/docs/optimum/main/en/exporters/onnx/usage_guides/export_a_model)_,\n     an external tool, based on [PyTorch](https://pytorch.org/)_, that allows models direct\n     download and export to ONNX.\n\n```\npip install optimum[exporters]\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Get model and data\nBefore diving into the model incompatibilities and how to resolve them, we'll need to acquire\nsome sample data to test on, plus the pretrained model.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Data\n\nGiven that the reference model was trained on [ImageNet](https://www.image-net.org/)_ dataset\n(which is not publicly available), this tutorial uses a set of 10 copyright free images.\nA helper function ``imagenet.preprocessing.get_preprocessed_samples`` loads\nand preprocesses (decodes, crops and extracts a square 224x224x3 patch from an input image)\nthese images.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nfrom akida_models.imagenet import get_preprocessed_samples\nfrom akida_models.imagenet.imagenet_utils import IMAGENET_MEAN, IMAGENET_STD\n\n# Model specification and hyperparameters\nNUM_CHANNELS = 3\nIMAGE_SIZE = 224\n\n# Load the preprocessed images and their corresponding labels for the test set\nx_test_raw, labels_test = get_preprocessed_samples(IMAGE_SIZE, NUM_CHANNELS)\nnum_images = x_test_raw.shape[0]\n\n# Normalize images as models expects\nimagenet_mean_255 = np.array(IMAGENET_MEAN, dtype=\"float32\") * 255.0\nimagenet_std_255 = np.array(IMAGENET_STD, dtype=\"float32\") * 255.0\nx_test = ((x_test_raw - imagenet_mean_255) / imagenet_std_255)\n\n# Transpose the channels to the first axis as per the default for ONNX models\nx_test = np.transpose(x_test, (0, 3, 1, 2))\n\nprint(f'{num_images} images and their labels are loaded and preprocessed.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Download the model\n\nWe download MobileNetV4 small from the [HuggingFace hub](https://huggingface.co/timm/mobilenetv4_conv_small.e2400_r224_in1k).\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import onnx\nfrom optimum.exporters.onnx import main_export\n\n# Download and convert MobiletNetV4 to ONNX\nmain_export(model_name_or_path=\"timm/mobilenetv4_conv_small.e2400_r224_in1k\",\n            task=\"image-classification\", output=\"./\")\n\n# Load the model in memory\nonnx_model = onnx.load_model(\"./model.onnx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 Evaluate model performance\n\nThe [ONNXRuntime](https://onnxruntime.ai)_ package is a cross-platform\naccelerator capable of loading and running models described in ONNX format.\nWe use this framework to evaluate the performance of the loaded ResNet50\nmodel.\n\n.. Note:: For example purposes, we only compute accuracy on 10 images.\n   Accuracy on the full ImageNet validation set is reported at the end.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from onnxruntime import InferenceSession\n\n\ndef evaluate_onnx_model(model):\n    sess = InferenceSession(model.SerializeToString())\n    # Calculate outputs by running images through the session\n    outputs = sess.run(None, {model.graph.input[0].name: x_test})\n    # The class with the highest score is what we choose as prediction\n    predicted = np.squeeze(np.argmax(outputs[0], 1))\n    # Compute the number of valid predictions\n    return int((predicted == labels_test).sum())\n\n\n# Evaluate over test dataset\ncorrectly_classified_floating = evaluate_onnx_model(onnx_model)\nprint(f'Floating point model accuracy: {correctly_classified_floating}/{num_images}.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Quantize\n\nAkida processes integer inputs, activations and weights. Therefore, the first step in\npreparing a floating point model to run on Akida is to quantize it using [QuantizeML quantize()](../../api_reference/quantizeml_apis.html#quantizeml.models.quantize)_.\n\n.. Note::\n  Please refer to the [QuantizeML toolkit user guide](../../user_guide/quantizeml.html)_\n  and the [Advanced QuantizeML tutorial](./plot_0_advanced_quantizeml.html)_ for further details.\n  In particular here, for simplicity, we pass only the small number of samples we already prepared\n  for calibration. Typically, you will want to use many more samples for calibration, say 1000 if\n  you have them available; and not drawn from your test data. The akida_models package provides a\n  helper function, [extract_samples()](../../api_reference/akida_models_apis.html#extract-samples)_\n  which may be helpful in preparing those.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from quantizeml.models import quantize\n\nmodel_quantized = quantize(onnx_model, samples=x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model was quantized successfully, we can check its accuracy:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "correctly_classified = evaluate_onnx_model(model_quantized)\nprint(f'Quantized model accuracy: {correctly_classified}/{num_images}.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Conversion\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1. Incompatibility at Conversion\n\nWhile most imcompatibilities will be picked up at the quantization step, some constraints are\nspecific to the target hardware device, and can only be applied at the conversion step. We can\ndetect these either with the [check_model_compatibility](../../api_reference/cnn2snn_apis.html#cnn2snn.check_model_compatibility)_ tool,\nor by trying to [convert the model into Akida](../../api_reference/cnn2snn_apis.html#cnn2snn.convert)_.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from cnn2snn import convert\n\ntry:\n    akida_model = convert(model_quantized)\nexcept Exception as e:\n    print(f\"MobileNetV4 is not fully accelerated by Akida. Reason: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This error is raised because the MobileNetV4 padding scheme is specific and differs from the\nTF-Keras/Akida standard.\n\nIdeally, we should aim to swap incompatible operations with mathematically\nequivalent replacements. For issues of convolution kernel size or padding, we can\noften achieve that by putting the kernel weights within a larger kernel, placed\neccentrically to compensate for any padding issues etc. In this case, we'll\ntry simply modifying the padding to be Akida-compatible.\n\nTo achieve this, we'll define custom quantization pattern to modify the model before\nquantization. Rather than try to provide a general solution, we'll hard code this for\nthe problem layers.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2. About Patterns\n\nFor efficiency, Akida hardware actually groups certain commonly occuring\noperations together. For example, ReLU activation functions, where present,\nare almost always applied on the outputs of the hard-working computational\nlayers (Convolutions, Depthwise Convolutions, Dense layers etc.). So the ReLU\non Akida is tied to those operations. While efficient, this does mean that\nsome sequences of operations will not by default be considered Akida-compatible,\neven though the individual operations are known to be handled. That's the\ncause of the problem encountered here.\n\n\nTo properly see what's going on, and to resolve the problem, we'll need to\nunderstand the concept of \"patterns\". These are the objects that QuantizeML\nuses to map ONNX models to their Akida equivalents. A pattern is a sequence of\ncontinuous [ONNX operators](https://onnx.ai/onnx/operators/) in a graph that\n**can be converted** to an\n[Akida V2 layer](../../api_reference/akida_apis.html#akida-v2-layers).\nFor example, the following model would be converted to an [akida.InputConv2D](../../api_reference/akida_apis.html#akida.InputConv2D) layer:\n\n.. figure:: ../../img/onnx_input_conv2d.png\n   :target: ../../_images/onnx_input_conv2d.png\n   :alt: InputConv2D example model\n   :scale: 80 %\n   :align: center\n\n   One ONNX configuration that would map to an [InputConv2D](../../api_reference/akida_apis.html#akida.InputConv2D).\n\n\nThe sequence of operators [``Conv``, ``Clip``, ``MaxPool``] **is one valid pattern**\nfor conversion towards [InputConv2D](../../api_reference/akida_apis.html#akida.InputConv2D).\n\n\nCrucially, we can check the list of the currently supported patterns:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from quantizeml.onnx_support.quantization.register_patterns import PATTERNS_MAP\n\nprint(*PATTERNS_MAP, sep='\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. Note::\n  Before the conversion the following changes are automatically done to allow the\n  QuantizeML toolkit to see an ONNX graph suitable for quantization:\n\n      1. transforms the following operators for general purposes:\n\n         * ``Conv`` -> ``DepthwiseConv`` when kernel size is 1 x Kx x Ky and ``group`` is required\n         * ``Clip`` > ``Relu`` (if ``min = 0.0``)\n\n      2. uses [Graph Optimizations in ONNX Runtime](https://onnxruntime.ai/docs/performance/model-optimizations/graph-optimizations.html)\n         to optimize the graph (e.g. fuse BatchNorm into convolutions).\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3. Custom quantization patterns\n\nThe existing patterns won't allow us to map an isolated GlobalAveragePool operation. But, for\nexample, the pooling operation can be mapped when following a Conv layer, and we can easily\nimplement a Conv layer that performs an identity operation on its inputs, just by setting the\nkernel weights appropriately. We can implement this workaround by using custom quantization\npatterns to extend ``PATTERNS_MAP``.\n\nEvery pattern includes an ONNX layer that stores the ONNX graph information for the matching\nsequence of nodes. QuantizeML also allows for a function to create a compatible layer from\nan initially incompatible pattern. This pattern function has two input parameters: the graph\nand the pattern-matched sequence of nodes extracted from it.\n\nOnce a pattern function is defined for an unsupported pattern, both can be appended\nin the quantization context through the ``custom_pattern_scope`` function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from quantizeml.onnx_support import layers\nfrom quantizeml.onnx_support.quantization import custom_pattern_scope\n\n\ndef align_conv_with_akida(block_nodes, graph, tensor_ranges):\n    \"\"\"Pattern function that handles convolutions incompatible with Akida\n    \"\"\"\n    # Recover initial ONNXLayer from block nodes and graph\n    try:\n        qlayer = layers.get_qdepthwise(block_nodes, graph, tensor_ranges)\n    except RuntimeError:\n        qlayer = layers.get_qconv(block_nodes, graph, tensor_ranges)\n\n    # Force the pads in some convolution to Akida compatible values\n    target_pads = None\n    if qlayer.name in ['/conv_stem/Conv', '/blocks/blocks.0/blocks.0.0/conv/Conv',\n                       '/blocks/blocks.1/blocks.1.0/conv/Conv',\n                       '/blocks/blocks.3/blocks.3.0/dw_mid/conv/Conv']:\n        target_pads = np.array([0, 0, 0, 0, 0, 0, 1, 1], np.int64)\n    elif qlayer.name == '/blocks/blocks.2/blocks.2.0/dw_mid/conv/Conv':\n        target_pads = np.array([0, 0, 1, 1, 0, 0, 2, 2], np.int64)\n\n    if target_pads is not None:\n        print(f\"Setting Akida pads in {qlayer.name}...\")\n        # Note: pads in convolution include spatial dimension\n        qlayer.set_weight(\"pads\", target_pads)\n\n    return qlayer\n\n\n# Define a custom patterns map as a new pattern and associated replacement function\nquantization_pattern_map = {}\nfor qpattern in PATTERNS_MAP:\n    if \"Conv\" in qpattern.pattern:\n        # Update all patterns that contains Conv op_type\n        quantization_pattern_map.update({qpattern.pattern: align_conv_with_akida})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Quantize model with custom patterns\nwith custom_pattern_scope(quantization_pattern_map):\n    model_quantized = quantize(onnx_model, samples=x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Evaluate quantized model performance\ncorrectly_classified = evaluate_onnx_model(model_quantized)\nprint(f'Quantized model accuracy: {correctly_classified}/{num_images}.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "At this point we can re-check conversion:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "try:\n    akida_model = convert(model_quantized)\nexcept Exception as e:\n    print(f\"MobileNetV4 is not fully accelerated by Akida. Reason: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Another error is raised due to an Akida incompatiblity: the model comes with a depthwise layer\nwith a kernel size 5 and and stride of 2. Akida only supports stride 2 for kernel size 3 depthwise\nlayers.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.4. Custom sanitizing\nHandling the kernel 5 stride 2 depthwise layer cannot be done using custom quantization\npattern. Patterns can only be used to transform one or several nodes towards a single node that\nmatches the chain of operations of an Akida layer.\nIn this case, to overcome the compatibilty issue, the original layer will be replaced by an\nequivalent set of two layers, decoupling the kernel size and the stride into two distinct layers.\nA custom santizing step will then be defined and applied to the original model:\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The custom sanitizer is implemented using [onnxscript](https://github.com/microsoft/onnxscript)_. The ONNX Rewriter provides functionality to replace\ncertain patterns in an ONNX graph with replacement patterns based on user-defined rewrite rules,\nwhich fits our needs. A tutorial on how to use the ONNX Rewriter can be found at\nhttps://microsoft.github.io/onnxscript/tutorial/rewriter/index.html.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from onnxscript.rewriter import ir, pattern\nfrom quantizeml.onnx_support.quantization import ONNXModel\n\n\ndef make_depthwise_compatible(model):\n    # Parse all 'Conv' operations\n    def find_convs(op, x, w):\n        return op.Conv(x, w, _allow_other_inputs=True, _outputs=[\"conv\"])\n\n    # Edit out the depthwise layer so that it becomes: a convolution with kernel 5 and stride 1 with\n    # original weights followed by a kernel 3 stride 2 convolution with identity weights\n    def replace_depthwise(op, x, w, conv, **__):\n        ir_node = conv.producer()\n        attributes = ir_node.attributes\n\n        # Change strides to 1 and padding to the Akida expected paddings\n        attributes['strides'] = ir.AttrInt64s('strides', [1, 1])\n        attributes['pads'] = ir.AttrInt64s('pads', [2, 2, 2, 2])\n        dw_kernel_5 = op.Conv(*ir_node.inputs, **attributes)\n\n        # Apply \"identity\" with kernel size=3 and strides=2 and the Akida expected paddings\n        identity_w = np.zeros((w.shape[0], 1, 3, 3), dtype=\"float32\")\n        identity_w[np.arange(w.shape[0]), 0, 1, 1] = 1\n        identity_w = op.initializer(ir.tensor(identity_w), name=f\"{ir_node.name}_identity_weights\")\n\n        dw_stride_2 = op.Conv(dw_kernel_5, identity_w, kernel_shape=[3, 3], strides=[2, 2],\n                              pads=[0, 0, 1, 1], group=w.shape[0])\n\n        # Note that the new nodes will have different names, so custom patterns that are using names\n        # will not be applied.\n        return dw_stride_2\n\n    # Only trigger the replacement on the target nodes with group=input channels, kernel_size=5\n    # and strides=2\n    def is_depthwise_k5_s2(*_, w, conv, **__):\n        attributes = conv.producer().attributes\n        group = attributes.get('group', ir.AttrInt64('group', 1))\n        strides = attributes.get('strides', ir.AttrInt64s('strides', [1]))\n        return group.value == w.shape[0] and w.shape[2:] == (5, 5) and strides.value == [2, 2]\n\n    # Define transformation rules\n    rules = [pattern.RewriteRule(find_convs, replace_depthwise, is_depthwise_k5_s2)]\n\n    # Apply rewrites\n    model.rewrite(rules)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The helper above will replace the depthwise layer with two layers, one with kernel size 5 and\nstride 1, and the second with kernel size 3 and stride 2:\n\n.. figure:: ../../img/sanitized_mbv4.png\n   :target: ../../_images/sanitized_mbv4.png\n   :alt: Sanitized Depthwise layer\n   :scale: 120 %\n   :align: center\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Wrap in an ONNXModel: in addition to rewriting, it will infer the shapes and check the model\n# after transformations\nmodel_to_sanitize = ONNXModel(onnx_model)\nmake_depthwise_compatible(model_to_sanitize)\nsanitized_model = model_to_sanitize.model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Evaluate the transformed model\ncorrectly_classified = evaluate_onnx_model(sanitized_model)\nprint(f'Sanitized model accuracy: {correctly_classified}/{num_images}.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Quantize again\nwith custom_pattern_scope(quantization_pattern_map):\n    model_quantized = quantize(sanitized_model, samples=x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Re-evaluate quantized model performance\ncorrectly_classified = evaluate_onnx_model(model_quantized)\nprint(f'Quantized model accuracy: {correctly_classified}/{num_images}.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.5. Successful Conversion\nTime to check conversion again\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "akida_model = convert(model_quantized)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. Note:: paddings are compatible in the modified depthwise layer because we've explicitely set\n          paddings values when rewriting the model (custom sanitize) and they were not overwritten\n          by the quantization patterns since the new nodes have different names.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Great - the model is now both quantized successfully, and can be\nentirely converted for acceleration on Akida. To check its\nperformance, we need to bear in mind that\n\n1. images must be numpy-raw, with an 8-bit unsigned integer data type and\n2. the channel dimension must be in the last dimension.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Evaluate performance\nakida_accuracy = akida_model.evaluate(x_test_raw, labels_test)\nprint(f'Akida model accuracy: {100 * akida_accuracy:.2f} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. Note::\n   The images shown in this tutorial are produced through [Netron](https://netron.app/).\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}