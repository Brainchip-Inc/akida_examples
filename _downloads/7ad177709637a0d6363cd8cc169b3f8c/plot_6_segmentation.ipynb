{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Segmentation tutorial\n\nThis tutorial demonstrates that Akida-compatible models can perform segmentation tasks.\n\nThis is illustrated through a person segmentation problem using the [Portrait128 dataset](https://github.com/anilsathyan7/Portrait-Segmentation)_.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load the dataset\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport numpy as np\nfrom tensorflow.keras.utils import get_file\n\n# Download validation set from Brainchip data server, it contains 10% of the original dataset\ndata_path = get_file(\"val.tar.gz\",\n                     \"https://data.brainchip.com/dataset-mirror/portrait128/val.tar.gz\",\n                     cache_subdir=os.path.join(\"datasets\", \"portrait128\"),\n                     extract=True)\n\ndata_dir = os.path.join(os.path.dirname(data_path), \"val\")\nx_val = np.load(os.path.join(data_dir, \"val_img.npy\"))\ny_val = np.load(os.path.join(data_dir, \"val_msk.npy\")).astype('uint8')\nbatch_size = 32\nsteps = x_val.shape[0] // 32\n\n# Visualize some data\nimport matplotlib.pyplot as plt\n\nid = np.random.randint(0, x_val.shape[0])\n\nfig, axs = plt.subplots(3, 3, constrained_layout=True)\nfor col in range(3):\n    axs[0, col].imshow(x_val[id + col] / 255.)\n    axs[0, col].axis('off')\n    axs[1, col].imshow(1 - y_val[id + col], cmap='Greys')\n    axs[1, col].axis('off')\n    axs[2, col].imshow(x_val[id + col] / 255. * y_val[id + col])\n    axs[2, col].axis('off')\n\nfig.suptitle('Image, mask and masked image', fontsize=10)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load a pre-trained native Keras model\n\nThe model is an AkidaUNet, that has an AkidaNet (0.5) backbone to extract features,\ncombined with a succession of [separable transposed convolutional](../../api_reference/akida_models_apis.html#akida_models.layer_blocks.sepconv_transpose_block)_\nblocks to build a segmentation map.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>- The \"transposed\" convolutional feature has been introduced in Akida 2.0.\n  - The \"separable transposed\" operation is obtained combining the QuantizeML custom\n    [DepthwiseConv2DTranspose](../../api_reference/quantizeml_apis.html#quantizeml.layers.DepthwiseConv2DTranspose)_ layer\n    with a standard pointwise convolution.</p></div>\n\nThe performance of the model is evaluated using both the pixel accuracy that describes how well\nthe model can predict the segmentation mask pixel by pixel and the [Binary IoU](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/BinaryIoU)_ that better takes into\naccount how close the predicted mask is to the ground truth.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import get_file\nfrom akida_models.model_io import load_model\n\n# Retrieve the model file from Brainchip data server\nmodel_file = get_file(\"akida_unet_portrait128.h5\",\n                      \"https://data.brainchip.com/models/AkidaV2/akida_unet/akida_unet_portrait128.h5\",\n                      cache_subdir='models')\n\n# Load the native Keras pre-trained model\nmodel_keras = load_model(model_file)\nmodel_keras.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from keras.metrics import BinaryIoU\n\n# Compile the native Keras model (required to evaluate the metrics)\nmodel_keras.compile(loss='binary_crossentropy', metrics=[BinaryIoU(), 'accuracy'])\n\n# Check Keras model performance\n_, biou, acc = model_keras.evaluate(x_val, y_val, steps=steps, verbose=0)\n\nprint(f\"Keras binary IoU / pixel accuracy: {biou:.4f} / {100*acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load a pre-trained quantized Keras model\n\nThe above native Keras model is quantized to 8-bit (all weights and activations) and fine-tuned\n(QAT).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from akida_models import akida_unet_portrait128_pretrained\n\n# Load the pre-trained quantized model\nmodel_quantized_keras = akida_unet_portrait128_pretrained()\nmodel_quantized_keras.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Compile the quantized Keras model (required to evaluate the metrics)\nmodel_quantized_keras.compile(loss='binary_crossentropy', metrics=[BinaryIoU(), 'accuracy'])\n\n# Check Keras model performance\n_, biou, acc = model_quantized_keras.evaluate(x_val, y_val, steps=steps, verbose=0)\n\nprint(f\"Keras quantized binary IoU / pixel accuracy: {biou:.4f} / {100*acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Conversion to Akida\n\nThe quantized Keras model is now converted into an Akida model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from cnn2snn import convert\n\n# Convert the model\nmodel_akida = convert(model_quantized_keras)\nmodel_akida.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n\n# Check Akida model performance\nlabels, pots = None, None\n\nfor s in range(steps):\n    batch = x_val[s * batch_size: (s + 1) * batch_size, :]\n    label_batch = y_val[s * batch_size: (s + 1) * batch_size, :]\n    pots_batch = model_akida.predict(batch.astype('uint8'))\n\n    if labels is None:\n        labels = label_batch\n        pots = pots_batch\n    else:\n        labels = np.concatenate((labels, label_batch))\n        pots = np.concatenate((pots, pots_batch))\npreds = tf.keras.activations.sigmoid(pots)\n\nm_binary_iou = tf.keras.metrics.BinaryIoU(target_class_ids=[0, 1], threshold=0.5)\nm_binary_iou.update_state(labels, preds)\nbinary_iou = m_binary_iou.result().numpy()\n\nm_accuracy = tf.keras.metrics.Accuracy()\nm_accuracy.update_state(labels, preds > 0.5)\naccuracy = m_accuracy.result().numpy()\nprint(f\"Akida binary IoU / pixel accuracy: {binary_iou:.4f} / {100*accuracy:.2f}%\")\n\n# For non-regression purpose\nassert binary_iou > 0.9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Segment a single image\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\n# Estimate age on a random single image and display Keras and Akida outputs\nsample = np.expand_dims(x_val[id, :], 0)\nkeras_out = model_keras(sample)\nakida_out = tf.keras.activations.sigmoid(model_akida.forward(sample.astype('uint8')))\n\nfig, axs = plt.subplots(1, 3, constrained_layout=True)\naxs[0].imshow(keras_out[0] * sample[0] / 255.)\naxs[0].set_title('Keras segmentation', fontsize=10)\naxs[0].axis('off')\n\naxs[1].imshow(akida_out[0] * sample[0] / 255.)\naxs[1].set_title('Akida segmentation', fontsize=10)\naxs[1].axis('off')\n\naxs[2].imshow(y_val[id] * sample[0] / 255.)\naxs[2].set_title('Expected segmentation', fontsize=10)\naxs[2].axis('off')\n\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}