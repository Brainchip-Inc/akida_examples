{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Transfer learning with MobileNet for cats vs. dogs\n\nThis tutorial presents a demonstration of how transfer learning is applied\nwith our quantized models to get an Akida model.\n\nThe transfer learning example is derived from the `Tensorflow\ntutorial <https://www.tensorflow.org/tutorials/images/transfer_learning>`__:\n\n    * Our base model is an Akida-compatible version of **MobileNet v1**,\n      trained on ImageNet.\n    * The new dataset for transfer learning is **cats vs. dogs**\n      (`link <https://www.tensorflow.org/datasets/catalog/cats_vs_dogs>`__).\n    * We use transfer learning to customize the model to the new task of\n      classifying cats and dogs.\n\n.. Note:: This tutorial only shows the inference of the trained Keras\n          model and its conversion to an Akida network. A textual explanation\n          of the training is given below.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transfer learning process\n.. figure:: https://s2.qwant.com/thumbr/0x380/7/0/7b7386531ea24ab1294fdf9b8698b008a51e38a3c57e81427fbef626ff226c/1*6ACbDsBMeDZcLg9W8CFT_Q.png?u=https%3A%2F%2Fcdn-images-1.medium.com%2Fmax%2F1600%2F1%2A6ACbDsBMeDZcLg9W8CFT_Q.png&q=0&b=1&p=0&a=1\n   :alt: transfer_learning_image\n   :target: https://s2.qwant.com/thumbr/0x380/7/0/7b7386531ea24ab1294fdf9b8698b008a51e38a3c57e81427fbef626ff226c/1*6ACbDsBMeDZcLg9W8CFT_Q.png?u=https%3A%2F%2Fcdn-images-1.medium.com%2Fmax%2F1600%2F1%2A6ACbDsBMeDZcLg9W8CFT_Q.png&q=0&b=1&p=0&a=1\n   :align: center\n\nTransfer learning allows to classify on a specific task by using a\npre-trained base model. For an introduction to transfer learning, please\nrefer to the `Tensorflow transfer learning\ntutorial <https://www.tensorflow.org/tutorials/images/transfer_learning>`__\nbefore exploring this tutorial. Here, we focus on how to quantize the\nKeras model in order to convert it to an Akida one.\n\nThe model is composed of:\n\n  * a base quantized MobileNet model used to extract image features\n  * a top layer to classify cats and dogs\n  * a sigmoid activation function to interpret model outputs as a probability\n\n**Base model**\n\nThe base model is a quantized version of MobileNet v1. This\nmodel was trained and quantized using the ImageNet dataset. Please refer\nto the corresponding `example <plot_2_mobilenet_imagenet.html>`__ for\nmore information. The layers have 4-bit weights (except for the first\nlayer having 8-bit weights) and the activations are quantized to 4 bits.\nThis base model ends with a classification layer for 1000 classes. To\nclassify cats and dogs, the feature extractor is preserved but the\nclassification layer must be removed to be replaced by a new top layer\nfocusing on the new task.\n\nIn our transfer learning process, the base model is frozen, i.e., the\nweights are not updated during training. Pre-trained weights for the\nfrozen quantized model are provided on our\n`data server <http://data.brainchip.com/models/mobilenet/>`__.\n\n**Top layer**\n\nWhile a fully-connected top layer is added in the Tensorflow tutorial, we\ndecided to use a separable convolutional layer with one output neuron for the\ntop layer of our model. The reason is that the separable convolutional layer\nis the only Akida layer supporting 4-bit weights (see `hardware compatibility\n<../../user_guide/hw_constraints.html>`__).\n\n**Training process**\n\nThe transfer learning process for quantized models can be handled in different\nways:\n\n  1. **From a quantized base model**, the new transferred model is composed\n     of a frozen base model and a float top layer. The top layer is trained.\n     Then, the top layer is quantized and fine-tuned. If necessary, the base\n     model can be unfrozen to be slightly trained to improve accuracy.\n  2. **From a float base model**, the new transferred model is also composed\n     of a frozen base model (with float weights/activations) and a float top\n     layer. The top layer is trained. Then the full model is quantized,\n     unfrozen and fine-tuned. This option requires longer training\n     operations since we don't take advantage of an already quantized base\n     model. Option 2 can be used alternatively if option 1 doesn't give\n     suitable performance.\n\nIn this example, option 1 is chosen. The training steps are described below.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and preprocess data\n\nIn this section, we will load and preprocess the 'cats_vs_dogs' dataset\nto match the required model's inputs.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.A - Load and split data\n\nThe ``cats_vs_dogs``\n`dataset <https://www.tensorflow.org/datasets/catalog/cats_vs_dogs>`__\nis loaded and split into train, validation and test sets. The train and\nvalidation sets were used for the transfer learning process. Here only\nthe test set is used. We use here ``tf.Dataset`` objects to load and\npreprocess batches of data (one can look at the TensorFlow guide\n`here <https://www.tensorflow.org/guide/data>`__ for more information).\n\n.. Note:: The ``cats_vs_dogs`` dataset version used here is 4.0.0.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n\nsplits = ['train[:80%]', 'train[80%:90%]', 'train[90%:]']\n\ntfds.disable_progress_bar()\n(raw_train, raw_validation,\n raw_test), metadata = tfds.load('cats_vs_dogs:4.0.0',\n                                 split=splits,\n                                 with_info=True,\n                                 as_supervised=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.B - Preprocess the test set\n\nWe must apply the same preprocessing as for training: rescaling and\nresizing. Since Akida models directly accept integer-valued images, we\nalso define a preprocessing function for Akida:\n\n  - for Keras: images are rescaled between 0 and 1, and resized to 160x160\n  - for Akida: images are only resized to 160x160 (uint8 values).\n\nKeras and Akida models require 4-dimensional (N,H,W,C) arrays as inputs.\nWe must then create batches of images to feed the model. For inference,\nthe batch size is not relevant; you can set it such that the batch of\nimages can be loaded in memory depending on your CPU/GPU.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n\nIMG_SIZE = 160\ninput_scaling = (127.5, 127.5)\n\n\ndef format_example_keras(image, label):\n    image = tf.cast(image, tf.float32)\n    image = (image - input_scaling[1]) / input_scaling[0]\n    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n    return image, label\n\n\ndef format_example_akida(image, label):\n    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n    image = tf.cast(image, tf.uint8)\n    return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\ntest_batches_keras = raw_test.map(format_example_keras).batch(BATCH_SIZE)\ntest_batches_akida = raw_test.map(format_example_akida).batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.C - Get labels\n\nLabels are contained in the test set as '0' for cats and '1' for dogs.\nWe read through the batches to extract the labels.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nlabels = np.array([])\nfor _, label_batch in test_batches_keras:\n    labels = np.concatenate((labels, label_batch))\n\nnum_images = labels.shape[0]\n\nprint(f\"Test set composed of {num_images} images: \"\n      f\"{np.count_nonzero(labels==0)} cats and \"\n      f\"{np.count_nonzero(labels==1)} dogs.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Modify a pre-trained base Keras model\n\nIn this section, we will describe how to modify a base model to specify\nthe classification for ``cats_vs_dogs``.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.A - Instantiate a Keras base model\n\nHere, we instantiate a quantized Keras model based on a MobileNet model.\nThis base model was previously trained using the 1000 classes of the\nImageNet dataset. For more information, please see the `ImageNet\ntutorial <plot_2_mobilenet_imagenet.html>`__.\n\nThe quantized MobileNet model satisfies the Akida NSoC requirements:\n\n  * The model relies on a convolutional layer (first layer) and separable\n    convolutional layers, all being Akida-compatible.\n  * All the separable convolutional layers have 4-bit weights, the first\n    convolutional layer has 8-bit weights.\n  * The activations are quantized with 4 bits.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from akida_models import mobilenet_imagenet\n\n# Instantiate a quantized MobileNet model\nbase_model_keras = mobilenet_imagenet(input_shape=(IMG_SIZE, IMG_SIZE, 3),\n                                      weight_quantization=4,\n                                      activ_quantization=4,\n                                      input_weight_quantization=8)\n\n# Load pre-trained weights for the base model\npretrained_weights = tf.keras.utils.get_file(\n    \"mobilenet_imagenet_iq8_wq4_aq4.h5\",\n    \"http://data.brainchip.com/models/mobilenet/mobilenet_imagenet_iq8_wq4_aq4.h5\",\n    file_hash=\"07780D7B6A12B764AF1372D792BDF032301508FB997BFD044C397CA2C8AD5747\",\n    cache_subdir='models/mobilenet')\nbase_model_keras.load_weights(pretrained_weights)\n\nbase_model_keras.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.B - Modify the network for the new task\n\nAs explained in `section 1 <#transfer-learning-process>`__,\nwe replace the 1000-class top layer with a separable convolutional layer with\none output neuron.\nThe new model is now appropriate for the ``cats_vs_dogs`` dataset and is\nAkida-compatible. Note that a sigmoid activation is added at the end of\nthe model: the output neuron returns a probability between 0 and 1 that\nthe input image is a dog.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from akida_models.layer_blocks import separable_conv_block\n\n# Add a top layer for \"cats_vs_dogs\" classification\nx = base_model_keras.get_layer('reshape_1').output\nx = separable_conv_block(x,\n                         filters=1,\n                         kernel_size=(3, 3),\n                         padding='same',\n                         use_bias=False,\n                         add_activation=False,\n                         name='top_layer_separable')\nx = tf.keras.layers.Activation('sigmoid')(x)\npreds = tf.keras.layers.Reshape((1,), name='reshape_2')(x)\nmodel_keras = tf.keras.Model(inputs=base_model_keras.input,\n                             outputs=preds,\n                             name=\"model_cats_vs_dogs\")\n\nmodel_keras.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Train the transferred model for the new task\n\nThe transferred model must be trained to learn how to classify cats and dogs.\nThe quantized base model is frozen: only the float top layer will effectively\nbe trained. One can take a look at the\n`training section <https://www.tensorflow.org/tutorials/images/transfer_learning#compile_the_model>`__\nof the corresponding TensorFlow tutorial to reproduce the training stage.\n\nThe float top layer is trained for 20 epochs. We don't illustrate the training\nphase in this tutorial; instead we directly load the pre-trained weights\nobtained after the 20 epochs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Freeze the base model part of the new model\nbase_model_keras.trainable = False\n\n# Load pre-trained weights\npretrained_weights = tf.keras.utils.get_file(\n    \"mobilenet_cats_vs_dogs_iq8_wq4_aq4.h5\",\n    \"http://data.brainchip.com/models/mobilenet/mobilenet_cats_vs_dogs_iq8_wq4_aq4.h5\",\n    file_hash=\"85a169b78b426647a7cff3c4d6caf902dcfcb56ea41d5ea50455d7ae466bfdd3\",\n    cache_subdir='models')\nmodel_keras.load_weights(pretrained_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Check performance on the test set\nmodel_keras.compile(metrics=['accuracy'])\n_, keras_accuracy = model_keras.evaluate(test_batches_keras)\n\nprint(f\"Keras accuracy (float top layer): {keras_accuracy*100:.2f} %\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4 Quantize the top layer\n\nTo get an Akida-compatible model, the float top layer must be quantized.\nWe decide to quantize its weights to 4 bits. The performance of the\nnew quantized model is then assessed.\n\nHere, the quantized model gives suitable performance compared to the model\nwith the float top layer. If that had not been the case, a fine-tuning step\nwould have been necessary to recover the drop in accuracy.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from cnn2snn import quantize_layer\n\n# Quantize the top layer to 4 bits\nmodel_keras = quantize_layer(model_keras, 'top_layer_separable', bitwidth=4)\n\n# Check performance for the quantized Keras model\nmodel_keras.compile(metrics=['accuracy'])\n_, keras_accuracy = model_keras.evaluate(test_batches_keras)\n\nprint(f\"Quantized Keras accuracy: {keras_accuracy*100:.2f} %\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Convert to Akida\n\nThe new quantized Keras model is now converted to an Akida model. The\n'sigmoid' final activation has no SNN equivalent and will be simply ignored\nduring the conversion.\n\nPerformance of the Akida model is then computed. Compared to Keras inference,\nremember that:\n\n  * Input images in Akida are uint8 and not scaled like Keras inputs. But\n    remember that the conversion process needs to know what scaling was\n    applied during Keras training, in order to compensate (see\n    `CNN2SNN guide <../../user_guide/cnn2snn.html#input-scaling>`__)\n  * The Akida `evaluate <../../api_reference/aee_apis.html#akida.Model.evaluate>`__\n    function takes a NumPy array containing the images and returns potentials\n    before the sigmoid activation. We must therefore explicitly apply the\n    'sigmoid' activation on the model outputs to obtain the Akida\n    probabilities.\n\nSince activations sparsity has a great impact on Akida inference time, we\nalso have a look at the average input and output sparsity of each layer on\none batch of the test set.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from cnn2snn import convert\n\n# Convert the model\nmodel_akida = convert(model_keras, input_scaling=input_scaling)\nmodel_akida.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from timeit import default_timer as timer\nfrom progressbar import ProgressBar\n\n# Run inference with Akida model\nn_batches = num_images // BATCH_SIZE + 1\npots_akida = np.array([], dtype=np.float32)\n\npbar = ProgressBar(maxval=n_batches)\npbar.start()\nstart = timer()\ni = 1\nfor batch, _ in test_batches_akida:\n    pots_batch_akida = model_akida.evaluate(batch.numpy())\n    pots_akida = np.concatenate((pots_akida, pots_batch_akida.squeeze()))\n    pbar.update(i)\n    i = i + 1\npbar.finish()\nend = timer()\nprint(f\"Akida inference on {num_images} images took {end-start:.2f} s.\\n\")\n\n# Compute predictions and accuracy\npreds_akida = tf.keras.layers.Activation('sigmoid')(pots_akida) > 0.5\nakida_accuracy = np.mean(np.equal(preds_akida, labels))\nprint(f\"Akida accuracy: {akida_accuracy*100:.2f} %\")\n\n# For non-regression purpose\nassert akida_accuracy > 0.97"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Print model statistics\nstats = model_akida.get_statistics()\nbatch, _ = iter(test_batches_akida).get_next()\nmodel_akida.evaluate(batch[:20].numpy())\n\nprint(\"Model statistics\")\nfor _, stat in stats.items():\n    print(stat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's summarize the accuracy for the quantized Keras and the Akida model.\n\n+-----------------+----------+\n| Model           | Accuracy |\n+=================+==========+\n| quantized Keras | 97.46 %  |\n+-----------------+----------+\n| Akida           | 97.55 %  |\n+-----------------+----------+\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Plot confusion matrix\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\n\ndef confusion_matrix_2classes(labels, predictions):\n    tp = np.count_nonzero(labels + predictions == 2)\n    tn = np.count_nonzero(labels + predictions == 0)\n    fp = np.count_nonzero(predictions - labels == 1)\n    fn = np.count_nonzero(labels - predictions == 1)\n\n    return np.array([[tp, fn], [fp, tn]])\n\n\ndef plot_confusion_matrix_2classes(cm, classes):\n    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.xticks([0, 1], classes)\n    plt.yticks([0, 1], classes)\n\n    for i, j in zip([0, 0, 1, 1], [0, 1, 0, 1]):\n        plt.text(j,\n                 i,\n                 f\"{cm[i, j]:.2f}\",\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > cm.max() / 2. else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.autoscale()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Plot confusion matrix for Akida\ncm_akida = confusion_matrix_2classes(labels, preds_akida.numpy())\nplot_confusion_matrix_2classes(cm_akida, ['dog', 'cat'])\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}