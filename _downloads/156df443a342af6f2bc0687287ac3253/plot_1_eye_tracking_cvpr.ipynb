{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Efficient online eye tracking with a lightweight spatiotemporal network and event cameras\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introduction\n\nEvent cameras are biologically inspired sensors that output asynchronous streams of per-pixel\nbrightness changes, rather than fixed-rate frames. This modality is especially well suited for\nhigh-speed, low-power applications like real-time eye tracking on embedded hardware. Traditional\ndeep learning models, however, are often ill-suited for exploiting the unique characteristics of\nevent data \u2014 particularly they lack the tools to leverage their temporal precision and sparsity.\n\nThis tutorial presents a lightweight spatiotemporal neural network architecture designed\nspecifically for online inference on event camera data. The model is:\n\n* **Causal and streaming-capable**, using FIFO buffering for minimal-latency inference.\n* **Highly efficient**, with a small compute and memory footprint.\n* **Accurate**, achieving state-of-the-art results on a competitive eye tracking benchmark.\n* **Further optimizable** via activation sparsification, maintaining performance while reducing\n  computational load.\n\nThe following sections outline the architecture, dataset characteristics, evaluation results,\nbuffering mechanism, and advanced optimization strategies.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Network architecture\n\nThe proposed architecture is a stack of **spatiotemporal convolutional blocks**, each consisting\nof a **temporal convolution followed by a spatial convolution**. These are designed to extract\nboth fine-grained temporal features and local spatial structure from event-based input tensors.\nThe figure below shows the details of the model architecture.\n\n.. figure:: ../../img/eye_tracking_model_figure.png\n   :target: ../../_images/eye_tracking_model_figure.png\n   :alt: Model architecture overview\n   :scale: 70 %\n   :align: center\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Key design features\n\n1. **Causal Temporal Convolutions**\n\n   Temporal convolutions are strictly causal\u2014output at time *t* depends only on input at time \u2264\n   *t*. This property is critical for real-time, online inference, allowing inference from the\n   first received frame from the sensor.\n\n2. **Factorized 3D Convolution Scheme**\n\n   Our spatiotemporal blocks perform temporal convolutions first, followed by spatial\n   convolutions. Decomposing the 3D convolutions into temporal and spatial layers greatly\n   reduces computation (in much the same way that depthwise separable layers do for 2D\n   convolutions).\n\n   .. figure:: ../../img/eye_tracking_block_description.png\n      :target: ../../_images/eye_tracking_block_description.png\n      :alt: Factorization of conv3D into temporal and spatial convolutions\n      :scale: 80 %\n      :align: center\n\n3. **Depthwise-Separable Convolutions (DWS)**\n\n   Both temporal and spatial layers can optionally be configured as depthwise-separable to\n   further reduce computation with minimal loss in accuracy.\n\n4. **No Residual Connections**\n\n   To conserve memory and simplify deployment on edge devices, residual connections are omitted.\n   Since the model has a reduced number of layers, they are not critical to achieve SOTA\n   performance.\n\n5. **Detection Head**\n\n   A lightweight head, inspired by CenterNet [Zhou et al. 2019](https://arxiv.org/abs/1904.07850)_, predicts a confidence score and local spatial offsets\n   for the pupil position over a coarse spatial grid. The predicted position of the pupil can\n   then be reconstructed.\n\n   .. figure:: ../../img/eye_tracking_post_processing.png\n      :target: ../../_images/eye_tracking_post_processing.png\n      :alt: Centernet head and post processing\n      :scale: 80 %\n      :align: center\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Instantiating the spatiotemporal blocks\n\nQuantizeML and Akida Models natively work with TF-Keras layers: akida_models has all the\nnecessary functions to instantiate a network based on spatiotemporal layers as well as training\npipelines available to train models on the jester dataset, the dvs128 dataset or this dataset.\n\nIn this tutorial, we'll use PyTorch and introduce the\n[tenns_modules](https://pypi.org/project/tenns-modules/)_ package which is available to create\nAkida compatible spatiotemporal blocks. The package contains a [spatio-temporal block](../../api_reference/tenns_modules_apis.html#tenns_modules.SpatioTemporalBlock)_\ncomposed of a [spatial](../../api_reference/tenns_modules_apis.html#tenns_modules.SpatialBlock)_\nand a [temporal](../../api_reference/tenns_modules_apis.html#tenns_modules.TemporalBlock)_\nblock.\n\nThe code below shows how to instantiate the simple 10 layers architecture we used to track the\npupil coordinates in time using the tenns_modules package.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Show how to load and create the model\nimport torch\nimport torch.nn as nn\n\nfrom tenns_modules import SpatioTemporalBlock\nfrom torchinfo import summary\n\nn_depthwise_layers = 4\nchannels = [2, 8, 16, 32, 48, 64, 80, 96, 112, 128, 256]\nt_kernel_size = 5  # can vary from 1 to 10\ns_kernel_size = 3  # can vary in [1, 3, 5, 7] (1 only when depthwise is False)\n\n\nclass TennSt(nn.Module):\n    def __init__(self, channels, t_kernel_size, s_kernel_size, n_depthwise_layers):\n        super().__init__()\n\n        depthwises = [False] * (10 - n_depthwise_layers) + [True] * n_depthwise_layers\n        self.backbone = nn.Sequential()\n        for i in range(0, len(depthwises), 2):\n            in_channels, med_channels, out_channels = channels[i], channels[i + 1], channels[i + 2]\n            t_depthwise, s_depthwise = depthwises[i], depthwises[i]\n\n            self.backbone.append(\n                SpatioTemporalBlock(in_channels=in_channels, med_channels=med_channels,\n                                    out_channels=out_channels, t_kernel_size=t_kernel_size,\n                                    s_kernel_size=s_kernel_size, s_stride=2, bias=False,\n                                    t_depthwise=t_depthwise, s_depthwise=s_depthwise))\n\n        self.head = nn.Sequential(\n            SpatioTemporalBlock(channels[-1], channels[-1], channels[-1],\n                                t_kernel_size=t_kernel_size, s_kernel_size=s_kernel_size,\n                                t_depthwise=False, s_depthwise=False),\n            nn.Conv3d(channels[-1], 3, 1)\n        )\n\n    def forward(self, input):\n        return self.head((self.backbone(input)))\n\n\nmodel = TennSt(channels, t_kernel_size, s_kernel_size, n_depthwise_layers)\nsummary(model, input_size=(1, 2, 50, 96, 128), depth=4, verbose=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Dataset and preprocessing\n\nThe model is trained and evaluated on the\n[AIS 2024 Event-Based Eye Tracking Challenge Dataset](https://www.kaggle.com/competitions/event-based-eye-tracking-ais2024)_, which contains\nrecordings from 13 participants, captured using 480\u00d7640-resolution event camera. Each participant\nhas between 2 and 6 recording sessions. The ground truth pupil (x- and y-) coordinates are\nprovided at a resolution of 100Hz. The evaluation of the predictions is done at 20Hz at a\nresolution of 60x80 when the eyes are opened.\n\nThe video below shows you an example of the reconstructed frames (note that the video has been\nsped up). The ground truth pupil location is represented by a cross: the cross is green when the\neye is opened and it turns red when the eye closes.\n\n.. video:: ../../img/eye_tracking_valdata_gt_only_fast.mp4\n   :nocontrols:\n   :autoplay:\n   :playsinline:\n   :muted:\n   :loop:\n   :width: 50%\n   :align: center\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Preprocessing\n\nThe following preprocessing is applied to the event data:\n\n- temporal augmentations (for training only)\n- spatial downsampling (by 5) and event binning to create segments with fixed temporal length\n- spatial affine transforms\n- frames where the eye is labeled as closed are ignored during training\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3.1.1 Event binning\n\nEvents are represented as 4-tuples: *(polarity, x, y, timestamp)*. These are converted into\ntensors of shape **(P=2, T, H, W)** using **causal event volume binning**, a method that preserves\ntemporal fidelity while avoiding future context. Binning uses a causal triangle kernel to\napproximate each event\u2019s influence over space and time, as you can see from the graph below.\n\n.. figure:: ../../img/eye_tracking_causal_event_binning.png\n   :target: ../../_images/eye_tracking_causal_event_binning.png\n   :alt: Example of causal event binning\n   :scale: 80 %\n   :align: center\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3.1.2 Augmentation\n\nTo improve generalization in a data-limited regime, the following transforms are applied to the\nevents (and the corresponding pupil coordinates) during training only:\n\n* **Spatial affine transforms** are applied such as scaling, rotation, translation.\n* **Temporal augmentations** including random time scaling and flipping (with polarity inversion).\n* **Random temporal flip** with probability 0.5 is applied to the time and polarity dimension.\n\nThese transforms are applied to each segment independently (but not varied within a segment).\nFor better legibility, the dataset was preprocessed offline and made available for evaluation\npurposes only.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Evaluation metric\n\nFor the competition, the primary metric for model evaluation was the \u201cp10\u201d accuracy: the\npercentage of predictions falling within 10 pixels of the ground truth (i.e. if the predicted\npupil center falls within the blue dashed circle in the figure below). We can also consider more\nstringent measures, such as a p3 accuracy (3 pixels); or simpler linear measures, such as the\nEuclidean distance (L2).\n\n.. figure:: ../../img/eye_tracking_pixel_accuracy_euclidean.png\n   :target: ../../_images/eye_tracking_pixel_accuracy_euclidean.png\n   :alt: Metrics used in the competition\n   :scale: 80 %\n   :align: center\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model training & evaluation\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Training details\n\nThe following hyperparameters were used for training:\n\n- batch size of 32\n- 50 event frames per segment\n- 200 epochs\n- AdamW optimizer with base LR of 0.002 and weight decay of 0.005\n- learning rate scheduler with linear warm up (for 2.5% of total epochs) and a cosine decay\n\n.. Note::\n  We don't train the model here as it requires access to a GPU but rather load a pre-trained model\n  for convenience.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Load the pretrained weights in our model\nfrom akida_models import fetch_file\n\nckpt_file = fetch_file(\n    fname=\"tenn_spatiotemporal_eye.ckpt\",\n    origin=\"https://data.brainchip.com/models/AkidaV2/tenn_spatiotemporal/tenn_spatiotemporal_eye.ckpt\",\n    cache_subdir='models')\n\ncheckpoint = torch.load(ckpt_file, map_location=\"cpu\")\nnew_state_dict = {k.replace('model._orig_mod.', ''): v for k, v in checkpoint[\"state_dict\"].items()}\nmodel.load_state_dict(new_state_dict)\n_ = model.eval().cpu()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Evaluation\n\nThe preprocessed test data have been set aside and can be loaded from the archive available\nonline.\n\n.. Note::\n  To optimize storage and reduce processing time, only the first 400 frames from each test file\n  have been mirrored on the dataset server. This subset is representative and sufficient for\n  evaluation purposes in this tutorial.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nsamples = fetch_file(\"https://data.brainchip.com/dataset-mirror/eye_tracking_ais2024_cvpr/eye_tracking_preprocessed_400frames_test.npz\",\n                     fname=\"eye_tracking_preprocessed_400frames_test.npz\")\ndata = np.load(samples, allow_pickle=True)\nevents, centers = data[\"events\"], data[\"centers\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To evaluate the model, we pass the data through our spatiotemporal model. Once we have the output,\nwe need to post process the model's output to reconstruct the predicted pupil coordinates in the\nprediction space (60, 80).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def process_detector_prediction(pred):\n    \"\"\"Post-processing of model predictions to extract the predicted pupil coordinates for a model\n    that has a centernet like head.\n\n    Args:\n        preds (torch.Tensor): shape (B, C, T, H, W)\n\n    Returns:\n        torch tensor of (B, 2) containing the x and y predicted coordinates\n    \"\"\"\n    torch_device = pred.device\n    batch_size, _, frames, height, width = pred.shape\n    # Extract the center heatmap, and the x and y offset maps\n    pred_pupil, pred_x_mod, pred_y_mod = pred.moveaxis(1, 0)\n    pred_x_mod = torch.sigmoid(pred_x_mod)\n    pred_y_mod = torch.sigmoid(pred_y_mod)\n\n    # Find the stronger peak in the center heatmap and it's coordinates\n    pupil_ind = pred_pupil.flatten(-2, -1).argmax(-1)  # (batch, frames)\n    pupil_ind_x = pupil_ind % width\n    pupil_ind_y = pupil_ind // width\n\n    # Reconstruct the predicted offset\n    batch_range = torch.arange(batch_size, device=torch_device).repeat_interleave(frames)\n    frames_range = torch.arange(frames, device=torch_device).repeat(batch_size)\n    pred_x_mod = pred_x_mod[batch_range, frames_range, pupil_ind_y.flatten(), pupil_ind_x.flatten()]\n    pred_y_mod = pred_y_mod[batch_range, frames_range, pupil_ind_y.flatten(), pupil_ind_x.flatten()]\n\n    # Express the coordinates in size agnostic terms (between 0 and 1)\n    x = (pupil_ind_x + pred_x_mod.view(batch_size, frames)) / width\n    y = (pupil_ind_y + pred_y_mod.view(batch_size, frames)) / height\n    return torch.stack([x, y], dim=1)\n\n\ndef compute_distance(pred, center):\n    \"\"\"Computes the L2 distance for a prediction and center matrice\n\n    Args:\n        pred: torch tensor of shape (2, T)\n        center: torch tensor of shape (2, T)\n    \"\"\"\n    height, width = 60, 80\n    pred = pred.detach().clone()\n    center = center.detach().clone()\n    pred[0, :] *= width\n    pred[1, :] *= height\n    center[0, :] *= width\n    center[1, :] *= height\n    l2_distances = torch.norm(center - pred, dim=0)\n    return l2_distances\n\n\ndef pretty_print_results(collected_distances):\n    \"\"\"Prints the distance and accuracy within different pixel tolerance.\n\n    By default, only the results at 20Hz will be printed (to be compatible with the\n    metrics of the challenge). To print the results computed on the whole trial,\n    use downsample=False. In practice, this changes very little to the final performance\n    of the model.\n    \"\"\"\n    for t in [10, 5, 3, 1]:\n        p_acc = (collected_distances < t).sum() / collected_distances.size\n        print(f'- p{t}: {p_acc:.3f}')\n    print(f'- Euc. Dist: {collected_distances.mean():.3f} ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Get the model device to propagate the events properly\ntorch_device = next(model.parameters()).device\n\n# Compute the distances across all 9 trials\ncollected_l2_distances = np.zeros((0,))\nfor trial_idx, event in enumerate(events):\n    center = torch.from_numpy(centers[trial_idx]).float().to(torch_device)\n    event = torch.from_numpy(event).unsqueeze(0).float().to(torch_device)\n    pred = model(event)\n    pred = process_detector_prediction(pred).squeeze(0)\n    l2_distances = compute_distance(pred, center)\n    collected_l2_distances = np.concatenate((collected_l2_distances, l2_distances), axis=0)\n\npretty_print_results(collected_l2_distances)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Official competition results\n\nThe results for the competition are on the test set (labels are not available). The main metric in\nthe challenge was the p10. Using this metric, our model ranked 3rd (see table below copied from\nthe [original challenge survey paper](https://openaccess.thecvf.com/content/CVPR2024W/AI4Streaming/papers/Wang_Event-Based_Eye_Tracking._AIS_2024_Challenge_Survey_CVPRW_2024_paper.pdf)_).\n\nHowever, other metrics were reported in the original challenge survey: the accuracy within 5 (p5),\n3 (p3) or 1 pixel (p1), as well as metrics directly measuring the distance between ground truth\nand predicted pupil location (L2 and L1, i.e. smaller values are better). On these more stringent\nmetrics, our model outperforms the other models on all the other metrics.\n\n.. list-table::\n   :header-rows: 1\n\n   * - **Team**\n     - **Rank**\n     - p10 private (primary)\n     - p10 \u2191\n     - p5 \u2191\n     - p3 \u2191\n     - p1 \u2191\n     - *L2* \u2193\n     - *L1* \u2193\n   * - USTCEventGroup\n     - 1\n     - **99.58**\n     - **99.42**\n     - 97.05\n     - 90.73\n     - 33.75\n     - 1.67\n     - 2.11\n   * - FreeEvs\n     - 2\n     - 99.27\n     - 99.26\n     - 94.31\n     - 83.83\n     - 23.91\n     - 2.03\n     - 2.56\n   * - **Brainchip**\n     - 3\n     - 99.16\n     - 99.00\n     - **97.79**\n     - **94.58**\n     - **45.50**\n     - **1.44**\n     - **1.82**\n   * - Go Sparse\n     - 4\n     - 98.74\n     - 99.00\n     - 77.20\n     - 47.97\n     - 7.32\n     - 3.51\n     - 4.63\n   * - MeMo\n     - 4\n     - 98.74\n     - 99.05\n     - 89.36\n     - 50.87\n     - 6.53\n     - 3.2\n     - 4.04\n\nThe best metric in class is highlighted in bold, \u2191 means higher values are best, \u2193 means lower\nvalues are best.\n\nThe code below shows an inference on the model using the *test* dataset. Note that the\nresults below differ from the challenge metrics reported above because our submission model was\ntrained on both the train and validation data to achieve the best possible performance (as allowed\nby the rules), but the model below that was used for the ablation studies was trained on the train\nset only.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Ablation studies and efficiency optimization\n\nFigure reproduced from the original paper.\n\n.. figure:: ../../img/paper_figure3.png\n   :target: ../../_images/paper_figure3.png\n   :alt: Figure 3 from the original paper\n   :scale: 80 %\n   :align: center\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 Ablation studies\n\nTo test the robustness of our design choices, we performed a series of ablation studies. To\nprovide a baseline model for the ablation study, we trained a model on the 'train' split only and\ntested it on the validation dataset. This model gets a p10 of 0.963 and an l2 distance of 2.79.\n\nThis showed that:\n\n1. Removing spatial affine augmentation reduces performance dramatically (from 0.963 \u2192 0.588).\n2. Causal event binning performs equivalently to other methods while enabling streaming inference.\n3. Larger temporal kernels (e.g., size 5 vs. 3) offer small but consistent improvements in\n   accuracy.\n4. Using only batch normalization (BN) layers gave a small improvement over group norm (GN) only\n   or a mix of BN/GN(96.9 vs 96.0 or 96.3).\n\nFor more details you can refer to the [paper](https://openaccess.thecvf.com/content/CVPR2024W/AI4Streaming/papers/Pei_A_Lightweight_Spatiotemporal_Network_for_Online_Eye_Tracking_with_Event_CVPRW_2024_paper.pdf)_.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Efficiency-accuracy trade-offs\n\nIn certain environments, such as edge or low-power devices, the balance between model size and\ncomputational demand often matters more than achieving state-of-the-art accuracy. This section\nexplores the trade-off between maximizing accuracy and maintaining model efficiency along 3 axis.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 6.2.1 Spatial resolution\n\nWe looked at how reducing input image size affects model performance (see [figure 3.A](./plot_1_eye_tracking_cvpr.html#ablation-studies-and-efficiency-optimization)_). Even with an\ninput size of 60 x 80 (downsampling by a factor of 8), the model still performs almost as well\nas with our default setting (downsampling by a factor of 5), while requiring only a third of the\ncomputation.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 6.2.2 Depthwise separable convolutions\n\nFrom the outset, we decided to further decompose our factorized convolutions into depthwise and\npointwise convolutions (similar to depthwise separable convolutions introduced in MobileNet V1).\nWe explored how these impacted model performance (see [figure 3.B](./plot_1_eye_tracking_cvpr.html#ablation-studies-and-efficiency-optimization)_): as the number\nof separable convolutions used increases, the MACs of the model decrease, with a relatively small\nimpact on the validation distance. When no separable layers are used, the final validation\ndistance is 2.6 vs. 3.1 when all layers are separable. Our baseline model had the last 4 layers\nconfigured as separable. Changing just 2 more to separable could lead to a reduction of almost 30%\nin compute, with almost no impact on performance (compare the turquoise with green lines on the\n[figure 3.B](./plot_1_eye_tracking_cvpr.html#ablation-studies-and-efficiency-optimization)_).\n\nThe combination of these techniques results in a highly efficient model with a computational cost\nof just 55M MACs/frame, and even less when sparsity is exploited.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 6.2.3 Activity regularization\n\nEvent camera data is inherently sparse. However, intermediate layers in a neural network may still\nproduce dense activations unless explicitly regularized. When measuring the baseline sparsity in\nthe network, we found it to be on average 50% (about what one would expect given ReLU activation\nfunctions), much of which may not be informative given the very high spatial sparsity of the input\nto the network. By applying L1 regularization to ReLU activations during training, the model is\nencouraged to silence unnecessary activations. We applied 5 different levels of regularization to\nour model: [figure 3.C](./plot_1_eye_tracking_cvpr.html#ablation-studies-and-efficiency-optimization)_ shows how the\naverage distance varies depending on the regularization strength while [figure 3.D](./plot_1_eye_tracking_cvpr.html#ablation-studies-and-efficiency-optimization)_ shows how the\nsparse aware MACs (i.e. MACs multiplied by the model's mean sparsity per layer) is affected by\nregularization. We can see that over 90% activation sparsity is achievable with a negligible\nperformance degradation (p10 remains >0.96).\n\nThis is especially interesting because Akida is an event based hardware: it is capable of skipping\nzero operations. In such hardware, high level of activation sparsity can translate into ~5\u00d7 speedups.\n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>Based on these ablation studies, the model made available through the\n  [model zoo](../../model_zoo_performance.html#eye-tracking)_ has been optimized for the inference\n  on Akida Hardware (downsampling by a factor of 6, use of depthwise separable convolutions), so the\n  number of parameters and accuracy reported differ.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. FIFO buffering for streaming inference\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.1 Key mechanism\n\nEach temporal convolutional layer maintains a fixed-length FIFO buffer of its input history (equal\nto the kernel size). At each time step:\n\n- The buffer is updated with the newest frame.\n- A dot product is computed between the buffer contents and the kernel weights.\n- The result is passed through normalization and spatial convolution.\n\nThis approach mimics the operation of a sliding temporal convolution but avoids recomputation and memory\nredundancy, ensuring minimal latency and efficient real-time processing.\nFor more details of this approach, see the tutorial that [introduced spatiotemporal models](./plot_0_introduction_to_spatiotemporal_models.html#streaming-inference-making-real-time-predictions)_.\n\n.. figure:: ../../img/fifo_buffer.png\n   :target: ../../_images/fifo_buffer.png\n   :alt: Fifo buffer\n   :scale: 80 %\n   :align: center\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.2 Exporting to ONNX\n\nThe transformation to buffer mode is done during quantization step (see dedicated section below).\nThe first step is to export the model to ONNX format. This is made very easy using the\n[tenns_modules](https://pypi.org/project/tenns-modules/)_ package and the [export_to_onnx](../../api_reference/tenns_modules_apis.html#tenns_modules.export_to_onnx)_ function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tenns_modules import export_to_onnx\n\n# Using a batch size of 10 to export with a dynamic batch size\nonnx_checkpoint_path = \"tenns_modules_onnx.onnx\"\nexport_to_onnx(model, (10, 2, 50, 96, 128), out_path=onnx_checkpoint_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the ONNX model that was automatically saved\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import onnx\n\nmodel = onnx.load(onnx_checkpoint_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Quantization and conversion to Akida\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.1 Quantization\n\nTo be deployable on Akida, the model needs to be quantized. This can easily be done using the\nQuantizeML package. For more details on the quantization scheme with the ONNX package see this\nexample on [off-the-shelf model quantization](../quantization/plot_2_off_the_shelf_quantization.html)_.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from quantizeml.models import quantize\nfrom quantizeml.layers import QuantizationParams\n\n# Retrieve calibration samples:\nsamples = fetch_file(\"https://data.brainchip.com/dataset-mirror/samples/eye_tracking/eye_tracking_onnx_samples_bs100.npz\",\n                     fname=\"eye_tracking_onnx_samples_bs100.npz\")\n\n# Define quantization parameters and load quantization samples\nqparams = QuantizationParams(per_tensor_activations=True, input_dtype='int8')\ndata = np.load(samples)\nsamples = np.concatenate([data[item] for item in data.files])\n\n# Quantize the model\nmodel_quant = quantize(model, qparams=qparams, epochs=1, batch_size=100, samples=samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. Note::\n  During this step, the model is also bufferized, meaning that the FIFOs of the temporal\n  convolutions are automatically created and initialized from the 3D convolutions.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.2 ONNX model evaluation\n\nThis model can be evaluated using the same process as before with a few differences:\n\n- We need to pass each frame to the model independently (i.e. the model now has a 4-D input shape\n  (B, C, H,  W) - batch, channels, height, width).\n- The post processing function needs to be modified to use numpy functions (instead of torch)\n- Once all frames from a given trial have been passed through, the FIFO buffers of the temporal\n  convolutions need to be reset using the [reset_buffers](../../api_reference/quantizeml_apis.html#quantizeml.models.reset_buffers)_ available from\n  QuantizeML.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def custom_process_detector_prediction(pred):\n    \"\"\" Post-processing of the model's output heatmap.\n\n    Reconstructs the predicted x- and y- center location using numpy functions to post-process\n    the output of a ONNX model.\n    \"\"\"\n    def sigmoid(x):\n        return 1 / (1 + np.exp(-x))\n    # Pred shape is (batch, channels, height, width)\n    batch_size, _, height, width = pred.shape\n\n    # Split channels - reshape to move frames dimension after batch\n    # Now (batch, height, width, channels)\n    pred = np.moveaxis(pred, 1, -1)\n    pred_pupil = pred[..., 0]\n    pred_x_mod = sigmoid(pred[..., 1])\n    pred_y_mod = sigmoid(pred[..., 2])\n\n    # Find pupil location\n    pred_pupil_flat = pred_pupil.reshape(batch_size, -1)\n    pupil_ind = np.argmax(pred_pupil_flat, axis=-1)\n    pupil_ind_x = pupil_ind % width\n    pupil_ind_y = pupil_ind // width\n\n    # Get the learned x- y- offset\n    batch_idx = np.repeat(np.arange(batch_size)[:, None], 1, axis=1)\n    x_mods = pred_x_mod[batch_idx, pupil_ind_y, pupil_ind_x]\n    y_mods = pred_y_mod[batch_idx, pupil_ind_y, pupil_ind_x]\n\n    # Calculate final coordinates\n    x = (pupil_ind_x + x_mods) / width\n    y = (pupil_ind_y + y_mods) / height\n\n    return np.stack([x, y], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create the inference session for the ONNX model and evaluate\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from onnxruntime import InferenceSession, SessionOptions\nfrom onnxruntime_extensions import get_library_path\nfrom quantizeml.onnx_support.quantization import ONNXModel\n\nsess_options = SessionOptions()\nsess_options.register_custom_ops_library(get_library_path())\nmodel_quant = ONNXModel(model_quant)\nsession = InferenceSession(model_quant.serialized, sess_options=sess_options,\n                           providers=['CPUExecutionProvider'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from quantizeml.models import reset_buffers\nfrom tqdm import tqdm\n\n# And then evaluate the model\ncollected_l2_distances = []\nfor trial_idx, event in enumerate(events):\n    center = centers[trial_idx]\n    for frame_idx in tqdm(range(event.shape[1])):\n        frame = event[:, frame_idx, ...][None, ...].astype(np.float32)\n        pred = session.run(None, {model_quant.input[0].name: frame})[0]\n        pred = custom_process_detector_prediction(pred).squeeze()\n        y_pred_x = pred[0] * 80\n        y_pred_y = pred[1] * 60\n        center_x = center[0, frame_idx] * 80\n        center_y = center[1, frame_idx] * 60\n        collected_l2_distances.append(np.sqrt(np.square(\n            center_x - y_pred_x) + np.square(center_y - y_pred_y)))\n    # Reset FIFOs between each file\n    reset_buffers(model_quant)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pretty_print_results(np.array(collected_l2_distances))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.3 Conversion to Akida\n\nThe quantized model can be easily converted to Akida using the cnn2snn package.\nThe [convert](../../api_reference/cnn2snn_apis.html#cnn2snn.convert)_ function\nreturns a model in Akida format ready for inference.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from cnn2snn import convert\n\nakida_model = convert(model_quant.model)\nakida_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. Note::\n  - For more information you can refer to the paper available [here](https://openaccess.thecvf.com/content/CVPR2024W/AI4Streaming/papers/Pei_A_Lightweight_Spatiotemporal_Network_for_Online_Eye_Tracking_with_Event_CVPRW_2024_paper.pdf)_.\n  - There is also a full training pipeline available in TF-Keras from the akida_models\n    package that reproduces the performance presented in the paper available with the\n    [akida_models.tenn_spatiotemporal](../../api_reference/akida_models_apis.html#akida_models.tenn_spatiotemporal_eye)_ function.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}