{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nDS-CNN/KWS inference\n=======================\n\nThis tutorial illustrates how to build a basic speech recognition\nAkida network that recognizes ten different words.\n\nThe model will be first defined as a CNN and trained in Keras, then\nconverted using the `CNN2SNN toolkit <../user_guide/cnn2snn.html>`__.\n\nThis example uses a Keyword Spotting Dataset prepared using\n**TensorFlow** `audio recognition\nexample <https://www.tensorflow.org/tutorials/sequences/audio_recognition>`__\nutils.\n\nThe words to recognize are first converted to `spectrogram\nimages <https://github.com/tensorflow/docs/blob/master/site/en/r1/tutorials/sequences/audio_recognition.md#how-does-this-model-work>`__\nthat allows us to use a model architecture that is typically used for\nimage recognition tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Load CNN2SNN tool dependencies\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# System imports\nimport os\nimport sys\nimport numpy as np\nimport pickle\nfrom sklearn.metrics import accuracy_score\nimport itertools\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\n\n# TensorFlow imports\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.utils import get_file\n\n# KWS model imports\nfrom akida_models import ds_cnn_kws_pretrained"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Load the preprocessed dataset\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "wanted_words = [\n    'down', 'go', 'left', 'no', 'off', 'on', 'right', 'stop', 'up', 'yes'\n]\nall_words = ['_silence_', '_unknown_'] + wanted_words\n\n# Preprocessed dataset parameters\nCHANNELS = 1\nCLASSES = len(all_words)\nSPECTROGRAM_LENGTH = 49\nFINGERPRINT_WIDTH = 10\n\ninput_shape = (SPECTROGRAM_LENGTH, FINGERPRINT_WIDTH, CHANNELS)\n\n# Try to load pre-processed dataset\nfname = get_file(\n    \"preprocessed_data.pkl\",\n    \"http://data.brainchip.com/dataset-mirror/kws/preprocessed_data.pkl\",\n    cache_subdir='datasets/kws')\nif os.path.isfile(fname):\n    print('Re-loading previously preprocessed dataset...')\n    f = open(fname, 'rb')\n    [x_train, y_train, x_valid, y_valid, train_files, val_files,\n     word_to_index] = pickle.load(f)\n    f.close()\nelse:\n    raise ValueError(\"Unable to load the pre-processed KWS dataset.\")\n\n# Transform the data to uint8\nx_train_min = x_train.min()\nx_train_max = x_train.max()\nmax_int_value = 255.0\n\n# For akida hardware training and validation range [0, 255] inclusive uint8\nx_train_akida = ((x_train - x_train_min) * max_int_value /\n                 (x_train_max - x_train_min)).astype(np.uint8)\nx_valid_akida = ((x_valid - x_train_min) * max_int_value /\n                 (x_train_max - x_train_min)).astype(np.uint8)\n\n# For cnn2snn training and validation range [0,1] inclusive float32\nx_train_rescaled_cnn = (x_train_akida.astype(np.float32)) / max_int_value\nx_valid_rescaled_cnn = (x_valid_akida.astype(np.float32)) / max_int_value\n\ninput_scaling = (max_int_value, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Create a Keras model satisfying Akida NSoC requirements\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe model consists of:\n\n* a first Convolutional layer accepting dense inputs (images),\n* several Separable Convolutional layers preserving spatial dimensions,\n* a global pooling reducing the spatial dimensions to a single pixel,\n* a last Separable Convolutional layer to reduce the number of outputs\n  to the number of words to predict.\n\nAll layers are followed by a batch normalization and a ReLU activation,\nexcept the last one that is followed by a SoftMax.\n\nThe first convolutional layer uses 8 bits weights, but other layers use\n4 bits weights.\n\nAll activations are 4 bits.\n\n.. Note:: The reason why we do not use a simple FullyConnected layer as the\n          last layer is precisely because of the 4 bits activations, that are\n          only supported as inputs by the Separable Convolutional layers.\n\nPre-trained weights were obtained after three training episodes:\n\n* first, we train the model with unconstrained float weights and\n  activations for 30 epochs,\n* then, we train the model with quantized activations only, with\n  weights initialized from those trained in the previous episode,\n* finally, we train the model with quantized weights and activations,\n  with weights initialized from those trained in the previous episode.\n\nThe table below summarizes the results obtained when preparing the\nweights stored under `<http://data.brainchip.com/models/ds_cnn/>`__ :\n\n+---------+----------------+---------------+----------+--------+\n| Episode | Weights Quant. | Activ. Quant. | Accuracy | Epochs |\n+=========+================+===============+==========+========+\n| 1       | N/A            | N/A           | 91.98 %  | 30     |\n+---------+----------------+---------------+----------+--------+\n| 2       | N/A            | 4 bits        | 92.13 %  | 30     |\n+---------+----------------+---------------+----------+--------+\n| 3       | 8/4 bits       | 4 bits        | 91.67 %  | 30     |\n+---------+----------------+---------------+----------+--------+\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "K.clear_session()\nmodel_keras = ds_cnn_kws_pretrained()\nmodel_keras.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. Check performance\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Check Model performance\npotentials_keras = model_keras.predict(x_valid_rescaled_cnn)\npreds_keras = np.squeeze(np.argmax(potentials_keras, 1))\n\naccuracy = accuracy_score(y_valid, preds_keras)\nprint(\"Accuracy: \" + \"{0:.2f}\".format(100 * accuracy) + \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5. Conversion to Akida\n~~~~~~~~~~~~~~~~~~~~~~\n\n5.1 Convert the trained Keras model to Akida\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nWe convert the model to Akida and verify that it is compatible with the\nAkida NSoC (**HW** column in summary).\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Convert the model\nfrom cnn2snn import convert\n\nmodel_akida = convert(model_keras, input_scaling=input_scaling)\nmodel_akida.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5.2 Check prediction accuracy\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "preds_akida = model_akida.predict(x_valid_akida, num_classes=CLASSES)\n\naccuracy = accuracy_score(y_valid, preds_akida)\nprint(\"Accuracy: \" + \"{0:.2f}\".format(100 * accuracy) + \"%\")\n\n# For non-regression purpose\nassert accuracy > 0.83\n\n# Print model statistics\nprint(\"Model statistics\")\nstats = model_akida.get_statistics()\nmodel_akida.predict(x_valid_akida[:20], num_classes=CLASSES)\nfor _, stat in stats.items():\n    print(stat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5.3 Confusion matrix\n^^^^^^^^^^^^^^^^^^^^\n\nThe confusion matrix provides a good summary of what mistakes the\nnetwork is making.\n\nPer scikit-learn convention it displays the true class in each row (ie\non each row you can see what the network predicted for the corresponding\nword).\n\nPlease refer to the Tensorflow `audio\nrecognition <https://github.com/tensorflow/docs/blob/master/site/en/r1/tutorials/sequences/audio_recognition.md#confusion-matrix>`__\nexample for a detailed explaination of the confusion matrix.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Create confusion matrix\nlabel_mapping = dict(zip(all_words, range(len(all_words))))\n\ncm = confusion_matrix(y_valid, preds_akida, list(label_mapping.values()))\n\n# Normalize\ncm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n# Display confusion matrix\nplt.rcParams[\"figure.figsize\"] = (8, 8)\nplt.figure()\n\nclasses = label_mapping\ntitle = 'Confusion matrix'\ncmap = plt.cm.Blues\n\nplt.imshow(cm, interpolation='nearest', cmap=cmap)\nplt.title(title)\nplt.colorbar()\ntick_marks = np.arange(len(classes))\nplt.xticks(tick_marks, classes, rotation=45)\nplt.yticks(tick_marks, classes)\n\nthresh = cm.max() / 2.\nfor i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n    plt.text(j,\n             i,\n             format(cm[i, j], '.2f'),\n             horizontalalignment=\"center\",\n             color=\"white\" if cm[i, j] > thresh else \"black\")\n\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.autoscale()\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}