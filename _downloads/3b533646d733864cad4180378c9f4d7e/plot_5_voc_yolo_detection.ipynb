{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# YOLO/PASCAL-VOC detection tutorial\n\nThis tutorial demonstrates that Akida can perform object detection. This is illustrated using a\nsubset of the\n[PASCAL-VOC 2007 dataset](http://host.robots.ox.ac.uk/pascal/VOC/voc2007/htmldoc/index.html)_\nwhich contains 20 classes. The YOLOv2 architecture from\n[Redmon et al (2016)](https://arxiv.org/pdf/1506.02640.pdf) has been chosen to\ntackle this object detection problem.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introduction\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Object detection\n\nObject detection is a computer vision task that combines two elemental tasks:\n\n - object classification that consists in assigning a class label to an image\n   like shown in the [AkidaNet/ImageNet inference](./plot_1_akidanet_imagenet.html)\n   example\n - object localization that consists of drawing a bounding box around one or\n   several objects in an image\n\nOne can learn more about the subject by reading this [introduction to object\ndetection blog article](https://machinelearningmastery.com/object-recognition-with-deep-learning/).\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 YOLO key concepts\n\nYou Only Look Once (YOLO) is a deep neural network architecture dedicated to\nobject detection.\n\nAs opposed to classic networks that handle object detection, YOLO predicts\nbounding boxes (localization task) and class probabilities (classification\ntask) from a single neural network in a single evaluation. The object\ndetection task is reduced to a regression problem to spatially separated boxes\nand associated class probabilities.\n\nYOLO base concept is to divide an input image into regions, forming a grid,\nand to predict bounding boxes and probabilities for each region. The bounding\nboxes are weighted by the prediction probabilities.\n\nYOLO also uses the concept of \"anchors boxes\" or \"prior boxes\". The network\ndoes not actually predict the actual bounding boxes but offsets from anchors\nboxes which are templates (width/height ratio) computed by clustering the\ndimensions of the ground truth boxes from the training dataset. The anchors\nthen represent the average shape and size of the objects to detect. More\ndetails on the anchors boxes concept are given in [this blog article](https://medium.com/@andersasac/anchor-boxes-the-key-to-quality-object-detection-ddf9d612d4f9).\n\nAdditional information about YOLO can be found on the [Darknet website](https://pjreddie.com/darknet/yolov2/) and source code for the preprocessing\nand postprocessing functions that are included in akida_models package (see\nthe [processing section](../../api_reference/akida_models_apis.html#processing)\nin the model zoo) is largely inspired from\n[experiencor github](https://github.com/experiencor/keras-yolo2).\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Preprocessing tools\n\nA subset of VOC has been prepared with test images from VOC2007\nthat contains 5 examples of each class. The dataset is represented as\na tfrecord file, containing images, labels, and bounding boxes.\n\nThe `load_tf_dataset` function is a helper function that facilitates the loading\nand parsing of the tfrecord file.\n\nThe [YOLO toolkit](../../api_reference/akida_models_apis.html#yolo-toolkit)\noffers several methods to prepare data for processing, see\n[load_image](../../api_reference/akida_models_apis.html#akida_models.detection.processing.load_image),\n[preprocess_image](../../api_reference/akida_models_apis.html#akida_models.detection.processing.preprocess_image).\n\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n\nfrom akida_models import fetch_file\n\n# Download TFrecords test set from Brainchip data server\ndata_path = fetch_file(\n    fname=\"voc_test_20_classes.tfrecord\",\n    origin=\"https://data.brainchip.com/dataset-mirror/voc/test_20_classes.tfrecord\",\n    cache_subdir='datasets/voc',\n    extract=True)\n\n\n# Helper function to load and parse the Tfrecord file.\ndef load_tf_dataset(tf_record_file_path):\n    tfrecord_files = [tf_record_file_path]\n\n    # Feature description for parsing the TFRecord\n    feature_description = {\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'objects/bbox': tf.io.VarLenFeature(tf.float32),\n        'objects/label': tf.io.VarLenFeature(tf.int64),\n    }\n\n    def _count_tfrecord_examples(dataset):\n        return len(list(dataset.as_numpy_iterator()))\n\n    def _parse_tfrecord_fn(example_proto):\n        example = tf.io.parse_single_example(example_proto, feature_description)\n\n        # Decode the image from bytes\n        example['image'] = tf.io.decode_jpeg(example['image'], channels=3)\n\n        # Convert the VarLenFeature to a dense tensor\n        example['objects/label'] = tf.sparse.to_dense(example['objects/label'], default_value=0)\n\n        example['objects/bbox'] = tf.sparse.to_dense(example['objects/bbox'])\n        # Boxes were flattenned that's why we need to reshape them\n        example['objects/bbox'] = tf.reshape(example['objects/bbox'],\n                                             (tf.shape(example['objects/label'])[0], 4))\n        # Create a new dictionary structure\n        objects = {\n            'label': example['objects/label'],\n            'bbox': example['objects/bbox'],\n        }\n\n        # Remove unnecessary keys\n        example.pop('objects/label')\n        example.pop('objects/bbox')\n\n        # Add 'objects' key to the main dictionary\n        example['objects'] = objects\n\n        return example\n\n    # Create a TFRecordDataset\n    dataset = tf.data.TFRecordDataset(tfrecord_files)\n    len_dataset = _count_tfrecord_examples(dataset)\n    parsed_dataset = dataset.map(_parse_tfrecord_fn)\n\n    return parsed_dataset, len_dataset\n\n\nlabels = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n          'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse',\n          'motorbike', 'person', 'pottedplant', 'sheep', 'sofa',\n          'train', 'tvmonitor']\n\nval_dataset, len_val_dataset = load_tf_dataset(data_path)\nprint(f\"Loaded VOC2007 sample test data: {len_val_dataset} images.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Anchors can also be computed easily using YOLO toolkit.\n\n.. Note:: The following code is given as an example. In a real use case\n          scenario, anchors are computed on the training dataset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from akida_models.detection.generate_anchors import generate_anchors\n\nnum_anchors = 5\ngrid_size = (7, 7)\nanchors_example = generate_anchors(val_dataset, num_anchors, grid_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model architecture\n\nThe [model zoo](../../api_reference/akida_models_apis.html#yolo) contains a\nYOLO model that is built upon the [AkidaNet architecture](../../api_reference/akida_models_apis.html#akida_models.akidanet_imagenet)\nand 3 separable convolutional layers at the top for bounding box and class\nestimation followed by a final separable convolutional which is the detection\nlayer. Note that for efficiency, the alpha parameter in AkidaNet (network\nwidth or number of filter in each layer) is set to 0.5.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from akida_models import yolo_base\n\n# Create a yolo model for 20 classes with 5 anchors and grid size of 7\nclasses = len(labels)\n\nmodel = yolo_base(input_shape=(224, 224, 3),\n                  classes=classes,\n                  nb_box=num_anchors,\n                  alpha=0.5)\nmodel.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model output can be reshaped to a more natural shape of:\n\n (grid_height, grid_width, anchors_box, 4 + 1 + num_classes)\n\nwhere the \"4 + 1\" term represents the coordinates of the estimated bounding\nboxes (top left x, top left y, width and height) and a confidence score. In\nother words, the output channels are actually grouped by anchor boxes, and in\neach group one channel provides either a coordinate, a global confidence score\nor a class confidence score. This process is done automatically in the\n[decode_output](../../api_reference/akida_models_apis.html#akida_models.detection.processing.decode_output)_\nfunction.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import Model\nfrom tensorflow.keras.layers import Reshape\n\n# Define a reshape output to be added to the YOLO model\noutput = Reshape((grid_size[1], grid_size[0], num_anchors, 4 + 1 + classes),\n                 name=\"YOLO_output\")(model.output)\n\n# Build the complete model\nfull_model = Model(model.input, output)\nfull_model.output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Training\n\nAs the YOLO model relies on Brainchip AkidaNet/ImageNet network, it is\npossible to perform transfer learning from ImageNet pretrained weights when\ntraining a YOLO model. See the [PlantVillage transfer learning example](./plot_4_transfer_learning.html) for a detail explanation on transfer\nlearning principles.\nAdditionally, for achieving optimal results, consider the following approach:\n\n1. Initially, train the model on the COCO dataset. This process helps in learning\ngeneral object detection features and improves the model's ability to detect various\nobjects across different contexts.\n\n2. After training on COCO, transfer the learned weights to a model equipped with a\nVOC head.\n\n3. Fine-tune the transferred weights on the VOC dataset. This step allows the model\nto adapt to the specific characteristics and nuances of the VOC dataset, further\nenhancing its performance on VOC-related tasks.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Performance\n\nThe model zoo also contains an [helper method](../../api_reference/akida_models_apis.html#akida_models.yolo_voc_pretrained)\nthat allows to create a YOLO model for VOC and load pretrained weights for the\ndetection task and the corresponding anchors. The anchors are used to interpret\nthe model outputs.\n\nThe metric used to evaluate YOLO is the mean average precision (mAP) which is\nthe percentage of correct prediction and is given for an intersection over\nunion (IoU) ratio. Scores in this example are given for the standard IoU of\n0.5, meaning that a detection is considered valid if the intersection over union\nratio with its ground truth equivalent is above 0.5 (mAP 50).\n\n .. Note:: A call to [evaluate_map](../../api_reference/akida_models_apis.html#akida_models.detection.map_evaluation.MapEvaluation.evaluate_map)\n           will preprocess the images, make the call to ``Model.predict`` and\n           use [decode_output](../../api_reference/akida_models_apis.html#akida_models.detection.processing.decode_output)_\n           before computing precision for all classes.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from timeit import default_timer as timer\nfrom akida_models import yolo_voc_pretrained\nfrom akida_models.detection.map_evaluation import MapEvaluation\n\n# Load the pretrained model along with anchors\nmodel_keras, anchors = yolo_voc_pretrained()\nmodel_keras.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Define the final reshape and build the model\noutput = Reshape((grid_size[1], grid_size[0], num_anchors, 4 + 1 + classes),\n                 name=\"YOLO_output\")(model_keras.output)\nmodel_keras = Model(model_keras.input, output)\n\n# Create the mAP evaluator object\nmap_evaluator = MapEvaluation(model_keras, val_dataset,\n                              len_val_dataset, labels, anchors)\n\n# Compute the scores for all validation images\nstart = timer()\n\nmap_dict, average_precisions = map_evaluator.evaluate_map()\nmAP = sum(map_dict.values()) / len(map_dict)\nend = timer()\n\nfor label, average_precision in average_precisions.items():\n    print(labels[label], '{:.4f}'.format(average_precision))\nprint('mAP 50: {:.4f}'.format(map_dict[0.5]))\nprint(f'Keras inference on {len_val_dataset} images took {end-start:.2f} s.\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Conversion to Akida\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 Convert to Akida model\nThe last YOLO_output layer that was added for splitting channels into values\nfor each box must be removed before Akida conversion.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Rebuild a model without the last layer\ncompatible_model = Model(model_keras.input, model_keras.layers[-2].output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When converting to an Akida model, we just need to pass the Keras model\nto [cnn2snn.convert](../../api_reference/cnn2snn_apis.html#convert).\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from cnn2snn import convert\n\nmodel_akida = convert(compatible_model)\nmodel_akida.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 Check performance\n\nAkida model accuracy is tested on the first *n* images of the validation set.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Create the mAP evaluator object\nmap_evaluator_ak = MapEvaluation(model_akida,\n                                 val_dataset,\n                                 len_val_dataset,\n                                 labels,\n                                 anchors,\n                                 is_keras_model=False)\n\n# Compute the scores for all validation images\nstart = timer()\nmap_ak_dict, average_precisions_ak = map_evaluator_ak.evaluate_map()\nmAP_ak = sum(map_ak_dict.values()) / len(map_ak_dict)\nend = timer()\n\nfor label, average_precision in average_precisions_ak.items():\n    print(labels[label], '{:.4f}'.format(average_precision))\nprint('mAP 50: {:.4f}'.format(map_ak_dict[0.5]))\nprint(f'Akida inference on {len_val_dataset} images took {end-start:.2f} s.\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Show predictions for a random image\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\nfrom akida_models.detection.processing import preprocess_image, decode_output\n\n# Shuffle the data to take a random test image\nval_dataset = val_dataset.shuffle(buffer_size=len_val_dataset)\n\ninput_shape = model_akida.layers[0].input_dims\n\n# Load the image\nraw_image = next(iter(val_dataset))['image']\n\n# Keep the original image size for later bounding boxes rescaling\nraw_height, raw_width, _ = raw_image.shape\n\n# Pre-process the image\nimage = preprocess_image(raw_image, input_shape)\ninput_image = image[np.newaxis, :].astype(np.uint8)\n\n# Call evaluate on the image\npots = model_akida.predict(input_image)[0]\n\n# Reshape the potentials to prepare for decoding\nh, w, c = pots.shape\npots = pots.reshape((h, w, len(anchors), 4 + 1 + len(labels)))\n\n# Decode potentials into bounding boxes\nraw_boxes = decode_output(pots, anchors, len(labels))\n\n# Rescale boxes to the original image size\npred_boxes = np.array([[\n    box.x1 * raw_width, box.y1 * raw_height, box.x2 * raw_width,\n    box.y2 * raw_height,\n    box.get_label(),\n    box.get_score()\n] for box in raw_boxes])\n\nfig = plt.figure(num='VOC detection by Akida')\nax = fig.subplots(1)\nimg_plot = ax.imshow(np.zeros(raw_image.shape, dtype=np.uint8))\nimg_plot.set_data(raw_image)\n\nfor box in pred_boxes:\n    rect = patches.Rectangle((box[0], box[1]),\n                             box[2] - box[0],\n                             box[3] - box[1],\n                             linewidth=1,\n                             edgecolor='r',\n                             facecolor='none')\n    ax.add_patch(rect)\n    class_score = ax.text(box[0],\n                          box[1] - 5,\n                          f\"{labels[int(box[4])]} - {box[5]:.2f}\",\n                          color='red')\n\nplt.axis('off')\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}