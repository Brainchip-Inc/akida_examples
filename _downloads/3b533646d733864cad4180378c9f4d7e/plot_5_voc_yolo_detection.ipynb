{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# YOLO/PASCAL-VOC detection tutorial\n\nThis tutorial demonstrates that Akida can perform object detection. This is illustrated using a\nsubset of the\n[PASCAL-VOC 2007 dataset](http://host.robots.ox.ac.uk/pascal/VOC/voc2007/htmldoc/index.html)_\nwith \"car\" and \"person\" classes only. The YOLOv2 architecture from\n[Redmon et al (2016)](https://arxiv.org/pdf/1506.02640.pdf) has been chosen to\ntackle this object detection problem.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introduction\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Object detection\n\nObject detection is a computer vision task that combines two elemental tasks:\n\n - object classification that consists in assigning a class label to an image\n   like shown in the [AkidaNet/ImageNet inference](plot_1_akidanet_imagenet.html)\n   example\n - object localization that consists in drawing a bounding box around one or\n   several objects in an image\n\nOne can learn more about the subject reading this [introduction to object\ndetection blog article](https://machinelearningmastery.com/object-recognition-with-deep-learning/).\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 YOLO key concepts\n\nYou Only Look Once (YOLO) is a deep neural network architecture dedicated to\nobject detection.\n\nAs opposed to classic networks that handle object detection, YOLO predicts\nbounding boxes (localization task) and class probabilities (classification\ntask) from a single neural network in a single evaluation. The object\ndetection task is reduced to a regression problem to spatially separated boxes\nand associated class probabilities.\n\nYOLO base concept is to divide an input image into regions, forming a grid,\nand to predict bounding boxes and probabilities for each region. The bounding\nboxes are weighted by the prediction probabilities.\n\nYOLO also uses the concept of \"anchors boxes\" or \"prior boxes\". The network\ndoes not actually predict the actual bounding boxes but offsets from anchors\nboxes which are templates (width/height ratio) computed by clustering the\ndimensions of the ground truth boxes from the training dataset. The anchors\nthen represent the average shape and size of the objects to detect. More\ndetails on the anchors boxes concept are given in [this blog article](https://medium.com/@andersasac/anchor-boxes-the-key-to-quality-object-detection-ddf9d612d4f9).\n\nAdditional information about YOLO can be found on the [Darknet website](https://pjreddie.com/darknet/yolov2/) and source code for the preprocessing\nand postprocessing functions that are included in akida_models package (see\nthe [processing section](../../api_reference/akida_models_apis.html#processing)\nin the model zoo) is largely inspired from\n[experiencor github](https://github.com/experiencor/keras-yolo2).\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Preprocessing tools\n\nAs this example focuses on car and person detection only, a subset of VOC has\nbeen prepared with test images from VOC2007 that contains at least one\nof the occurence of the two classes. Just like the VOC dataset, the subset\ncontains an image folder, an annotation folder and a text file listing the\nfile names of interest.\n\nThe [YOLO toolkit](../../api_reference/akida_models_apis.html#yolo-toolkit)\noffers several methods to prepare data for processing, see\n[load_image](../../api_reference/akida_models_apis.html#akida_models.detection.processing.load_image),\n[preprocess_image](../../api_reference/akida_models_apis.html#akida_models.detection.processing.preprocess_image)\nor [parse_voc_annotations](../../api_reference/akida_models_apis.html#akida_models.detection.processing.parse_voc_annotations).\n\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\n\nfrom akida_models import fetch_file\nfrom akida_models.detection.processing import parse_voc_annotations\n\n# Download validation set from Brainchip data server\ndata_path = fetch_file(\n    fname=\"voc_test_car_person.tar.gz\",\n    origin=\"https://data.brainchip.com/dataset-mirror/voc/voc_test_car_person.tar.gz\",\n    cache_subdir='datasets/voc',\n    extract=True)\n\ndata_dir = os.path.dirname(data_path)\ngt_folder = os.path.join(data_dir, 'voc_test_car_person', 'Annotations')\nimage_folder = os.path.join(data_dir, 'voc_test_car_person', 'JPEGImages')\nfile_path = os.path.join(\n    data_dir, 'voc_test_car_person', 'test_car_person.txt')\nlabels = ['car', 'person']\n\nval_data = parse_voc_annotations(gt_folder, image_folder, file_path, labels)\nprint(\"Loaded VOC2007 test data for car and person classes: \"\n      f\"{len(val_data)} images.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Anchors can also be computed easily using YOLO toolkit.\n\n.. Note:: The following code is given as an example. In a real use case\n          scenario, anchors are computed on the training dataset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from akida_models.detection.generate_anchors import generate_anchors\n\nnum_anchors = 5\ngrid_size = (7, 7)\nanchors_example = generate_anchors(val_data, num_anchors, grid_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model architecture\n\nThe [model zoo](../../api_reference/akida_models_apis.html#yolo) contains a\nYOLO model that is built upon the [AkidaNet architecture](../../api_reference/akida_models_apis.html#akida_models.akidanet_imagenet)\nand 3 separable convolutional layers at the top for bounding box and class\nestimation followed by a final separable convolutional which is the detection\nlayer. Note that for efficiency, the alpha parameter in AkidaNet (network\nwidth or number of filter in each layer) is set to 0.5.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from akida_models import yolo_base\n\n# Create a yolo model for 2 classes with 5 anchors and grid size of 7\nclasses = 2\n\nmodel = yolo_base(input_shape=(224, 224, 3),\n                  classes=classes,\n                  nb_box=num_anchors,\n                  alpha=0.5)\nmodel.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model output can be reshaped to a more natural shape of:\n\n (grid_height, grid_width, anchors_box, 4 + 1 + num_classes)\n\nwhere the \"4 + 1\" term represents the coordinates of the estimated bounding\nboxes (top left x, top left y, width and height) and a confidence score. In\nother words, the output channels are actually grouped by anchor boxes, and in\neach group one channel provides either a coordinate, a global confidence score\nor a class confidence score. This process is done automatically in the\n[decode_output](../../api_reference/akida_models_apis.html#akida_models.detection.processing.decode_output)_\nfunction.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import Model\nfrom tensorflow.keras.layers import Reshape\n\n# Define a reshape output to be added to the YOLO model\noutput = Reshape((grid_size[1], grid_size[0], num_anchors, 4 + 1 + classes),\n                 name=\"YOLO_output\")(model.output)\n\n# Build the complete model\nfull_model = Model(model.input, output)\nfull_model.output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Training\n\nAs the YOLO model relies on Brainchip AkidaNet/ImageNet network, it is\npossible to perform transfer learning from ImageNet pretrained weights when\ntraining a YOLO model. See the [PlantVillage transfer learning example](plot_4_transfer_learning.html) for a detail explanation on transfer\nlearning principles.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Performance\n\nThe model zoo also contains an [helper method](../../api_reference/akida_models_apis.html#akida_models.yolo_voc_pretrained)\nthat allows to create a YOLO model for VOC and load pretrained weights for the\ncar and person detection task and the corresponding anchors. The anchors are\nused to interpret the model outputs.\n\nThe metric used to evaluate YOLO is the mean average precision (mAP) which is\nthe percentage of correct prediction and is given for an intersection over\nunion (IoU) ratio. Scores in this example are given for the standard IoU of\n0.5 meaning that a detection is considered valid if the intersection over\nunion ratio with its ground truth equivalent is above 0.5.\n\n .. Note:: A call to [evaluate_map](../../api_reference/akida_models_apis.html#akida_models.detection.map_evaluation.MapEvaluation.evaluate_map)\n           will preprocess the images, make the call to ``Model.predict`` and\n           use [decode_output](../../api_reference/akida_models_apis.html#akida_models.detection.processing.decode_output)_\n           before computing precision for all classes.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from timeit import default_timer as timer\nfrom akida_models import yolo_voc_pretrained\nfrom akida_models.detection.map_evaluation import MapEvaluation\n\n# Load the pretrained model along with anchors\nmodel_keras, anchors = yolo_voc_pretrained()\nmodel_keras.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Define the final reshape and build the model\noutput = Reshape((grid_size[1], grid_size[0], num_anchors, 4 + 1 + classes),\n                 name=\"YOLO_output\")(model_keras.output)\nmodel_keras = Model(model_keras.input, output)\n\n# Create the mAP evaluator object\nnum_images = 100\n\nmap_evaluator = MapEvaluation(model_keras, val_data[:num_images], labels,\n                              anchors)\n\n# Compute the scores for all validation images\nstart = timer()\nmAP, average_precisions = map_evaluator.evaluate_map()\nend = timer()\n\nfor label, average_precision in average_precisions.items():\n    print(labels[label], '{:.4f}'.format(average_precision))\nprint('mAP: {:.4f}'.format(mAP))\nprint(f'Keras inference on {num_images} images took {end-start:.2f} s.\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Conversion to Akida\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 Convert to Akida model\nThe last YOLO_output layer that was added for splitting channels into values\nfor each box must be removed before Akida conversion.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Rebuild a model without the last layer\ncompatible_model = Model(model_keras.input, model_keras.layers[-2].output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When converting to an Akida model, we just need to pass the Keras model\nto [cnn2snn.convert](../../api_reference/cnn2snn_apis.html#convert).\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from cnn2snn import convert\n\nmodel_akida = convert(compatible_model)\nmodel_akida.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 Check performance\n\nAkida model accuracy is tested on the first *n* images of the validation set.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Create the mAP evaluator object\nmap_evaluator_ak = MapEvaluation(model_akida,\n                                 val_data[:num_images],\n                                 labels,\n                                 anchors,\n                                 is_keras_model=False)\n\n# Compute the scores for all validation images\nstart = timer()\nmAP_ak, average_precisions_ak = map_evaluator_ak.evaluate_map()\nend = timer()\n\nfor label, average_precision in average_precisions_ak.items():\n    print(labels[label], '{:.4f}'.format(average_precision))\nprint('mAP: {:.4f}'.format(mAP_ak))\nprint(f'Akida inference on {num_images} images took {end-start:.2f} s.\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Show predictions for a random image\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\nfrom akida_models.detection.processing import load_image, preprocess_image, decode_output\n\n# Take a random test image\ni = np.random.randint(len(val_data))\n\ninput_shape = model_akida.layers[0].input_dims\n\n# Load the image\nraw_image = load_image(val_data[i]['image_path'])\n\n# Keep the original image size for later bounding boxes rescaling\nraw_height, raw_width, _ = raw_image.shape\n\n# Pre-process the image\nimage = preprocess_image(raw_image, input_shape)\ninput_image = image[np.newaxis, :].astype(np.uint8)\n\n# Call evaluate on the image\npots = model_akida.predict(input_image)[0]\n\n# Reshape the potentials to prepare for decoding\nh, w, c = pots.shape\npots = pots.reshape((h, w, len(anchors), 4 + 1 + len(labels)))\n\n# Decode potentials into bounding boxes\nraw_boxes = decode_output(pots, anchors, len(labels))\n\n# Rescale boxes to the original image size\npred_boxes = np.array([[\n    box.x1 * raw_width, box.y1 * raw_height, box.x2 * raw_width,\n    box.y2 * raw_height,\n    box.get_label(),\n    box.get_score()\n] for box in raw_boxes])\n\nfig = plt.figure(num='VOC2012 car and person detection by Akida')\nax = fig.subplots(1)\nimg_plot = ax.imshow(np.zeros(raw_image.shape, dtype=np.uint8))\nimg_plot.set_data(raw_image)\n\nfor box in pred_boxes:\n    rect = patches.Rectangle((box[0], box[1]),\n                             box[2] - box[0],\n                             box[3] - box[1],\n                             linewidth=1,\n                             edgecolor='r',\n                             facecolor='none')\n    ax.add_patch(rect)\n    class_score = ax.text(box[0],\n                          box[1] - 5,\n                          f\"{labels[int(box[4])]} - {box[5]:.2f}\",\n                          color='red')\n\nplt.axis('off')\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}