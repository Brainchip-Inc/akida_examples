{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Upgrading to Akida 2.0\n\nThis tutorial targets Akida 1.0 users that are looking for advice on how to migrate their Akida 1.0\nmodel towards Akida 2.0. It also lists the major differences in model architecture compatibilities\nbetween 1.0 and 2.0.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Workflow differences\n\n.. figure:: ../../img/1.0vs2.0_flow.png\n   :target: ../../_images/1.0vs2.0_flow.png\n   :alt: 1.0 vs. 2.0 flow\n   :scale: 25 %\n   :align: center\n\n   Akida 1.0 and 2.0 workflows\n\nAs shown in the figure above, the main difference between 1.0 and 2.0 workflows is the\nquantization step that was based on CNN2SNN and that is now based on QuantizeML.\n\nProviding your model architecture is 2.0 compatible ([next section](plot_1_upgrading_to_2.0.html#models-architecture-differences)_ lists differences), upgrading to\n2.0 is limited to moving from a [cnn2snn.quantize](../../api_reference/cnn2snn_apis.html#cnn2snn.quantize)_ call to a [quantizeml.models.quantize](../../api_reference/quantizeml_apis.html#quantizeml.models.quantize)_ call. The code snippets\nbelow show the two different calls.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import keras\n\n# Build a simple model that is cross-compatible\ninput = keras.layers.Input((32, 32, 3))\nx = keras.layers.Conv2D(kernel_size=3, filters=32, strides=2, padding='same')(input)\nx = keras.layers.BatchNormalization()(x)\nx = keras.layers.ReLU(max_value=6.0)(x)\nx = keras.layers.Flatten()(x)\nx = keras.layers.Dense(units=10)(x)\n\nmodel = keras.Model(input, x)\nmodel.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import cnn2snn\n\n# Akida 1.0 flow\nquantized_model_1_0 = cnn2snn.quantize(model, input_weight_quantization=8, weight_quantization=4,\n                                       activ_quantization=4)\nakida_model_1_0 = cnn2snn.convert(quantized_model_1_0)\nakida_model_1_0.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import quantizeml\n\n# Akida 2.0 flow\nqparams = quantizeml.layers.QuantizationParams(input_weight_bits=8, weight_bits=4,\n                                               activation_bits=4)\nquantized_model_2_0 = quantizeml.models.quantize(model, qparams=qparams)\nakida_model_2_0 = cnn2snn.convert(quantized_model_2_0)\nakida_model_2_0.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>Here we use 8/4/4 quantization to match the CNN2SNN version above, but most users will\n          typically use the default 8-bit quantization that comes with QuantizeML.</p></div>\n\nQuantizeML quantization API is close to the legacy CNN2SNN quantization API and further details on\nhow to use it are given in the [global workflow tutorial](../general/plot_0_global_workflow.html)_ and the [advanced QuantizeML tutorial](plot_0_advanced_quantizeml.html)_.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Models architecture differences\n\n### 2.1. Separable convolution\n\nIn Akida 1.0, a Keras [SeparableConv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/SeparableConv2D)_ used to be\nquantized as a [QuantizedSeparableConv2D](../../api_reference/cnn2snn_apis.html#cnn2snn.QuantizedSeparableConv2D)_ and converted to an\nAkida [SeparableConvolutional](../../api_reference/akida_apis.html#akida.SeparableConvolutional)_ layer. These 3 layers each\nperform a \"fused\" operation where the depthwise and pointwise operations are grouped together in a\nsingle layer.\n\nIn Akida 2.0, the fused separable layer support has been dropped in favor of a more commonly used\nunfused operation where the depthwise and the pointwise operations are computed in independent\nlayers. The akida_models package offers a [separable_conv_block](../../api_reference/akida_models_apis.html#akida_models.layer_blocks.separable_conv_block)_\nwith a ``fused=False`` parameter that will create the [DepthwiseConv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/DepthwiseConv2D)_ and the pointwise\n[Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D)_ layers under the\nhood. This block will then be quantized towards a [QuantizedDepthwiseConv2D](../../api_reference/quantizeml_apis.html#quantizeml.layers.QuantizedDepthwiseConv2D)_ and a\npointwise [QuantizedConv2D](../../api_reference/quantizeml_apis.html#quantizeml.layers.QuantizedConv2D)_ before\nconversion into [DepthwiseConv2D](../../api_reference/akida_apis.html#akida.DepthwiseConv2D)_\nand pointwise [Conv2D](../../api_reference/akida_apis.html#akida.Conv2D)_ respectively.\n\nNote that while the resulting model topography is slightly different, the fused and unfused\nmathematical operations are strictly equivalent.\n\nIn order to ease 1.0 to 2.0 migration of existing models, akida_models offers an\n[unfuse_sepconv2d](../../api_reference/akida_models_apis.html#akida_models.unfuse_sepconv2d)_\nAPI that takes a model with fused layers and transforms it into an unfused equivalent version. For\nconvenience, an ``unfuse`` CLI action is also provided.\n\n```bash\nakida_models unfuse -m model.h5\n```\n### 2.2. Global average pooling operation\n\nThe supported position of the [GlobalAveragePooling2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling2D)_ operation\nhas changed in Akida 2.0 as it now must come after the ReLU activation (when there is one). In\nother words, in Akida 1.0 the layers were organized as follows:\n\n* ... > Neural layer > GlobalAveragePooling > (BatchNormalization) > ReLU >  Neural layer > ...\n\nIn Akida 2.0 the supported sequence of layer is:\n\n* ... > Neural layer > (BatchNormalization) > (ReLU) > GlobalAveragePooling >  Neural layer > ...\n\nThis can also be configured using the ``post_relu_gap`` parameter of akida_models [layer_blocks](../../api_reference/akida_models_apis.html#layer-blocks)_.\n\nTo migrate an existing model from 1.0 to 2.0, it is possible to load 1.0 weights into a 2.0\noriented architecture using [Keras save and load APIs](https://www.tensorflow.org/tutorials/keras/save_and_load)_ because the global average pooling\nposition does not have an effect on model weights. However, the sequences between 1.0 and 2.0 are\nnot mathematically equivalent so it might be required to tune or even retrain the model.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Using ``AkidaVersion``\n\nIt is still possible to build, quantize and convert models towards a 1.0 target using the\n[AkidaVersion API](../../api_reference/cnn2snn_apis.html#akida-version)_.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Reusing the previously defined 2.0 model but converting to a 1.0 target this time\nwith cnn2snn.set_akida_version(cnn2snn.AkidaVersion.v1):\n    akida_model = cnn2snn.convert(quantized_model_2_0)\nakida_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "One will notice the different Akida layers types as detailed in [Akida user guide](../../user_guide/akida.html#akida-layers)_.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}