{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Akida edge learning for keyword spotting\n\nThis tutorial demonstrates the Akida NSoC **edge learning** capabilities using\nits built-in learning algorithm.\n\nIt focuses on a keyword spotting (KWS) example, where an existing Akida network\nis re-trained to be able to classify new audio keywords.\n\nJust a few samples (few-shot learning) of the new words are sufficient to\naugment the Akida model with extra classes, while preserving high accuracy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Edge learning process\n\nBy \"edge learning\", we mean the process of network learning in an edge device.\nAside from technical requirements imposed by the device (low power, latency,\netc.), the task itself will often present particular challenges:\n\n1. The application cannot know which, or indeed, how many classes it will\n   be trained on ultimately, so it must be possible to **add new classes**\n   to the classifier online, i.e. requires **continual learning**.\n2. Often, there will be no large labelled dataset for new classes, which\n   must instead be learned from just a few samples, i.e. requires **few-shot\n   learning**.\n\nThe Akida NSoC has a built-in learning algorithm designed for training the\nlast layer of a model and well suited for edge learning.\nThe specific use case in this tutorial mimics the process of a mobile\nphone user who wants to add new speech commands, i.e. new keywords, to a\npre-trained voice recognition system with a few preset keywords.\nTo achieve this using the Akida NSoC, learning occurs in 3 stages:\n\n1. The Akida model preparation: an Akida model must meet specific conditions\n   to be compatible for [Akida learning](../../user_guide/akida.html#using-akida-edge-learning)_.\n2. The \"offline\" Akida learning: the last layer of the Akida model is trained\n   from scratch with a large dataset. In this KWS case, the model is trained\n   with 32 keywords from the Google \"Speech Commands dataset\".\n3. The \"online\" (edge learning) stage: new keywords are learned with few\n   samples, adding to the pre-trained words from stage 2.\n\n\n### 1.1 Akida model preparation\n\nThe Akida NSoC embeds a native learning algorithm allowing training of the\nlast layer of an Akida model. The overall model can then be seen as the\ncombination of two parts:\n\n- a feature extractor (or spike generator) corresponding to all but the last\n  layer of a standard (back-propagation trained) neural network. This part of\n  the model cannot be trained on the Akida NSoC, and would typically be\n  prepared in advance, e.g. using the CNN2SNN conversion tool. Also, to be\n  compatible with Akida learning, the feature extractor must return binary\n  spikes (1-bit activations).\n- the classification layer (i.e. the last layer). It must have 1-bit weights\n  and usually has several output neurons per class. This layer will be the\n  only one trained using the built-in learning algorithm.\n\nNote that, unlike a standard CNN network where each class is represented by\na single output neuron, an Akida native training requires several neurons\nfor each class. The number of neurons per class can be seen as the number\nof centroids to represent a class; there is an analogy with k-means clustering\napplied to one-class samples, k being the number of neurons. The choice of the\nnumber of neurons is a trade-off: too many neurons per class may be\ncomputationally inefficient; in contrast too few neurons per class may have\ndifficulty representing within-class heterogeneity. Like k-means\nclustering, the choice of k depends on the cluster representation of the data.\n\nLike any training process, hyper-parameters must be set appropriately.\nThe only mandatory parameter is the number of weights (i.e. number of\nconnections for each neuron) which must be correlated to the number of spikes\nat the end of the feature extractor. Other parameters, such as\n``min_plasticity`` or ``learning_competition``, are optional and mainly used\nfor model fine-tuning: one can set them to default for a first try.\n\n\n### 1.2 \"Offline\" Akida learning\n\nThe model is ready for training. Remember that the feature extractor has\nalready been trained in stage 1. Here, only the last Akida layer is\ntrainable. Training is still \"offline\" though, corresponding to the\npreparation of the network with the \"preset\" command keywords. The last layer\nis trained from scratch: its binary weights are randomly initialized.\n\nA large dataset is passed through the Akida network and the on-chip learning\nalgorithm updates the weights of the classification layer accordingly.\nIn this KWS example, we take a dataset containing 32 words + a \"silence\"\nclass (33 classes) for a total of about 94,000 samples.\n\nNote that the dataset on which the feature extractor was trained does not need\nto be the same as the one used for \"offline\" training of the classification\nlayer. What is important is that the features extracted are as good as\npossible for the expected inputs. Since the \"edge\" classes are, by\ndefinition, not known in advance, in practice that typically means making\nyour feature extractor as general as possible.\n\n\n### 1.3 \"Online\" edge learning\n\n\"Online\" edge learning consists in adding and learning new classes to a\nformer pre-trained model. This stage is meant to be performed on a chip with\nfew examples for each new class.\n\nIn practice, edge learning with Akida is similar to \"offline\" learning,\nexcept that:\n\n- the network has already been trained on a set of classes which need to be\n  kept, and so the novel classes are in addition to those.\n- only few samples are available for training.\n\nIn this KWS example, 3 new keywords are learned using 4 samples per word from\na single user. Applying data augmentation on these samples adds variability\nto improve generalization. After edge learning, the model is able to classify\nthe 3 new classes with similar accuracy to the 33 existing classes (and\nperformance on the existing classes is unaffected).\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Dataset preparation\n\nThe data comes from the Google \"Speech Commands\" dataset containing audio\nfiles for 35 keywords. The number of utterances for each word varies from\n1,500 to 4,000.\n32 words are used to train the Akida model and 3 new words are added for\nedge learning.\n\nTwo datasets are loaded:\n\n- The first dataset contains all samples of the 32 following keywords\n  extended with the \"silence\" samples (see the\n  [original paper](https://arxiv.org/pdf/1804.03209.pdf)_ for details on\n  the dataset). In total, 94,252 samples are used. These are split into a\n  training set (90%) and a validation set (10%), used to train the model\n  \"offline\" (stage 2).\n- The second dataset contains samples of the 3 new keywords from a single\n  speaker: 'backward', 'follow' and 'forward'. Since the aim of edge learning\n  is to train with few samples, only 4 utterances will be used for\n  training and the rest for testing (ideally, one would test with many more\n  samples, but the number of repetitions per individual speaker in the\n  database makes this impossible). Data augmentation is applied with time\n  shift and additional background noise, generating 40 training samples per\n  utterances, therefore 4 x 40 = 160 training samples per new word.\n\nThe audio files are pre-processed: the mel-frequency cepstral coefficients\n(MFCC) are computed as features to represent each audio sample. The obtained\nfeatures for one sample are stored in an array of shape (49, 10, 1). This\narray of features is chosen as input in the Akida network.\n\nFor the sake of simplicity, the pre-processing step is not detailed here;\nthis tutorial directly fetches the pre-processed audio data for both datasets.\nThe pre-processed utility methods to generate these MFCC data are available in\nthe ``akida_models`` package.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from akida import FullyConnected, evaluate_sparsity, Model\n\nimport pickle\n\nfrom akida_models import fetch_file\n\n# Fetch pre-processed data for 32 keywords\nfname = fetch_file(\n    fname='kws_preprocessed_all_words_except_backward_follow_forward.pkl',\n    origin=\"https://data.brainchip.com/dataset-mirror/kws/kws_preprocessed_all_words_except_backward_follow_forward.pkl\",\n    cache_subdir='datasets/kws')\nwith open(fname, 'rb') as f:\n    [x_train, y_train, x_val, y_val, _, _, word_to_index,\n     data_transform] = pickle.load(f)\n\n# Fetch pre-processed data for the 3 new keywords\nfname2 = fetch_file(\n    fname='kws_preprocessed_edge_backward_follow_forward.pkl',\n    origin=\"https://data.brainchip.com/dataset-mirror/kws/kws_preprocessed_edge_backward_follow_forward.pkl\",\n    cache_subdir='datasets/kws')\nwith open(fname2, 'rb') as f:\n    [\n        x_train_new, y_train_new, x_val_new, y_val_new, files_train, files_val,\n        word_to_index_new, dt2\n    ] = pickle.load(f)\n\nprint(\"Wanted words and labels:\\n\", word_to_index)\nprint(\"New words:\\n\", word_to_index_new)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Prepare Akida model for learning\n\nAs explained above, to be compatible with Akida:\n\n- the feature extractor must return **binary spikes**.\n- the classification layer must have **binary weights**.\n\nFor this example, we load a pre-trained model from which we keep the feature\nextractor, returning binary spikes. This model was previously trained and\nquantized with Keras and the CNN2SNN tool. The first dataset with 33 classes\n(32 keywords + \"silence\") was used for training.\n\nHowever, the last layer of this pre-trained model is not compatible for Akida\nlearning since it doesn't have binary weights. We then remove this last layer\nand add a new classification layer with 33 classes and\n15 neurons per class. One can try with different values of neurons per\nclass, e.g. from 1 to 500 neurons per class, and see the effects on\nperformance and time cost.\n\nMoreover, as for any training algorithm, the learning hyper-parameters have to\nbe correctly set. For the Akida learning algorithm, one important parameter\nis the **number of weights**: because of the way the Akida learning algorithm\nworks, the number of spikes at the end of the feature extractor provides a\ngood starting point for this hyper-parameter. Here, we estimate this number\nof output spikes using 10% of the training set, which is enough to have a\nreasonable estimation.\n\n.. Note:: Edge learning is only supported for Akida 1.0 models for now.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from akida_models import ds_cnn_kws_pretrained\nfrom cnn2snn import set_akida_version, AkidaVersion\n\n# Instantiate a quantized model with pretrained quantized weights\nwith set_akida_version(AkidaVersion.v1):\n    model = ds_cnn_kws_pretrained()\nmodel.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nfrom math import ceil\n\nfrom cnn2snn import convert\n\n#  Convert to an Akida model\nmodel_ak = convert(model)\nmodel_ak.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Measure Akida accuracy on validation set\nbatch_size = 1000\npreds_ak = np.zeros(y_val.shape[0])\nnum_batches_val = ceil(x_val.shape[0] / batch_size)\nfor i in range(num_batches_val):\n    s = slice(i * batch_size, (i + 1) * batch_size)\n    preds_ak[s] = model_ak.predict_classes(x_val[s])\n\nacc_val_ak = np.sum(preds_ak == y_val) / y_val.shape[0]\nprint(f\"Akida CNN2SNN validation set accuracy: {100 * acc_val_ak:.2f} %\")\n\n# For non-regression purposes\nassert acc_val_ak > 0.88"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Replace the last layer by a classification layer with binary weights\n# Here, we choose to set 15 neurons per class.\nnum_classes = 33\nnum_neurons_per_class = 15\n\nmodel_ak.pop_layer()\nlayer_fc = FullyConnected(name='akida_edge_layer',\n                          units=num_classes * num_neurons_per_class,\n                          activation=False)\nmodel_ak.add(layer_fc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Compute sparsity information for the model using 10% of the training data\n# which is enough for a good estimate\nnum_samples = ceil(0.1 * x_train.shape[0])\nsparsities = evaluate_sparsity(model_ak, x_train[:num_samples])\n\n# Retrieve the number of output spikes from the feature extractor output\noutput_density = 1 - sparsities[model_ak.get_layer('separable_4')]\navg_spikes = model_ak.get_layer('separable_4').output_dims[-1] * output_density\nprint(f\"Average number of spikes: {avg_spikes}\")\n\n# Fix the number of weights to 1.2 times the average number of output spikes\nnum_weights = int(1.2 * avg_spikes)\nprint(\"The number of weights is then set to:\", num_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Learn with Akida using the training set\n\nThis stage shows how to train the Akida model using the built-in learning\nalgorithm in an \"offline\" stage, i.e. training the classification layer\nfrom scratch using a large training set.\nThe dataset containing the 33 classes (32 keywords + \"silence\") is used.\n\nNow that the Akida model is ready for training, the hyper-parameters\nmust be set using the [compile](../../api_reference/akida_apis.html#akida.Model.compile)_\nmethod of the last layer. Compiling a layer means that this layer is\nconfigured for training and ready to be trained. For more information about\nthe learning hyper-parameters, check the [user guide](../../user_guide/akida.html#compiling-a-layer)_.\nNote that we set the `learning_competition` to 0.1, which gives a little\ncompetition between neurons to prevent learning similar features.\n\nOnce the last layer is compiled, the\n[fit](../../api_reference/akida_apis.html#akida.Model.fit) method is used to\npass the dataset for training. This call is similar to the `fit` method in\ntf.keras.\n\nAfter training, the model is assessed on the validation set using the\n[predict](../../api_reference/akida_apis.html#akida.Model.predict) method. It\nreturns the estimated labels for the validation samples.\nThe model is then saved to a ``.fbz`` file.\n\nNote that in this specific case, the same dataset was used to train the\nfeature extractor using the CNN2SNN tool in an early stage, and to train this\nclassification layer using the native learning algorithm. However, the edge\nlearning in the next stage passes completely new data in the network.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Compile Akida model with learning parameters\nfrom akida import AkidaUnsupervised\nmodel_ak.compile(optimizer=AkidaUnsupervised(num_weights=num_weights,\n                                             num_classes=num_classes,\n                                             learning_competition=0.1))\nmodel_ak.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from time import time\n\n# Train the last layer using Akida `fit` method\nprint(f\"Akida learning with {num_classes} classes... \\\n        (this step can take a few minutes)\")\nnum_batches = ceil(x_train.shape[0] / batch_size)\nstart = time()\nfor i in range(num_batches):\n    s = slice(i * batch_size, (i + 1) * batch_size)\n    model_ak.fit(x_train[s], y_train[s].astype(np.int32))\nend = time()\n\nprint(f\"Elapsed time for Akida training: {end-start:.2f} s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Measure Akida accuracy on validation set\npreds_val_ak = np.zeros(y_val.shape[0])\nfor i in range(num_batches_val):\n    s = slice(i * batch_size, (i + 1) * batch_size)\n    preds_val_ak[s] = model_ak.predict_classes(x_val[s],\n                                               num_classes=num_classes)\n\nacc_val_ak = np.sum(preds_val_ak == y_val) / y_val.shape[0]\nprint(f\"Akida validation set accuracy: {100 * acc_val_ak:.2f} %\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\n\nfrom tempfile import TemporaryDirectory\n\n# Save Akida model\ntemp_dir = TemporaryDirectory(prefix='edge_learning_kws')\nmodel_file = os.path.join(temp_dir.name, 'ds_cnn_edge_kws.fbz')\nmodel_ak.save(model_file)\ndel model_ak"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Edge learning\n\nAfter the \"offline\" training stage, we emulate the use case where the\npre-trained Akida model is loaded on an Akida chip, ready to learn new\nclasses. Our previously saved Akida model has 33 output classes with learned\nweights.\nWe now add 3 classes to the existing model using the\n[add_classes](../../api_reference/akida_apis.html#akida.Model.add_classes) method\nand learn the 3 new keywords without changing the already learned weights.\n\nThere is no need to compile the final layer again; the new neurons were\ninitialized along with the old ones, based on the learning hyper-parameters\ngiven in the [compile](../../api_reference/akida_apis.html#akida.Model.compile)\ncall. The edge learning then uses the same scheme as for the \"offline\" Akida\nlearning - only the number of samples used is much more restricted.\n\nHere, each new class is trained using 160 samples, stored in the second\ndataset: 4 utterances per word from a single speaker, augmented 40 times each.\nThe validation set for new words ['backward', 'follow', 'forward'] contains\nrespectively 6, 7 and 6 utterances.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"Validation set of new words ({y_val_new.shape[0]} samples):\")\nfor word, label in word_to_index_new.items():\n    print(f\" - {word} (label {label}): {np.sum(y_val_new == label)} samples\")\n\n# Update new labels following the numbering of the old keywords, i.e, new word\n# with label '0' becomes label '34', new word label '1' becomes '35', etc.\ny_train_new += num_classes\ny_val_new += num_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Load the pre-trained model (no need to compile it again)\nmodel_edge = Model(model_file)\nmodel_edge.add_classes(3)\n\n# Train the Akida model with new keywords; only few samples are used.\nprint(\"\\nEdge learning with 3 new classes ...\")\nstart = time()\nmodel_edge.fit(x_train_new, y_train_new.astype(np.int32))\nend = time()\nprint(f\"Elapsed time for Akida edge learning: {end-start:.2f} s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Predict on the new validation set\npreds_ak_new = model_edge.predict_classes(x_val_new,\n                                          num_classes=num_classes + 3)\ngood_preds_val_new_ak = np.sum(preds_ak_new == y_val_new)\nprint(f\"Akida validation set accuracy on 3 new keywords: \\\n        {good_preds_val_new_ak}/{y_val_new.shape[0]}\")\n\n# Predict on the old validation set. Edge learning of the 3 new keywords barely\n# affects the accuracy of the old classes.\npreds_ak_old = np.zeros(y_val.shape[0])\nfor i in range(num_batches_val):\n    s = slice(i * batch_size, (i + 1) * batch_size)\n    preds_ak_old[s] = model_edge.predict_classes(x_val[s],\n                                                 num_classes=num_classes + 3)\n\nacc_val_old_ak = np.sum(preds_ak_old == y_val) / y_val.shape[0]\nprint(f\"Akida validation set accuracy on 33 old classes: \\\n        {100 * acc_val_old_ak:.2f} %\")\n\n# For non-regression purposes\nassert acc_val_old_ak > 0.82"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}