{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# DS-CNN/KWS inference\n\nThis tutorial illustrates the process of developing an Akida-compatible speech recognition\nmodel that can identify thirty-two different keywords.\n\nInitially, the model is defined as a CNN in Keras and trained regularly. Next, it undergoes\nquantization using [QuantizeML](../../user_guide/quantizeml.html)_ and finally converted\nto Akida using [CNN2SNN](../../user_guide/cnn2snn.html)_.\n\nThis example uses a Keyword Spotting Dataset prepared using **TensorFlow** [audio recognition\nexample](https://www.tensorflow.org/tutorials/audio/simple_audio)_ utils.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load the preprocessed dataset\n\nThe TensorFlow [speech_commands](https://www.tensorflow.org/datasets/catalog/speech_commands)_\ndataset is used for training and validation. All keywords except \"backward\",\n\"follow\" and \"forward\", are retrieved. These three words are kept to\nillustrate the edge learning in this\n[edge example](../edge/plot_1_edge_learning_kws.html)_.\n\nThe words to recognize have been converted to [spectrogram images](https://github.com/tensorflow/docs/blob/master/site/en/r1/tutorials/sequences/audio_recognition.md#how-does-this-model-work)_\nthat allows us to use a model architecture that is typically used for image recognition tasks.\nThe raw audio data have been preprocessed, transforming the audio files into MFCC features,\nwell-suited for CNN networks.\nA pickle file containing the preprocessed data is available on Brainchip data server.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pickle\n\nfrom tensorflow.keras.utils import get_file\n\n# Fetch pre-processed data for 32 keywords\nfname = get_file(\n    fname='kws_preprocessed_all_words_except_backward_follow_forward.pkl',\n    origin=\"https://data.brainchip.com/dataset-mirror/kws/kws_preprocessed_all_words_except_backward_follow_forward.pkl\",\n    cache_subdir='datasets/kws')\nwith open(fname, 'rb') as f:\n    [_, _, x_valid, y_valid, _, _, word_to_index, _] = pickle.load(f)\n\n# Preprocessed dataset parameters\nnum_classes = len(word_to_index)\n\nprint(\"Wanted words and labels:\\n\", word_to_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load a pre-trained native Keras model\n\nThe model consists of:\n\n* a first convolutional layer accepting dense inputs (images),\n* several separable convolutional layers preserving spatial dimensions,\n* a global pooling reducing the spatial dimensions to a single pixel,\n* a final dense layer to classify words.\n\nAll layers are followed by a batch normalization and a ReLU activation.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n\n# Retrieve the model file from the BrainChip data server\nmodel_file = get_file(\"ds_cnn_kws.h5\",\n                      \"https://data.brainchip.com/models/AkidaV2/ds_cnn/ds_cnn_kws.h5\",\n                      cache_subdir='models')\n\n# Load the native Keras pre-trained model\nmodel_keras = load_model(model_file)\nmodel_keras.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nfrom sklearn.metrics import accuracy_score\n\n# Check Keras Model performance\npotentials_keras = model_keras.predict(x_valid)\npreds_keras = np.squeeze(np.argmax(potentials_keras, 1))\n\naccuracy = accuracy_score(y_valid, preds_keras)\nprint(\"Accuracy: \" + \"{0:.2f}\".format(100 * accuracy) + \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load a pre-trained quantized Keras model\n\nThe above native Keras model has been quantized to 8-bit. Note that\na 4-bit version is also available from the [model zoo](../../model_zoo_performance.html#id10).\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from quantizeml.models import load_model\n\n# Load the pre-trained quantized model\nmodel_file = get_file(\n    \"ds_cnn_kws_i8_w8_a8.h5\",\n    \"https://data.brainchip.com/models/AkidaV2/ds_cnn/ds_cnn_kws_i8_w8_a8.h5\",\n    cache_subdir='models')\nmodel_keras_quantized = load_model(model_file)\nmodel_keras_quantized.summary()\n\n# Check Model performance\npotentials_keras_q = model_keras_quantized.predict(x_valid)\npreds_keras_q = np.squeeze(np.argmax(potentials_keras_q, 1))\n\naccuracy_q = accuracy_score(y_valid, preds_keras_q)\nprint(\"Accuracy: \" + \"{0:.2f}\".format(100 * accuracy_q) + \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Conversion to Akida\n\nThe converted model is Akida 2.0 compatible and its performance\nevaluation is done using the Akida simulator.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from cnn2snn import convert\n\n# Convert the model\nmodel_akida = convert(model_keras_quantized)\nmodel_akida.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Check Akida model performance\npreds_akida = model_akida.predict_classes(x_valid, num_classes=num_classes)\n\naccuracy = accuracy_score(y_valid, preds_akida)\nprint(\"Accuracy: \" + \"{0:.2f}\".format(100 * accuracy) + \"%\")\n\n# For non-regression purposes\nassert accuracy > 0.9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Confusion matrix\n\nThe confusion matrix provides a good summary of what mistakes the\nnetwork is making.\n\nPer scikit-learn convention it displays the true class in each row (ie\non each row you can see what the network predicted for the corresponding\nword).\n\nPlease refer to the Tensorflow [audio\nrecognition](https://github.com/tensorflow/docs/blob/master/site/en/r1/tutorials/sequences/audio_recognition.md#confusion-matrix)_\nexample for a detailed explanation of the confusion matrix.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import itertools\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import confusion_matrix\n\n# Create confusion matrix\ncm = confusion_matrix(y_valid, preds_akida,\n                      labels=list(word_to_index.values()))\n\n# Normalize\ncm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n# Display confusion matrix\nplt.rcParams[\"figure.figsize\"] = (16, 16)\nplt.figure()\n\ntitle = 'Confusion matrix'\ncmap = plt.cm.Blues\n\nplt.imshow(cm, interpolation='nearest', cmap=cmap)\nplt.title(title)\nplt.colorbar()\ntick_marks = np.arange(len(word_to_index))\nplt.xticks(tick_marks, word_to_index, rotation=45)\nplt.yticks(tick_marks, word_to_index)\n\nthresh = cm.max() / 2.\nfor i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n    plt.text(j,\n             i,\n             format(cm[i, j], '.2f'),\n             horizontalalignment=\"center\",\n             color=\"white\" if cm[i, j] > thresh else \"black\")\n\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.autoscale()\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}