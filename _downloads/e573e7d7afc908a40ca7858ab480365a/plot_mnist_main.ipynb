{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nLearning and inference on MNIST\n===============================\n\nThe MNIST dataset is a handwritten digits database. It has a training\nset of 60,000 images, and a test set of 10,000 images. An image has\n28x28 pixels (784 features) and an associated label.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Loading the MNIST dataset\n----------------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Various imports needed for the tutorial\nimport os\nimport numpy as np\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.datasets import mnist\nfrom sklearn.metrics import accuracy_score\nimport warnings\n\n# Akida specific imports\nfrom akida import Model, InputBCSpike, FullyConnected, LearningType"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "#  Retrieve MNIST dataset\n(train_set, train_label), (test_set, test_label) = mnist.load_data()\n# add a dimension to images sets as akida expects 4 dimensions inputs\ntrain_set = np.expand_dims(train_set, -1)\ntest_set = np.expand_dims(test_set, -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Look at some images from the dataset\n---------------------------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Display a few images from the train set\nf, axarr = plt.subplots(1, 4)\nfor i in range (0, 4):\n    axarr[i].imshow(train_set[i].reshape((28,28)), cmap=cm.Greys_r)\n    axarr[i].set_title('Class %d' % train_label[i])\nplt.show()\nprint ('Note that an MNIST image is a %s '  % (train_set[0].shape,) + \"numpy array (filled with 8 bit values, i.e. grayscale)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Configuring Akida model\n--------------------------\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A neural network model can be sequentially defined. Check the `Akida\nExecution Engine documentation <../../api_reference/aee_apis.html>`__ for a\nfull description of the parameters and layer types available.\nNote that first layer matches MNIST image properties (InputBCSpike with\ninput_width: 28 and input_height: 28)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "#Create a model\nmodel = Model()\nmodel.add(InputBCSpike(\"inputBC\", input_width=28, input_height=28))\nfully = FullyConnected(\"fully\", num_neurons=1000, activations_enabled=False)\nmodel.add(fully)\n# Configure the last layer for semi-supervised training\nfully.compile(num_weights=500, num_classes=10)\nmodel.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. Testing performance\n----------------------\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Akida Execution Engine provides a simple performance routine. We can\ntry a test of baseline performance without any training:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Dumb try with an untrained model ...\nnum_samples = 10000\n\nstats = model.get_statistics()\npred_label = model.predict(test_set[:int(num_samples)], 10)\naccuracy = accuracy_score(test_label[:num_samples], pred_label[:num_samples])\n\nprint(\"Accuracy: \"+\"{0:.2f}\".format(100*accuracy)+\"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Print model statistics\nprint(\"Model statistics\")\nfor _, stat in stats.items():\n    print(stat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5. Learning and inference\n-------------------------\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's train the model on the MNIST training dataset (60 000 images\navailable). Out of interest, we can run a performance test at regular\nintervals, to see how it evolves as a function of the number of training\nsamples (and remember that the end of the training run here corresponds\nto just one training 'epoch').\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Routine to update intermediate results plot\ndef plot_update(performance, histogram, axes, figure, nb_samples):\n\n    # Update performance subplot\n    axes[0].plot(nb_samples, 100*performance, 'b.')\n    axes[0].set(xlabel='\\nTraining samples: '+str(nb_samples),\n                ylabel='Accuracy: '+'{0:.2f}'.format(100*performance))\n    # Update learning rate subplot\n    x_hist = [x[0] for x in histogram]\n    y_hist = [x[1] for x in histogram]\n    axes[1].plot(x_hist, y_hist, dash_joinstyle='round')\n\n    figure.canvas.draw()\n    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Check model performance every 'checkpoint' samples\nin_images = train_set\ncheckpoints = [5, 10, 20, 50, 100, 200, 500, 1000, 2000, 3000, 5000, 7000, 10000]\ncounter = 0\n\n# Change Matplotlib backend for dynamic display\n\n# Adjust Plot parameters\nplt.rcParams['axes.grid'] = True\nfig, ax = plt.subplots(1, 2)\nplt.subplots_adjust(wspace = 0.5, right=0.9)\n\n# Set axis limit values and labels\nfor n, subplot in np.ndenumerate(ax):\n    subplot.set_xlim(0, 100)\n    subplot.set_ylim(0, 100)\nax[0].set_xlim(0, checkpoints[-1])\nax[0].set(xlabel='\\nTraining samples: ', ylabel='Accuracy', aspect=checkpoints[-1]/100)\nax[0].tick_params(bottom=False, labelbottom=False)\nax[1].set(xlabel='Neuron learning rate', ylabel='Number of neurons', aspect=100/100)\n\nfig.canvas.draw()\n\n# Get a reference to the layer for data extraction\nfully = model.get_layer('fully')\n\n# Start learning and plot performances all along\nfor i in range(in_images.shape[0]):\n    model.fit(in_images[i:i+1], input_labels=train_label[i])\n    # Plot intermediate accuracy and learning rate for the defined checkpoints\n    if counter < len(checkpoints) and i == (checkpoints[counter]):\n        pred_label = model.predict(test_set[:int(num_samples)], 10)\n        accuracy = accuracy_score(test_label[:num_samples], pred_label[:num_samples])\n        hist       = fully.get_learning_histogram()\n        plot_update(accuracy, hist, ax, fig, i)\n        counter+=1\n\n    # Then plot learning rate every 10000 samples\n    elif (i+1)%10000 == 0:\n        pred_label = model.predict(test_set[:int(num_samples)], 10)\n        accuracy = accuracy_score(test_label[:num_samples], pred_label[:num_samples])\n        hist       = fully.get_learning_histogram()\n        plot_update(accuracy, hist, ax, fig, i+1)\n\nprint(\"Accuracy: \"+\"{0:.2f}\".format(100*accuracy)+\"%\")\n\n# For non-regression purpose\nassert accuracy > 0.93"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}