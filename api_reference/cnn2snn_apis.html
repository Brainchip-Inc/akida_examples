

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>CNN2SNN Toolkit API &mdash; Akida Examples  documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery-rendered-html.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Akida models API" href="akida_models_apis.html" />
    <link rel="prev" title="Akida Execution Engine API" href="aee_apis.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #78b3ff" >
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/akida.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                MetaTF 1.9.4
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../index.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../installation.html#requirements">Requirements</a></li>
<li class="toctree-l2"><a class="reference internal" href="../installation.html#quick-installation">Quick installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../installation.html#running-examples">Running examples</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../user_guide/user_guide.html">User guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../user_guide/getting_started.html">Getting started</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/getting_started.html#for-beginners">For beginners</a></li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/getting_started.html#for-users-familiar-with-deep-learning">For users familiar with deep-learning</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../user_guide/aee.html">Akida user guide</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/aee.html#the-akida-execution-engine">The Akida Execution Engine</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#id1">1. The Spiking Neural Network model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#id2">2. Input data format</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#id3">3. Determine training mode</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#id4">4. Interpreting outputs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/aee.html#neural-network-model">Neural Network model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#specifying-the-neural-network-model">Specifying the Neural Network model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#saving-and-loading">Saving and loading</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#input-layer-types">Input layer types</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#data-processing-layer-types">Data-Processing layer types</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/aee.html#id5">Using Akida Unsupervised Learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#learning-constraints">Learning constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#compiling-a-layer">Compiling a layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#id8">Learning parameters</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../user_guide/cnn2snn.html">CNN2SNN toolkit</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/cnn2snn.html#overview">Overview</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#conversion-workflow">Conversion workflow</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#typical-training-scenario">Typical training scenario</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#design-compatibility-constraints">Design compatibility constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#quantization-compatibility-constraints">Quantization compatibility constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#command-line-interface">Command-line interface</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/cnn2snn.html#layers-considerations">Layers Considerations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#supported-layer-types">Supported layer types</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#cnn2snn-quantization-aware-layers">CNN2SNN Quantization-aware layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#training-only-layers">Training-Only Layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#first-layers">First Layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#id6">Final Layers</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/cnn2snn.html#tips-and-tricks">Tips and Tricks</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../user_guide/akida_models.html">Akida models zoo</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/akida_models.html#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/akida_models.html#command-line-interface-for-model-creation">Command-line interface for model creation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/akida_models.html#command-line-interface-for-model-training">Command-line interface for model training</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida_models.html#cifar10-training-and-tuning">CIFAR10 training and tuning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida_models.html#utk-face-training">UTK Face training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida_models.html#kws-training">KWS training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida_models.html#yolo-training">YOLO training</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/akida_models.html#id1">Layer Blocks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida_models.html#conv-block"><code class="docutils literal notranslate"><span class="pre">conv_block</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida_models.html#dense-block"><code class="docutils literal notranslate"><span class="pre">dense_block</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida_models.html#separable-conv-block"><code class="docutils literal notranslate"><span class="pre">separable_conv_block</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../user_guide/hw_constraints.html">Hardware constraints</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/hw_constraints.html#akida-nsoc-pre-production">Akida NSoC (Pre-production)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/hw_constraints.html#inputconvolutional">InputConvolutional</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/hw_constraints.html#convolutional">Convolutional</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/hw_constraints.html#separableconvolutional">SeparableConvolutional</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/hw_constraints.html#fullyconnected">FullyConnected</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/hw_constraints.html#akida-nsoc-production">Akida NSoC (Production)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/hw_constraints.html#id1">InputConvolutional</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/hw_constraints.html#id2">Convolutional</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/hw_constraints.html#id3">SeparableConvolutional</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/hw_constraints.html#id4">FullyConnected</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../user_guide/compatibility.html">Akida versions compatibility</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/compatibility.html#upgrading-models-with-legacy-quantizers">Upgrading models with legacy quantizers</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="api_reference.html">API reference</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="aee_apis.html">Akida Execution Engine</a><ul>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#model">Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#layer">Layer</a></li>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#layerstatistics">LayerStatistics</a></li>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#inputdata">InputData</a></li>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#inputconvolutional">InputConvolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#fullyconnected">FullyConnected</a></li>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#convolutional">Convolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#separableconvolutional">SeparableConvolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#concat">Concat</a></li>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#dense">Dense</a></li>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#backendtype">BackendType</a></li>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#convolutionmode">ConvolutionMode</a></li>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#poolingtype">PoolingType</a></li>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#learningtype">LearningType</a></li>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#hwversion">HwVersion</a></li>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#compatibility">Compatibility</a></li>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#device">Device</a></li>
</ul>
</li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">CNN2SNN</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#tool-functions">Tool functions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#quantize">quantize</a></li>
<li class="toctree-l4"><a class="reference internal" href="#quantize-layer">quantize_layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="#convert">convert</a></li>
<li class="toctree-l4"><a class="reference internal" href="#check-model-compatibility">check_model_compatibility</a></li>
<li class="toctree-l4"><a class="reference internal" href="#load-quantized-model">load_quantized_model</a></li>
<li class="toctree-l4"><a class="reference internal" href="#load-partial-weights">load_partial_weights</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#quantizers">Quantizers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#weightquantizer">WeightQuantizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="#linearweightquantizer">LinearWeightQuantizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="#stdweightquantizer">StdWeightQuantizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="#trainablestdweightquantizer">TrainableStdWeightQuantizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="#maxquantizer">MaxQuantizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="#maxperaxisquantizer">MaxPerAxisQuantizer</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#quantized-layers">Quantized layers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#quantizedconv2d">QuantizedConv2D</a></li>
<li class="toctree-l4"><a class="reference internal" href="#quantizeddepthwiseconv2d">QuantizedDepthwiseConv2D</a></li>
<li class="toctree-l4"><a class="reference internal" href="#quantizeddense">QuantizedDense</a></li>
<li class="toctree-l4"><a class="reference internal" href="#quantizedseparableconv2d">QuantizedSeparableConv2D</a></li>
<li class="toctree-l4"><a class="reference internal" href="#quantizedactivation">QuantizedActivation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#activationdiscreterelu">ActivationDiscreteRelu</a></li>
<li class="toctree-l4"><a class="reference internal" href="#quantizedrelu">QuantizedReLU</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="akida_models_apis.html">Akida models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="akida_models_apis.html#layer-blocks">Layer blocks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="akida_models_apis.html#conv-block">conv_block</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida_models_apis.html#separable-conv-block">separable_conv_block</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida_models_apis.html#dense-block">dense_block</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="akida_models_apis.html#helpers">Helpers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="akida_models_apis.html#batchnormalization-gamma-constraint">BatchNormalization gamma constraint</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="akida_models_apis.html#model-zoo">Model zoo</a><ul>
<li class="toctree-l4"><a class="reference internal" href="akida_models_apis.html#mobilenet">Mobilenet</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida_models_apis.html#ds-cnn">DS-CNN</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida_models_apis.html#vgg">VGG</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida_models_apis.html#yolo">YOLO</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../examples/index.html">Examples</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../examples/index.html#general-examples">General examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_0_gxnor_mnist.html">GXNOR/MNIST inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_0_gxnor_mnist.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_0_gxnor_mnist.html#load-the-pre-trained-akida-model">2. Load the pre-trained Akida model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_0_gxnor_mnist.html#show-predictions-for-a-single-image">3. Show predictions for a single image</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_0_gxnor_mnist.html#check-performance">4. Check performance</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_1_ds_cnn_cifar10.html">DS-CNN CIFAR10 inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_1_ds_cnn_cifar10.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_1_ds_cnn_cifar10.html#create-a-keras-ds-cnn-model">2. Create a Keras DS-CNN model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_1_ds_cnn_cifar10.html#quantized-model">3. Quantized model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_1_ds_cnn_cifar10.html#pretrained-quantized-model">4. Pretrained quantized model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_1_ds_cnn_cifar10.html#conversion-to-akida">5. Conversion to Akida</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_2_mobilenet_imagenet.html">MobileNet/ImageNet inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_2_mobilenet_imagenet.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_2_mobilenet_imagenet.html#create-a-keras-mobilenet-model">2. Create a Keras MobileNet model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_2_mobilenet_imagenet.html#quantized-model">3. Quantized model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_2_mobilenet_imagenet.html#pretrained-quantized-model">4. Pretrained quantized model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_2_mobilenet_imagenet.html#conversion-to-akida">5. Conversion to Akida</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_3_ds_cnn_kws.html">DS-CNN/KWS inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_3_ds_cnn_kws.html#load-the-preprocessed-dataset">1. Load the preprocessed dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_3_ds_cnn_kws.html#load-a-pre-trained-native-keras-model">2. Load a pre-trained native Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_3_ds_cnn_kws.html#load-a-pre-trained-quantized-keras-model-satisfying-akida-nsoc-requirements">3. Load a pre-trained quantized Keras model satisfying Akida NSoC requirements</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_3_ds_cnn_kws.html#conversion-to-akida">4. Conversion to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_3_ds_cnn_kws.html#confusion-matrix">5. Confusion matrix</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_4_regression.html">Regression tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_regression.html#load-the-dataset">1. Load the dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_regression.html#load-a-pre-trained-native-keras-model">2. Load a pre-trained native Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_regression.html#load-a-pre-trained-quantized-keras-model-satisfying-akida-nsoc-requirements">3. Load a pre-trained quantized Keras model satisfying Akida NSoC requirements</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_regression.html#conversion-to-akida">4. Conversion to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_regression.html#estimate-age-on-a-single-image">5. Estimate age on a single image</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_5_transfer_learning.html">Transfer learning with MobileNet for cats vs. dogs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_5_transfer_learning.html#transfer-learning-process">Transfer learning process</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_5_transfer_learning.html#load-and-preprocess-data">1. Load and preprocess data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_5_transfer_learning.html#modify-a-pre-trained-base-keras-model">2. Modify a pre-trained base Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_5_transfer_learning.html#train-the-transferred-model-for-the-new-task">3. Train the transferred model for the new task</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_5_transfer_learning.html#quantize-the-top-layer">4 Quantize the top layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_5_transfer_learning.html#convert-to-akida">5. Convert to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_5_transfer_learning.html#plot-confusion-matrix">6. Plot confusion matrix</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_6_voc_yolo_detection.html">YOLO/PASCAL-VOC detection tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_6_voc_yolo_detection.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_6_voc_yolo_detection.html#preprocessing-tools">2. Preprocessing tools</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_6_voc_yolo_detection.html#model-architecture">3. Model architecture</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_6_voc_yolo_detection.html#training">4. Training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_6_voc_yolo_detection.html#performance">5. Performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_6_voc_yolo_detection.html#conversion-to-akida">6. Conversion to Akida</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/index.html#cnn2snn-tutorials">CNN2SNN tutorials</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/cnn2snn/plot_0_cnn_flow.html">CNN conversion flow tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_0_cnn_flow.html#load-and-reshape-mnist-dataset">1. Load and reshape MNIST dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_0_cnn_flow.html#model-definition">2. Model definition</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_0_cnn_flow.html#model-training">3. Model training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_0_cnn_flow.html#model-quantization">4. Model quantization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_0_cnn_flow.html#model-fine-tuning-quantization-aware-training">5. Model fine tuning (quantization-aware training)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_0_cnn_flow.html#model-conversion">6. Model conversion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/cnn2snn/plot_1_advanced_cnn2snn.html">Advanced CNN2SNN tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_1_advanced_cnn2snn.html#design-a-cnn2snn-quantized-model">1. Design a CNN2SNN quantized model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_1_advanced_cnn2snn.html#weight-quantizer-details">2. Weight Quantizer Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_1_advanced_cnn2snn.html#quantized-activation-layer-details">3. Quantized Activation Layer Details</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/index.html#edge-examples">Edge examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/edge/plot_0_edge_learning_vision.html">Akida vision edge learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_0_edge_learning_vision.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_0_edge_learning_vision.html#prepare-akida-model-for-learning">2. Prepare Akida model for learning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_0_edge_learning_vision.html#edge-learning-with-akida">3. Edge learning with Akida</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/edge/plot_1_edge_learning_kws.html">Akida edge learning for keyword spotting</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_1_edge_learning_kws.html#edge-learning-process">1. Edge learning process</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_1_edge_learning_kws.html#dataset-preparation">2. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_1_edge_learning_kws.html#prepare-akida-model-for-learning">3. Prepare Akida model for learning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_1_edge_learning_kws.html#learn-with-akida-using-the-training-set">4. Learn with Akida using the training set</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_1_edge_learning_kws.html#edge-learning">5. Edge learning</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/edge/plot_2_edge_learning_parameters.html">Tips to set Akida learning parameters</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_2_edge_learning_parameters.html#akida-learning-parameters">1. Akida learning parameters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_2_edge_learning_parameters.html#create-akida-model">2. Create Akida model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_2_edge_learning_parameters.html#estimate-the-required-number-of-weights-of-the-trainable-layer">3. Estimate the required number of weights of the trainable layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_2_edge_learning_parameters.html#estimate-the-number-of-neurons-per-class">4. Estimate the number of neurons per class</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/Brainchip-Inc/akida_examples/releases">Changelog</a></li>
<li class="toctree-l1"><a class="reference external" href="https://support.brainchip.com/portal/home">Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Akida Examples</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="api_reference.html">API reference</a> &raquo;</li>
        
      <li>CNN2SNN Toolkit API</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="module-cnn2snn">
<span id="cnn2snn-toolkit-api"></span><h1>CNN2SNN Toolkit API<a class="headerlink" href="#module-cnn2snn" title="Permalink to this headline">¶</a></h1>
<p>CNN2SNN toolkit to quantize and convert Keras models into Akida-compatible
models.</p>
<div class="section" id="tool-functions">
<h2>Tool functions<a class="headerlink" href="#tool-functions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="quantize">
<h3>quantize<a class="headerlink" href="#quantize" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt id="cnn2snn.quantize">
<code class="sig-prename descclassname">cnn2snn.</code><code class="sig-name descname">quantize</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">model</span></em>, <em class="sig-param"><span class="n">weight_quantization</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">activ_quantization</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">input_weight_quantization</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">fold_BN</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">quantizer_function</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/cnn2snn/quantization.html#quantize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cnn2snn.quantize" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts a standard sequential Keras model to a CNN2SNN Keras quantized
model, compatible for Akida conversion.</p>
<p>This function returns a Keras model where the standard neural layers
(Conv2D, SeparableConv2D, Dense) and the ReLU activations are replaced with
CNN2SNN quantized layers (QuantizedConv2D, QuantizedSeparableConv2D,
QuantizedDense, ActivationDiscreteRelu).</p>
<p>Several transformations are applied to the model:
- the order of MaxPool and BatchNormalization layers are inverted so that
BatchNormalization always happens first,
- the batch normalization layers are folded into the previous layers.</p>
<p>This new model can be either directly converted to akida, or first
retrained for a few epochs to recover any accuracy loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>tf.keras.Model</em>) – a standard Keras model</p></li>
<li><p><strong>weight_quantization</strong> (<em>int</em>) – <p>sets all weights in the model to have
a particular quantization bitwidth except for the weights in the
first layer.</p>
<ul>
<li><p>’0’ implements floating point 32-bit weights.</p></li>
<li><p>’2’ through ‘8’ implements n-bit weights where n is from 2-8 bits.</p></li>
</ul>
</p></li>
<li><p><strong>activ_quantization</strong> (<em>int</em>) – <p>sets all activations in the model to have a
particular activation quantization bitwidth.</p>
<ul>
<li><p>’0’ implements floating point 32-bit activations.</p></li>
<li><p>’1’ through ‘8’ implements n-bit weights where n is from 1-8 bits.</p></li>
</ul>
</p></li>
<li><p><strong>input_weight_quantization</strong> (<em>int</em>) – <p>sets weight quantization in the first
layer. Defaults to weight_quantization value.</p>
<ul>
<li><p>’None’ implements the same bitwidth as the other weights.</p></li>
<li><p>’0’ implements floating point 32-bit weights.</p></li>
<li><p>’2’ through ‘8’ implements n-bit weights where n is from 2-8 bits.</p></li>
</ul>
</p></li>
<li><p><strong>fold_BN</strong> (<em>bool</em>) – enable folding batch normalization layers with their
corresponding neural layer.</p></li>
<li><p><strong>quantizer_function</strong> (<em>function</em>) – callable that takes as argument the layer
instance to be quantized and the corresponding default quantizer and
returns the quantizer to use.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a quantized Keras model</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.keras.Model</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="quantize-layer">
<h3>quantize_layer<a class="headerlink" href="#quantize-layer" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt id="cnn2snn.quantize_layer">
<code class="sig-prename descclassname">cnn2snn.</code><code class="sig-name descname">quantize_layer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">model</span></em>, <em class="sig-param"><span class="n">target_layer</span></em>, <em class="sig-param"><span class="n">bitwidth</span></em>, <em class="sig-param"><span class="n">quantizer_function</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/cnn2snn/quantization.html#quantize_layer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cnn2snn.quantize_layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Quantizes a specific layer with the given bitwidth.</p>
<p>This function returns a Keras model where the target layer is quantized.
All other layers are preserved.
If the target layer is a native Keras layer (Conv2D, SeparableConv2D, Dense,
ReLU), it is replaced by a CNN2SNN quantized layer (QuantizedConv2D,
QuantizedSeparableConv2D, QuantizedDense, ActivationDiscreteRelu). If
the target layer is an already quantized layer, only the bitwidth is
modified.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Quantize a layer of a native Keras model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
<span class="gp">... </span>    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,)),</span>
<span class="gp">... </span>    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Softmax</span><span class="p">()])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model_quantized</span> <span class="o">=</span> <span class="n">cnn2snn</span><span class="o">.</span><span class="n">quantize_layer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span>
<span class="gp">... </span>                                         <span class="n">target_layer</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>                                         <span class="n">bitwidth</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model_quantized</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cnn2snn</span><span class="o">.</span><span class="n">QuantizedDense</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">model_quantized</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">quantizer</span><span class="o">.</span><span class="n">bitwidth</span><span class="p">)</span>
<span class="go">4</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Quantize a layer of an an already quantized layer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model_quantized</span> <span class="o">=</span> <span class="n">cnn2snn</span><span class="o">.</span><span class="n">quantize_layer</span><span class="p">(</span><span class="n">model_quantized</span><span class="p">,</span>
<span class="gp">... </span>                                         <span class="n">target_layer</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">bitwidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">model_quantized</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">quantizer</span><span class="o">.</span><span class="n">bitwidth</span><span class="p">)</span>
<span class="go">2</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>tf.keras.Model</em>) – a standard Keras model</p></li>
<li><p><strong>target_layer</strong> – a standard or quantized Keras layer to be
converted, or the index or name of the target layer.</p></li>
<li><p><strong>bitwidth</strong> (<em>int</em>) – the desired quantization bitwidth. If zero, no
quantization will be applied.</p></li>
<li><p><strong>quantizer_function</strong> (<em>function</em>) – callable that takes as argument the layer
instance to be quantized and the corresponding default quantizer and
returns the quantizer to use.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a quantized Keras model</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.keras.Model</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>ValueError</strong> – In case of invalid target layer</p></li>
<li><p><strong>ValueError</strong> – If bitwidth is not greater than zero</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="convert">
<h3>convert<a class="headerlink" href="#convert" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt id="cnn2snn.convert">
<code class="sig-prename descclassname">cnn2snn.</code><code class="sig-name descname">convert</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">model</span></em>, <em class="sig-param"><span class="n">file_path</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">input_scaling</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">input_is_image</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/cnn2snn/converter.html#convert"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cnn2snn.convert" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts a Keras quantized model to an Akida one.</p>
<p>After quantizing a Keras model with <a class="reference internal" href="#cnn2snn.quantize" title="cnn2snn.quantize"><code class="xref py py-func docutils literal notranslate"><span class="pre">cnn2snn.quantize()</span></code></a>, it can be
converted to an Akida model. By default, the conversion expects that the
Akida model takes 8-bit images as inputs. <code class="docutils literal notranslate"><span class="pre">input_scaling</span></code> defines how the
images have been rescaled to be fed into the Keras model (see note below).</p>
<p>If inputs are spikes, you can set <code class="docutils literal notranslate"><span class="pre">input_is_image=False</span></code>. In this case,
Akida inputs are then expected to be integers between 0 and 15.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The relationship between Keras and Akida inputs is defined as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">input_akida</span> <span class="o">=</span> <span class="n">input_scaling</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">input_keras</span> <span class="o">+</span> <span class="n">input_scaling</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span>
</pre></div>
</div>
<p>If a <code class="xref py py-class docutils literal notranslate"><span class="pre">tf.keras.layers.experimental.preprocessing.Rescaling</span></code>
layer is present as first layer of the model, <code class="docutils literal notranslate"><span class="pre">input_scaling</span></code> must
be None: the <code class="xref py py-class docutils literal notranslate"><span class="pre">Rescaling</span></code> parameters will be used to compute the
input scaling.</p>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Convert a quantized Keras model with Keras inputs as images</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># rescaled between -1 and 1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs_akida</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;uint8&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs_keras</span> <span class="o">=</span> <span class="p">(</span><span class="n">images</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span> <span class="o">-</span> <span class="mi">128</span><span class="p">)</span> <span class="o">/</span> <span class="mi">128</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model_akida</span> <span class="o">=</span> <span class="n">cnn2snn</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">model_keras</span><span class="p">,</span> <span class="n">input_scaling</span><span class="o">=</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model_akida</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">inputs_akida</span><span class="p">)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Convert a quantized Keras model with Keras inputs as spikes and</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># input scaling of (2.5, 0). Akida spikes must be integers between</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 0 and 15</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs_akida</span> <span class="o">=</span> <span class="n">spikes</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;uint8&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs_keras</span> <span class="o">=</span> <span class="n">spikes</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span> <span class="o">/</span> <span class="mf">2.5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model_akida</span> <span class="o">=</span> <span class="n">cnn2snn</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">model_keras</span><span class="p">,</span> <span class="n">input_scaling</span><span class="o">=</span><span class="p">(</span><span class="mf">2.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model_akida</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">inputs_akida</span><span class="p">)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Convert and directly save the Akida model to fbz file.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cnn2snn</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">model_keras</span><span class="p">,</span> <span class="s1">&#39;model_akida.fbz&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.Model</span></code>) – a tf.keras model</p></li>
<li><p><strong>file_path</strong> (<em>str</em><em>, </em><em>optional</em>) – destination for the akida model.
(Default value = None)</p></li>
<li><p><strong>input_scaling</strong> (<em>2 elements tuple</em><em>, </em><em>optional</em>) – value of the input scaling.
(Default value = None)</p></li>
<li><p><strong>input_is_image</strong> (<em>bool</em><em>, </em><em>optional</em>) – True if input is an image (3-D 8-bit
input with 1 or 3 channels) followed by QuantizedConv2D. Akida model
input will be InputConvolutional. If False, Akida model input will
be InputData. (Default value = True)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>an Akida model.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="aee_apis.html#akida.Model" title="akida.Model"><code class="xref py py-obj docutils literal notranslate"><span class="pre">akida.Model</span></code></a></p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>ValueError</strong> – If <code class="docutils literal notranslate"><span class="pre">input_scaling[0]</span></code> is null or negative.</p></li>
<li><p><strong>ValueError</strong> – If a <code class="xref py py-class docutils literal notranslate"><span class="pre">Rescaling</span></code> layer is present and
    <code class="docutils literal notranslate"><span class="pre">input_scaling</span></code> is not None.</p></li>
<li><p><strong>SystemError</strong> – If Tensorflow is not run in eager mode.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<p>A detailed description of the input_scaling parameter is given in the
<a class="reference external" href="../user_guide/cnn2snn.html#input-scaling">user guide</a>.</p>
</div>
<div class="section" id="check-model-compatibility">
<h3>check_model_compatibility<a class="headerlink" href="#check-model-compatibility" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt id="cnn2snn.check_model_compatibility">
<code class="sig-prename descclassname">cnn2snn.</code><code class="sig-name descname">check_model_compatibility</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">model</span></em>, <em class="sig-param"><span class="n">input_is_image</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/cnn2snn/compatibility_checks.html#check_model_compatibility"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cnn2snn.check_model_compatibility" title="Permalink to this definition">¶</a></dt>
<dd><p>Checks if a Keras model is compatible for cnn2snn conversion.</p>
<p>This function doesn’t convert the Keras model to an Akida model
but only checks if the model design is compatible.</p>
<p>Note that this function doesn’t check if the quantization bitwidths (weights
or activations) are supported by the Akida Execution Engine or by the Akida
NSoC.</p>
<p><strong>1. How to build a compatible Keras quantized model?</strong></p>
<p>The following lines give details and constraints on how to build a Keras
model compatible for the conversion to an Akida model.</p>
<p><strong>2. General information about layers</strong></p>
<p>An Akida layer must be seen as a block of Keras layers starting with a
processing layer (Conv2D, SeparableConv2D,
Dense). All blocks of Keras layers except the last block must have
exactly one activation layer (ReLU or ActivationDiscreteRelu). Other
optional layers can be present in a block such as a pooling layer or a
batch normalization layer.
Here are all the supported Keras layers for an Akida-compatible model:</p>
<ul class="simple">
<li><p>Processing layers:</p>
<ul>
<li><p>tf.keras Conv2D/SeparableConv2D/Dense</p></li>
<li><p>cnn2snn QuantizedConv2D/QuantizedSeparableConv2D/QuantizedDense</p></li>
</ul>
</li>
<li><p>Activation layers:</p>
<ul>
<li><p>tf.keras ReLU</p></li>
<li><p>cnn2snn ActivationDiscreteRelu</p></li>
<li><p>any increasing activation function (only for the last block of layers)
such as softmax, sigmoid set as last layer. This layer must derive from
tf.keras.layers.Activation, and it will be removed during conversion to
an Akida model.</p></li>
</ul>
</li>
<li><p>Pooling layers:</p>
<ul>
<li><p>MaxPool2D</p></li>
<li><p>GlobalAvgPool2D</p></li>
</ul>
</li>
<li><p>BatchNormalization</p></li>
<li><p>Dropout</p></li>
<li><p>Flatten</p></li>
<li><p>Input</p></li>
<li><p>Reshape</p></li>
</ul>
<p>Example of a block of Keras layers:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>       <span class="o">----------</span>
       <span class="o">|</span> <span class="n">Conv2D</span> <span class="o">|</span>
       <span class="o">----------</span>
           <span class="o">||</span>
           \<span class="o">/</span>
 <span class="o">----------------------</span>
 <span class="o">|</span> <span class="n">BatchNormalization</span> <span class="o">|</span>
 <span class="o">----------------------</span>
           <span class="o">||</span>
           \<span class="o">/</span>
      <span class="o">-------------</span>
      <span class="o">|</span> <span class="n">MaxPool2D</span> <span class="o">|</span>
      <span class="o">-------------</span>
           <span class="o">||</span>
           \<span class="o">/</span>
<span class="o">--------------------------</span>
<span class="o">|</span> <span class="n">ActivationDiscreteRelu</span> <span class="o">|</span>
<span class="o">--------------------------</span>
</pre></div>
</div>
<p><strong>3. Constraints about inputs</strong></p>
<p>An Akida model can accept two types of inputs: sparse events or 8-bit
images. Whatever the input type, the Keras inputs must respect the
following relation:</p>
<blockquote>
<div><p>input_akida = scale * input_keras + shift</p>
</div></blockquote>
<p>where the Akida inputs must be positive integers, the input scale must be
a float value and the input shift must be an integer. In other words,
scale * input_keras must be integers.</p>
<p>Depending on the input type:</p>
<ul class="simple">
<li><p>if the inputs are events (sparse), the first layer of the Keras model can
be any processing layer. The input shift must be zero.</p></li>
<li><p>if the inputs are images, the first layer must be a Conv2D
layer.</p></li>
</ul>
<p><strong>4. Constraints about layers’ parameters</strong></p>
<p>To be Akida-compatible, the Keras layers must observe the following rules:</p>
<ul class="simple">
<li><p>all layers with the ‘data_format’ parameter must be ‘channels_last’</p></li>
<li><p>all processing quantized layers and ActivationDiscreteRelu must have a
valid quantization bitwidth</p></li>
<li><p>a Dense layer must have an input shape of (N,) or (1, 1, N)</p></li>
<li><p>a BatchNormalization layer must have ‘axis’ set to -1 (default)</p></li>
<li><p>a BatchNormalization layer cannot have negative gammas</p></li>
<li><p>Reshape layers can only be used to transform a tensor of shape (N,) to a
tensor of shape (1, 1, N), and vice-versa</p></li>
<li><p>only one pooling layer can be used in each block</p></li>
<li><p>a MaxPool2D layer must have the same ‘padding’ as the corresponding
processing quantized layer</p></li>
</ul>
<p><strong>5. Constraints about the order of layers</strong></p>
<p>To be Akida-compatible, the order of Keras layers must observe the following
rules:</p>
<ul class="simple">
<li><p>a block of Keras layers must start with a processing quantized layer</p></li>
<li><p>where present, a BatchNormalization/GlobalAvgPool2D layer must be placed
before the activation</p></li>
<li><p>a Flatten layer can only be used before a Dense layer</p></li>
<li><p>an Activation layer other than ActivationDiscreteRelu can only be used
in the last layer</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.Model</span></code>) – the model to check.</p></li>
<li><p><strong>input_is_image</strong> (<em>bool</em><em>, </em><em>optional</em>) – True if input is an image (8-bit input
with 1 or 3 channels) followed by QuantizedConv2D. Akida model
input will be InputConvolutional. If False, Akida model input will
be InputData. (Default value = True)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="load-quantized-model">
<h3>load_quantized_model<a class="headerlink" href="#load-quantized-model" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt id="cnn2snn.load_quantized_model">
<code class="sig-prename descclassname">cnn2snn.</code><code class="sig-name descname">load_quantized_model</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">filepath</span></em>, <em class="sig-param"><span class="n">custom_objects</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">compile_model</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/cnn2snn/utils.html#load_quantized_model"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cnn2snn.load_quantized_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads a quantized model saved in TF or HDF5 format.</p>
<p>If the model was compiled and trained before saving, its training state
will be loaded as well.
This function is a wrapper of <cite>tf.keras.models.load_model</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>filepath</strong> (<em>string</em>) – path to the saved model.</p></li>
<li><p><strong>custom_objects</strong> (<em>dict</em>) – optional dictionary mapping names (strings) to
custom classes or functions to be considered during deserialization.</p></li>
<li><p><strong>compile_model</strong> (<em>bool</em>) – whether to compile the model after loading.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a Keras model instance.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tensorflow.keras.Model</span></code></p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="load-partial-weights">
<h3>load_partial_weights<a class="headerlink" href="#load-partial-weights" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt id="cnn2snn.load_partial_weights">
<code class="sig-prename descclassname">cnn2snn.</code><code class="sig-name descname">load_partial_weights</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dest_model</span></em>, <em class="sig-param"><span class="n">src_model</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/cnn2snn/utils.html#load_partial_weights"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cnn2snn.load_partial_weights" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads a subset of weights from one Keras model to another</p>
<p>This goes through each layers of the source model, looking for a matching
layer in the destination model.
If a layer with the same name is found, then this method assumes that one
of the two layer has the same set of weights as the other plus some extra
weights at the end, and loads only the first common weights into the
destination layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dest_model</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tensorflow.keras.Model</span></code>) – the destination Model</p></li>
<li><p><strong>src_model</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tensorflow.keras.Model</span></code>) – the source Model</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
</div>
<div class="section" id="quantizers">
<h2>Quantizers<a class="headerlink" href="#quantizers" title="Permalink to this headline">¶</a></h2>
<div class="section" id="weightquantizer">
<h3>WeightQuantizer<a class="headerlink" href="#weightquantizer" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="cnn2snn.quantization_ops.WeightQuantizer">
<em class="property">class </em><code class="sig-prename descclassname">cnn2snn.quantization_ops.</code><code class="sig-name descname">WeightQuantizer</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/cnn2snn/quantization_ops.html#WeightQuantizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cnn2snn.quantization_ops.WeightQuantizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.keras.engine.base_layer.Layer</span></code></p>
<p>The base class for all weight quantizers.</p>
<p>This base class must be overloaded as well as the two functions <cite>quantize</cite>
and <cite>scale_factor</cite>.</p>
<p>Quantizers derived from this class must be symmetric uniform mid-tread
quantizers, in order to be compatible with the conversion into an Akida
model. Quantization is usually done in two steps:</p>
<ol class="arabic simple">
<li><p>The weights must be first quantized on integer values in the
range imposed by the bitwidth, e.g. from -7 to 7 for a 4-bit
quantization.</p></li>
<li><p>These integer weights are then reconstructed to float discretized
values, in the range of the original weights. For example, 4-bit integer
weights are reconstructed on a grid from -7*qstep to 7*qstep, where
qstep is the quantization step size between two levels of the uniform
grid.</p></li>
</ol>
<p>For a full explanation about mid-tread uniform quantization, one can take a
look at <a class="reference external" href="https://en.wikipedia.org/wiki/Quantization_(signal_processing)#Example">the Wikipedia page</a>.</p>
<p>The <cite>quantize</cite> function takes as inputs the original weights and must
return the reconstructed float values after quantization. The
<cite>scale_factor</cite> function must return the factor used to transform the float
reconstructed weights into the integer values obtained after step 1. In other
words, given a set of float weights “w”:</p>
<blockquote>
<div><p>quantize(w) * scale_factor(w) is a set of integer weights.</p>
</div></blockquote>
<p>The bitwidth defines the number of quantization levels on which the
weights will be quantized. For instance, a 4-bit quantization gives
integer values between -7 and 7. More generally, for a n-bit
quantization, values are ranged from -kmax to kmax where kmax is
(2^(n-1) - 1).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>bitwidth</strong> (<em>int</em>) – the quantization bitwidth.</p>
</dd>
</dl>
<p><strong>Attributes:</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cnn2snn.quantization_ops.WeightQuantizer.bitwidth" title="cnn2snn.quantization_ops.WeightQuantizer.bitwidth"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bitwidth</span></code></a></p></td>
<td><p>Returns the bitwidth of the quantizer</p></td>
</tr>
</tbody>
</table>
<p><strong>Methods:</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cnn2snn.quantization_ops.WeightQuantizer.get_config" title="cnn2snn.quantization_ops.WeightQuantizer.get_config"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_config</span></code></a>()</p></td>
<td><p>Returns the config of the layer.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cnn2snn.quantization_ops.WeightQuantizer.quantize" title="cnn2snn.quantization_ops.WeightQuantizer.quantize"><code class="xref py py-obj docutils literal notranslate"><span class="pre">quantize</span></code></a>(w)</p></td>
<td><p>Quantizes the specified weights Tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cnn2snn.quantization_ops.WeightQuantizer.scale_factor" title="cnn2snn.quantization_ops.WeightQuantizer.scale_factor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">scale_factor</span></code></a>(w)</p></td>
<td><p>Evaluates the scale factor for the specified weights tf.Tensor.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cnn2snn.quantization_ops.WeightQuantizer.bitwidth">
<em class="property">property </em><code class="sig-name descname">bitwidth</code><a class="headerlink" href="#cnn2snn.quantization_ops.WeightQuantizer.bitwidth" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the bitwidth of the quantizer</p>
</dd></dl>

<dl class="py method">
<dt id="cnn2snn.quantization_ops.WeightQuantizer.get_config">
<code class="sig-name descname">get_config</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/cnn2snn/quantization_ops.html#WeightQuantizer.get_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cnn2snn.quantization_ops.WeightQuantizer.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Python dictionary.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="cnn2snn.quantization_ops.WeightQuantizer.quantize">
<code class="sig-name descname">quantize</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">w</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/cnn2snn/quantization_ops.html#WeightQuantizer.quantize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cnn2snn.quantization_ops.WeightQuantizer.quantize" title="Permalink to this definition">¶</a></dt>
<dd><p>Quantizes the specified weights Tensor.</p>
<p>This function must return a tf.Tensor containing float weights
discretized on a uniform grid based on the scale factor “sf”.
In other words, the discretized weights must be values among:
-kmax/sf, …, -2/sf, -1/sf, 0, 1/sf, 2/sf, …, kmax/sf</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>w</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tensorflow.Tensor</span></code>) – the weights Tensor to quantize.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a Tensor of quantized weights.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tensorflow.Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="cnn2snn.quantization_ops.WeightQuantizer.scale_factor">
<code class="sig-name descname">scale_factor</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">w</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/cnn2snn/quantization_ops.html#WeightQuantizer.scale_factor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cnn2snn.quantization_ops.WeightQuantizer.scale_factor" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluates the scale factor for the specified weights tf.Tensor.</p>
<p>This function returns the scale factor to get the quantized integer
weights from the reconstructed float weights. It is equal to the
inverse of the quantization step size.</p>
<p>The scale factor can be a scalar that is applied on the whole tensor
of weights. It can also be a vector of length the number of filters,
where each value applies to the weights of the corresponding output
filter. This is called a per-axis quantization, as opposed to a
per-tensor quantization. The number of filters is usually the last
dimension of the weights tensor. More details are given
<a class="reference external" href="https://www.tensorflow.org/lite/performance/quantization_spec?hl=sv#per-axis_vs_per-tensor">here</a></p>
<p>Note that the <cite>quantizer_dw</cite> of a depthwise convolution in a
QuantizedSeparableConv2D layer must imperatively return a scalar scale
factor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>w</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tensorflow.Tensor</span></code>) – the weights Tensor to quantize.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a Tensor containing a single scalar value.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tensorflow.Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="linearweightquantizer">
<h3>LinearWeightQuantizer<a class="headerlink" href="#linearweightquantizer" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="cnn2snn.quantization_ops.LinearWeightQuantizer">
<em class="property">class </em><code class="sig-prename descclassname">cnn2snn.quantization_ops.</code><code class="sig-name descname">LinearWeightQuantizer</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/cnn2snn/quantization_ops.html#LinearWeightQuantizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cnn2snn.quantization_ops.LinearWeightQuantizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#cnn2snn.quantization_ops.WeightQuantizer" title="cnn2snn.quantization_ops.WeightQuantizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">cnn2snn.quantization_ops.WeightQuantizer</span></code></a></p>
<p>An abstract linear weight quantizer</p>
<p>This abstract class proposes a linear symmetric and uniform quantization
function. The “linear” term here means that there is no non-linear
transformation of the weights before the uniform quantization.</p>
<p>The <cite>scale_factor</cite> function must be overloaded.</p>
<p><strong>Methods:</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cnn2snn.quantization_ops.LinearWeightQuantizer.quantize" title="cnn2snn.quantization_ops.LinearWeightQuantizer.quantize"><code class="xref py py-obj docutils literal notranslate"><span class="pre">quantize</span></code></a>(w)</p></td>
<td><p>Linearly quantizes the input weights on a symmetric uniform grid based on the scale factor.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cnn2snn.quantization_ops.LinearWeightQuantizer.scale_factor" title="cnn2snn.quantization_ops.LinearWeightQuantizer.scale_factor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">scale_factor</span></code></a>(w)</p></td>
<td><p>Evaluates the scale factor for the specified weights tf.Tensor.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cnn2snn.quantization_ops.LinearWeightQuantizer.quantize">
<code class="sig-name descname">quantize</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">w</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/cnn2snn/quantization_ops.html#LinearWeightQuantizer.quantize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cnn2snn.quantization_ops.LinearWeightQuantizer.quantize" title="Permalink to this definition">¶</a></dt>
<dd><p>Linearly quantizes the input weights on a symmetric uniform grid
based on the scale factor.</p>
<p>The input weights are directly rounded to the closest discretized
value, without any transformation on the input weights.</p>
<p>The gradient is estimated using the Straight-Through Estimator (STE),
i.e. the gradient is computed as if there were no quantization.</p>
</dd></dl>

<dl class="py method">
<dt id="cnn2snn.quantization_ops.LinearWeightQuantizer.scale_factor">
<code class="sig-name descname">scale_factor</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">w</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/cnn2snn/quantization_ops.html#LinearWeightQuantizer.scale_factor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cnn2snn.quantization_ops.LinearWeightQuantizer.scale_factor" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluates the scale factor for the specified weights tf.Tensor.</p>
<p>This function returns the scale factor to get the quantized integer
weights from the reconstructed float weights. It is equal to the
inverse of the quantization step size.</p>
<p>The scale factor can be a scalar that is applied on the whole tensor
of weights. It can also be a vector of length the number of filters,
where each value applies to the weights of the corresponding output
filter. This is called a per-axis quantization, as opposed to a
per-tensor quantization. The number of filters is usually the last
dimension of the weights tensor. More details are given
<a class="reference external" href="https://www.tensorflow.org/lite/performance/quantization_spec?hl=sv#per-axis_vs_per-tensor">here</a></p>
<p>Note that the <cite>quantizer_dw</cite> of a depthwise convolution in a
QuantizedSeparableConv2D layer must imperatively return a scalar scale
factor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>w</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tensorflow.Tensor</span></code>) – the weights Tensor to quantize.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a Tensor containing a single scalar value.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tensorflow.Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="stdweightquantizer">
<h3>StdWeightQuantizer<a class="headerlink" href="#stdweightquantizer" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="cnn2snn.StdWeightQuantizer">
<em class="property">class </em><code class="sig-prename descclassname">cnn2snn.</code><code class="sig-name descname">StdWeightQuantizer</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/cnn2snn/quantization_ops.html#StdWeightQuantizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cnn2snn.StdWeightQuantizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#cnn2snn.quantization_ops.LinearWeightQuantizer" title="cnn2snn.quantization_ops.LinearWeightQuantizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">cnn2snn.quantization_ops.LinearWeightQuantizer</span></code></a></p>
<p>A uniform quantizer.</p>
<p>Quantizes the specified weights into 2^bitwidth-1 values centered on zero.
E.g. with bitwidth = 4, 15 quantization levels: from -7 * qstep to 7 * qstep
with qstep being the quantization step. The quantization step is defined by:</p>
<blockquote>
<div><p>qstep = threshold * std(W) / max_value</p>
</div></blockquote>
<p>with max_value being 2^(bitwidth-1) - 1. E.g with bitwidth = 4, max_value = 7.</p>
<p>All values below or above threshold * std(W) are automatically assigned to
the min (resp max) value.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>threshold</strong> (<em>int</em>) – the standard deviation multiplier used to exclude
outliers.</p></li>
<li><p><strong>bitwidth</strong> (<em>int</em>) – the quantizer bitwidth defining the number of
quantized values.</p></li>
</ul>
</dd>
</dl>
<p><strong>Methods:</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cnn2snn.StdWeightQuantizer.get_config" title="cnn2snn.StdWeightQuantizer.get_config"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_config</span></code></a>()</p></td>
<td><p>Returns the config of the layer.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cnn2snn.StdWeightQuantizer.scale_factor" title="cnn2snn.StdWeightQuantizer.scale_factor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">scale_factor</span></code></a>(w)</p></td>
<td><p>Evaluates the scale factor for the specified weights tf.Tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cnn2snn.StdWeightQuantizer.sigma_scaled_" title="cnn2snn.StdWeightQuantizer.sigma_scaled_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sigma_scaled_</span></code></a>(w)</p></td>
<td><p>Get the range on which the weights are quantized.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cnn2snn.StdWeightQuantizer.get_config">
<code class="sig-name descname">get_config</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/cnn2snn/quantization_ops.html#StdWeightQuantizer.get_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cnn2snn.StdWeightQuantizer.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Python dictionary.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="cnn2snn.StdWeightQuantizer.scale_factor">
<code class="sig-name descname">scale_factor</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">w</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/cnn2snn/quantization_ops.html#StdWeightQuantizer.scale_factor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cnn2snn.StdWeightQuantizer.scale_factor" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluates the scale factor for the specified weights tf.Tensor.</p>
<p>This function returns the scale factor to get the quantized integer
weights from the reconstructed float weights. It is equal to the
inverse of the quantization step size.</p>
<p>The scale factor can be a scalar that is applied on the whole tensor
of weights. It can also be a vector of length the number of filters,
where each value applies to the weights of the corresponding output
filter. This is called a per-axis quantization, as opposed to a
per-tensor quantization. The number of filters is usually the last
dimension of the weights tensor. More details are given
<a class="reference external" href="https://www.tensorflow.org/lite/performance/quantization_spec?hl=sv#per-axis_vs_per-tensor">here</a></p>
<p>Note that the <cite>quantizer_dw</cite> of a depthwise convolution in a
QuantizedSeparableConv2D layer must imperatively return a scalar scale
factor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>w</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tensorflow.Tensor</span></code>) – the weights Tensor to quantize.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a Tensor containing a single scalar value.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tensorflow.Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="cnn2snn.StdWeightQuantizer.sigma_scaled_">
<code class="sig-name descname">sigma_scaled_</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">w</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/cnn2snn/quantization_ops.html#StdWeightQuantizer.sigma_scaled_"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cnn2snn.StdWeightQuantizer.sigma_scaled_" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the range on which the weights are quantized. This quantizer
discretizes weights in the range:</p>
<blockquote>
<div><p>[-threshold * std(weights) ; threshold * std(weights)]</p>
</div></blockquote>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="trainablestdweightquantizer">
<h3>TrainableStdWeightQuantizer<a class="headerlink" href="#trainablestdweightquantizer" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="cnn2snn.TrainableStdWeightQuantizer">
<em class="property">class </em><code class="sig-prename descclassname">cnn2snn.</code><code class="sig-name descname">TrainableStdWeightQuantizer</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/cnn2snn/quantization_ops.html#TrainableStdWeightQuantizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cnn2snn.TrainableStdWeightQuantizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#cnn2snn.quantization_ops.LinearWeightQuantizer" title="cnn2snn.quantization_ops.LinearWeightQuantizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">cnn2snn.quantization_ops.LinearWeightQuantizer</span></code></a></p>
<p>A trainable weight quantizer.</p>
<p>Quantizes the specified weights into 2^bitwidth-1 values centered on zero.
E.g. with bitwidth = 4, 15 quantization levels: from -7 * qstep to 7 * qstep
with qstep being the quantization step. The quantization step is defined by:</p>
<blockquote>
<div><p>qstep = threshold * std(W) / max_value</p>
</div></blockquote>
<p>with:</p>
<blockquote>
<div><ul class="simple">
<li><p>max_value being 2^(bitwidth-1) - 1. E.g with bitwidth = 4, max_value = 7.</p></li>
<li><p>threshold a trainable parameter whose initial value can be specified.</p></li>
</ul>
</div></blockquote>
<p>All values below or above threshold * std(W) are automatically assigned to
the min (resp max) value.</p>
<p>This is the trainable version of the StdWeightQuantizer class.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>threshold</strong> (<em>int</em>) – the initial value of the standard deviation
multiplier used to exclude outliers.</p></li>
<li><p><strong>bitwidth</strong> (<em>int</em>) – the quantizer bitwidth defining the number of
quantized values.</p></li>
</ul>
</dd>
</dl>
<p><strong>Methods:</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cnn2snn.TrainableStdWeightQuantizer.get_config" title="cnn2snn.TrainableStdWeightQuantizer.get_config"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_config</span></code></a>()</p></td>
<td><p>Returns the config of the layer.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cnn2snn.TrainableStdWeightQuantizer.scale_factor" title="cnn2snn.TrainableStdWeightQuantizer.scale_factor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">scale_factor</span></code></a>(w)</p></td>
<td><p>Evaluates the scale factor for the specified weights tf.Tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cnn2snn.TrainableStdWeightQuantizer.sigma_scaled_" title="cnn2snn.TrainableStdWeightQuantizer.sigma_scaled_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sigma_scaled_</span></code></a>(w)</p></td>
<td><p>Get the range on which the weights are quantized.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cnn2snn.TrainableStdWeightQuantizer.get_config">
<code class="sig-name descname">get_config</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/cnn2snn/quantization_ops.html#TrainableStdWeightQuantizer.get_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cnn2snn.TrainableStdWeightQuantizer.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Python dictionary.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="cnn2snn.TrainableStdWeightQuantizer.scale_factor">
<code class="sig-name descname">scale_factor</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">w</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/cnn2snn/quantization_ops.html#TrainableStdWeightQuantizer.scale_factor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cnn2snn.TrainableStdWeightQuantizer.scale_factor" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluates the scale factor for the specified weights tf.Tensor.</p>
<p>This function returns the scale factor to get the quantized integer
weights from the reconstructed float weights. It is equal to the
inverse of the quantization step size.</p>
<p>The scale factor can be a scalar that is applied on the whole tensor
of weights. It can also be a vector of length the number of filters,
where each value applies to the weights of the corresponding output
filter. This is called a per-axis quantization, as opposed to a
per-tensor quantization. The number of filters is usually the last
dimension of the weights tensor. More details are given
<a class="reference external" href="https://www.tensorflow.org/lite/performance/quantization_spec?hl=sv#per-axis_vs_per-tensor">here</a></p>
<p>Note that the <cite>quantizer_dw</cite> of a depthwise convolution in a
QuantizedSeparableConv2D layer must imperatively return a scalar scale
factor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>w</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tensorflow.Tensor</span></code>) – the weights Tensor to quantize.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a Tensor containing a single scalar value.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tensorflow.Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="cnn2snn.TrainableStdWeightQuantizer.sigma_scaled_">
<code class="sig-name descname">sigma_scaled_</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">w</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/cnn2snn/quantization_ops.html#TrainableStdWeightQuantizer.sigma_scaled_"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cnn2snn.TrainableStdWeightQuantizer.sigma_scaled_" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the range on which the weights are quantized. This quantizer
discretizes weights in the range:</p>
<blockquote>
<div><p>[-threshold * std(weights) ; threshold * std(weights)]</p>
</div></blockquote>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="maxquantizer">
<h3>MaxQuantizer<a class="headerlink" href="#maxquantizer" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="cnn2snn.MaxQuantizer">
<em class="property">class </em><code class="sig-prename descclassname">cnn2snn.</code><code class="sig-name descname">MaxQuantizer</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/cnn2snn/quantization_ops.html#MaxQuantizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cnn2snn.MaxQuantizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#cnn2snn.quantization_ops.LinearWeightQuantizer" title="cnn2snn.quantization_ops.LinearWeightQuantizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">cnn2snn.quantization_ops.LinearWeightQuantizer</span></code></a></p>
<p>A quantizer that relies on maximum range.</p>
<p>Quantizes the specified weights into 2^bitwidth-1 values centered on zero.
E.g. with bitwidth = 4, 15 quantization levels: from -7 * qstep to 7 * qstep
with qstep being the quantization step. The quantization step is defined by:</p>
<blockquote>
<div><p>qstep = max_range / max_value</p>
</div></blockquote>
<p>with:</p>
<blockquote>
<div><ul class="simple">
<li><p>max_range = max(abs(W))</p></li>
<li><p>max_value = 2^(bitwidth-1) - 1. E.g with bitwidth = 4, max_value = 7.</p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>bitwidth</strong> (<em>int</em>) – the quantizer bitwidth defining the number of
quantized values.</p>
</dd>
</dl>
<p><strong>Methods:</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cnn2snn.MaxQuantizer.max_range_" title="cnn2snn.MaxQuantizer.max_range_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">max_range_</span></code></a>(w)</p></td>
<td><p>Get the range on which the weights are quantized.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cnn2snn.MaxQuantizer.scale_factor" title="cnn2snn.MaxQuantizer.scale_factor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">scale_factor</span></code></a>(w)</p></td>
<td><p>Evaluates the scale factor for the specified weights tf.Tensor.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cnn2snn.MaxQuantizer.max_range_">
<em class="property">static </em><code class="sig-name descname">max_range_</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">w</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/cnn2snn/quantization_ops.html#MaxQuantizer.max_range_"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cnn2snn.MaxQuantizer.max_range_" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the range on which the weights are quantized. This quantizer
discretizes weights in the range:</p>
<blockquote>
<div><p>[-max(weights) ; max(weights)]</p>
</div></blockquote>
</dd></dl>

<dl class="py method">
<dt id="cnn2snn.MaxQuantizer.scale_factor">
<code class="sig-name descname">scale_factor</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">w</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/cnn2snn/quantization_ops.html#MaxQuantizer.scale_factor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cnn2snn.MaxQuantizer.scale_factor" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluates the scale factor for the specified weights tf.Tensor.</p>
<p>This function returns the scale factor to get the quantized integer
weights from the reconstructed float weights. It is equal to the
inverse of the quantization step size.</p>
<p>The scale factor can be a scalar that is applied on the whole tensor
of weights. It can also be a vector of length the number of filters,
where each value applies to the weights of the corresponding output
filter. This is called a per-axis quantization, as opposed to a
per-tensor quantization. The number of filters is usually the last
dimension of the weights tensor. More details are given
<a class="reference external" href="https://www.tensorflow.org/lite/performance/quantization_spec?hl=sv#per-axis_vs_per-tensor">here</a></p>
<p>Note that the <cite>quantizer_dw</cite> of a depthwise convolution in a
QuantizedSeparableConv2D layer must imperatively return a scalar scale
factor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>w</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tensorflow.Tensor</span></code>) – the weights Tensor to quantize.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a Tensor containing a single scalar value.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tensorflow.Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="maxperaxisquantizer">
<h3>MaxPerAxisQuantizer<a class="headerlink" href="#maxperaxisquantizer" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="cnn2snn.MaxPerAxisQuantizer">
<em class="property">class </em><code class="sig-prename descclassname">cnn2snn.</code><code class="sig-name descname">MaxPerAxisQuantizer</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/cnn2snn/quantization_ops.html#MaxPerAxisQuantizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cnn2snn.MaxPerAxisQuantizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">cnn2snn.quantization_ops.MaxQuantizer</span></code></p>
<p>A quantizer that relies on maximum range per axis.</p>
<p>Quantizes the specified weights into 2^bitwidth-1 values centered on zero.
E.g. with bitwidth = 4, 15 quantization levels: from -7 * qstep to 7 * qstep
with qstep being the quantization step. The quantization step is defined by:</p>
<blockquote>
<div><p>qstep = max_range / max_value</p>
</div></blockquote>
<p>with:</p>
<blockquote>
<div><ul class="simple">
<li><p>max_range = max(abs(W))</p></li>
<li><p>max_value = 2^(bitwidth-1) - 1. E.g with bitwidth = 4, max_value = 7.</p></li>
</ul>
</div></blockquote>
<p>This is an evolution of the MaxQuantizer that defines the max_range per
axis.</p>
<p>The last dimension is used as axis, meaning that the scaling factor is a
vector with as many values as “filters”, or “neurons”.</p>
<p>Note: for a DepthwiseConv2D layer that has a single filter, this
quantizer is strictly equivalent to the MaxQuantizer.</p>
<p><strong>Methods:</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cnn2snn.MaxPerAxisQuantizer.max_range_" title="cnn2snn.MaxPerAxisQuantizer.max_range_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">max_range_</span></code></a>(w)</p></td>
<td><p>Get the range on which the weights are quantized.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cnn2snn.MaxPerAxisQuantizer.max_range_">
<code class="sig-name descname">max_range_</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">w</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/cnn2snn/quantization_ops.html#MaxPerAxisQuantizer.max_range_"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cnn2snn.MaxPerAxisQuantizer.max_range_" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the range on which the weights are quantized. This quantizer
discretizes weights in the range:</p>
<blockquote>
<div><p>[-max(weights) ; max(weights)]</p>
</div></blockquote>
</dd></dl>

</dd></dl>

</div>
</div>
<div class="section" id="quantized-layers">
<h2>Quantized layers<a class="headerlink" href="#quantized-layers" title="Permalink to this headline">¶</a></h2>
<div class="section" id="quantizedconv2d">
<h3>QuantizedConv2D<a class="headerlink" href="#quantizedconv2d" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="cnn2snn.QuantizedConv2D">
<em class="property">class </em><code class="sig-prename descclassname">cnn2snn.</code><code class="sig-name descname">QuantizedConv2D</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/cnn2snn/quantization_layers.html#QuantizedConv2D"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cnn2snn.QuantizedConv2D" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.keras.layers.convolutional.Conv2D</span></code></p>
<p>A quantization-aware Keras convolutional layer.</p>
<p>Inherits from Keras Conv2D layer, applying a quantization on weights during
the forward pass.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>filters</strong> (<em>int</em>) – the number of filters.</p></li>
<li><p><strong>kernel_size</strong> (<em>tuple of integer</em>) – the kernel spatial dimensions.</p></li>
<li><p><strong>quantizer</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">cnn2snn.WeightQuantizer</span></code>) – the quantizer
to apply during the forward pass.</p></li>
<li><p><strong>strides</strong> (<em>integer</em><em>, or </em><em>tuple of integers</em><em>, </em><em>optional</em>) – strides of the
convolution along height and width.</p></li>
<li><p><strong>padding</strong> (<em>str</em><em>, </em><em>optional</em>) – one of ‘valid’ or ‘same’.</p></li>
<li><p><strong>use_bias</strong> (<em>boolean</em><em>, </em><em>optional</em>) – whether the layer uses a bias vector.</p></li>
<li><p><strong>kernel_initializer</strong> (str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.initializer</span></code>, optional) – initializer for the weights matrix.</p></li>
<li><p><strong>bias_initializer</strong> (str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.initializer</span></code>, optional) – initializer for the bias vector.</p></li>
<li><p><strong>kernel_regularizer</strong> (str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.regularizer</span></code>, optional) – regularization applied to the weights.</p></li>
<li><p><strong>bias_regularizer</strong> (str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.regularizer</span></code>, optional) – regularization applied to the bias.</p></li>
<li><p><strong>(</strong><strong>str</strong> (<em>activity_regularizer</em>) – optional): regularization applied to the output of the layer.</p></li>
</ul>
</dd>
</dl>
<p>:param or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.regularizer</span></code>: optional): regularization applied to the output of the layer.
:param : optional): regularization applied to the output of the layer.
:param kernel_constraint: constraint applied to the weights.
:type kernel_constraint: str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.constraint</span></code>, optional
:param bias_constraint: constraint applied to the bias.
:type bias_constraint: str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.constraint</span></code>, optional</p>
<p><strong>Methods:</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cnn2snn.QuantizedConv2D.call" title="cnn2snn.QuantizedConv2D.call"><code class="xref py py-obj docutils literal notranslate"><span class="pre">call</span></code></a>(inputs)</p></td>
<td><p>Evaluates input Tensor.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cnn2snn.QuantizedConv2D.get_config" title="cnn2snn.QuantizedConv2D.get_config"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_config</span></code></a>()</p></td>
<td><p>Returns the config of the layer.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cnn2snn.QuantizedConv2D.call">
<code class="sig-name descname">call</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inputs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/cnn2snn/quantization_layers.html#QuantizedConv2D.call"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cnn2snn.QuantizedConv2D.call" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluates input Tensor.</p>
<p>This applies the quantization on weights, then evaluates the input
Tensor and produces the output Tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>inputs</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tensorflow.Tensor</span></code>) – input Tensor.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>output Tensor.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tensorflow.Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="cnn2snn.QuantizedConv2D.get_config">
<code class="sig-name descname">get_config</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/cnn2snn/quantization_layers.html#QuantizedConv2D.get_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cnn2snn.QuantizedConv2D.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Python dictionary.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="quantizeddepthwiseconv2d">
<h3>QuantizedDepthwiseConv2D<a class="headerlink" href="#quantizeddepthwiseconv2d" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="cnn2snn.QuantizedDepthwiseConv2D">
<em class="property">class </em><code class="sig-prename descclassname">cnn2snn.</code><code class="sig-name descname">QuantizedDepthwiseConv2D</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/cnn2snn/quantization_layers.html#QuantizedDepthwiseConv2D"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cnn2snn.QuantizedDepthwiseConv2D" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.keras.layers.convolutional.DepthwiseConv2D</span></code></p>
<p>A quantization-aware Keras depthwise convolutional layer.</p>
<p>Inherits from Keras DepthwiseConv2D layer, applying a quantization on
weights during the forward pass.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>kernel_size</strong> (<em>a tuple of integer</em>) – the kernel spatial dimensions.</p></li>
<li><p><strong>strides</strong> (<em>integer</em><em>, or </em><em>tuple of integers</em><em>, </em><em>optional</em>) – strides of the
convolution along height and width.</p></li>
<li><p><strong>quantizer</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">cnn2snn.WeightQuantizer</span></code>) – the quantizer
to apply during the forward pass.</p></li>
<li><p><strong>padding</strong> (<em>str</em><em>, </em><em>optional</em>) – One of ‘valid’ or ‘same’.</p></li>
<li><p><strong>use_bias</strong> (<em>boolean</em><em>, </em><em>optional</em>) – whether the layer uses a bias vector.</p></li>
<li><p><strong>depthwise_initializer</strong> (str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.initializer</span></code>, optional) – initializer for the weights matrix.</p></li>
<li><p><strong>bias_initializer</strong> (str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.initializer</span></code>, optional) – initializer for the bias vector.</p></li>
<li><p><strong>depthwise_regularizer</strong> (str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.initializer</span></code>, optional) – regularization applied to the weights.</p></li>
<li><p><strong>bias_regularizer</strong> (str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.initializer</span></code>, optional) – regularization applied to the bias.</p></li>
<li><p><strong>(</strong><strong>str</strong> (<em>activity_regularizer</em>) – optional): regularization applied to the output of the layer.</p></li>
</ul>
</dd>
</dl>
<p>:param or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.regularizer</span></code>: optional): regularization applied to the output of the layer.
:param : optional): regularization applied to the output of the layer.
:param depthwise_constraint: constraint applied to the weights.
:type depthwise_constraint: str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.initializer</span></code>, optional
:param bias_constraint: constraint applied to the bias.
:type bias_constraint: str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.initializer</span></code>, optional</p>
<p><strong>Methods:</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cnn2snn.QuantizedDepthwiseConv2D.call" title="cnn2snn.QuantizedDepthwiseConv2D.call"><code class="xref py py-obj docutils literal notranslate"><span class="pre">call</span></code></a>(inputs)</p></td>
<td><p>Evaluates input Tensor.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cnn2snn.QuantizedDepthwiseConv2D.get_config" title="cnn2snn.QuantizedDepthwiseConv2D.get_config"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_config</span></code></a>()</p></td>
<td><p>Returns the config of the layer.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cnn2snn.QuantizedDepthwiseConv2D.call">
<code class="sig-name descname">call</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inputs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/cnn2snn/quantization_layers.html#QuantizedDepthwiseConv2D.call"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cnn2snn.QuantizedDepthwiseConv2D.call" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluates input Tensor.</p>
<p>This applies the quantization on weights, then evaluates the input
Tensor and produces the output Tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>inputs</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tensorflow.Tensor</span></code>) – input Tensor.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>output Tensor.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tensorflow.Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="cnn2snn.QuantizedDepthwiseConv2D.get_config">
<code class="sig-name descname">get_config</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/cnn2snn/quantization_layers.html#QuantizedDepthwiseConv2D.get_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cnn2snn.QuantizedDepthwiseConv2D.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Python dictionary.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="quantizeddense">
<h3>QuantizedDense<a class="headerlink" href="#quantizeddense" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="cnn2snn.QuantizedDense">
<em class="property">class </em><code class="sig-prename descclassname">cnn2snn.</code><code class="sig-name descname">QuantizedDense</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/cnn2snn/quantization_layers.html#QuantizedDense"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cnn2snn.QuantizedDense" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.keras.layers.core.Dense</span></code></p>
<p>A quantization-aware Keras dense layer.</p>
<p>Inherits from Keras Dense layer, applying a quantization on weights during
the forward pass.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>units</strong> (<em>int</em>) – the number of neurons.</p></li>
<li><p><strong>use_bias</strong> (<em>boolean</em><em>, </em><em>optional</em>) – whether the layer uses a bias vector.</p></li>
<li><p><strong>quantizer</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">cnn2snn.WeightQuantizer</span></code>) – the quantizer
to apply during the forward pass.</p></li>
<li><p><strong>kernel_initializer</strong> (str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.initializer</span></code>, optional) – initializer for the weights matrix.</p></li>
<li><p><strong>bias_initializer</strong> (str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.initializer</span></code>, optional) – initializer for the bias vector.</p></li>
<li><p><strong>kernel_regularizer</strong> (str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.regularizer</span></code>, optional) – regularization applied to the weights.</p></li>
<li><p><strong>bias_regularizer</strong> (str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.regularizer</span></code>, optional) – regularization applied to the bias.</p></li>
<li><p><strong>(</strong><strong>str</strong> (<em>activity_regularizer</em>) – optional): regularization applied to the output of the layer.</p></li>
</ul>
</dd>
</dl>
<p>:param or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.regularizer</span></code>: optional): regularization applied to the output of the layer.
:param : optional): regularization applied to the output of the layer.
:param kernel_constraint: constraint applied to the weights.
:type kernel_constraint: str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.constraint</span></code>, optional
:param bias_constraint: constraint applied to the bias.
:type bias_constraint: str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.constraint</span></code>, optional</p>
<p><strong>Methods:</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cnn2snn.QuantizedDense.call" title="cnn2snn.QuantizedDense.call"><code class="xref py py-obj docutils literal notranslate"><span class="pre">call</span></code></a>(inputs)</p></td>
<td><p>Evaluates input Tensor.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cnn2snn.QuantizedDense.get_config" title="cnn2snn.QuantizedDense.get_config"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_config</span></code></a>()</p></td>
<td><p>Returns the config of the layer.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cnn2snn.QuantizedDense.call">
<code class="sig-name descname">call</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inputs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/cnn2snn/quantization_layers.html#QuantizedDense.call"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cnn2snn.QuantizedDense.call" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluates input Tensor.</p>
<p>This applies the quantization on weights, then evaluates the input
Tensor and produces the output Tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>inputs</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tensorflow.Tensor</span></code>) – input Tensor.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>output Tensor.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tensorflow.Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="cnn2snn.QuantizedDense.get_config">
<code class="sig-name descname">get_config</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/cnn2snn/quantization_layers.html#QuantizedDense.get_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cnn2snn.QuantizedDense.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Python dictionary.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="quantizedseparableconv2d">
<h3>QuantizedSeparableConv2D<a class="headerlink" href="#quantizedseparableconv2d" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="cnn2snn.QuantizedSeparableConv2D">
<em class="property">class </em><code class="sig-prename descclassname">cnn2snn.</code><code class="sig-name descname">QuantizedSeparableConv2D</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/cnn2snn/quantization_layers.html#QuantizedSeparableConv2D"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cnn2snn.QuantizedSeparableConv2D" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.keras.layers.convolutional.SeparableConv2D</span></code></p>
<p>A quantization-aware Keras separable convolutional layer.</p>
<p>Inherits from Keras SeparableConv2D layer, applying a quantization on
weights during the forward pass.</p>
<p>Creates a quantization-aware separable convolutional layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>filters</strong> (<em>int</em>) – the number of filters.</p></li>
<li><p><strong>kernel_size</strong> (<em>tuple of integer</em>) – the kernel spatial dimensions.</p></li>
<li><p><strong>quantizer</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">cnn2snn.WeightQuantizer</span></code>) – the quantizer to apply
during the forward pass.</p></li>
<li><p><strong>quantizer_dw</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">cnn2snn.WeightQuantizer</span></code>, optional) – the
depthwise quantizer to apply during the forward pass.</p></li>
<li><p><strong>strides</strong> (<em>integer</em><em>, or </em><em>tuple of integers</em><em>, </em><em>optional</em>) – strides of the
convolution along height and width.</p></li>
<li><p><strong>padding</strong> (<em>str</em><em>, </em><em>optional</em>) – One of ‘valid’ or ‘same’.</p></li>
<li><p><strong>use_bias</strong> (<em>boolean</em><em>, </em><em>optional</em>) – Whether the layer uses a bias vector.</p></li>
<li><p><strong>depthwise_initializer</strong> (str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.initializer</span></code>, optional) – initializer for the depthwise kernel.</p></li>
<li><p><strong>pointwise_initializer</strong> (str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.initializer</span></code>, optional) – initializer for the pointwise kernel.</p></li>
<li><p><strong>bias_initializer</strong> (str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.initializer</span></code>, optional) – initializer for the bias vector.</p></li>
<li><p><strong>depthwise_regularizer</strong> (str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.regularizer</span></code>, optional) – regularization applied to the depthwise kernel.</p></li>
<li><p><strong>pointwise_regularizer</strong> (str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.regularizer</span></code>, optional) – regularization applied to the pointwise kernel.</p></li>
<li><p><strong>bias_regularizer</strong> (str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.regularizer</span></code>, optional) – regularization applied to the bias.</p></li>
<li><p><strong>(</strong><strong>str</strong> (<em>activity_regularizer</em>) – optional): regularization applied to the output of the layer.</p></li>
</ul>
</dd>
</dl>
<p>:param or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.regularizer</span></code>: optional): regularization applied to the output of the layer.
:param : optional): regularization applied to the output of the layer.
:param depthwise_constraint: constraint applied to the depthwise kernel.
:type depthwise_constraint: str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.constraint</span></code>, optional
:param pointwise_constraint: constraint applied to the pointwise kernel.
:type pointwise_constraint: str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.constraint</span></code>, optional
:param bias_constraint: constraint applied to the bias.
:type bias_constraint: str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.constraint</span></code>, optional</p>
<p><strong>Methods:</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cnn2snn.QuantizedSeparableConv2D.call" title="cnn2snn.QuantizedSeparableConv2D.call"><code class="xref py py-obj docutils literal notranslate"><span class="pre">call</span></code></a>(inputs)</p></td>
<td><p>Evaluates input Tensor.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cnn2snn.QuantizedSeparableConv2D.get_config" title="cnn2snn.QuantizedSeparableConv2D.get_config"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_config</span></code></a>()</p></td>
<td><p>Returns the config of the layer.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cnn2snn.QuantizedSeparableConv2D.call">
<code class="sig-name descname">call</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inputs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/cnn2snn/quantization_layers.html#QuantizedSeparableConv2D.call"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cnn2snn.QuantizedSeparableConv2D.call" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluates input Tensor.</p>
<p>This applies the quantization on weights, then evaluates the input
Tensor and produces the output Tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>inputs</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tensorflow.Tensor</span></code>) – input Tensor.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a Tensor.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tensorflow.Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="cnn2snn.QuantizedSeparableConv2D.get_config">
<code class="sig-name descname">get_config</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/cnn2snn/quantization_layers.html#QuantizedSeparableConv2D.get_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cnn2snn.QuantizedSeparableConv2D.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Python dictionary.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="quantizedactivation">
<h3>QuantizedActivation<a class="headerlink" href="#quantizedactivation" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="cnn2snn.QuantizedActivation">
<em class="property">class </em><code class="sig-prename descclassname">cnn2snn.</code><code class="sig-name descname">QuantizedActivation</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/cnn2snn/quantization_layers.html#QuantizedActivation"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cnn2snn.QuantizedActivation" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tensorflow.python.keras.engine.base_layer.Layer</span></code></p>
<p>Base class for quantized activation layers.</p>
<p>This base class must be overloaded as well as the three &#64;property
functions:</p>
<ul class="simple">
<li><p><cite>threshold</cite></p></li>
<li><p><cite>step_height</cite></p></li>
<li><p><cite>step_width</cite></p></li>
</ul>
<p>These &#64;property functions must return TensorFlow objects (e.g. tf.Tensor
or tf.Variable) of scalar values. The <cite>.numpy()</cite> method must be callable on
them. They can be fixed at initialization or can be trainable variables.</p>
<p>The CNN2SNN toolkit only support linear quantized activation as defined in
the <cite>quantized_activation</cite> function.</p>
<p>The bitwidth defines the number of quantization levels on which the
activation will be quantized. For instance, a 4-bit quantization gives
15 activation levels. More generally, a n-bit quantization gives 2^n-1
levels.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>bitwidth</strong> (<em>int</em>) – the quantization bitwidth</p>
</dd>
</dl>
<p><strong>Attributes:</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cnn2snn.QuantizedActivation.bitwidth" title="cnn2snn.QuantizedActivation.bitwidth"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bitwidth</span></code></a></p></td>
<td><p>Returns the bitwidth of the quantized activation</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cnn2snn.QuantizedActivation.step_height" title="cnn2snn.QuantizedActivation.step_height"><code class="xref py py-obj docutils literal notranslate"><span class="pre">step_height</span></code></a></p></td>
<td><p>Returns the step height of the quantized activation</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cnn2snn.QuantizedActivation.step_width" title="cnn2snn.QuantizedActivation.step_width"><code class="xref py py-obj docutils literal notranslate"><span class="pre">step_width</span></code></a></p></td>
<td><p>Returns the step width of the quantized activation</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cnn2snn.QuantizedActivation.threshold" title="cnn2snn.QuantizedActivation.threshold"><code class="xref py py-obj docutils literal notranslate"><span class="pre">threshold</span></code></a></p></td>
<td><p>Returns the activation threshold</p></td>
</tr>
</tbody>
</table>
<p><strong>Methods:</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cnn2snn.QuantizedActivation.call" title="cnn2snn.QuantizedActivation.call"><code class="xref py py-obj docutils literal notranslate"><span class="pre">call</span></code></a>(inputs, **kwargs)</p></td>
<td><p>This is where the layer’s logic lives.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cnn2snn.QuantizedActivation.get_config" title="cnn2snn.QuantizedActivation.get_config"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_config</span></code></a>()</p></td>
<td><p>Returns the config of the layer.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cnn2snn.QuantizedActivation.quantized_activation" title="cnn2snn.QuantizedActivation.quantized_activation"><code class="xref py py-obj docutils literal notranslate"><span class="pre">quantized_activation</span></code></a>(x)</p></td>
<td><p>Evaluates the quantized activations for the specified input Tensor.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cnn2snn.QuantizedActivation.bitwidth">
<em class="property">property </em><code class="sig-name descname">bitwidth</code><a class="headerlink" href="#cnn2snn.QuantizedActivation.bitwidth" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the bitwidth of the quantized activation</p>
</dd></dl>

<dl class="py method">
<dt id="cnn2snn.QuantizedActivation.call">
<code class="sig-name descname">call</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inputs</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/cnn2snn/quantization_layers.html#QuantizedActivation.call"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cnn2snn.QuantizedActivation.call" title="Permalink to this definition">¶</a></dt>
<dd><p>This is where the layer’s logic lives.</p>
<p>Note here that <cite>call()</cite> method in <cite>tf.keras</cite> is little bit different
from <cite>keras</cite> API. In <cite>keras</cite> API, you can pass support masking for
layers as additional arguments. Whereas <cite>tf.keras</cite> has <cite>compute_mask()</cite>
method to support masking.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> – Input tensor, or list/tuple of input tensors.</p></li>
<li><p><strong>**kwargs</strong> – Additional keyword arguments. Currently unused.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tensor or list/tuple of tensors.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="cnn2snn.QuantizedActivation.get_config">
<code class="sig-name descname">get_config</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/cnn2snn/quantization_layers.html#QuantizedActivation.get_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cnn2snn.QuantizedActivation.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Python dictionary.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="cnn2snn.QuantizedActivation.quantized_activation">
<code class="sig-name descname">quantized_activation</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/cnn2snn/quantization_layers.html#QuantizedActivation.quantized_activation"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cnn2snn.QuantizedActivation.quantized_activation" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluates the quantized activations for the specified input Tensor.</p>
<p>The quantization is defined by three parameters:</p>
<ul class="simple">
<li><p>the activation threshold, ‘threshold’</p></li>
<li><p>the quantization step size, ‘step_height’</p></li>
<li><p>the step width, ‘step_width’</p></li>
</ul>
<p>For any potential x, the activation output is as follows:</p>
<ul class="simple">
<li><p>if x &lt;= threshold, activation is zero</p></li>
<li><p>if threshold &lt; x &lt;= threshold + step_width, activation is
step_height</p></li>
<li><p>if threshold + step_width &lt; x &lt;= threshold + 2*step_width,
activation is 2*step_height</p></li>
<li><p>and so on…</p></li>
<li><p>if x &gt; threshold + levels*step_width, activation is
levels*step_height</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tensorflow.Tensor</span></code>) – the input values.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="cnn2snn.QuantizedActivation.step_height">
<em class="property">property </em><code class="sig-name descname">step_height</code><a class="headerlink" href="#cnn2snn.QuantizedActivation.step_height" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the step height of the quantized activation</p>
</dd></dl>

<dl class="py method">
<dt id="cnn2snn.QuantizedActivation.step_width">
<em class="property">property </em><code class="sig-name descname">step_width</code><a class="headerlink" href="#cnn2snn.QuantizedActivation.step_width" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the step width of the quantized activation</p>
</dd></dl>

<dl class="py method">
<dt id="cnn2snn.QuantizedActivation.threshold">
<em class="property">property </em><code class="sig-name descname">threshold</code><a class="headerlink" href="#cnn2snn.QuantizedActivation.threshold" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the activation threshold</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="activationdiscreterelu">
<h3>ActivationDiscreteRelu<a class="headerlink" href="#activationdiscreterelu" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="cnn2snn.ActivationDiscreteRelu">
<em class="property">class </em><code class="sig-prename descclassname">cnn2snn.</code><code class="sig-name descname">ActivationDiscreteRelu</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/cnn2snn/quantization_layers.html#ActivationDiscreteRelu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cnn2snn.ActivationDiscreteRelu" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">cnn2snn.quantization_layers.QuantizedActivation</span></code></p>
<p>A discrete ReLU Keras Activation.</p>
<p>Activations will be quantized and will have 2^bitwidth values in the range
[0,6].</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>bitwidth</strong> (<em>int</em>) – the activation bitwidth.</p>
</dd>
</dl>
<p><strong>Attributes:</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cnn2snn.ActivationDiscreteRelu.step_height" title="cnn2snn.ActivationDiscreteRelu.step_height"><code class="xref py py-obj docutils literal notranslate"><span class="pre">step_height</span></code></a></p></td>
<td><p>Returns the step height of the quantized activation</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cnn2snn.ActivationDiscreteRelu.step_width" title="cnn2snn.ActivationDiscreteRelu.step_width"><code class="xref py py-obj docutils literal notranslate"><span class="pre">step_width</span></code></a></p></td>
<td><p>Returns the step width of the quantized activation</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cnn2snn.ActivationDiscreteRelu.threshold" title="cnn2snn.ActivationDiscreteRelu.threshold"><code class="xref py py-obj docutils literal notranslate"><span class="pre">threshold</span></code></a></p></td>
<td><p>Returns the activation threshold</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cnn2snn.ActivationDiscreteRelu.step_height">
<em class="property">property </em><code class="sig-name descname">step_height</code><a class="headerlink" href="#cnn2snn.ActivationDiscreteRelu.step_height" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the step height of the quantized activation</p>
</dd></dl>

<dl class="py method">
<dt id="cnn2snn.ActivationDiscreteRelu.step_width">
<em class="property">property </em><code class="sig-name descname">step_width</code><a class="headerlink" href="#cnn2snn.ActivationDiscreteRelu.step_width" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the step width of the quantized activation</p>
</dd></dl>

<dl class="py method">
<dt id="cnn2snn.ActivationDiscreteRelu.threshold">
<em class="property">property </em><code class="sig-name descname">threshold</code><a class="headerlink" href="#cnn2snn.ActivationDiscreteRelu.threshold" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the activation threshold</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="quantizedrelu">
<h3>QuantizedReLU<a class="headerlink" href="#quantizedrelu" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="cnn2snn.QuantizedReLU">
<em class="property">class </em><code class="sig-prename descclassname">cnn2snn.</code><code class="sig-name descname">QuantizedReLU</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/cnn2snn/quantization_layers.html#QuantizedReLU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#cnn2snn.QuantizedReLU" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">cnn2snn.quantization_layers.QuantizedActivation</span></code></p>
<p>A Trainable Quantized ReLU Keras Activation.</p>
<p>Activations will be clipped to a trainable range, and quantized to a number
of values defined by the bitwidth: N = (2^bitwidth - 1) values plus zero</p>
<p>More specifically, this class uses two trainable variables:</p>
<ul class="simple">
<li><p>threshold represents the lower bound of the activation range,</p></li>
<li><p>step_height represents the step between two quantized activation values.</p></li>
</ul>
<p>The activation range is therefore [threshold, tN_k], with:</p>
<blockquote>
<div><p>tN_k = threshold + N * step_height = (2^bitwidth - 1) * step_height</p>
</div></blockquote>
<p>In other words:</p>
<ul class="simple">
<li><p>inputs below threshold will result in no activation</p></li>
<li><p>inputs between threshold and threshold + tN_k will be ceiled to the nearest
threshold + n * step_height, and result in a activation of n * step_height</p></li>
<li><p>inputs above threshold + tN_k will result in a activation of N * step_height</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>bitwidth</strong> (<em>int</em>) – the activation bitwidth.</p>
</dd>
</dl>
<p><strong>Attributes:</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cnn2snn.QuantizedReLU.step_height" title="cnn2snn.QuantizedReLU.step_height"><code class="xref py py-obj docutils literal notranslate"><span class="pre">step_height</span></code></a></p></td>
<td><p>Returns the step height of the quantized activation</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cnn2snn.QuantizedReLU.step_width" title="cnn2snn.QuantizedReLU.step_width"><code class="xref py py-obj docutils literal notranslate"><span class="pre">step_width</span></code></a></p></td>
<td><p>Returns the step width of the quantized activation</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#cnn2snn.QuantizedReLU.threshold" title="cnn2snn.QuantizedReLU.threshold"><code class="xref py py-obj docutils literal notranslate"><span class="pre">threshold</span></code></a></p></td>
<td><p>Returns the activation threshold</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="cnn2snn.QuantizedReLU.step_height">
<em class="property">property </em><code class="sig-name descname">step_height</code><a class="headerlink" href="#cnn2snn.QuantizedReLU.step_height" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the step height of the quantized activation</p>
</dd></dl>

<dl class="py method">
<dt id="cnn2snn.QuantizedReLU.step_width">
<em class="property">property </em><code class="sig-name descname">step_width</code><a class="headerlink" href="#cnn2snn.QuantizedReLU.step_width" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the step width of the quantized activation</p>
</dd></dl>

<dl class="py method">
<dt id="cnn2snn.QuantizedReLU.threshold">
<em class="property">property </em><code class="sig-name descname">threshold</code><a class="headerlink" href="#cnn2snn.QuantizedReLU.threshold" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the activation threshold</p>
</dd></dl>

</dd></dl>

</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="akida_models_apis.html" class="btn btn-neutral float-right" title="Akida models API" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="aee_apis.html" class="btn btn-neutral float-left" title="Akida Execution Engine API" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020, BrainChip Holdings Ltd. All Rights Reserved.

    </p>
  </div> 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>