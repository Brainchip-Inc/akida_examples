

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>CNN2SNN Toolkit API &mdash; Akida Examples  documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Akida models API" href="akida_models_apis.html" />
    <link rel="prev" title="Akida Execution Engine API" href="aee_apis.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #3f51b5" >
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/akida.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                Akida 1.8.2
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../index.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../installation.html#requirements">Requirements</a></li>
<li class="toctree-l2"><a class="reference internal" href="../installation.html#quick-installation">Quick installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../installation.html#running-examples">Running examples</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../user_guide/user_guide.html">User guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../user_guide/getting_started.html">Getting started</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/getting_started.html#for-beginners">For beginners</a></li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/getting_started.html#for-users-familiar-with-deep-learning">For users familiar with deep-learning</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../user_guide/aee.html">Akida user guide</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/aee.html#the-akida-execution-engine">The Akida Execution Engine</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#id1">1. The Spiking Neural Network model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#id2">2. Input data format</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#id3">3. Determine training mode</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#id4">4. Interpreting outputs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/aee.html#neural-network-model">Neural Network model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#specifying-the-neural-network-model">Specifying the Neural Network model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#saving-and-loading">Saving and loading</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#input-layer-types">Input layer types</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#data-processing-layer-types">Data-Processing layer types</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/aee.html#id5">Using Akida Unsupervised Learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#learning-constraints">Learning constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#compiling-a-layer">Compiling a layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#id7">Learning parameters</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../user_guide/cnn2snn.html">CNN2SNN toolkit</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/cnn2snn.html#overview">Overview</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#conversion-workflow">Conversion Workflow</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#compatibility-constraints">Compatibility Constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#typical-training-scenario">Typical training scenario</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/cnn2snn.html#layers-considerations">Layers Considerations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#supported-layer-types">Supported layer types</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#quantization-aware-layers">Quantization-aware layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#training-only-layers">Training-Only Layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#first-layers">First Layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#id6">Final Layers</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/cnn2snn.html#layer-blocks">Layer Blocks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#id7">Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#conv-block"><code class="docutils literal notranslate"><span class="pre">conv_block</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#dense-block"><code class="docutils literal notranslate"><span class="pre">dense_block</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#separable-conv-block"><code class="docutils literal notranslate"><span class="pre">separable_conv_block</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/cnn2snn.html#tips-and-tricks">Tips and Tricks</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../user_guide/hw_constraints.html">Hardware constraints</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/hw_constraints.html#input-layer">Input layer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/hw_constraints.html#data-processing-layers">Data-Processing layers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/hw_constraints.html#convolutional-layer">Convolutional layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/hw_constraints.html#fully-connected-layer">Fully connected layer</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="api_reference.html">API reference</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="aee_apis.html">Akida Execution Engine</a><ul>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#model">Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#layer">Layer</a></li>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#layerstatistics">LayerStatistics</a></li>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#observer">Observer</a></li>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#inputdata">InputData</a></li>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#inputconvolutional">InputConvolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#inputbcspike">InputBCSpike</a></li>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#fullyconnected">FullyConnected</a></li>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#convolutional">Convolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#separableconvolutional">SeparableConvolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#dense">Dense</a></li>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#sparse">Sparse</a></li>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#coords-to-sparse">coords_to_sparse</a></li>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#dense-to-sparse">dense_to_sparse</a></li>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#packetize">packetize</a></li>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#backendtype">BackendType</a></li>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#convolutionmode">ConvolutionMode</a></li>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#poolingtype">PoolingType</a></li>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#learningtype">LearningType</a></li>
</ul>
</li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">CNN2SNN</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#convert">convert</a></li>
<li class="toctree-l3"><a class="reference internal" href="#check-model-compatibility">check_model_compatibility</a></li>
<li class="toctree-l3"><a class="reference internal" href="#weightquantizer">WeightQuantizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="#weightfloat">WeightFloat</a></li>
<li class="toctree-l3"><a class="reference internal" href="#quantizedconv2d">QuantizedConv2D</a></li>
<li class="toctree-l3"><a class="reference internal" href="#quantizeddepthwiseconv2d">QuantizedDepthwiseConv2D</a></li>
<li class="toctree-l3"><a class="reference internal" href="#quantizeddense">QuantizedDense</a></li>
<li class="toctree-l3"><a class="reference internal" href="#quantizedseparableconv2d">QuantizedSeparableConv2D</a></li>
<li class="toctree-l3"><a class="reference internal" href="#activationdiscreterelu">ActivationDiscreteRelu</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="akida_models_apis.html">Akida models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="akida_models_apis.html#quantization-blocks">Quantization blocks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="akida_models_apis.html#conv-block">conv_block</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida_models_apis.html#separable-conv-block">separable_conv_block</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida_models_apis.html#dense-block">dense_block</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="akida_models_apis.html#model-zoo">Model zoo</a><ul>
<li class="toctree-l4"><a class="reference internal" href="akida_models_apis.html#mobilenet">Mobilenet</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida_models_apis.html#vgg">VGG</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../examples/index.html">Examples</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../examples/plot_gxnor_mnist.html">GXNOR/MNIST inference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_gxnor_mnist.html#loading-the-mnist-dataset">1. Loading the MNIST dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_gxnor_mnist.html#look-at-some-images-from-the-test-dataset">2. Look at some images from the test dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_gxnor_mnist.html#load-the-pre-trained-akida-model">3. Load the pre-trained Akida model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_gxnor_mnist.html#classify-a-single-image">4. Classify a single image</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_gxnor_mnist.html#check-performance-across-a-number-of-samples">5. Check performance across a number of samples</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/plot_regression.html">Regression tutorial</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_regression.html#load-dependencies">1. Load dependencies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_regression.html#load-the-dataset">2. Load the dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_regression.html#create-a-keras-model-satisfying-akida-nsoc-requirements">3. Create a Keras model satisfying Akida NSoC requirements</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_regression.html#check-performance">4. Check performance</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_regression.html#conversion-to-akida">5. Conversion to Akida</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_regression.html#convert-the-trained-keras-model-to-akida">5.1 Convert the trained Keras model to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_regression.html#check-akida-model-accuracy">5.2 Check Akida model accuracy</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_regression.html#estimate-age-on-a-single-image">6. Estimate age on a single image</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/plot_mobilenet_kws.html">MobileNet/KWS inference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_mobilenet_kws.html#load-cnn2snn-tool-dependencies">1. Load CNN2SNN tool dependencies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_mobilenet_kws.html#load-the-preprocessed-dataset">2. Load the preprocessed dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_mobilenet_kws.html#create-a-keras-model-satisfying-akida-nsoc-requirements">3. Create a Keras model satisfying Akida NSoC requirements</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_mobilenet_kws.html#check-performance">4. Check performance</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_mobilenet_kws.html#conversion-to-akida">5. Conversion to Akida</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_mobilenet_kws.html#convert-the-trained-keras-model-to-akida">5.1 Convert the trained Keras model to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_mobilenet_kws.html#check-prediction-accuracy">5.2 Check prediction accuracy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_mobilenet_kws.html#confusion-matrix">5.3 Confusion matrix</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/plot_vgg_mobilenet_cifar10.html">VGG and MobileNet/CIFAR10 inference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_vgg_mobilenet_cifar10.html#load-cnn2snn-tool-dependencies">1. Load CNN2SNN tool dependencies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_vgg_mobilenet_cifar10.html#load-and-reshape-cifar10-dataset">2. Load and reshape CIFAR10 dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_vgg_mobilenet_cifar10.html#create-a-quantized-keras-vgg-model">3. Create a quantized Keras VGG model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_vgg_mobilenet_cifar10.html#a-instantiate-keras-model">3.A Instantiate Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_vgg_mobilenet_cifar10.html#b-check-performance">3.B Check performance</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_vgg_mobilenet_cifar10.html#conversion-to-akida">4. Conversion to Akida</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_vgg_mobilenet_cifar10.html#a-convert-to-akida-model">4.A Convert to Akida model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_vgg_mobilenet_cifar10.html#b-check-hardware-compliancy">4.B Check hardware compliancy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_vgg_mobilenet_cifar10.html#c-check-performance">4.C Check performance</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_vgg_mobilenet_cifar10.html#create-a-quantized-keras-mobilenet-model">5. Create a quantized Keras MobileNet model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_vgg_mobilenet_cifar10.html#a-instantiate-keras-mobilenet-model">5.A Instantiate Keras MobileNet model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_vgg_mobilenet_cifar10.html#id1">5.B Check performance</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_vgg_mobilenet_cifar10.html#id2">6. Conversion to Akida</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_vgg_mobilenet_cifar10.html#id3">6.A Convert to Akida model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_vgg_mobilenet_cifar10.html#id4">6.B Check hardware compliancy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_vgg_mobilenet_cifar10.html#id5">6.C Check performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_vgg_mobilenet_cifar10.html#d-show-predictions-for-a-random-image">6D. Show predictions for a random image</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/plot_transfer_learning.html">Transfer learning with MobileNet for cats vs. dogs</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_transfer_learning.html#transfer-learning-process">1. Transfer learning process</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_transfer_learning.html#load-and-preprocess-data">2. Load and preprocess data</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_transfer_learning.html#a-load-and-split-data">2.A - Load and split data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_transfer_learning.html#b-preprocess-the-test-set">2.B - Preprocess the test set</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_transfer_learning.html#c-get-labels">2.C - Get labels</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_transfer_learning.html#convert-a-quantized-keras-model-to-akida">3. Convert a quantized Keras model to Akida</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_transfer_learning.html#a-instantiate-a-keras-base-model">3.A - Instantiate a Keras base model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_transfer_learning.html#b-modify-the-network-and-load-pre-trained-weights">3.B - Modify the network and load pre-trained weights</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_transfer_learning.html#c-convert-to-akida">3.C - Convert to Akida</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_transfer_learning.html#classify-test-images">4. Classify test images</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_transfer_learning.html#a-classify-test-images">4.A Classify test images</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_transfer_learning.html#b-compare-results">4.B Compare results</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/plot_mobilenet_imagenet.html">MobileNet/ImageNet inference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_mobilenet_imagenet.html#load-cnn2snn-tool-dependencies">1. Load CNN2SNN tool dependencies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_mobilenet_imagenet.html#load-test-images-from-imagenet">2. Load test images from ImageNet</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_mobilenet_imagenet.html#load-test-images-and-preprocess-test-images">2.1 Load test images and preprocess test images</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_mobilenet_imagenet.html#load-labels">2.2 Load labels</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_mobilenet_imagenet.html#create-a-quantized-keras-model">3. Create a quantized Keras model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_mobilenet_imagenet.html#instantiate-keras-model">3.1 Instantiate Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_mobilenet_imagenet.html#check-performance-of-the-keras-model">3.2 Check performance of the Keras model</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_mobilenet_imagenet.html#convert-keras-model-for-akida-nsoc">4. Convert Keras model for Akida NSoC</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_mobilenet_imagenet.html#convert-keras-model-to-an-akida-compatible-model">4.1 Convert Keras model to an Akida compatible model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_mobilenet_imagenet.html#test-performance-of-the-akida-model">4.2 Test performance of the Akida model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_mobilenet_imagenet.html#show-predictions-for-a-random-test-image">4.3 Show predictions for a random test image</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/plot_cnn_flow.html">CNN conversion flow tutorial</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_cnn_flow.html#system-configuration">1. System configuration</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_cnn_flow.html#load-cnn2snn-tool-dependencies">1.1 Load CNN2SNN tool dependencies</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_cnn_flow.html#load-and-reshape-mnist-dataset">1.2 Load and reshape MNIST dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_cnn_flow.html#set-training-parameters">1.3 Set training parameters</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_cnn_flow.html#model-creation-and-performance-check">2. Model creation and performance check</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_cnn_flow.html#model-creation">2.1 Model creation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_cnn_flow.html#performance-check">2.2 Performance check</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_cnn_flow.html#model-akida-compatibility-check-and-changes">3. Model Akida-compatibility check and changes</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_cnn_flow.html#compatibility-check">3.1 Compatibility check</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_cnn_flow.html#model-adaptation">3.2 Model adaptation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_cnn_flow.html#id1">3.3 Performance check</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_cnn_flow.html#model-quantization-and-training">4. Model quantization and training</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_cnn_flow.html#quantize-the-model">4.1 Quantize the model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_cnn_flow.html#id2">4.2 Performance check</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_cnn_flow.html#convert-trained-model-for-akida-and-test">5. Convert trained model for Akida and test</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_cnn_flow.html#final-conversion">5.1 Final conversion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_cnn_flow.html#performances-check-with-the-akida-execution-engine">5.2 Performances check with the Akida Execution Engine</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/Brainchip-Inc/akida_examples/releases">Changelog</a></li>
<li class="toctree-l1"><a class="reference external" href="https://support.brainchip.com/portal/home">Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Akida Examples</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="api_reference.html">API reference</a> &raquo;</li>
        
      <li>CNN2SNN Toolkit API</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="module-cnn2snn">
<span id="cnn2snn-toolkit-api"></span><h1>CNN2SNN Toolkit API<a class="headerlink" href="#module-cnn2snn" title="Permalink to this headline">¶</a></h1>
<div class="section" id="convert">
<h2>convert<a class="headerlink" href="#convert" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="cnn2snn.convert">
<code class="sig-prename descclassname">cnn2snn.</code><code class="sig-name descname">convert</code><span class="sig-paren">(</span><em class="sig-param">model</em>, <em class="sig-param">file_path=None</em>, <em class="sig-param">input_scaling=(1.0</em>, <em class="sig-param">0)</em>, <em class="sig-param">input_is_sparse=False</em><span class="sig-paren">)</span><a class="headerlink" href="#cnn2snn.convert" title="Permalink to this definition">¶</a></dt>
<dd><p>Simple function to convert a Keras model to an Akida one.</p>
<p>These steps are performed:</p>
<ol class="arabic simple">
<li><p>Merge the depthwise+conv layers into a separable_conv one.</p></li>
<li><p>Generate an Akida model based on that model.</p></li>
<li><p>Convert weights from the Keras model to Akida.</p></li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The relationship between Keras and Akida inputs is:
input_akida = alpha * input_keras + beta, optional</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.Model</span></code>) – a tf.keras model</p></li>
<li><p><strong>file_path</strong> (<em>str</em><em>, </em><em>optional</em>) – destination for the akida model.
(Default value = None)</p></li>
<li><p><strong>input_scaling</strong> (<em>2 elements tuple</em><em>, </em><em>optional</em>) – value of the input scaling.
(Default value = (1.0,0))</p></li>
<li><p><strong>input_is_sparse</strong> (<em>bool</em><em>, </em><em>optional</em>) – if True, input will be an InputData layer,
otherwise it will be InputConvolutional. (Default value = False)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>an Akida model.</p>
</dd>
</dl>
</dd></dl>

<p>A detailed description of the input_scaling parameter is given in the
<a class="reference external" href="../user_guide/cnn2snn.html#input-scaling">user guide</a>.</p>
</div>
<div class="section" id="check-model-compatibility">
<h2>check_model_compatibility<a class="headerlink" href="#check-model-compatibility" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="cnn2snn.check_model_compatibility">
<code class="sig-prename descclassname">cnn2snn.</code><code class="sig-name descname">check_model_compatibility</code><span class="sig-paren">(</span><em class="sig-param">model_keras</em>, <em class="sig-param">input_is_sparse</em><span class="sig-paren">)</span><a class="headerlink" href="#cnn2snn.check_model_compatibility" title="Permalink to this definition">¶</a></dt>
<dd><p>Checks if a Keras quantized model is compatible for cnn2snn conversion.</p>
<p>This function doesn’t convert the Keras quantized model to an Akida model
but only checks if the model is compatible. The checks are performed at two
different levels:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Some checks are done when the Keras model is scanned, during the
generation of the model map.</p></li>
<li><p>Other checks are then done based on the model map.</p></li>
</ol>
</div></blockquote>
<p>Note that this function doesn’t check if the quantization bitwidths (weights
or activations) are supported by the Akida Execution Engine or by the Akida
NSoC.</p>
<p><strong>1. How to build a compatible Keras quantized model?</strong></p>
<p>The following lines give details and constraints on how to build a Keras
model compatible for the conversion to an Akida model.</p>
<p><strong>2. General information about layers</strong></p>
<p>An Akida layer must be seen as a block of Keras layers starting with a
processing layer (QuantizedConv2D, QuantizedSeparableConv2D,
QuantizedDense). All blocks of Keras layers except the last block must have
exactly one quantized activation layer (ActivationDiscreteRelu). Other
optional layers can be present in a block such as a pooling layer or a
batch normalization layer.
Here are all the supported Keras layers for an Akida-compatible model:</p>
<ul class="simple">
<li><p>Processing layers:</p>
<ul>
<li><p>cnn2snn.QuantizedConv2D</p></li>
<li><p>cnn2snn.QuantizedSeparableConv2D</p></li>
<li><p>cnn2snn.QuantizedDense</p></li>
</ul>
</li>
<li><p>Activation layers:</p>
<ul>
<li><p>cnn2snn.ActivationDiscreteRelu</p></li>
<li><p>any increasing activation function (only for the last block of layers)
such as softmax, sigmoid set as last layer. This layer must derive from
tf.keras.layers.Activation, and it will be removed during conversion to
Akida-compatible model.</p></li>
</ul>
</li>
<li><p>Pooling layers:</p>
<ul>
<li><p>MaxPool2D</p></li>
<li><p>GlobalAvgPool2D</p></li>
</ul>
</li>
<li><p>BatchNormalization</p></li>
<li><p>Dropout</p></li>
<li><p>Flatten</p></li>
<li><p>Input</p></li>
<li><p>Reshape</p></li>
</ul>
<p>Example of a block of Keras layers:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>   <span class="o">-------------------</span>
   <span class="o">|</span> <span class="n">QuantizedConv2D</span> <span class="o">|</span>
   <span class="o">-------------------</span>
           <span class="o">||</span>
           \<span class="o">/</span>
 <span class="o">----------------------</span>
 <span class="o">|</span> <span class="n">BatchNormalization</span> <span class="o">|</span>
 <span class="o">----------------------</span>
           <span class="o">||</span>
           \<span class="o">/</span>
      <span class="o">-------------</span>
      <span class="o">|</span> <span class="n">MaxPool2D</span> <span class="o">|</span>
      <span class="o">-------------</span>
           <span class="o">||</span>
           \<span class="o">/</span>
<span class="o">--------------------------</span>
<span class="o">|</span> <span class="n">ActivationDiscreteRelu</span> <span class="o">|</span>
<span class="o">--------------------------</span>
</pre></div>
</div>
<p><strong>3. Constraints about inputs</strong></p>
<p>An Akida model can accept two types of inputs: sparse events or 8-bit
images. Whatever the input type, the Keras inputs must respect the
following relation:</p>
<blockquote>
<div><p>input_akida = scale * input_keras + shift</p>
</div></blockquote>
<p>where the Akida inputs must be positive integers, the input scale must be
a float value and the input shift must be an integer. In other words,
scale * input_keras must be integers.</p>
<p>Depending on the input type:</p>
<ul class="simple">
<li><p>if the inputs are events (sparse), the first layer of the Keras model can
be any quantized processing layer. The input shift must be zero.</p></li>
<li><p>if the inputs are images, the first layer must be a QuantizedConv2D
layer.</p></li>
</ul>
<p><strong>4. Constraints about layers’ parameters</strong></p>
<p>To be Akida-compatible, the Keras layers must observe the following rules:</p>
<ul class="simple">
<li><p>all layers with the ‘data_format’ parameter must be ‘channels_last’</p></li>
<li><p>all processing quantized layers and ActivationDiscreteRelu must have a
valid quantization bitwidth</p></li>
<li><p>a QuantizedConv2D layer must have strides of (1, 1). Exception: if the
inputs are images, the first convolutional layer can have any value
for strides.</p></li>
<li><p>a QuantizedDense layer must have an input shape of (N,) or (1, 1, N)</p></li>
<li><p>a BatchNormalization layer must have ‘axis’ set to -1 (default)</p></li>
<li><p>a BatchNormalization layer cannot have negative gammas</p></li>
<li><p>Reshape layers can only be used to transform a tensor of shape (N,) to a
tensor of shape (1, 1, N), and vice-versa</p></li>
<li><p>only one pooling layer can be used in each block</p></li>
<li><p>a MaxPool2D layer must have the same ‘padding’ as the corresponding
processing quantized layer</p></li>
</ul>
<p><strong>5. Constraints about the order of layers</strong></p>
<p>To be Akida-compatible, the order of Keras layers must observe the following
rules:</p>
<ul class="simple">
<li><p>a block of Keras layers must start with a processing quantized layer</p></li>
<li><p>where present, a BatchNormalization/GlobalAvgPool2D layer must be placed
before the activation</p></li>
<li><p>a Flatten layer can only be used before a QuantizedDense layer</p></li>
<li><p>an Activation layer other than ActivationDiscreteRelu can only be used
in the last layer</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>tf.keras model</em>) – the model to parse.</p></li>
<li><p><strong>input_is_sparse</strong> (<em>bool</em>) – if True, input will be an inputData layer,
otherwise it will be inputConvolutional.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="weightquantizer">
<h2>WeightQuantizer<a class="headerlink" href="#weightquantizer" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="cnn2snn.WeightQuantizer">
<em class="property">class </em><code class="sig-prename descclassname">cnn2snn.</code><code class="sig-name descname">WeightQuantizer</code><span class="sig-paren">(</span><em class="sig-param">threshold=3</em>, <em class="sig-param">bitwidth=4</em><span class="sig-paren">)</span><a class="headerlink" href="#cnn2snn.WeightQuantizer" title="Permalink to this definition">¶</a></dt>
<dd><p>A uniform quantizer.</p>
<p>Quantizes the specified weights into 2^bitwidth-1 values centered on zero.
E.g. with bitwidth = 4, 15 quantization levels: from -7 * qstep to 7 * qstep
with qstep being the quantization step. The quantization step is defined by:</p>
<p>qstep = threshold * std(W) / max_value</p>
<p>with max_value being 2^(bitwidth-1) - 1. E.g with bitwidth = 4, max_value = 7.</p>
<p>All values below or above threshold * std(W) are automatically assigned to
the min (resp max) value.</p>
<p><strong>Methods</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cnn2snn.WeightQuantizer.quantize" title="cnn2snn.WeightQuantizer.quantize"><code class="xref py py-obj docutils literal notranslate"><span class="pre">quantize</span></code></a>(w)</p></td>
<td><p>Quantizes the specified weights Tensor.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cnn2snn.WeightQuantizer.scale_factor" title="cnn2snn.WeightQuantizer.scale_factor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">scale_factor</span></code></a>(w)</p></td>
<td><p>Evaluates the scale factor for the specified weights Tensor.</p></td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="cnn2snn.WeightQuantizer.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">threshold=3</em>, <em class="sig-param">bitwidth=4</em><span class="sig-paren">)</span><a class="headerlink" href="#cnn2snn.WeightQuantizer.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a Weights quantizer for the specified bitwidth.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>threshold</strong> (<em>integer</em>) – the standard deviation multiplier used to exclude
outliers.</p></li>
<li><p><strong>bitwidth</strong> (<em>integer</em>) – the quantizer bitwidth defining the number of
quantized values.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="cnn2snn.WeightQuantizer.quantize">
<code class="sig-name descname">quantize</code><span class="sig-paren">(</span><em class="sig-param">w</em><span class="sig-paren">)</span><a class="headerlink" href="#cnn2snn.WeightQuantizer.quantize" title="Permalink to this definition">¶</a></dt>
<dd><p>Quantizes the specified weights Tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>w</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tensorflow.Tensor</span></code>) – the weights Tensor to quantize.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a Tensor of quantized weights.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tensorflow.Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="cnn2snn.WeightQuantizer.scale_factor">
<code class="sig-name descname">scale_factor</code><span class="sig-paren">(</span><em class="sig-param">w</em><span class="sig-paren">)</span><a class="headerlink" href="#cnn2snn.WeightQuantizer.scale_factor" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluates the scale factor for the specified weights Tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>w</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tensorflow.Tensor</span></code>) – the weights Tensor to quantize.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a Tensor containing a single scalar value.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tensorflow.Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="weightfloat">
<h2>WeightFloat<a class="headerlink" href="#weightfloat" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="cnn2snn.WeightFloat">
<em class="property">class </em><code class="sig-prename descclassname">cnn2snn.</code><code class="sig-name descname">WeightFloat</code><a class="headerlink" href="#cnn2snn.WeightFloat" title="Permalink to this definition">¶</a></dt>
<dd><p>This quantizer actually does not perform any quantization, and it might
be used for training.</p>
<p><strong>Methods</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cnn2snn.WeightFloat.quantize" title="cnn2snn.WeightFloat.quantize"><code class="xref py py-obj docutils literal notranslate"><span class="pre">quantize</span></code></a>(w)</p></td>
<td><p>Quantizes the specified weights Tensor.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cnn2snn.WeightFloat.scale_factor" title="cnn2snn.WeightFloat.scale_factor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">scale_factor</span></code></a>(w)</p></td>
<td><p>Evaluates the scale factor for the specified weights Tensor.</p></td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="cnn2snn.WeightFloat.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#cnn2snn.WeightFloat.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a Weights quantizer for the specified bitwidth.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>bitwidth</strong> (<em>integer</em>) – the quantizer bitwidth defining the number of
quantized values.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="cnn2snn.WeightFloat.quantize">
<code class="sig-name descname">quantize</code><span class="sig-paren">(</span><em class="sig-param">w</em><span class="sig-paren">)</span><a class="headerlink" href="#cnn2snn.WeightFloat.quantize" title="Permalink to this definition">¶</a></dt>
<dd><p>Quantizes the specified weights Tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>w</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tensorflow.Tensor</span></code>) – the weights Tensor to quantize.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a Tensor of quantized weights.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tensorflow.Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="cnn2snn.WeightFloat.scale_factor">
<code class="sig-name descname">scale_factor</code><span class="sig-paren">(</span><em class="sig-param">w</em><span class="sig-paren">)</span><a class="headerlink" href="#cnn2snn.WeightFloat.scale_factor" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluates the scale factor for the specified weights Tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>w</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tensorflow.Tensor</span></code>) – the weights Tensor to quantize.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a Tensor containing a single scalar value.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tensorflow.Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="quantizedconv2d">
<h2>QuantizedConv2D<a class="headerlink" href="#quantizedconv2d" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="cnn2snn.QuantizedConv2D">
<em class="property">class </em><code class="sig-prename descclassname">cnn2snn.</code><code class="sig-name descname">QuantizedConv2D</code><span class="sig-paren">(</span><em class="sig-param">filters</em>, <em class="sig-param">kernel_size</em>, <em class="sig-param">strides=(1</em>, <em class="sig-param">1)</em>, <em class="sig-param">padding='valid'</em>, <em class="sig-param">use_bias=True</em>, <em class="sig-param">kernel_initializer='glorot_uniform'</em>, <em class="sig-param">bias_initializer='zeros'</em>, <em class="sig-param">kernel_regularizer=None</em>, <em class="sig-param">bias_regularizer=None</em>, <em class="sig-param">kernel_constraint=None</em>, <em class="sig-param">bias_constraint=None</em>, <em class="sig-param">quantizer=&lt;cnn2snn.quantization_ops.WeightFloat object&gt;</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#cnn2snn.QuantizedConv2D" title="Permalink to this definition">¶</a></dt>
<dd><p>A quantization-aware Keras convolutional layer.</p>
<p>Inherits from Keras Conv2D layer, applying a quantization on weights during
the forward pass.</p>
<p><strong>Methods</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cnn2snn.QuantizedConv2D.call" title="cnn2snn.QuantizedConv2D.call"><code class="xref py py-obj docutils literal notranslate"><span class="pre">call</span></code></a>(inputs)</p></td>
<td><p>Evaluates input Tensor.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cnn2snn.QuantizedConv2D.get_config" title="cnn2snn.QuantizedConv2D.get_config"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_config</span></code></a>()</p></td>
<td><p>Returns the config of the layer.</p></td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="cnn2snn.QuantizedConv2D.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">filters</em>, <em class="sig-param">kernel_size</em>, <em class="sig-param">strides=(1</em>, <em class="sig-param">1)</em>, <em class="sig-param">padding='valid'</em>, <em class="sig-param">use_bias=True</em>, <em class="sig-param">kernel_initializer='glorot_uniform'</em>, <em class="sig-param">bias_initializer='zeros'</em>, <em class="sig-param">kernel_regularizer=None</em>, <em class="sig-param">bias_regularizer=None</em>, <em class="sig-param">kernel_constraint=None</em>, <em class="sig-param">bias_constraint=None</em>, <em class="sig-param">quantizer=&lt;cnn2snn.quantization_ops.WeightFloat object&gt;</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#cnn2snn.QuantizedConv2D.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a quantization-aware convolutional layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>filters</strong> (<em>integer</em>) – the number of filters.</p></li>
<li><p><strong>kernel_size</strong> (<em>tuple of integer</em>) – the kernel spatial dimensions.</p></li>
<li><p><strong>strides</strong> (<em>integer</em><em>, or </em><em>tuple of integers</em><em>, </em><em>optional</em>) – strides of the
convolution along height and width.</p></li>
<li><p><strong>padding</strong> (<em>str</em><em>, </em><em>optional</em>) – one of ‘valid’ or ‘same’.</p></li>
<li><p><strong>use_bias</strong> (<em>boolean</em><em>, </em><em>optional</em>) – whether the layer uses a bias vector.</p></li>
<li><p><strong>kernel_initializer</strong> (str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.initializer</span></code>, optional) – initializer for the weights matrix.</p></li>
<li><p><strong>bias_initializer</strong> (str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.initializer</span></code>, optional) – initializer for the bias vector.</p></li>
<li><p><strong>kernel_regularizer</strong> (str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.regularizer</span></code>, optional) – regularization applied to the weights.</p></li>
<li><p><strong>bias_regularizer</strong> (str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.regularizer</span></code>, optional) – regularization applied to the bias.</p></li>
<li><p><strong>kernel_constraint</strong> (str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.constraint</span></code>, optional) – constraint applied to the weights.</p></li>
<li><p><strong>bias_constraint</strong> (str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.constraint</span></code>, optional) – constraint applied to the bias.</p></li>
<li><p><strong>quantizer</strong> (<a class="reference internal" href="#cnn2snn.WeightQuantizer" title="cnn2snn.WeightQuantizer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cnn2snn.WeightQuantizer</span></code></a>, optional) – the quantizer
to apply during the forward pass.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="cnn2snn.QuantizedConv2D.call">
<code class="sig-name descname">call</code><span class="sig-paren">(</span><em class="sig-param">inputs</em><span class="sig-paren">)</span><a class="headerlink" href="#cnn2snn.QuantizedConv2D.call" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluates input Tensor.</p>
<p>This applies the quantization on weights, then evaluates the input
Tensor and produces the output Tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>inputs</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tensorflow.Tensor</span></code>) – input Tensor.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>output Tensor.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tensorflow.Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="cnn2snn.QuantizedConv2D.get_config">
<code class="sig-name descname">get_config</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#cnn2snn.QuantizedConv2D.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Python dictionary.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="quantizeddepthwiseconv2d">
<h2>QuantizedDepthwiseConv2D<a class="headerlink" href="#quantizeddepthwiseconv2d" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="cnn2snn.QuantizedDepthwiseConv2D">
<em class="property">class </em><code class="sig-prename descclassname">cnn2snn.</code><code class="sig-name descname">QuantizedDepthwiseConv2D</code><span class="sig-paren">(</span><em class="sig-param">kernel_size</em>, <em class="sig-param">padding='valid'</em>, <em class="sig-param">use_bias=True</em>, <em class="sig-param">depthwise_initializer='glorot_uniform'</em>, <em class="sig-param">bias_initializer='zeros'</em>, <em class="sig-param">depthwise_regularizer=None</em>, <em class="sig-param">bias_regularizer=None</em>, <em class="sig-param">depthwise_constraint=None</em>, <em class="sig-param">bias_constraint=None</em>, <em class="sig-param">quantizer=&lt;cnn2snn.quantization_ops.WeightFloat object&gt;</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#cnn2snn.QuantizedDepthwiseConv2D" title="Permalink to this definition">¶</a></dt>
<dd><p>A quantization-aware Keras depthwise convolutional layer.</p>
<p>Inherits from Keras DepthwiseConv2D layer, applying a quantization on
weights during the forward pass.</p>
<p><strong>Methods</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cnn2snn.QuantizedDepthwiseConv2D.call" title="cnn2snn.QuantizedDepthwiseConv2D.call"><code class="xref py py-obj docutils literal notranslate"><span class="pre">call</span></code></a>(inputs)</p></td>
<td><p>Evaluates input Tensor.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cnn2snn.QuantizedDepthwiseConv2D.get_config" title="cnn2snn.QuantizedDepthwiseConv2D.get_config"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_config</span></code></a>()</p></td>
<td><p>Returns the config of the layer.</p></td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="cnn2snn.QuantizedDepthwiseConv2D.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">kernel_size</em>, <em class="sig-param">padding='valid'</em>, <em class="sig-param">use_bias=True</em>, <em class="sig-param">depthwise_initializer='glorot_uniform'</em>, <em class="sig-param">bias_initializer='zeros'</em>, <em class="sig-param">depthwise_regularizer=None</em>, <em class="sig-param">bias_regularizer=None</em>, <em class="sig-param">depthwise_constraint=None</em>, <em class="sig-param">bias_constraint=None</em>, <em class="sig-param">quantizer=&lt;cnn2snn.quantization_ops.WeightFloat object&gt;</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#cnn2snn.QuantizedDepthwiseConv2D.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a quantization-aware depthwise convolutional layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>kernel_size</strong> (<em>a tuple of integer</em>) – the kernel spatial dimensions.</p></li>
<li><p><strong>padding</strong> (<em>str</em><em>, </em><em>optional</em>) – One of ‘valid’ or ‘same’.</p></li>
<li><p><strong>use_bias</strong> (<em>boolean</em><em>, </em><em>optional</em>) – whether the layer uses a bias vector.</p></li>
<li><p><strong>depthwise_initializer</strong> (str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.initializer</span></code>, optional) – initializer for the weights matrix.</p></li>
<li><p><strong>bias_initializer</strong> (str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.initializer</span></code>, optional) – initializer for the bias vector.</p></li>
<li><p><strong>depthwise_regularizer</strong> (str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.initializer</span></code>, optional) – regularization applied to the weights.</p></li>
<li><p><strong>bias_regularizer</strong> (str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.initializer</span></code>, optional) – regularization applied to the bias.</p></li>
<li><p><strong>depthwise_constraint</strong> (str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.initializer</span></code>, optional) – constraint applied to the weights.</p></li>
<li><p><strong>bias_constraint</strong> (str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.initializer</span></code>, optional) – constraint applied to the bias.</p></li>
<li><p><strong>quantizer</strong> (<a class="reference internal" href="#cnn2snn.WeightQuantizer" title="cnn2snn.WeightQuantizer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cnn2snn.WeightQuantizer</span></code></a>, optional) – the quantizer
to apply during the forward pass.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="cnn2snn.QuantizedDepthwiseConv2D.call">
<code class="sig-name descname">call</code><span class="sig-paren">(</span><em class="sig-param">inputs</em><span class="sig-paren">)</span><a class="headerlink" href="#cnn2snn.QuantizedDepthwiseConv2D.call" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluates input Tensor.</p>
<p>This applies the quantization on weights, then evaluates the input
Tensor and produces the output Tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>inputs</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tensorflow.Tensor</span></code>) – input Tensor.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>output Tensor.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tensorflow.Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="cnn2snn.QuantizedDepthwiseConv2D.get_config">
<code class="sig-name descname">get_config</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#cnn2snn.QuantizedDepthwiseConv2D.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Python dictionary.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="quantizeddense">
<h2>QuantizedDense<a class="headerlink" href="#quantizeddense" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="cnn2snn.QuantizedDense">
<em class="property">class </em><code class="sig-prename descclassname">cnn2snn.</code><code class="sig-name descname">QuantizedDense</code><span class="sig-paren">(</span><em class="sig-param">units</em>, <em class="sig-param">use_bias=True</em>, <em class="sig-param">kernel_initializer='glorot_uniform'</em>, <em class="sig-param">bias_initializer='zeros'</em>, <em class="sig-param">kernel_regularizer=None</em>, <em class="sig-param">bias_regularizer=None</em>, <em class="sig-param">kernel_constraint=None</em>, <em class="sig-param">bias_constraint=None</em>, <em class="sig-param">quantizer=&lt;cnn2snn.quantization_ops.WeightFloat object&gt;</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#cnn2snn.QuantizedDense" title="Permalink to this definition">¶</a></dt>
<dd><p>A quantization-aware Keras dense layer.</p>
<p>Inherits from Keras Dense layer, applying a quantization on weights during
the forward pass.</p>
<p><strong>Methods</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cnn2snn.QuantizedDense.call" title="cnn2snn.QuantizedDense.call"><code class="xref py py-obj docutils literal notranslate"><span class="pre">call</span></code></a>(inputs)</p></td>
<td><p>Evaluates input Tensor.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cnn2snn.QuantizedDense.get_config" title="cnn2snn.QuantizedDense.get_config"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_config</span></code></a>()</p></td>
<td><p>Returns the config of the layer.</p></td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="cnn2snn.QuantizedDense.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">units</em>, <em class="sig-param">use_bias=True</em>, <em class="sig-param">kernel_initializer='glorot_uniform'</em>, <em class="sig-param">bias_initializer='zeros'</em>, <em class="sig-param">kernel_regularizer=None</em>, <em class="sig-param">bias_regularizer=None</em>, <em class="sig-param">kernel_constraint=None</em>, <em class="sig-param">bias_constraint=None</em>, <em class="sig-param">quantizer=&lt;cnn2snn.quantization_ops.WeightFloat object&gt;</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#cnn2snn.QuantizedDense.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a quantization-aware dense layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>units</strong> (<em>integer</em>) – the number of neurons.</p></li>
<li><p><strong>use_bias</strong> (<em>boolean</em><em>, </em><em>optional</em>) – whether the layer uses a bias vector.</p></li>
<li><p><strong>kernel_initializer</strong> (str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.initializer</span></code>, optional) – initializer for the weights matrix.</p></li>
<li><p><strong>bias_initializer</strong> (str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.initializer</span></code>, optional) – initializer for the bias vector.</p></li>
<li><p><strong>kernel_regularizer</strong> (str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.regularizer</span></code>, optional) – regularization applied to the weights.</p></li>
<li><p><strong>bias_regularizer</strong> (str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.regularizer</span></code>, optional) – regularization applied to the bias.</p></li>
<li><p><strong>kernel_constraint</strong> (str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.constraint</span></code>, optional) – constraint applied to the weights.</p></li>
<li><p><strong>bias_constraint</strong> (str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.constraint</span></code>, optional) – constraint applied to the bias.</p></li>
<li><p><strong>quantizer</strong> (<a class="reference internal" href="#cnn2snn.WeightQuantizer" title="cnn2snn.WeightQuantizer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cnn2snn.WeightQuantizer</span></code></a>, optional) – the quantizer
to apply during the forward pass.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="cnn2snn.QuantizedDense.call">
<code class="sig-name descname">call</code><span class="sig-paren">(</span><em class="sig-param">inputs</em><span class="sig-paren">)</span><a class="headerlink" href="#cnn2snn.QuantizedDense.call" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluates input Tensor.</p>
<p>This applies the quantization on weights, then evaluates the input
Tensor and produces the output Tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>inputs</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tensorflow.Tensor</span></code>) – input Tensor.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>output Tensor.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tensorflow.Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="cnn2snn.QuantizedDense.get_config">
<code class="sig-name descname">get_config</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#cnn2snn.QuantizedDense.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Python dictionary.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="quantizedseparableconv2d">
<h2>QuantizedSeparableConv2D<a class="headerlink" href="#quantizedseparableconv2d" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="cnn2snn.QuantizedSeparableConv2D">
<em class="property">class </em><code class="sig-prename descclassname">cnn2snn.</code><code class="sig-name descname">QuantizedSeparableConv2D</code><span class="sig-paren">(</span><em class="sig-param">filters</em>, <em class="sig-param">kernel_size</em>, <em class="sig-param">padding='valid'</em>, <em class="sig-param">use_bias=True</em>, <em class="sig-param">depthwise_initializer='glorot_uniform'</em>, <em class="sig-param">pointwise_initializer='glorot_uniform'</em>, <em class="sig-param">bias_initializer='zeros'</em>, <em class="sig-param">depthwise_regularizer=None</em>, <em class="sig-param">pointwise_regularizer=None</em>, <em class="sig-param">bias_regularizer=None</em>, <em class="sig-param">depthwise_constraint=None</em>, <em class="sig-param">pointwise_constraint=None</em>, <em class="sig-param">bias_constraint=None</em>, <em class="sig-param">quantizer=&lt;cnn2snn.quantization_ops.WeightFloat object&gt;</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#cnn2snn.QuantizedSeparableConv2D" title="Permalink to this definition">¶</a></dt>
<dd><p>A quantization-aware Keras separable convolutional layer.</p>
<p>Inherits from Keras SeparableConv2D layer, applying a quantization on
weights during the forward pass.</p>
<p><strong>Methods</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cnn2snn.QuantizedSeparableConv2D.call" title="cnn2snn.QuantizedSeparableConv2D.call"><code class="xref py py-obj docutils literal notranslate"><span class="pre">call</span></code></a>(inputs)</p></td>
<td><p>Evaluates input Tensor.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cnn2snn.QuantizedSeparableConv2D.get_config" title="cnn2snn.QuantizedSeparableConv2D.get_config"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_config</span></code></a>()</p></td>
<td><p>Returns the config of the layer.</p></td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="cnn2snn.QuantizedSeparableConv2D.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">filters</em>, <em class="sig-param">kernel_size</em>, <em class="sig-param">padding='valid'</em>, <em class="sig-param">use_bias=True</em>, <em class="sig-param">depthwise_initializer='glorot_uniform'</em>, <em class="sig-param">pointwise_initializer='glorot_uniform'</em>, <em class="sig-param">bias_initializer='zeros'</em>, <em class="sig-param">depthwise_regularizer=None</em>, <em class="sig-param">pointwise_regularizer=None</em>, <em class="sig-param">bias_regularizer=None</em>, <em class="sig-param">depthwise_constraint=None</em>, <em class="sig-param">pointwise_constraint=None</em>, <em class="sig-param">bias_constraint=None</em>, <em class="sig-param">quantizer=&lt;cnn2snn.quantization_ops.WeightFloat object&gt;</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#cnn2snn.QuantizedSeparableConv2D.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a quantization-aware separable convolutional layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>filters</strong> (<em>integer</em>) – the number of filters.</p></li>
<li><p><strong>kernel_size</strong> (<em>tuple of integer</em>) – the kernel spatial dimensions.</p></li>
<li><p><strong>padding</strong> (<em>str</em><em>, </em><em>optional</em>) – One of ‘valid’ or ‘same’.</p></li>
<li><p><strong>use_bias</strong> (<em>boolean</em><em>, </em><em>optional</em>) – Whether the layer uses a bias vector.</p></li>
<li><p><strong>depthwise_initializer</strong> (str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.initializer</span></code>, optional) – initializer for the depthwise kernel.</p></li>
<li><p><strong>pointwise_initializer</strong> (str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.initializer</span></code>, optional) – initializer for the pointwise kernel.</p></li>
<li><p><strong>bias_initializer</strong> (str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.initializer</span></code>, optional) – initializer for the bias vector.</p></li>
<li><p><strong>depthwise_regularizer</strong> (str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.regularizer</span></code>, optional) – regularization applied to the depthwise kernel.</p></li>
<li><p><strong>pointwise_regularizer</strong> (str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.regularizer</span></code>, optional) – regularization applied to the pointwise kernel.</p></li>
<li><p><strong>bias_regularizer</strong> (str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.regularizer</span></code>, optional) – regularization applied to the bias.</p></li>
<li><p><strong>depthwise_constraint</strong> (str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.constraint</span></code>, optional) – constraint applied to the depthwise kernel.</p></li>
<li><p><strong>pointwise_constraint</strong> (str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.constraint</span></code>, optional) – constraint applied to the pointwise kernel.</p></li>
<li><p><strong>bias_constraint</strong> (str, or a <code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.keras.constraint</span></code>, optional) – constraint applied to the bias.</p></li>
<li><p><strong>quantizer</strong> (<a class="reference internal" href="#cnn2snn.WeightQuantizer" title="cnn2snn.WeightQuantizer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cnn2snn.WeightQuantizer</span></code></a>) – the quantizer to apply
during the forward pass.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="cnn2snn.QuantizedSeparableConv2D.call">
<code class="sig-name descname">call</code><span class="sig-paren">(</span><em class="sig-param">inputs</em><span class="sig-paren">)</span><a class="headerlink" href="#cnn2snn.QuantizedSeparableConv2D.call" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluates input Tensor.</p>
<p>This applies the quantization on weights, then evaluates the input
Tensor and produces the output Tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>inputs</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tensorflow.Tensor</span></code>) – input Tensor.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a Tensor.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tensorflow.Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="cnn2snn.QuantizedSeparableConv2D.get_config">
<code class="sig-name descname">get_config</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#cnn2snn.QuantizedSeparableConv2D.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Python dictionary.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="activationdiscreterelu">
<h2>ActivationDiscreteRelu<a class="headerlink" href="#activationdiscreterelu" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="cnn2snn.ActivationDiscreteRelu">
<em class="property">class </em><code class="sig-prename descclassname">cnn2snn.</code><code class="sig-name descname">ActivationDiscreteRelu</code><span class="sig-paren">(</span><em class="sig-param">bitwidth=1</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#cnn2snn.ActivationDiscreteRelu" title="Permalink to this definition">¶</a></dt>
<dd><p>A discrete ReLU Keras Activation.</p>
<p>Activations will be quantized and will have 2^bitwidth values in the range
[0,6].</p>
<p><strong>Methods</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#cnn2snn.ActivationDiscreteRelu.get_config" title="cnn2snn.ActivationDiscreteRelu.get_config"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_config</span></code></a>()</p></td>
<td><p>Returns the config of the layer.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#cnn2snn.ActivationDiscreteRelu.quantized_activation" title="cnn2snn.ActivationDiscreteRelu.quantized_activation"><code class="xref py py-obj docutils literal notranslate"><span class="pre">quantized_activation</span></code></a>(x)</p></td>
<td><p>Evaluates the activations for the specified input Tensor.</p></td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="cnn2snn.ActivationDiscreteRelu.__init__">
<code class="sig-name descname">__init__</code><span class="sig-paren">(</span><em class="sig-param">bitwidth=1</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#cnn2snn.ActivationDiscreteRelu.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a discrete ReLU for the specified bitwidth.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>bitwidth</strong> (<em>int</em>) – the activation bitwidth.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="cnn2snn.ActivationDiscreteRelu.get_config">
<code class="sig-name descname">get_config</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#cnn2snn.ActivationDiscreteRelu.get_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Python dictionary.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="cnn2snn.ActivationDiscreteRelu.quantized_activation">
<code class="sig-name descname">quantized_activation</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="headerlink" href="#cnn2snn.ActivationDiscreteRelu.quantized_activation" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluates the activations for the specified input Tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tensorflow.Tensor</span></code>) – the input values.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="akida_models_apis.html" class="btn btn-neutral float-right" title="Akida models API" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="aee_apis.html" class="btn btn-neutral float-left" title="Akida Execution Engine API" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright Copyright 2020, BrainChip Holdings Ltd. All Rights Reserved.

    </p>
  </div> 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>