

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Akida Execution Engine API &mdash; Akida Examples  documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery-rendered-html.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="CNN2SNN Toolkit API" href="cnn2snn_apis.html" />
    <link rel="prev" title="API reference" href="api_reference.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #3f51b5" >
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/akida.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                Akida 1.8.13
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../index.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../installation.html#requirements">Requirements</a></li>
<li class="toctree-l2"><a class="reference internal" href="../installation.html#quick-installation">Quick installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../installation.html#running-examples">Running examples</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../user_guide/user_guide.html">User guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../user_guide/getting_started.html">Getting started</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/getting_started.html#for-beginners">For beginners</a></li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/getting_started.html#for-users-familiar-with-deep-learning">For users familiar with deep-learning</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../user_guide/aee.html">Akida user guide</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/aee.html#the-akida-execution-engine">The Akida Execution Engine</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#id1">1. The Spiking Neural Network model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#id2">2. Input data format</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#id3">3. Determine training mode</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#id4">4. Interpreting outputs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/aee.html#neural-network-model">Neural Network model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#specifying-the-neural-network-model">Specifying the Neural Network model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#saving-and-loading">Saving and loading</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#input-layer-types">Input layer types</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#data-processing-layer-types">Data-Processing layer types</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/aee.html#id5">Using Akida Unsupervised Learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#learning-constraints">Learning constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#compiling-a-layer">Compiling a layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#id6">Learning parameters</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../user_guide/cnn2snn.html">CNN2SNN toolkit</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/cnn2snn.html#overview">Overview</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#conversion-workflow">Conversion workflow</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#typical-training-scenario">Typical training scenario</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#design-compatibility-constraints">Design compatibility constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#quantization-compatibility-constraints">Quantization compatibility constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#command-line-interface">Command-line interface</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/cnn2snn.html#layers-considerations">Layers Considerations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#supported-layer-types">Supported layer types</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#cnn2snn-quantization-aware-layers">CNN2SNN Quantization-aware layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#training-only-layers">Training-Only Layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#first-layers">First Layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#id6">Final Layers</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/cnn2snn.html#tips-and-tricks">Tips and Tricks</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../user_guide/akida_models.html">Akida models zoo</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/akida_models.html#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/akida_models.html#command-line-interface-for-model-creation">Command-line interface for model creation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/akida_models.html#command-line-interface-for-model-training">Command-line interface for model training</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida_models.html#cifar10-training-and-tuning">CIFAR10 training and tuning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida_models.html#utk-face-training">UTK Face training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida_models.html#kws-training">KWS training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida_models.html#yolo-training">YOLO training</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/akida_models.html#id1">Layer Blocks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida_models.html#conv-block"><code class="docutils literal notranslate"><span class="pre">conv_block</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida_models.html#dense-block"><code class="docutils literal notranslate"><span class="pre">dense_block</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida_models.html#separable-conv-block"><code class="docutils literal notranslate"><span class="pre">separable_conv_block</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../user_guide/hw_constraints.html">Hardware constraints</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/hw_constraints.html#akida-nsoc-pre-production">Akida NSoC (Pre-production)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/hw_constraints.html#inputconvolutional">InputConvolutional</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/hw_constraints.html#convolutional">Convolutional</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/hw_constraints.html#separableconvolutional">SeparableConvolutional</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/hw_constraints.html#fullyconnected">FullyConnected</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/hw_constraints.html#akida-nsoc-production">Akida NSoC (Production)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/hw_constraints.html#id1">InputConvolutional</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/hw_constraints.html#id2">Convolutional</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/hw_constraints.html#id3">SeparableConvolutional</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/hw_constraints.html#id4">FullyConnected</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../user_guide/compatibility.html">Akida versions compatibility</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/compatibility.html#upgrading-models-with-legacy-quantizers">Upgrading models with legacy quantizers</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="api_reference.html">API reference</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Akida Execution Engine</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#model">Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#layer">Layer</a></li>
<li class="toctree-l3"><a class="reference internal" href="#layerstatistics">LayerStatistics</a></li>
<li class="toctree-l3"><a class="reference internal" href="#inputdata">InputData</a></li>
<li class="toctree-l3"><a class="reference internal" href="#inputconvolutional">InputConvolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="#fullyconnected">FullyConnected</a></li>
<li class="toctree-l3"><a class="reference internal" href="#convolutional">Convolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="#separableconvolutional">SeparableConvolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="#concat">Concat</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dense">Dense</a></li>
<li class="toctree-l3"><a class="reference internal" href="#backend">Backend</a></li>
<li class="toctree-l3"><a class="reference internal" href="#convolutionmode">ConvolutionMode</a></li>
<li class="toctree-l3"><a class="reference internal" href="#poolingtype">PoolingType</a></li>
<li class="toctree-l3"><a class="reference internal" href="#learningtype">LearningType</a></li>
<li class="toctree-l3"><a class="reference internal" href="#compatibility">Compatibility</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="cnn2snn_apis.html">CNN2SNN</a><ul>
<li class="toctree-l3"><a class="reference internal" href="cnn2snn_apis.html#tool-functions">Tool functions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn_apis.html#quantize">quantize</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn_apis.html#quantize-layer">quantize_layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn_apis.html#convert">convert</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn_apis.html#check-model-compatibility">check_model_compatibility</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn_apis.html#load-quantized-model">load_quantized_model</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn_apis.html#load-partial-weights">load_partial_weights</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cnn2snn_apis.html#quantizers">Quantizers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn_apis.html#weightquantizer">WeightQuantizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn_apis.html#linearweightquantizer">LinearWeightQuantizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn_apis.html#stdweightquantizer">StdWeightQuantizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn_apis.html#trainablestdweightquantizer">TrainableStdWeightQuantizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn_apis.html#maxquantizer">MaxQuantizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn_apis.html#maxperaxisquantizer">MaxPerAxisQuantizer</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cnn2snn_apis.html#quantized-layers">Quantized layers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn_apis.html#quantizedconv2d">QuantizedConv2D</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn_apis.html#quantizeddepthwiseconv2d">QuantizedDepthwiseConv2D</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn_apis.html#quantizeddense">QuantizedDense</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn_apis.html#quantizedseparableconv2d">QuantizedSeparableConv2D</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn_apis.html#quantizedactivation">QuantizedActivation</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn_apis.html#activationdiscreterelu">ActivationDiscreteRelu</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn_apis.html#quantizedrelu">QuantizedReLU</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="akida_models_apis.html">Akida models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="akida_models_apis.html#layer-blocks">Layer blocks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="akida_models_apis.html#conv-block">conv_block</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida_models_apis.html#separable-conv-block">separable_conv_block</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida_models_apis.html#dense-block">dense_block</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="akida_models_apis.html#model-zoo">Model zoo</a><ul>
<li class="toctree-l4"><a class="reference internal" href="akida_models_apis.html#mobilenet">Mobilenet</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida_models_apis.html#ds-cnn">DS-CNN</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida_models_apis.html#vgg">VGG</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida_models_apis.html#yolo">YOLO</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../examples/index.html">Examples</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../examples/index.html#general-examples">General examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_0_gxnor_mnist.html">GXNOR/MNIST inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_0_gxnor_mnist.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_0_gxnor_mnist.html#load-the-pre-trained-akida-model">2. Load the pre-trained Akida model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_0_gxnor_mnist.html#show-predictions-for-a-single-image">3. Show predictions for a single image</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_0_gxnor_mnist.html#check-performance">4. Check performance</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_1_ds_cnn_cifar10.html">DS-CNN CIFAR10 inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_1_ds_cnn_cifar10.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_1_ds_cnn_cifar10.html#create-a-keras-ds-cnn-model">2. Create a Keras DS-CNN model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_1_ds_cnn_cifar10.html#quantized-model">3. Quantized model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_1_ds_cnn_cifar10.html#pretrained-quantized-model">4. Pretrained quantized model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_1_ds_cnn_cifar10.html#conversion-to-akida">5. Conversion to Akida</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_2_mobilenet_imagenet.html">MobileNet/ImageNet inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_2_mobilenet_imagenet.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_2_mobilenet_imagenet.html#create-a-keras-mobilenet-model">2. Create a Keras MobileNet model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_2_mobilenet_imagenet.html#quantized-model">3. Quantized model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_2_mobilenet_imagenet.html#pretrained-quantized-model">4. Pretrained quantized model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_2_mobilenet_imagenet.html#conversion-to-akida">5. Conversion to Akida</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_3_ds_cnn_kws.html">DS-CNN/KWS inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_3_ds_cnn_kws.html#load-the-preprocessed-dataset">1. Load the preprocessed dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_3_ds_cnn_kws.html#load-a-pre-trained-native-keras-model">2. Load a pre-trained native Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_3_ds_cnn_kws.html#load-a-pre-trained-quantized-keras-model-satisfying-akida-nsoc-requirements">3. Load a pre-trained quantized Keras model satisfying Akida NSoC requirements</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_3_ds_cnn_kws.html#conversion-to-akida">4. Conversion to Akida</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_4_regression.html">Regression tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_regression.html#load-the-dataset">1. Load the dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_regression.html#load-a-pre-trained-native-keras-model">2. Load a pre-trained native Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_regression.html#load-a-pre-trained-quantized-keras-model-satisfying-akida-nsoc-requirements">3. Load a pre-trained quantized Keras model satisfying Akida NSoC requirements</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_regression.html#conversion-to-akida">4. Conversion to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_regression.html#estimate-age-on-a-single-image">5. Estimate age on a single image</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_5_transfer_learning.html">Transfer learning with MobileNet for cats vs. dogs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_5_transfer_learning.html#transfer-learning-process">Transfer learning process</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_5_transfer_learning.html#load-and-preprocess-data">1. Load and preprocess data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_5_transfer_learning.html#modify-a-pre-trained-base-keras-model">2. Modify a pre-trained base Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_5_transfer_learning.html#train-the-transferred-model-for-the-new-task">3. Train the transferred model for the new task</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_5_transfer_learning.html#quantize-the-top-layer">4 Quantize the top layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_5_transfer_learning.html#convert-to-akida">5. Convert to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_5_transfer_learning.html#plot-confusion-matrix">6. Plot confusion matrix</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_6_voc_yolo_detection.html">YOLO/PASCAL-VOC detection tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_6_voc_yolo_detection.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_6_voc_yolo_detection.html#preprocessing-tools">2. Preprocessing tools</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_6_voc_yolo_detection.html#model-architecture">3. Model architecture</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_6_voc_yolo_detection.html#training">4. Training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_6_voc_yolo_detection.html#performance">5. Performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_6_voc_yolo_detection.html#conversion-to-akida">6. Conversion to Akida</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/index.html#cnn2snn-tutorials">CNN2SNN tutorials</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/cnn2snn/plot_0_cnn_flow.html">CNN conversion flow tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_0_cnn_flow.html#load-and-reshape-mnist-dataset">1. Load and reshape MNIST dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_0_cnn_flow.html#model-definition">2. Model definition</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_0_cnn_flow.html#model-quantization">4. Model quantization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_0_cnn_flow.html#model-fine-tuning-quantization-aware-training">5. Model fine tuning (quantization-aware training)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_0_cnn_flow.html#model-conversion">6. Model conversion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/cnn2snn/plot_1_advanced_cnn2snn.html">Advanced CNN2SNN tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_1_advanced_cnn2snn.html#design-a-cnn2snn-quantized-model">1. Design a CNN2SNN quantized model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_1_advanced_cnn2snn.html#weight-quantizer-details">2. Weight Quantizer Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_1_advanced_cnn2snn.html#quantized-activation-layer-details">3. Quantized Activation Layer Details</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/index.html#edge-examples">Edge examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/edge/plot_0_edge_learning_vision.html">Akida vision edge learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_0_edge_learning_vision.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_0_edge_learning_vision.html#prepare-akida-model-for-learning">2. Prepare Akida model for learning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_0_edge_learning_vision.html#edge-learning-with-akida">3. Edge learning with Akida</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/edge/plot_1_edge_learning_kws.html">Akida edge learning for keyword spotting</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_1_edge_learning_kws.html#edge-learning-process">1. Edge learning process</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_1_edge_learning_kws.html#dataset-preparation">2. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_1_edge_learning_kws.html#prepare-akida-model-for-learning">3. Prepare Akida model for learning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_1_edge_learning_kws.html#learn-with-akida-using-the-training-set">4. Learn with Akida using the training set</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_1_edge_learning_kws.html#edge-learning">4. Edge learning</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/edge/plot_2_edge_learning_parameters.html">Tips to set Akida learning parameters</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_2_edge_learning_parameters.html#akida-learning-parameters">1. Akida learning parameters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_2_edge_learning_parameters.html#create-akida-model">2. Create Akida model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_2_edge_learning_parameters.html#estimate-the-required-number-of-weights-of-the-trainable-layer">3. Estimate the required number of weights of the trainable layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_2_edge_learning_parameters.html#estimate-the-number-of-neurons-per-class">4. Estimate the number of neurons per class</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/Brainchip-Inc/akida_examples/releases">Changelog</a></li>
<li class="toctree-l1"><a class="reference external" href="https://support.brainchip.com/portal/home">Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Akida Examples</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="api_reference.html">API reference</a> &raquo;</li>
        
      <li>Akida Execution Engine API</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="module-akida">
<span id="akida-execution-engine-api"></span><h1>Akida Execution Engine API<a class="headerlink" href="#module-akida" title="Permalink to this headline">¶</a></h1>
<dl class="py attribute">
<dt id="akida.__version__">
<code class="sig-prename descclassname">akida.</code><code class="sig-name descname">__version__</code><a class="headerlink" href="#akida.__version__" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the current version of the akida module.</p>
</dd></dl>

<div class="section" id="model">
<h2>Model<a class="headerlink" href="#model" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="akida.Model">
<em class="property">class </em><code class="sig-prename descclassname">akida.</code><code class="sig-name descname">Model</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">filename</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">serialized_buffer</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">layers</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">backend</span><span class="o">=</span><span class="default_value">BackendType.Software</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida/model.html#Model"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#akida.Model" title="Permalink to this definition">¶</a></dt>
<dd><p>An Akida neural <code class="docutils literal notranslate"><span class="pre">Model</span></code>, represented as a hierarchy of layers.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">Model</span></code> class is the main interface to Akida and allows to creates an
empty <code class="docutils literal notranslate"><span class="pre">Model</span></code>, a <code class="docutils literal notranslate"><span class="pre">Model</span></code> template from a YAML file, or a full <code class="docutils literal notranslate"><span class="pre">Model</span></code>
from a serialized file.</p>
<p>It provides methods to instantiate, train, test and save models.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>filename</strong> (<em>str</em><em>, </em><em>optional</em>) – path of the YAML file containing the model
architecture, or a serialized Model.
If None, an empty sequential model will be created, or filled
with the layers in the layers parameter.</p></li>
<li><p><strong>serialized_buffer</strong> (<em>bytes</em><em>, </em><em>optional</em>) – binary buffer containing a
serialized Model.</p></li>
<li><p><strong>layers</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">list</span></code>, optional) – list of layers that will be copied
to the new model. If the list does not start with an input layer,
it will be added automatically.</p></li>
<li><p><strong>backend</strong> (<a class="reference internal" href="#akida.BackendType" title="akida.BackendType"><code class="xref py py-obj docutils literal notranslate"><span class="pre">BackendType</span></code></a>, optional) – backend to run the model on.</p></li>
</ul>
</dd>
</dl>
<p><strong>Methods:</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#akida.Model.add" title="akida.Model.add"><code class="xref py py-obj docutils literal notranslate"><span class="pre">add</span></code></a>(self, layer, inbound_layers)</p></td>
<td><p>Add a layer to the current model.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#akida.Model.add_classes" title="akida.Model.add_classes"><code class="xref py py-obj docutils literal notranslate"><span class="pre">add_classes</span></code></a>(num_add_classes)</p></td>
<td><p>Adds classes to the last layer of the model.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#akida.Model.compile" title="akida.Model.compile"><code class="xref py py-obj docutils literal notranslate"><span class="pre">compile</span></code></a>(self, num_weights, num_classes, …)</p></td>
<td><p>Prepare the internal parameters of the last layer of the model for training</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#akida.Model.evaluate" title="akida.Model.evaluate"><code class="xref py py-obj docutils literal notranslate"><span class="pre">evaluate</span></code></a>(inputs)</p></td>
<td><p>Evaluates a set of images or events through the model.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#akida.Model.fit" title="akida.Model.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a>(inputs[, input_labels])</p></td>
<td><p>Trains a set of images or events through the model.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#akida.Model.forward" title="akida.Model.forward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">forward</span></code></a>(inputs)</p></td>
<td><p>Forwards a set of images or events through the model.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#akida.Model.get_layer" title="akida.Model.get_layer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_layer</span></code></a>(*args, **kwargs)</p></td>
<td><p>Overloaded function.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#akida.Model.get_layer_count" title="akida.Model.get_layer_count"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_layer_count</span></code></a>(self)</p></td>
<td><p>The number of layers.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#akida.Model.get_statistics" title="akida.Model.get_statistics"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_statistics</span></code></a>()</p></td>
<td><p>Get statistics by layer for this network.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#akida.Model.pop_layer" title="akida.Model.pop_layer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">pop_layer</span></code></a>(self)</p></td>
<td><p>Remove the last layer of the current model.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#akida.Model.predict" title="akida.Model.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a>(inputs[, num_classes])</p></td>
<td><p>Returns the model class predictions.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#akida.Model.save" title="akida.Model.save"><code class="xref py py-obj docutils literal notranslate"><span class="pre">save</span></code></a>(self, arg0)</p></td>
<td><p>Saves all the model configuration (all layers and weights) to a file on disk.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#akida.Model.summary" title="akida.Model.summary"><code class="xref py py-obj docutils literal notranslate"><span class="pre">summary</span></code></a>()</p></td>
<td><p>Prints a string summary of the model.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#akida.Model.to_buffer" title="akida.Model.to_buffer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">to_buffer</span></code></a>(self)</p></td>
<td><p>Serializes all the model configuration (all layers and weights) to a bytes buffer.</p></td>
</tr>
</tbody>
</table>
<p><strong>Attributes:</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#akida.Model.backend" title="akida.Model.backend"><code class="xref py py-obj docutils literal notranslate"><span class="pre">backend</span></code></a></p></td>
<td><p>The backend the model is running on.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#akida.Model.input_dims" title="akida.Model.input_dims"><code class="xref py py-obj docutils literal notranslate"><span class="pre">input_dims</span></code></a></p></td>
<td><p>The model input dimensions (width, height, features).</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#akida.Model.layers" title="akida.Model.layers"><code class="xref py py-obj docutils literal notranslate"><span class="pre">layers</span></code></a></p></td>
<td><p>Get a list of layers in current model.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#akida.Model.metrics" title="akida.Model.metrics"><code class="xref py py-obj docutils literal notranslate"><span class="pre">metrics</span></code></a></p></td>
<td><p>The model metrics.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#akida.Model.output_dims" title="akida.Model.output_dims"><code class="xref py py-obj docutils literal notranslate"><span class="pre">output_dims</span></code></a></p></td>
<td><p>The model output dimensions (width, height, features).</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="akida.Model.add">
<code class="sig-name descname">add</code><span class="sig-paren">(</span><em class="sig-param">self: akida.core.ModelBase</em>, <em class="sig-param">layer: akida::Layer</em>, <em class="sig-param">inbound_layers: List[akida::Layer] = []</em><span class="sig-paren">)</span> &#x2192; None<a class="headerlink" href="#akida.Model.add" title="Permalink to this definition">¶</a></dt>
<dd><p>Add a layer to the current model.</p>
<p>A list of inbound layers can optionally be specified.
These layers must already be included in the model.
if no inbound layer is specified, and the layer is not the first layer
in the model, the last included layer will be used as inbound layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>layer</strong> (<em>one of the available layers</em>) – layer instance to be added to the model</p></li>
<li><p><strong>inbound_layers</strong> (a list of <cite>Layer</cite>) – an optional list of inbound layers</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="akida.Model.add_classes">
<code class="sig-name descname">add_classes</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">num_add_classes</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida/model.html#Model.add_classes"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#akida.Model.add_classes" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds classes to the last layer of the model.</p>
<p>A model with a compiled last layer is ready to learn using the Akida
built-in learning algorithm. This function allows to add new classes
(i.e. new neurons) to the last layer, keeping the previously learned
neurons.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>num_add_classes</strong> (<em>int</em>) – number of classes to add to the last layer</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>RuntimeError</strong> – if the last layer is not compiled</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="akida.Model.backend">
<em class="property">property </em><code class="sig-name descname">backend</code><a class="headerlink" href="#akida.Model.backend" title="Permalink to this definition">¶</a></dt>
<dd><p>The backend the model is running on.</p>
</dd></dl>

<dl class="py method">
<dt id="akida.Model.compile">
<code class="sig-name descname">compile</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span><span class="p">:</span> <span class="n">akida.core.ModelBase</span></em>, <em class="sig-param"><span class="n">num_weights</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">num_classes</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em>, <em class="sig-param"><span class="n">initial_plasticity</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">learning_competition</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">0.0</span></em>, <em class="sig-param"><span class="n">min_plasticity</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">0.10000000149011612</span></em>, <em class="sig-param"><span class="n">plasticity_decay</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">0.25</span></em><span class="sig-paren">)</span> &#x2192; None<a class="headerlink" href="#akida.Model.compile" title="Permalink to this definition">¶</a></dt>
<dd><p>Prepare the internal parameters of the last layer of the model for training</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_weights</strong> (<em>int</em>) – number of connections for each neuron.</p></li>
<li><p><strong>num_classes</strong> (<em>int</em><em>, </em><em>optional</em>) – number of classes when running in a
‘labeled mode’.</p></li>
<li><p><strong>initial_plasticity</strong> (<em>float</em><em>, </em><em>optional</em>) – defines how easily the weights
will change when learning occurs.</p></li>
<li><p><strong>learning_competition</strong> (<em>float</em><em>, </em><em>optional</em>) – controls competition between
neurons.</p></li>
<li><p><strong>min_plasticity</strong> (<em>float</em><em>, </em><em>optional</em>) – defines the minimum level to which
plasticity will decay.</p></li>
<li><p><strong>plasticity_decay</strong> (<em>float</em><em>, </em><em>optional</em>) – defines the decay of plasticity
with each learning step.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="akida.Model.evaluate">
<code class="sig-name descname">evaluate</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inputs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida/model.html#Model.evaluate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#akida.Model.evaluate" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluates a set of images or events through the model.</p>
<p>Forwards an input tensor through the model and returns a float array.</p>
<p>It applies ONLY to models without an activation on the last layer.
The output values are obtained from the model discrete potentials by
applying a shift and a scale.</p>
<p>The expected input tensor dimensions are:</p>
<ul class="simple">
<li><p>n, representing the number of frames or samples,</p></li>
<li><p>w, representing the width,</p></li>
<li><p>h, representing the height,</p></li>
<li><p>c, representing the channel, or more generally the feature.</p></li>
</ul>
<p>If the inputs are events, the input shape must be (n, w, h, c), but if
the inputs are images (numpy array), their shape must be (n, h, w, c).</p>
<p>Note: only grayscale (c=1) or RGB (c=3) images (arrays) are supported.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>inputs</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code>) – a (n, w, h, c) numpy.ndarray</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a float array of shape (n, w, h, c).</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>TypeError</strong> – if the input is not a numpy.ndarray.</p></li>
<li><p><strong>RuntimeError</strong> – if the model last layer has an activation.</p></li>
<li><p><strong>ValueError</strong> – if the input doesn’t match the required shape,
    format, or if the model only has an InputData layer.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="akida.Model.fit">
<code class="sig-name descname">fit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inputs</span></em>, <em class="sig-param"><span class="n">input_labels</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida/model.html#Model.fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#akida.Model.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Trains a set of images or events through the model.</p>
<p>Trains the model with the specified input tensor (numpy array).</p>
<p>The expected input tensor dimensions are:</p>
<ul class="simple">
<li><p>n, representing the number of frames or samples,</p></li>
<li><p>w, representing the width,</p></li>
<li><p>h, representing the height,</p></li>
<li><p>c, representing the channel, or more generally the feature.</p></li>
</ul>
<p>If the inputs are events, the input shape must be (n, w, h, c), but if
the inputs are images, their shape must be (n, h, w, c).</p>
<p>Note: only grayscale (c=1) or RGB (c=3) images (arrays) are supported.</p>
<p>If activations are enabled for the last layer, the output is an uint8
tensor.</p>
<p>If activations are disabled for the last layer, the output is an int32
tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code>) – a numpy.ndarray</p></li>
<li><p><strong>input_labels</strong> (<em>list</em><em>(</em><em>int</em><em>)</em><em>, </em><em>optional</em>) – input labels.
Must have one label per input, or a single label for all inputs.
If a label exceeds the defined number of classes, the input will
be discarded. (Default value = None).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a numpy array of shape (n, out_w, out_h, out_c).</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>TypeError</strong> – if the input is not a numpy.ndarray.</p></li>
<li><p><strong>ValueError</strong> – if the input doesn’t match the required shape,
    format, etc.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="akida.Model.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inputs</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida/model.html#Model.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#akida.Model.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forwards a set of images or events through the model.</p>
<p>Forwards an input tensor through the model and returns an output tensor.</p>
<p>The expected input tensor dimensions are:</p>
<ul class="simple">
<li><p>n, representing the number of frames or samples,</p></li>
<li><p>w, representing the width,</p></li>
<li><p>h, representing the height,</p></li>
<li><p>c, representing the channel, or more generally the feature.</p></li>
</ul>
<p>If the inputs are events, the input shape must be (n, w, h, c), but if
the inputs are images, their shape must be (n, h, w, c).</p>
<p>Note: only grayscale (c=1) or RGB (c=3) images (arrays) are supported.</p>
<p>If activations are enabled for the last layer, the output is an uint8
tensor.</p>
<p>If activations are disabled for the last layer, the output is an int32
tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>inputs</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code>) – a numpy.ndarray</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a numpy array of shape (n, out_w, out_h, out_c).</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>TypeError</strong> – if the input is not a numpy.ndarray.</p></li>
<li><p><strong>ValueError</strong> – if the inputs doesn’t match the required shape,
    format, etc.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="akida.Model.get_layer">
<code class="sig-name descname">get_layer</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#akida.Model.get_layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Overloaded function.</p>
<ol class="arabic">
<li><p>get_layer(self: akida.core.ModelBase, layer_name: str) -&gt; akida::Layer</p>
<blockquote>
<div><p>Get a reference to a specific layer.</p>
<p>This method allows a deeper introspection of the model by providing
access to the underlying layers.</p>
<dl class="field-list simple">
<dt class="field-odd">param layer_name</dt>
<dd class="field-odd"><p>name of the layer to retrieve</p>
</dd>
<dt class="field-even">type layer_name</dt>
<dd class="field-even"><p>str</p>
</dd>
<dt class="field-odd">return</dt>
<dd class="field-odd"><p>a <code class="docutils literal notranslate"><span class="pre">Layer</span></code></p>
</dd>
</dl>
</div></blockquote>
</li>
<li><p>get_layer(self: akida.core.ModelBase, layer_index: int) -&gt; akida::Layer</p>
<blockquote>
<div><p>Get a reference to a specific layer.</p>
<p>This method allows a deeper introspection of the model by providing
access to the underlying layers.</p>
<dl class="field-list simple">
<dt class="field-odd">param layer_index</dt>
<dd class="field-odd"><p>index of the layer to retrieve</p>
</dd>
<dt class="field-even">type layer_index</dt>
<dd class="field-even"><p>int</p>
</dd>
<dt class="field-odd">return</dt>
<dd class="field-odd"><p>a <code class="docutils literal notranslate"><span class="pre">Layer</span></code></p>
</dd>
</dl>
</div></blockquote>
</li>
</ol>
</dd></dl>

<dl class="py method">
<dt id="akida.Model.get_layer_count">
<code class="sig-name descname">get_layer_count</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span><span class="p">:</span> <span class="n">akida.core.ModelBase</span></em><span class="sig-paren">)</span> &#x2192; int<a class="headerlink" href="#akida.Model.get_layer_count" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of layers.</p>
</dd></dl>

<dl class="py method">
<dt id="akida.Model.get_statistics">
<code class="sig-name descname">get_statistics</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida/model.html#Model.get_statistics"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#akida.Model.get_statistics" title="Permalink to this definition">¶</a></dt>
<dd><p>Get statistics by layer for this network.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><cite>LayerStatistics</cite> indexed by layer_name.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>a dictionary of obj</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="akida.Model.input_dims">
<em class="property">property </em><code class="sig-name descname">input_dims</code><a class="headerlink" href="#akida.Model.input_dims" title="Permalink to this definition">¶</a></dt>
<dd><p>The model input dimensions (width, height, features).</p>
</dd></dl>

<dl class="py method">
<dt id="akida.Model.layers">
<em class="property">property </em><code class="sig-name descname">layers</code><a class="headerlink" href="#akida.Model.layers" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a list of layers in current model.</p>
</dd></dl>

<dl class="py method">
<dt id="akida.Model.metrics">
<em class="property">property </em><code class="sig-name descname">metrics</code><a class="headerlink" href="#akida.Model.metrics" title="Permalink to this definition">¶</a></dt>
<dd><p>The model metrics.</p>
</dd></dl>

<dl class="py method">
<dt id="akida.Model.output_dims">
<em class="property">property </em><code class="sig-name descname">output_dims</code><a class="headerlink" href="#akida.Model.output_dims" title="Permalink to this definition">¶</a></dt>
<dd><p>The model output dimensions (width, height, features).</p>
</dd></dl>

<dl class="py method">
<dt id="akida.Model.pop_layer">
<code class="sig-name descname">pop_layer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span><span class="p">:</span> <span class="n">akida.core.ModelBase</span></em><span class="sig-paren">)</span> &#x2192; None<a class="headerlink" href="#akida.Model.pop_layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Remove the last layer of the current model.</p>
</dd></dl>

<dl class="py method">
<dt id="akida.Model.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inputs</span></em>, <em class="sig-param"><span class="n">num_classes</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida/model.html#Model.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#akida.Model.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the model class predictions.</p>
<p>Forwards an input tensor (images or events) through the model
and compute predictions based on the neuron id.
If the number of output neurons is greater than the number of classes,
the neurons are automatically assigned to a class by dividing their id
by the number of classes.</p>
<p>The expected input tensor dimensions are:</p>
<ul class="simple">
<li><p>n, representing the number of frames or samples,</p></li>
<li><p>w, representing the width,</p></li>
<li><p>h, representing the height,</p></li>
<li><p>c, representing the channel, or more generally the feature.</p></li>
</ul>
<p>If the inputs are events, the input shape must be (n, w, h, c), but if
the inputs are images their shape must be (n, h, w, c).</p>
<p>Note: only grayscale (c=1) or RGB (c=3) images (arrays) are supported.</p>
<p>Note that the predictions are based on the activation values of the last
layer: for most use cases, you may want to disable activations for that
layer (ie setting <code class="docutils literal notranslate"><span class="pre">activations_enabled=False</span></code>) to get a better
accuracy.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code>) – a numpy.ndarray</p></li>
<li><p><strong>num_classes</strong> (<em>int</em><em>, </em><em>optional</em>) – optional parameter (defaults to the
number of neurons in the last layer).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>an array of shape (n).</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>TypeError</strong> – if the input is not a numpy.ndarray.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="akida.Model.save">
<code class="sig-name descname">save</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span><span class="p">:</span> <span class="n">akida.core.ModelBase</span></em>, <em class="sig-param"><span class="n">arg0</span><span class="p">:</span> <span class="n">str</span></em><span class="sig-paren">)</span> &#x2192; None<a class="headerlink" href="#akida.Model.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves all the model configuration (all layers and weights) to a
file on disk.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>model_file</strong> (<em>str</em>) – full path of the serialized model (.fbz file).</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="akida.Model.summary">
<code class="sig-name descname">summary</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida/model.html#Model.summary"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#akida.Model.summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Prints a string summary of the model.</p>
<p>This method prints a summary of the model with details for every layer:</p>
<ul class="simple">
<li><p>name and type in the first column</p></li>
<li><p>output shape</p></li>
<li><p>kernel shape</p></li>
</ul>
<p>If there is any layer with unsupervised learning enabled, it will list
them, with these details:</p>
<ul class="simple">
<li><p>name of layer</p></li>
<li><p>number of incoming connections</p></li>
<li><p>number of weights per neuron</p></li>
</ul>
<p>It will also tell the input shape, the backend type and version.</p>
</dd></dl>

<dl class="py method">
<dt id="akida.Model.to_buffer">
<code class="sig-name descname">to_buffer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span><span class="p">:</span> <span class="n">akida.core.ModelBase</span></em><span class="sig-paren">)</span> &#x2192; bytes<a class="headerlink" href="#akida.Model.to_buffer" title="Permalink to this definition">¶</a></dt>
<dd><p>Serializes all the model configuration (all layers and weights) to a
bytes buffer.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="layer">
<h2>Layer<a class="headerlink" href="#layer" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="akida.Layer">
<em class="property">class </em><code class="sig-prename descclassname">akida.</code><code class="sig-name descname">Layer</code><a class="headerlink" href="#akida.Layer" title="Permalink to this definition">¶</a></dt>
<dd><p><strong>Methods:</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#akida.Layer.get_learning_histogram" title="akida.Layer.get_learning_histogram"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_learning_histogram</span></code></a>()</p></td>
<td><p>Returns an histogram of learning percentages.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#akida.Layer.get_variable" title="akida.Layer.get_variable"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_variable</span></code></a>(name)</p></td>
<td><p>Get the value of a layer variable.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#akida.Layer.get_variable_names" title="akida.Layer.get_variable_names"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_variable_names</span></code></a>()</p></td>
<td><p>Get the list of variable names for this layer.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#akida.Layer.set_variable" title="akida.Layer.set_variable"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_variable</span></code></a>(name, values)</p></td>
<td><p>Set the value of a layer variable.</p></td>
</tr>
</tbody>
</table>
<p><strong>Attributes:</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#akida.Layer.inbounds" title="akida.Layer.inbounds"><code class="xref py py-obj docutils literal notranslate"><span class="pre">inbounds</span></code></a></p></td>
<td><p>The layer inbound layers.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#akida.Layer.input_bits" title="akida.Layer.input_bits"><code class="xref py py-obj docutils literal notranslate"><span class="pre">input_bits</span></code></a></p></td>
<td><p>The layer input bits.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#akida.Layer.input_dims" title="akida.Layer.input_dims"><code class="xref py py-obj docutils literal notranslate"><span class="pre">input_dims</span></code></a></p></td>
<td><p>The layer input dimensions (width, height, channels).</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#akida.Layer.learning" title="akida.Layer.learning"><code class="xref py py-obj docutils literal notranslate"><span class="pre">learning</span></code></a></p></td>
<td><p>The layer learning parameters set.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#akida.Layer.name" title="akida.Layer.name"><code class="xref py py-obj docutils literal notranslate"><span class="pre">name</span></code></a></p></td>
<td><p>The layer name.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#akida.Layer.output_dims" title="akida.Layer.output_dims"><code class="xref py py-obj docutils literal notranslate"><span class="pre">output_dims</span></code></a></p></td>
<td><p>The layer output dimensions (width, height, features).</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#akida.Layer.parameters" title="akida.Layer.parameters"><code class="xref py py-obj docutils literal notranslate"><span class="pre">parameters</span></code></a></p></td>
<td><p>The layer parameters set.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#akida.Layer.variables" title="akida.Layer.variables"><code class="xref py py-obj docutils literal notranslate"><span class="pre">variables</span></code></a></p></td>
<td><p>The layer trainable variables.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="akida.Layer.get_learning_histogram">
<code class="sig-name descname">get_learning_histogram</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#akida.Layer.get_learning_histogram" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an histogram of learning percentages.</p>
<p>Returns a list of learning percentages and the associated number of
neurons.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a (n,2) numpy.ndarray containing the learning
percentages and the number of neurons.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="akida.Layer.get_variable">
<code class="sig-name descname">get_variable</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em><span class="sig-paren">)</span><a class="headerlink" href="#akida.Layer.get_variable" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the value of a layer variable.</p>
<p>Layer variables are named entities representing the weights or
thresholds used during inference:</p>
<ul class="simple">
<li><p>Weights variables are typically integer arrays of shape:
(width, height, features/channels, num_neurons) row-major (‘C’).</p></li>
<li><p>Threshold variables are typically integer or float arrays of shape:
(num_neurons).</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>name</strong> (<em>str</em>) – the variable name.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>an array containing the variable.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="akida.Layer.get_variable_names">
<code class="sig-name descname">get_variable_names</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#akida.Layer.get_variable_names" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the list of variable names for this layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a list of variable names.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="akida.Layer.inbounds">
<em class="property">property </em><code class="sig-name descname">inbounds</code><a class="headerlink" href="#akida.Layer.inbounds" title="Permalink to this definition">¶</a></dt>
<dd><p>The layer inbound layers.</p>
</dd></dl>

<dl class="py method">
<dt id="akida.Layer.input_bits">
<em class="property">property </em><code class="sig-name descname">input_bits</code><a class="headerlink" href="#akida.Layer.input_bits" title="Permalink to this definition">¶</a></dt>
<dd><p>The layer input bits.</p>
</dd></dl>

<dl class="py method">
<dt id="akida.Layer.input_dims">
<em class="property">property </em><code class="sig-name descname">input_dims</code><a class="headerlink" href="#akida.Layer.input_dims" title="Permalink to this definition">¶</a></dt>
<dd><p>The layer input dimensions (width, height, channels).</p>
</dd></dl>

<dl class="py method">
<dt id="akida.Layer.learning">
<em class="property">property </em><code class="sig-name descname">learning</code><a class="headerlink" href="#akida.Layer.learning" title="Permalink to this definition">¶</a></dt>
<dd><p>The layer learning parameters set.</p>
</dd></dl>

<dl class="py method">
<dt id="akida.Layer.name">
<em class="property">property </em><code class="sig-name descname">name</code><a class="headerlink" href="#akida.Layer.name" title="Permalink to this definition">¶</a></dt>
<dd><p>The layer name.</p>
</dd></dl>

<dl class="py method">
<dt id="akida.Layer.output_dims">
<em class="property">property </em><code class="sig-name descname">output_dims</code><a class="headerlink" href="#akida.Layer.output_dims" title="Permalink to this definition">¶</a></dt>
<dd><p>The layer output dimensions (width, height, features).</p>
</dd></dl>

<dl class="py method">
<dt id="akida.Layer.parameters">
<em class="property">property </em><code class="sig-name descname">parameters</code><a class="headerlink" href="#akida.Layer.parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>The layer parameters set.</p>
</dd></dl>

<dl class="py method">
<dt id="akida.Layer.set_variable">
<code class="sig-name descname">set_variable</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span></em>, <em class="sig-param"><span class="n">values</span></em><span class="sig-paren">)</span><a class="headerlink" href="#akida.Layer.set_variable" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the value of a layer variable.</p>
<p>Layer variables are named entities representing the weights or
thresholds used during inference:</p>
<ul>
<li><p>Weights variables are typically integer arrays of shape:</p>
<p>(num_neurons, features/channels, height, width) col-major ordered (‘F’)</p>
</li>
</ul>
<p>or equivalently:</p>
<blockquote>
<div><p>(width, height, features/channels, num_neurons) row-major (‘C’).</p>
</div></blockquote>
<ul class="simple">
<li><p>Threshold variables are typically integer or float arrays of shape:
(num_neurons).</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<em>str</em>) – the variable name.</p></li>
<li><p><strong>values</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.ndarray</span></code>) – a numpy.ndarray containing the variable values.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="akida.Layer.variables">
<em class="property">property </em><code class="sig-name descname">variables</code><a class="headerlink" href="#akida.Layer.variables" title="Permalink to this definition">¶</a></dt>
<dd><p>The layer trainable variables.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="layerstatistics">
<h2>LayerStatistics<a class="headerlink" href="#layerstatistics" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="akida.LayerStatistics">
<em class="property">class </em><code class="sig-prename descclassname">akida.</code><code class="sig-name descname">LayerStatistics</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">layer</span></em>, <em class="sig-param"><span class="n">nb_samples</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">nb_activations</span><span class="o">=</span><span class="default_value">0</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida/layer_statistics.html#LayerStatistics"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#akida.LayerStatistics" title="Permalink to this definition">¶</a></dt>
<dd><p>Container attached to an akida.Model and an akida.Layer that allows to
retrieve layer statistics:
(average input and output sparsity, number of operations, number of
possible spikes, row_sparsity).</p>
<p><strong>Attributes:</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#akida.LayerStatistics.layer_name" title="akida.LayerStatistics.layer_name"><code class="xref py py-obj docutils literal notranslate"><span class="pre">layer_name</span></code></a></p></td>
<td><p>Get the name of the corresponding layer.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#akida.LayerStatistics.output_sparsity" title="akida.LayerStatistics.output_sparsity"><code class="xref py py-obj docutils literal notranslate"><span class="pre">output_sparsity</span></code></a></p></td>
<td><p>Get average output sparsity for the layer.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#akida.LayerStatistics.possible_spikes" title="akida.LayerStatistics.possible_spikes"><code class="xref py py-obj docutils literal notranslate"><span class="pre">possible_spikes</span></code></a></p></td>
<td><p>Get possible spikes for the layer.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#akida.LayerStatistics.row_sparsity" title="akida.LayerStatistics.row_sparsity"><code class="xref py py-obj docutils literal notranslate"><span class="pre">row_sparsity</span></code></a></p></td>
<td><p>Get kernel row sparsity.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="akida.LayerStatistics.layer_name">
<em class="property">property </em><code class="sig-name descname">layer_name</code><a class="headerlink" href="#akida.LayerStatistics.layer_name" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the name of the corresponding layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>the layer name.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="akida.LayerStatistics.output_sparsity">
<em class="property">property </em><code class="sig-name descname">output_sparsity</code><a class="headerlink" href="#akida.LayerStatistics.output_sparsity" title="Permalink to this definition">¶</a></dt>
<dd><p>Get average output sparsity for the layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>the average output sparsity value.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="akida.LayerStatistics.possible_spikes">
<em class="property">property </em><code class="sig-name descname">possible_spikes</code><a class="headerlink" href="#akida.LayerStatistics.possible_spikes" title="Permalink to this definition">¶</a></dt>
<dd><p>Get possible spikes for the layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>the possible spike amount value.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="akida.LayerStatistics.row_sparsity">
<em class="property">property </em><code class="sig-name descname">row_sparsity</code><a class="headerlink" href="#akida.LayerStatistics.row_sparsity" title="Permalink to this definition">¶</a></dt>
<dd><p>Get kernel row sparsity.</p>
<p>Compute row sparsity for kernel weights.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>the kernel row sparsity value.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>float</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="inputdata">
<h2>InputData<a class="headerlink" href="#inputdata" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="akida.InputData">
<em class="property">class </em><code class="sig-prename descclassname">akida.</code><code class="sig-name descname">InputData</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_width</span></em>, <em class="sig-param"><span class="n">input_height</span></em>, <em class="sig-param"><span class="n">input_channels</span></em>, <em class="sig-param"><span class="n">input_bits</span><span class="o">=</span><span class="default_value">4</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">''</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida/input_data.html#InputData"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#akida.InputData" title="Permalink to this definition">¶</a></dt>
<dd><p>This is the general purpose input layer. It takes events in a simple
address-event data format; that is, each event is characterized by a trio
of values giving x, y and channel values.</p>
<p>Regarding the input dimension values, note that AEE expects inputs with
zero-based indexing, i.e., if input_width is defined as 12, then the model
expects all input events to have x-values in the range 0–11.</p>
<p>Where possible:</p>
<ul class="simple">
<li><p>The x and y dimensions should be used for discretely-sampled continuous
domains such as space (e.g., images) or time-series (e.g., an audio
signal).</p></li>
<li><p>The c dimension should be used for ‘category indices’, where there is no
particular relationship between neighboring values.</p></li>
</ul>
<p>The input dimension values are used for:</p>
<ul class="simple">
<li><p>Error checking – input events are checked and if any fall outside the
defined input range, then the whole set of events sent on that
processing call is rejected. An error will also be generated if the
defined values are larger than the true input dimensions.</p></li>
<li><p>Configuring the input and output dimensions of subsequent layers in the
model.</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_width</strong> (<em>int</em>) – input width.</p></li>
<li><p><strong>input_height</strong> (<em>int</em>) – input height.</p></li>
<li><p><strong>input_channels</strong> (<em>int</em>) – size of the third input dimension.</p></li>
<li><p><strong>input_bits</strong> (<em>int</em>) – input bitwidth.</p></li>
<li><p><strong>name</strong> (<em>str</em><em>, </em><em>optional</em>) – name of the layer.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="inputconvolutional">
<h2>InputConvolutional<a class="headerlink" href="#inputconvolutional" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="akida.InputConvolutional">
<em class="property">class </em><code class="sig-prename descclassname">akida.</code><code class="sig-name descname">InputConvolutional</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_width</span></em>, <em class="sig-param"><span class="n">input_height</span></em>, <em class="sig-param"><span class="n">input_channels</span></em>, <em class="sig-param"><span class="n">kernel_width</span></em>, <em class="sig-param"><span class="n">kernel_height</span></em>, <em class="sig-param"><span class="n">num_neurons</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">''</span></em>, <em class="sig-param"><span class="n">convolution_mode</span><span class="o">=</span><span class="default_value">ConvolutionMode.Same</span></em>, <em class="sig-param"><span class="n">stride_x</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">stride_y</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">weights_bits</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">pooling_width</span><span class="o">=</span><span class="default_value">- 1</span></em>, <em class="sig-param"><span class="n">pooling_height</span><span class="o">=</span><span class="default_value">- 1</span></em>, <em class="sig-param"><span class="n">pooling_type</span><span class="o">=</span><span class="default_value">PoolingType.NoPooling</span></em>, <em class="sig-param"><span class="n">pooling_stride_x</span><span class="o">=</span><span class="default_value">- 1</span></em>, <em class="sig-param"><span class="n">pooling_stride_y</span><span class="o">=</span><span class="default_value">- 1</span></em>, <em class="sig-param"><span class="n">activations_enabled</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">threshold_fire</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">threshold_fire_step</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">threshold_fire_bits</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">padding_value</span><span class="o">=</span><span class="default_value">0</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida/input_convolutional.html#InputConvolutional"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#akida.InputConvolutional" title="Permalink to this definition">¶</a></dt>
<dd><p>The <code class="docutils literal notranslate"><span class="pre">InputConvolutional</span></code> layer is an image-specific input layer.</p>
<p>It is used if images are sent directly to AEE without using the
event-generating method. If the User applies their own event-generating
method, the resulting events should be sent to an InputData type layer
instead.</p>
<p>The InputConvolutional layer accepts images in 8-bit pixels, either
grayscale or RGB. Images are converted to events using a combination of
convolution kernels, activation thresholds and winner-take-all (WTA)
policies. Note that since the layer input is dense, expect approximately one
event per pixel – fewer if there are large contrast-free regions in the
image, such as with the MNIST dataset.</p>
<p>Note that this format is not appropriate for neuromorphic camera type input
which data is natively event-based and should be sent to an InputData type
input layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_width</strong> (<em>int</em>) – input width.</p></li>
<li><p><strong>input_height</strong> (<em>int</em>) – input height.</p></li>
<li><p><strong>input_channels</strong> (<em>int</em>) – number of channels of the input image.</p></li>
<li><p><strong>kernel_width</strong> (<em>int</em>) – convolutional kernel width.</p></li>
<li><p><strong>kernel_height</strong> (<em>int</em>) – convolutional kernel height.</p></li>
<li><p><strong>num_neurons</strong> (<em>int</em>) – number of neurons (filters).</p></li>
<li><p><strong>name</strong> (<em>str</em><em>, </em><em>optional</em>) – name of the layer.</p></li>
<li><p><strong>convolution_mode</strong> (<a class="reference internal" href="#akida.ConvolutionMode" title="akida.ConvolutionMode"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvolutionMode</span></code></a>, optional) – type of
convolution.</p></li>
<li><p><strong>stride_x</strong> (<em>int</em><em>, </em><em>optional</em>) – convolution stride X.</p></li>
<li><p><strong>stride_y</strong> (<em>int</em><em>, </em><em>optional</em>) – convolution stride Y.</p></li>
<li><p><strong>weights_bits</strong> (<em>int</em><em>, </em><em>optional</em>) – number of bits used to quantize weights.</p></li>
<li><p><strong>pooling_width</strong> (<em>int</em><em>, </em><em>optional</em>) – pooling window width. If set to -1 it
will be global.</p></li>
<li><p><strong>pooling_height</strong> (<em>int</em><em>, </em><em>optional</em>) – pooling window height. If set to -1 it
will be global.</p></li>
<li><p><strong>pooling_type</strong> (<a class="reference internal" href="#akida.PoolingType" title="akida.PoolingType"><code class="xref py py-obj docutils literal notranslate"><span class="pre">PoolingType</span></code></a>, optional) – pooling type
(None, Max or Average).</p></li>
<li><p><strong>pooling_stride_x</strong> (<em>int</em><em>, </em><em>optional</em>) – pooling stride on x dimension.</p></li>
<li><p><strong>pooling_stride_y</strong> (<em>int</em><em>, </em><em>optional</em>) – pooling stride on y dimension.</p></li>
<li><p><strong>activations_enabled</strong> (<em>bool</em><em>, </em><em>optional</em>) – enable or disable activation
function.</p></li>
<li><p><strong>threshold_fire</strong> (<em>int</em><em>, </em><em>optional</em>) – threshold for neurons to fire or
generate an event.</p></li>
<li><p><strong>threshold_fire_step</strong> (<em>float</em><em>, </em><em>optional</em>) – length of the potential
quantization intervals.</p></li>
<li><p><strong>threshold_fire_bits</strong> (<em>int</em><em>, </em><em>optional</em>) – number of bits used to quantize
the neuron response.</p></li>
<li><p><strong>padding_value</strong> (<em>int</em><em>, </em><em>optional</em>) – value used when padding.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="fullyconnected">
<h2>FullyConnected<a class="headerlink" href="#fullyconnected" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="akida.FullyConnected">
<em class="property">class </em><code class="sig-prename descclassname">akida.</code><code class="sig-name descname">FullyConnected</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">num_neurons</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">''</span></em>, <em class="sig-param"><span class="n">weights_bits</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">activations_enabled</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">threshold_fire</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">threshold_fire_step</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">threshold_fire_bits</span><span class="o">=</span><span class="default_value">1</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida/fully_connected.html#FullyConnected"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#akida.FullyConnected" title="Permalink to this definition">¶</a></dt>
<dd><p>This is used for most processing purposes, since any neuron in the layer
can be connected to any input channel.</p>
<p>Outputs are returned from FullyConnected layers as a list of events, that
is, as a triplet of x, y and feature values. However, FullyConnected
models by definition have no intrinsic spatial organization. Thus, all
output events have x and y values of zero with only the f value being
meaningful – corresponding to the index of the event-generating neuron.
Note that each neuron can only generate a single event for each packet of
inputs processed.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_neurons</strong> (<em>int</em>) – number of neurons (filters).</p></li>
<li><p><strong>name</strong> (<em>str</em><em>, </em><em>optional</em>) – name of the layer.</p></li>
<li><p><strong>weights_bits</strong> (<em>int</em><em>, </em><em>optional</em>) – number of bits used to quantize weights.</p></li>
<li><p><strong>activations_enabled</strong> (<em>bool</em><em>, </em><em>optional</em>) – enable or disable activation
function.</p></li>
<li><p><strong>threshold_fire</strong> (<em>int</em><em>, </em><em>optional</em>) – threshold for neurons to fire or
generate an event.</p></li>
<li><p><strong>threshold_fire_step</strong> (<em>float</em><em>, </em><em>optional</em>) – length of the potential
quantization intervals.</p></li>
<li><p><strong>threshold_fire_bits</strong> (<em>int</em><em>, </em><em>optional</em>) – number of bits used to
quantize the neuron response.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="convolutional">
<h2>Convolutional<a class="headerlink" href="#convolutional" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="akida.Convolutional">
<em class="property">class </em><code class="sig-prename descclassname">akida.</code><code class="sig-name descname">Convolutional</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">kernel_width</span></em>, <em class="sig-param"><span class="n">kernel_height</span></em>, <em class="sig-param"><span class="n">num_neurons</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">''</span></em>, <em class="sig-param"><span class="n">convolution_mode</span><span class="o">=</span><span class="default_value">ConvolutionMode.Same</span></em>, <em class="sig-param"><span class="n">stride_x</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">stride_y</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">weights_bits</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">pooling_width</span><span class="o">=</span><span class="default_value">- 1</span></em>, <em class="sig-param"><span class="n">pooling_height</span><span class="o">=</span><span class="default_value">- 1</span></em>, <em class="sig-param"><span class="n">pooling_type</span><span class="o">=</span><span class="default_value">PoolingType.NoPooling</span></em>, <em class="sig-param"><span class="n">pooling_stride_x</span><span class="o">=</span><span class="default_value">- 1</span></em>, <em class="sig-param"><span class="n">pooling_stride_y</span><span class="o">=</span><span class="default_value">- 1</span></em>, <em class="sig-param"><span class="n">activations_enabled</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">threshold_fire</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">threshold_fire_step</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">threshold_fire_bits</span><span class="o">=</span><span class="default_value">1</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida/convolutional.html#Convolutional"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#akida.Convolutional" title="Permalink to this definition">¶</a></dt>
<dd><p>Convolutional or “weight-sharing” layers are commonly used in visual
processing. However, the convolution operation is extremely useful in any
domain where translational invariance is required – that is, where localized
patterns may be of interest regardless of absolute position within the
input. The convolution implemented here is typical of that used in visual
processing, i.e., it is a 2D convolution (across the x- and y-dimensions),
but a 3D input with a 3D filter. No convolution occurs across the third
dimension; events from input feature 1 only interact with connections to
input feature 1 – likewise for input feature 2 and so on. Typically,
the input feature is the identity of the event-emitting neuron in the
previous layer.</p>
<p>Outputs are returned from convolutional layers as a list of events, that is,
as a triplet of x, y and feature (neuron index) values. Note that for a
single packet processed, each neuron can only generate a single event at a
given location, but can generate events at multiple different locations and
that multiple neurons may all generate events at a single location.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>kernel_width</strong> (<em>int</em>) – convolutional kernel width.</p></li>
<li><p><strong>kernel_height</strong> (<em>int</em>) – convolutional kernel height.</p></li>
<li><p><strong>num_neurons</strong> (<em>int</em>) – number of neurons (filters).</p></li>
<li><p><strong>name</strong> (<em>str</em><em>, </em><em>optional</em>) – name of the layer.</p></li>
<li><p><strong>convolution_mode</strong> (<a class="reference internal" href="#akida.ConvolutionMode" title="akida.ConvolutionMode"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvolutionMode</span></code></a>, optional) – type of convolution.</p></li>
<li><p><strong>stride_x</strong> (<em>int</em><em>, </em><em>optional</em>) – convolution stride X.</p></li>
<li><p><strong>stride_y</strong> (<em>int</em><em>, </em><em>optional</em>) – convolution stride Y.</p></li>
<li><p><strong>weights_bits</strong> (<em>int</em><em>, </em><em>optional</em>) – number of bits used to quantize weights.</p></li>
<li><p><strong>pooling_width</strong> (<em>int</em><em>, </em><em>optional</em>) – pooling window width. If set to -1 it
will be global.</p></li>
<li><p><strong>pooling_height</strong> (<em>int</em><em>, </em><em>optional</em>) – pooling window height. If set to -1
it will be global.</p></li>
<li><p><strong>pooling_type</strong> (<a class="reference internal" href="#akida.PoolingType" title="akida.PoolingType"><code class="xref py py-obj docutils literal notranslate"><span class="pre">PoolingType</span></code></a>, optional) – pooling type
(None, Max or Average).</p></li>
<li><p><strong>pooling_stride_x</strong> (<em>int</em><em>, </em><em>optional</em>) – pooling stride on x dimension.</p></li>
<li><p><strong>pooling_stride_y</strong> (<em>int</em><em>, </em><em>optional</em>) – pooling stride on y dimension.</p></li>
<li><p><strong>activations_enabled</strong> (<em>bool</em><em>, </em><em>optional</em>) – enable or disable activation
function.</p></li>
<li><p><strong>threshold_fire</strong> (<em>int</em><em>, </em><em>optional</em>) – threshold for neurons to fire or
generate an event.</p></li>
<li><p><strong>threshold_fire_step</strong> (<em>float</em><em>, </em><em>optional</em>) – length of the potential
quantization intervals.</p></li>
<li><p><strong>threshold_fire_bits</strong> (<em>int</em><em>, </em><em>optional</em>) – number of bits used to quantize
the neuron response.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="separableconvolutional">
<h2>SeparableConvolutional<a class="headerlink" href="#separableconvolutional" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="akida.SeparableConvolutional">
<em class="property">class </em><code class="sig-prename descclassname">akida.</code><code class="sig-name descname">SeparableConvolutional</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">kernel_width</span></em>, <em class="sig-param"><span class="n">kernel_height</span></em>, <em class="sig-param"><span class="n">num_neurons</span></em>, <em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">''</span></em>, <em class="sig-param"><span class="n">convolution_mode</span><span class="o">=</span><span class="default_value">ConvolutionMode.Same</span></em>, <em class="sig-param"><span class="n">stride_x</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">stride_y</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">weights_bits</span><span class="o">=</span><span class="default_value">2</span></em>, <em class="sig-param"><span class="n">pooling_width</span><span class="o">=</span><span class="default_value">- 1</span></em>, <em class="sig-param"><span class="n">pooling_height</span><span class="o">=</span><span class="default_value">- 1</span></em>, <em class="sig-param"><span class="n">pooling_type</span><span class="o">=</span><span class="default_value">PoolingType.NoPooling</span></em>, <em class="sig-param"><span class="n">pooling_stride_x</span><span class="o">=</span><span class="default_value">- 1</span></em>, <em class="sig-param"><span class="n">pooling_stride_y</span><span class="o">=</span><span class="default_value">- 1</span></em>, <em class="sig-param"><span class="n">activations_enabled</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">threshold_fire</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">threshold_fire_step</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">threshold_fire_bits</span><span class="o">=</span><span class="default_value">1</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida/separable_convolutional.html#SeparableConvolutional"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#akida.SeparableConvolutional" title="Permalink to this definition">¶</a></dt>
<dd><p>Separable convolutions consist in first performing a depthwise spatial
convolution (which acts on each input channel separately) followed by a
pointwise convolution which mixes together the resulting output channels.
Intuitively, separable convolutions can be understood as a way to factorize
a convolution kernel into two smaller kernels, thus decreasing the number of
computations required to evaluate the output potentials. The
<code class="docutils literal notranslate"><span class="pre">SeparableConvolutional</span></code> layer can also integrate a final pooling
operation to reduce its spatial output dimensions.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>kernel_width</strong> (<em>int</em>) – convolutional kernel width.</p></li>
<li><p><strong>kernel_height</strong> (<em>int</em>) – convolutional kernel height.</p></li>
<li><p><strong>num_neurons</strong> (<em>int</em>) – number of pointwise neurons.</p></li>
<li><p><strong>name</strong> (<em>str</em><em>, </em><em>optional</em>) – name of the layer.</p></li>
<li><p><strong>convolution_mode</strong> (<a class="reference internal" href="#akida.ConvolutionMode" title="akida.ConvolutionMode"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvolutionMode</span></code></a>, optional) – type of convolution.</p></li>
<li><p><strong>stride_x</strong> (<em>int</em><em>, </em><em>optional</em>) – convolution stride X.</p></li>
<li><p><strong>stride_y</strong> (<em>int</em><em>, </em><em>optional</em>) – convolution stride Y.</p></li>
<li><p><strong>weights_bits</strong> (<em>int</em><em>, </em><em>optional</em>) – number of bits used to quantize weights.</p></li>
<li><p><strong>pooling_width</strong> (<em>int</em><em>, </em><em>optional</em>) – pooling window width. If set to -1 it
will be global.</p></li>
<li><p><strong>pooling_height</strong> (<em>int</em><em>, </em><em>optional</em>) – pooling window height. If set to -1
it will be global.</p></li>
<li><p><strong>pooling_type</strong> (<a class="reference internal" href="#akida.PoolingType" title="akida.PoolingType"><code class="xref py py-obj docutils literal notranslate"><span class="pre">PoolingType</span></code></a>, optional) – pooling type
(None, Max or Average).</p></li>
<li><p><strong>pooling_stride_x</strong> (<em>int</em><em>, </em><em>optional</em>) – pooling stride on x dimension.</p></li>
<li><p><strong>pooling_stride_y</strong> (<em>int</em><em>, </em><em>optional</em>) – pooling stride on y dimension.</p></li>
<li><p><strong>activations_enabled</strong> (<em>bool</em><em>, </em><em>optional</em>) – enable or disable activation
function.</p></li>
<li><p><strong>threshold_fire</strong> (<em>int</em><em>, </em><em>optional</em>) – threshold for neurons to fire or
generate an event.</p></li>
<li><p><strong>threshold_fire_step</strong> (<em>float</em><em>, </em><em>optional</em>) – length of the potential
quantization intervals.</p></li>
<li><p><strong>threshold_fire_bits</strong> (<em>int</em><em>, </em><em>optional</em>) – number of bits used to quantize
the neuron response.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="concat">
<h2>Concat<a class="headerlink" href="#concat" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="akida.Concat">
<em class="property">class </em><code class="sig-prename descclassname">akida.</code><code class="sig-name descname">Concat</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span><span class="o">=</span><span class="default_value">''</span></em>, <em class="sig-param"><span class="n">activations_enabled</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">threshold_fire</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">threshold_fire_step</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">threshold_fire_bits</span><span class="o">=</span><span class="default_value">1</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida/concat.html#Concat"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#akida.Concat" title="Permalink to this definition">¶</a></dt>
<dd><p>Concatenates its inputs along the last dimension</p>
<p>It takes as input a list of tensors, all of the same shape except for the
last dimension, and returns a single tensor that is the concatenation
of all inputs.</p>
<p>It accepts as inputs either potentials or activations.</p>
<p>It can perform an activation on the concatenated output with its own set of
activation parameters and variables.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<em>str</em><em>, </em><em>optional</em>) – name of the layer.</p></li>
<li><p><strong>activations_enabled</strong> (<em>bool</em><em>, </em><em>optional</em>) – enable or disable activation
function.</p></li>
<li><p><strong>threshold_fire</strong> (<em>int</em><em>, </em><em>optional</em>) – threshold for neurons to fire or
generate an event.</p></li>
<li><p><strong>threshold_fire_step</strong> (<em>float</em><em>, </em><em>optional</em>) – length of the potential
quantization intervals.</p></li>
<li><p><strong>threshold_fire_bits</strong> (<em>int</em><em>, </em><em>optional</em>) – number of bits used to
quantize the neuron response.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="dense">
<h2>Dense<a class="headerlink" href="#dense" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="akida.Dense">
<em class="property">class </em><code class="sig-prename descclassname">akida.</code><code class="sig-name descname">Dense</code><a class="headerlink" href="#akida.Dense" title="Permalink to this definition">¶</a></dt>
<dd><p><strong>Attributes:</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#akida.Dense.shape" title="akida.Dense.shape"><code class="xref py py-obj docutils literal notranslate"><span class="pre">shape</span></code></a></p></td>
<td><p>Returns the shape of this tensor.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#akida.Dense.size" title="akida.Dense.size"><code class="xref py py-obj docutils literal notranslate"><span class="pre">size</span></code></a></p></td>
<td><p>Returns the size of this tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#akida.Dense.type" title="akida.Dense.type"><code class="xref py py-obj docutils literal notranslate"><span class="pre">type</span></code></a></p></td>
<td><p>Returns the type of this tensor.</p></td>
</tr>
</tbody>
</table>
<p><strong>Methods:</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#akida.Dense.to_numpy" title="akida.Dense.to_numpy"><code class="xref py py-obj docutils literal notranslate"><span class="pre">to_numpy</span></code></a>(self)</p></td>
<td><p>Converts the tensor to a numpy.ndarray object.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="akida.Dense.shape">
<em class="property">property </em><code class="sig-name descname">shape</code><a class="headerlink" href="#akida.Dense.shape" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the shape of this tensor.</p>
</dd></dl>

<dl class="py method">
<dt id="akida.Dense.size">
<em class="property">property </em><code class="sig-name descname">size</code><a class="headerlink" href="#akida.Dense.size" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the size of this tensor.</p>
</dd></dl>

<dl class="py method">
<dt id="akida.Dense.to_numpy">
<code class="sig-name descname">to_numpy</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span><span class="p">:</span> <span class="n">akida.core.Tensor</span></em><span class="sig-paren">)</span> &#x2192; array<a class="headerlink" href="#akida.Dense.to_numpy" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts the tensor to a numpy.ndarray object.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a numpy.ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="akida.Dense.type">
<em class="property">property </em><code class="sig-name descname">type</code><a class="headerlink" href="#akida.Dense.type" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the type of this tensor.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="backend">
<h2>Backend<a class="headerlink" href="#backend" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="akida.BackendType">
<em class="property">class </em><code class="sig-prename descclassname">akida.</code><code class="sig-name descname">BackendType</code><a class="headerlink" href="#akida.BackendType" title="Permalink to this definition">¶</a></dt>
<dd><p>Members:</p>
<p>Software</p>
<p>Hardware</p>
<p>Hybrid</p>
</dd></dl>

<dl class="py function">
<dt id="akida.has_backend">
<code class="sig-prename descclassname">akida.</code><code class="sig-name descname">has_backend</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">backend</span><span class="p">:</span> <span class="n">akida.core.BackendType</span></em><span class="sig-paren">)</span> &#x2192; bool<a class="headerlink" href="#akida.has_backend" title="Permalink to this definition">¶</a></dt>
<dd><p>Checks if a given backend type is available</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>backend</strong> (<a class="reference internal" href="#akida.BackendType" title="akida.BackendType"><em>BackendType</em></a>) – the backend to check</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a bool</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="akida.backends">
<code class="sig-prename descclassname">akida.</code><code class="sig-name descname">backends</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; Dict<span class="p">[</span>akida.core.BackendType<span class="p">, </span>akida.core.Backend<span class="p">]</span><a class="headerlink" href="#akida.backends" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the full list of available backends</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>list of BackendType</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="convolutionmode">
<h2>ConvolutionMode<a class="headerlink" href="#convolutionmode" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="akida.ConvolutionMode">
<em class="property">class </em><code class="sig-prename descclassname">akida.</code><code class="sig-name descname">ConvolutionMode</code><a class="headerlink" href="#akida.ConvolutionMode" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the effective padding of the input for convolution, thereby determining the output dimensions. Naming conventions are the same as Keras/Tensorflow.</p>
<p>Members:</p>
<blockquote>
<div><p>Valid : No padding</p>
<p>Same : Padded so that output size is input size divided by the stride</p>
<p>Full : Padded so that convolution is computed at each point of overlap</p>
</div></blockquote>
</dd></dl>

</div>
<div class="section" id="poolingtype">
<h2>PoolingType<a class="headerlink" href="#poolingtype" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="akida.PoolingType">
<em class="property">class </em><code class="sig-prename descclassname">akida.</code><code class="sig-name descname">PoolingType</code><a class="headerlink" href="#akida.PoolingType" title="Permalink to this definition">¶</a></dt>
<dd><p>The pooling type</p>
<p>Members:</p>
<blockquote>
<div><p>NoPooling : No pooling applied</p>
<p>Max : Maximum pixel value is selected</p>
<p>Average : Average pixel value is selected</p>
</div></blockquote>
</dd></dl>

</div>
<div class="section" id="learningtype">
<h2>LearningType<a class="headerlink" href="#learningtype" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="akida.LearningType">
<em class="property">class </em><code class="sig-prename descclassname">akida.</code><code class="sig-name descname">LearningType</code><a class="headerlink" href="#akida.LearningType" title="Permalink to this definition">¶</a></dt>
<dd><p>The learning type</p>
<p>Members:</p>
<blockquote>
<div><p>NoLearning : Learning is disabled, inference-only mode</p>
<p>AkidaUnsupervised : Built-in unsupervised learning rules</p>
</div></blockquote>
</dd></dl>

</div>
<div class="section" id="compatibility">
<h2>Compatibility<a class="headerlink" href="#compatibility" title="Permalink to this headline">¶</a></h2>
<dl class="py function">
<dt id="akida.compatibility.model_hardware_incompatibilities">
<code class="sig-prename descclassname">akida.compatibility.</code><code class="sig-name descname">model_hardware_incompatibilities</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">model</span></em>, <em class="sig-param"><span class="n">nsoc_version</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida/compatibility/check.html#model_hardware_incompatibilities"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#akida.compatibility.model_hardware_incompatibilities" title="Permalink to this definition">¶</a></dt>
<dd><p>Checks a model compatibility with hardware.</p>
<p>This method performs parameters value checking for hardware
compatibility and returns incompatibility messages when needed.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Model</span></code>) – the Model to check hardware compatibility</p></li>
<li><p><strong>nsoc_version</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">NsocVersion</span></code>, optional) – the NSoC version to check</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a list of str containing the hardware incompatibilities of the model.
The list is empty if the model is hardware compatible.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="akida.compatibility.create_from_model">
<code class="sig-prename descclassname">akida.compatibility.</code><code class="sig-name descname">create_from_model</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">model</span></em>, <em class="sig-param"><span class="n">nsoc_version</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida/compatibility/conversion.html#create_from_model"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#akida.compatibility.create_from_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Tries to create a HW compatible model from an incompatible one</p>
<p>Tries to create a HW compatible model from an incompatible one, using SW
workarounds for known limitations. It returns a converted model that is not
guaranteed to be HW compatible, depending if workaround have been found.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">Model</span></code>) – a Model object to convert</p></li>
<li><p><strong>nsoc_version</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">NsocVersion</span></code>, optional) – version of the NSoC</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a new Model with no guarantee that it is HW compatible.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">Model</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt id="akida.NsocVersion">
<em class="property">class </em><code class="sig-prename descclassname">akida.</code><code class="sig-name descname">NsocVersion</code><a class="headerlink" href="#akida.NsocVersion" title="Permalink to this definition">¶</a></dt>
<dd><p>Members:</p>
<p>Unknown</p>
<p>v1</p>
</dd></dl>

</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="cnn2snn_apis.html" class="btn btn-neutral float-right" title="CNN2SNN Toolkit API" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="api_reference.html" class="btn btn-neutral float-left" title="API reference" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020, BrainChip Holdings Ltd. All Rights Reserved.

    </p>
  </div> 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>