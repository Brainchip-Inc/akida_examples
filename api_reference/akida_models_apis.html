<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Akida models API &mdash; Akida Examples  documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery-binder.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery-dataframe.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery-rendered-html.css" type="text/css" />
      <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Akida examples" href="../examples/index.html" />
    <link rel="prev" title="QuantizeML API" href="quantizeml_apis.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #989898" >

          
          
          <a href="../index.html">
            
              <img src="../_static/MetaTF_logo.png" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                Akida, 2nd Generation
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../index.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../installation.html#supported-configurations">Supported configurations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../installation.html#quick-installation">Quick installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../installation.html#running-examples">Running examples</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../user_guide/user_guide.html">User guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../user_guide/akida.html">Akida user guide</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/akida.html#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/akida.html#programming-interface">Programming interface</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida.html#the-akida-model">The Akida Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida.html#akida-layers">Akida layers</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/akida.html#model-hardware-mapping">Model Hardware Mapping</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida.html#devices">Devices</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida.html#model-mapping">Model mapping</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida.html#advanced-mapping-details-and-hardware-devices-usage">Advanced Mapping Details and Hardware Devices Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida.html#performance-measurement">Performance measurement</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/akida.html#using-akida-edge-learning">Using Akida Edge learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida.html#learning-constraints">Learning constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida.html#compiling-a-layer">Compiling a layer</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../user_guide/quantizeml.html">QuantizeML toolkit</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/quantizeml.html#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/quantizeml.html#the-fixedpoint-representation">The FixedPoint representation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/quantizeml.html#quantization-flow">Quantization flow</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/quantizeml.html#compatibility-constraints">Compatibility constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/quantizeml.html#model-loading">Model loading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/quantizeml.html#command-line-interface">Command line interface</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/quantizeml.html#quantize-cli">quantize CLI</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/quantizeml.html#config-cli">config CLI</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/quantizeml.html#check-cli">check CLI</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/quantizeml.html#insert-rescaling-cli">insert_rescaling CLI</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/quantizeml.html#supported-layer-types">Supported layer types</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../user_guide/cnn2snn.html">CNN2SNN toolkit</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/cnn2snn.html#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/cnn2snn.html#conversion-flow">Conversion flow</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#conversion-compatibility">Conversion compatibility</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#command-line-interface">Command-line interface</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/cnn2snn.html#handling-akida-1-0-and-akida-2-0-specificities">Handling Akida 1.0 and Akida 2.0 specificities</a></li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/cnn2snn.html#legacy-quantization-api">Legacy quantization API</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#typical-quantization-scenario">Typical quantization scenario</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#design-compatibility-constraints">Design compatibility constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#id3">Command-line interface</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#layers-considerations">Layers Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#tips-and-tricks">Tips and Tricks</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../user_guide/akida_models.html">Akida models zoo</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/akida_models.html#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/akida_models.html#command-line-interface-for-model-creation">Command-line interface for model creation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/akida_models.html#command-line-interface-for-model-training">Command-line interface for model training</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida_models.html#kws-training">KWS training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida_models.html#akidanet-training">AkidaNet training</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/akida_models.html#command-line-interface-for-model-evaluation">Command-line interface for model evaluation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/akida_models.html#command-line-interface-to-evaluate-model-macs">Command-line interface to evaluate model MACS</a></li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/akida_models.html#id1">Layer Blocks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/akida_models.html#handling-akida-1-0-and-akida-2-0-specificities">Handling Akida 1.0 and Akida 2.0 specificities</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../user_guide/hw_constraints.html">Hardware constraints</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/hw_constraints.html#inputconvolutional">InputConvolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/hw_constraints.html#convolutional">Convolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/hw_constraints.html#separableconvolutional">SeparableConvolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/hw_constraints.html#fullyconnected">FullyConnected</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="api_reference.html">API reference</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="akida_apis.html">Akida runtime</a><ul>
<li class="toctree-l3"><a class="reference internal" href="akida_apis.html#model">Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="akida_apis.html#layer">Layer</a><ul>
<li class="toctree-l4"><a class="reference internal" href="akida_apis.html#id1">Layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida_apis.html#mapping">Mapping</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="akida_apis.html#akida-v1-layers">Akida V1 layers</a></li>
<li class="toctree-l3"><a class="reference internal" href="akida_apis.html#akida-v2-layers">Akida V2 layers</a></li>
<li class="toctree-l3"><a class="reference internal" href="akida_apis.html#layer-parameters">Layer parameters</a><ul>
<li class="toctree-l4"><a class="reference internal" href="akida_apis.html#layertype">LayerType</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida_apis.html#padding">Padding</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida_apis.html#pooltype">PoolType</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="akida_apis.html#optimizers">Optimizers</a></li>
<li class="toctree-l3"><a class="reference internal" href="akida_apis.html#sequence">Sequence</a><ul>
<li class="toctree-l4"><a class="reference internal" href="akida_apis.html#id2">Sequence</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida_apis.html#backendtype">BackendType</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida_apis.html#pass">Pass</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="akida_apis.html#device">Device</a><ul>
<li class="toctree-l4"><a class="reference internal" href="akida_apis.html#id3">Device</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida_apis.html#hwversion">HwVersion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="akida_apis.html#hwdevice">HWDevice</a><ul>
<li class="toctree-l4"><a class="reference internal" href="akida_apis.html#id4">HWDevice</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida_apis.html#socdriver">SocDriver</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida_apis.html#clockmode">ClockMode</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="akida_apis.html#powermeter">PowerMeter</a></li>
<li class="toctree-l3"><a class="reference internal" href="akida_apis.html#np">NP</a></li>
<li class="toctree-l3"><a class="reference internal" href="akida_apis.html#tools">Tools</a><ul>
<li class="toctree-l4"><a class="reference internal" href="akida_apis.html#sparsity">Sparsity</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida_apis.html#compatibility">Compatibility</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="cnn2snn_apis.html">CNN2SNN</a><ul>
<li class="toctree-l3"><a class="reference internal" href="cnn2snn_apis.html#akida-version">Akida version</a></li>
<li class="toctree-l3"><a class="reference internal" href="cnn2snn_apis.html#conversion">Conversion</a></li>
<li class="toctree-l3"><a class="reference internal" href="cnn2snn_apis.html#legacy-quantization-api">Legacy quantization API</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn_apis.html#utils">Utils</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn_apis.html#calibration">Calibration</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn_apis.html#transforms">Transforms</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn_apis.html#constraint">Constraint</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn_apis.html#quantization">Quantization</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn_apis.html#quantizers">Quantizers</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn_apis.html#quantized-layers">Quantized layers</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="quantizeml_apis.html">QuantizeML</a><ul>
<li class="toctree-l3"><a class="reference internal" href="quantizeml_apis.html#layers">Layers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="quantizeml_apis.html#reshaping">Reshaping</a></li>
<li class="toctree-l4"><a class="reference internal" href="quantizeml_apis.html#activations">Activations</a></li>
<li class="toctree-l4"><a class="reference internal" href="quantizeml_apis.html#attention">Attention</a></li>
<li class="toctree-l4"><a class="reference internal" href="quantizeml_apis.html#normalization">Normalization</a></li>
<li class="toctree-l4"><a class="reference internal" href="quantizeml_apis.html#convolution">Convolution</a></li>
<li class="toctree-l4"><a class="reference internal" href="quantizeml_apis.html#depthwise-convolution">Depthwise convolution</a></li>
<li class="toctree-l4"><a class="reference internal" href="quantizeml_apis.html#separable-convolution">Separable convolution</a></li>
<li class="toctree-l4"><a class="reference internal" href="quantizeml_apis.html#dense">Dense</a></li>
<li class="toctree-l4"><a class="reference internal" href="quantizeml_apis.html#skip-connection">Skip connection</a></li>
<li class="toctree-l4"><a class="reference internal" href="quantizeml_apis.html#pooling">Pooling</a></li>
<li class="toctree-l4"><a class="reference internal" href="quantizeml_apis.html#shiftmax">Shiftmax</a></li>
<li class="toctree-l4"><a class="reference internal" href="quantizeml_apis.html#transformers">Transformers</a></li>
<li class="toctree-l4"><a class="reference internal" href="quantizeml_apis.html#rescaling">Rescaling</a></li>
<li class="toctree-l4"><a class="reference internal" href="quantizeml_apis.html#dropout">Dropout</a></li>
<li class="toctree-l4"><a class="reference internal" href="quantizeml_apis.html#quantizers">Quantizers</a></li>
<li class="toctree-l4"><a class="reference internal" href="quantizeml_apis.html#quantization-parameters">Quantization parameters</a></li>
<li class="toctree-l4"><a class="reference internal" href="quantizeml_apis.html#calibration">Calibration</a></li>
<li class="toctree-l4"><a class="reference internal" href="quantizeml_apis.html#recording">Recording</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="quantizeml_apis.html#models">Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="quantizeml_apis.html#transforms">Transforms</a></li>
<li class="toctree-l4"><a class="reference internal" href="quantizeml_apis.html#quantization">Quantization</a></li>
<li class="toctree-l4"><a class="reference internal" href="quantizeml_apis.html#id1">Calibration</a></li>
<li class="toctree-l4"><a class="reference internal" href="quantizeml_apis.html#utils">Utils</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="quantizeml_apis.html#tensors">Tensors</a><ul>
<li class="toctree-l4"><a class="reference internal" href="quantizeml_apis.html#qtensor">QTensor</a></li>
<li class="toctree-l4"><a class="reference internal" href="quantizeml_apis.html#fixedpoint">FixedPoint</a></li>
<li class="toctree-l4"><a class="reference internal" href="quantizeml_apis.html#qfloat">QFloat</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Akida models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#layer-blocks">Layer blocks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#cnn-blocks">CNN blocks</a></li>
<li class="toctree-l4"><a class="reference internal" href="#transformers-blocks">Transformers blocks</a></li>
<li class="toctree-l4"><a class="reference internal" href="#transposed-blocks">Transposed blocks</a></li>
<li class="toctree-l4"><a class="reference internal" href="#detection-block">Detection block</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#helpers">Helpers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#gamma-constraint">Gamma constraint</a></li>
<li class="toctree-l4"><a class="reference internal" href="#unfusing-separableconvolutional">Unfusing SeparableConvolutional</a></li>
<li class="toctree-l4"><a class="reference internal" href="#extract-samples">Extract samples</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#knowledge-distillation">Knowledge distillation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#macs">MACS</a></li>
<li class="toctree-l3"><a class="reference internal" href="#model-i-o">Model I/O</a></li>
<li class="toctree-l3"><a class="reference internal" href="#utils">Utils</a></li>
<li class="toctree-l3"><a class="reference internal" href="#model-zoo">Model zoo</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#akidanet">AkidaNet</a></li>
<li class="toctree-l4"><a class="reference internal" href="#mobilenet">Mobilenet</a></li>
<li class="toctree-l4"><a class="reference internal" href="#ds-cnn">DS-CNN</a></li>
<li class="toctree-l4"><a class="reference internal" href="#vgg">VGG</a></li>
<li class="toctree-l4"><a class="reference internal" href="#yolo">YOLO</a></li>
<li class="toctree-l4"><a class="reference internal" href="#pointnet">PointNet++</a></li>
<li class="toctree-l4"><a class="reference internal" href="#gxnor">GXNOR</a></li>
<li class="toctree-l4"><a class="reference internal" href="#centernet">CenterNet</a></li>
<li class="toctree-l4"><a class="reference internal" href="#akidaunet">AkidaUNet</a></li>
<li class="toctree-l4"><a class="reference internal" href="#transformers">Transformers</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../examples/index.html">Examples</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../examples/index.html#general-examples">General examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_0_global_workflow.html">Global Akida workflow</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_0_global_workflow.html#create-and-train">1. Create and train</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_0_global_workflow.html#quantize">2. Quantize</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_0_global_workflow.html#convert">3. Convert</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_0_global_workflow.html#gxnor-mnist">4. GXNOR/MNIST</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_1_akidanet_imagenet.html">AkidaNet/ImageNet inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_1_akidanet_imagenet.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_1_akidanet_imagenet.html#pretrained-quantized-model">2. Pretrained quantized model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_1_akidanet_imagenet.html#conversion-to-akida">3. Conversion to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_1_akidanet_imagenet.html#hardware-mapping-and-performance">4. Hardware mapping and performance</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_2_ds_cnn_kws.html">DS-CNN/KWS inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_2_ds_cnn_kws.html#load-the-preprocessed-dataset">1. Load the preprocessed dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_2_ds_cnn_kws.html#load-a-pre-trained-native-keras-model">2. Load a pre-trained native Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_2_ds_cnn_kws.html#load-a-pre-trained-quantized-keras-model">3. Load a pre-trained quantized Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_2_ds_cnn_kws.html#conversion-to-akida">4. Conversion to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_2_ds_cnn_kws.html#confusion-matrix">5. Confusion matrix</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_3_regression.html">Age estimation (regression) example</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_3_regression.html#load-the-utkface-dataset">1. Load the UTKFace Dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_3_regression.html#load-a-pre-trained-native-keras-model">2. Load a pre-trained native Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_3_regression.html#load-a-pre-trained-quantized-keras-model">3. Load a pre-trained quantized Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_3_regression.html#conversion-to-akida">4. Conversion to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_3_regression.html#estimate-age-on-a-single-image">5. Estimate age on a single image</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_4_transfer_learning.html">Transfer learning with AkidaNet for PlantVillage</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_transfer_learning.html#transfer-learning-process">Transfer learning process</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_transfer_learning.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_transfer_learning.html#get-a-trained-akidanet-base-model">2. Get a trained AkidaNet base model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_transfer_learning.html#add-a-classification-head-to-the-model">3. Add a classification head to the model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_transfer_learning.html#freeze-the-base-model">4. Freeze the base model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_transfer_learning.html#train-for-a-few-epochs">5. Train for a few epochs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_transfer_learning.html#quantize-the-model">6. Quantize the model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_transfer_learning.html#compute-accuracy">7. Compute accuracy</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_5_voc_yolo_detection.html">YOLO/PASCAL-VOC detection tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_5_voc_yolo_detection.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_5_voc_yolo_detection.html#preprocessing-tools">2. Preprocessing tools</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_5_voc_yolo_detection.html#model-architecture">3. Model architecture</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_5_voc_yolo_detection.html#training">4. Training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_5_voc_yolo_detection.html#performance">5. Performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_5_voc_yolo_detection.html#conversion-to-akida">6. Conversion to Akida</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_6_segmentation.html">Segmentation tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_6_segmentation.html#load-the-dataset">1. Load the dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_6_segmentation.html#load-a-pre-trained-native-keras-model">2. Load a pre-trained native Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_6_segmentation.html#load-a-pre-trained-quantized-keras-model">3. Load a pre-trained quantized Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_6_segmentation.html#conversion-to-akida">4. Conversion to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_6_segmentation.html#segment-a-single-image">5. Segment a single image</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_7_vision_transformer.html">Build Vision Transformers for Akida</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_7_vision_transformer.html#model-selection">1. Model selection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_7_vision_transformer.html#model-optimization-for-akida-hardware">2. Model optimization for Akida hardware</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_7_vision_transformer.html#model-training">3. Model Training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_7_vision_transformer.html#model-quantization">4. Model quantization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_7_vision_transformer.html#conversion-to-akida">5. Conversion to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_7_vision_transformer.html#displaying-results-attention-maps">6. Displaying results Attention Maps</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/index.html#quantization">Quantization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/quantization/plot_0_advanced_quantizeml.html">Advanced QuantizeML tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/quantization/plot_0_advanced_quantizeml.html#defining-a-quantization-scheme">1. Defining a quantization scheme</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/quantization/plot_0_advanced_quantizeml.html#calibration">2. Calibration</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/quantization/plot_1_upgrading_to_2.0.html">Upgrading to Akida 2.0</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/quantization/plot_1_upgrading_to_2.0.html#workflow-differences">1. Workflow differences</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/quantization/plot_1_upgrading_to_2.0.html#models-architecture-differences">2. Models architecture differences</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/quantization/plot_1_upgrading_to_2.0.html#using-akidaversion">3. Using <code class="docutils literal notranslate"><span class="pre">AkidaVersion</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/index.html#edge-examples">Edge examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/edge/plot_0_edge_learning_vision.html">Akida vision edge learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_0_edge_learning_vision.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_0_edge_learning_vision.html#prepare-akida-model-for-learning">2. Prepare Akida model for learning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_0_edge_learning_vision.html#edge-learning-with-akida">3. Edge learning with Akida</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/edge/plot_1_edge_learning_kws.html">Akida edge learning for keyword spotting</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_1_edge_learning_kws.html#edge-learning-process">1. Edge learning process</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_1_edge_learning_kws.html#dataset-preparation">2. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_1_edge_learning_kws.html#prepare-akida-model-for-learning">3. Prepare Akida model for learning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_1_edge_learning_kws.html#learn-with-akida-using-the-training-set">4. Learn with Akida using the training set</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_1_edge_learning_kws.html#edge-learning">5. Edge learning</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/edge/plot_2_edge_learning_parameters.html">Tips to set Akida learning parameters</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_2_edge_learning_parameters.html#akida-learning-parameters">1. Akida learning parameters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_2_edge_learning_parameters.html#create-akida-model">2. Create Akida model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_2_edge_learning_parameters.html#estimate-the-required-number-of-weights-of-the-trainable-layer">3. Estimate the required number of weights of the trainable layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_2_edge_learning_parameters.html#estimate-the-number-of-neurons-per-class">4. Estimate the number of neurons per class</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/index.html#deprecated-cnn2snn-tutorials">[Deprecated] CNN2SNN tutorials</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/cnn2snn/plot_1_advanced_cnn2snn.html">Advanced CNN2SNN tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_1_advanced_cnn2snn.html#design-a-cnn2snn-quantized-model">1. Design a CNN2SNN quantized model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_1_advanced_cnn2snn.html#weight-quantizer-details">2. Weight Quantizer Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_1_advanced_cnn2snn.html#understanding-quantized-activation">3. Understanding quantized activation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_1_advanced_cnn2snn.html#how-to-deal-with-too-high-scale-factors">4. How to deal with too high scale factors</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../model_zoo_performance.html">Model zoo performance</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../model_zoo_performance.html#akida-1-0-models">Akida 1.0 models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../model_zoo_performance.html#image-icon-ref-image-domain"> Image domain</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../model_zoo_performance.html#classification">Classification</a></li>
<li class="toctree-l4"><a class="reference internal" href="../model_zoo_performance.html#object-detection">Object detection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../model_zoo_performance.html#regression">Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="../model_zoo_performance.html#face-recognition">Face recognition</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../model_zoo_performance.html#audio-icon-ref-audio-domain"> Audio domain</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../model_zoo_performance.html#keyword-spotting">Keyword spotting</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../model_zoo_performance.html#pointcloud-icon-ref-point-cloud"> Point cloud</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../model_zoo_performance.html#id1">Classification</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../model_zoo_performance.html#akida-2-0-models">Akida 2.0 models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../model_zoo_performance.html#id2"> Image domain</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../model_zoo_performance.html#id3">Classification</a></li>
<li class="toctree-l4"><a class="reference internal" href="../model_zoo_performance.html#id6">Object detection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../model_zoo_performance.html#id7">Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="../model_zoo_performance.html#id8">Face recognition</a></li>
<li class="toctree-l4"><a class="reference internal" href="../model_zoo_performance.html#segmentation">Segmentation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../model_zoo_performance.html#id10"> Audio domain</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../model_zoo_performance.html#id11">Keyword spotting</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../model_zoo_performance.html#id12"> Point cloud</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../model_zoo_performance.html#id13">Classification</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../changelog.html">Changelog</a></li>
<li class="toctree-l1"><a class="reference external" href="https://support.brainchip.com/portal/home">Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #989898" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Akida Examples</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="api_reference.html">API reference</a></li>
      <li class="breadcrumb-item active">Akida models API</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="module-akida_models">
<span id="akida-models-api"></span><h1>Akida models API<a class="headerlink" href="#module-akida_models" title="Permalink to this headline"></a></h1>
<p>Imports models.</p>
<section id="layer-blocks">
<h2>Layer blocks<a class="headerlink" href="#layer-blocks" title="Permalink to this headline"></a></h2>
<section id="cnn-blocks">
<h3>CNN blocks<a class="headerlink" href="#cnn-blocks" title="Permalink to this headline"></a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="akida_models.layer_blocks.conv_block">
<span class="sig-prename descclassname"><span class="pre">akida_models.layer_blocks.</span></span><span class="sig-name descname"><span class="pre">conv_block</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filters</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pooling</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">post_relu_gap</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pool_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(2,</span> <span class="pre">2)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_batchnorm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">relu_activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ReLU3.75'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/layer_blocks.html#conv_block"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.layer_blocks.conv_block" title="Permalink to this definition"></a></dt>
<dd><p>Adds a convolutional layer with optional layers in the following order:
max pooling, batch normalization, activation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>tf.Tensor</em>) – input tensor of shape <cite>(rows, cols, channels)</cite></p></li>
<li><p><strong>filters</strong> (<em>int</em>) – the dimensionality of the output space
(i.e. the number of output filters in the convolution).</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em> or </em><em>tuple of 2 integers</em>) – specifying the
height and width of the 2D convolution kernel.
Can be a single integer to specify the same value for
all spatial dimensions.</p></li>
<li><p><strong>pooling</strong> (<em>str</em><em>, </em><em>optional</em>) – add a pooling layer of type ‘pooling’ among the values ‘max’ or
‘global_avg’, with pooling size set to pool_size. If ‘None’, no pooling will be added.</p></li>
<li><p><strong>post_relu_gap</strong> (<em>bool</em><em>, </em><em>optional</em>) – when pooling is ‘global_avg’, indicates if the pooling comes
before or after ReLU activation. Defaults to False.</p></li>
<li><p><strong>pool_size</strong> (<em>int</em><em> or </em><em>tuple of 2 integers</em><em>, </em><em>optional</em>) – factors by which to downscale (vertical,
horizontal). (2, 2) will halve the input in both spatial dimension. If only one integer
is specified, the same window length will be used for both dimensions.</p></li>
<li><p><strong>add_batchnorm</strong> (<em>bool</em><em>, </em><em>optional</em>) – add a BatchNormalization layer</p></li>
<li><p><strong>relu_activation</strong> (<em>str</em><em>, </em><em>optional</em>) – the ReLU activation to add to the layer in the form ‘ReLUx’
where ‘x’ is the max_value to use. Set to False to disable activation. Defaults to
‘ReLU3.75’.</p></li>
<li><p><strong>**kwargs</strong> – arguments passed to the keras.Conv2D layer, such as
strides, padding, use_bias, weight_regularizer, etc.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>output tensor of conv2D block.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.layer_blocks.separable_conv_block">
<span class="sig-prename descclassname"><span class="pre">akida_models.layer_blocks.</span></span><span class="sig-name descname"><span class="pre">separable_conv_block</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filters</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strides</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'same'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pooling</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">post_relu_gap</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pool_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(2,</span> <span class="pre">2)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_batchnorm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">relu_activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ReLU3.75'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fused</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'glorot_uniform'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pointwise_regularizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/layer_blocks.html#separable_conv_block"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.layer_blocks.separable_conv_block" title="Permalink to this definition"></a></dt>
<dd><p>Adds a separable convolutional layer with optional layers in the
following order: global average pooling, max pooling, batch normalization,
activation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>tf.Tensor</em>) – input tensor of shape <cite>(height, width, channels)</cite></p></li>
<li><p><strong>filters</strong> (<em>int</em>) – the dimensionality of the output space
(i.e. the number of output filters in the pointwise convolution).</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em> or </em><em>tuple of 2 integers</em>) – specifying the
height and width of the 2D convolution window. Can be a single
integer to specify the same value for all spatial dimensions.</p></li>
<li><p><strong>strides</strong> (<em>int</em><em> or </em><em>tuple of 2 integers</em><em>, </em><em>optional</em>) – strides of the depthwise convolution.
Defaults to 1.</p></li>
<li><p><strong>padding</strong> (<em>str</em><em>, </em><em>optional</em>) – padding mode for the depthwise convolution. Defaults to ‘same’.</p></li>
<li><p><strong>use_bias</strong> (<em>bool</em><em>, </em><em>optional</em>) – whether the layer uses a bias vector. Defaults to True.</p></li>
<li><p><strong>pooling</strong> (<em>str</em><em>, </em><em>optional</em>) – add a pooling layer of type ‘pooling’ among the values ‘max’, or
‘global_avg’, with pooling size set to pool_size. If ‘None’, no pooling will be added.</p></li>
<li><p><strong>post_relu_gap</strong> (<em>bool</em><em>, </em><em>optional</em>) – when pooling is ‘global_avg’, indicates if the pooling comes
before or after ReLU activation. Defaults to False.</p></li>
<li><p><strong>pool_size</strong> (<em>int</em><em> or </em><em>tuple of 2 integers</em><em>, </em><em>optional</em>) – factors by which to downscale (vertical,
horizontal). (2, 2) will halve the input in both spatial dimension. If only one integer
is specified, the same window length will be used for both dimensions.</p></li>
<li><p><strong>add_batchnorm</strong> (<em>bool</em><em>, </em><em>optional</em>) – add a BatchNormalization layer</p></li>
<li><p><strong>relu_activation</strong> (<em>str</em><em>, </em><em>optional</em>) – the ReLU activation to add to the layer in the form ‘ReLUx’
where ‘x’ is the max_value to use. Set to False to disable activation. Defaults to
‘ReLU3.75’.</p></li>
<li><p><strong>fused</strong> (<em>bool</em><em>, </em><em>optional</em>) – If True use a SeparableConv2D layer otherwise use a
DepthwiseConv2D + Conv2D layers. Defaults to True.</p></li>
<li><p><strong>name</strong> (<em>str</em><em>, </em><em>optional</em>) – name of the layer. Defaults to None.</p></li>
<li><p><strong>kernel_initializer</strong> (<em>keras.initializer</em><em>, </em><em>optional</em>) – initializer for both kernels. Defaults to
‘glorot_uniform’.</p></li>
<li><p><strong>pointwise_regularizer</strong> (<em>keras.regularizers</em><em>, </em><em>optional</em>) – regularizer function applied to the
pointwise kernel matrix. Defaults to None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>output tensor of separable conv block.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.layer_blocks.dense_block">
<span class="sig-prename descclassname"><span class="pre">akida_models.layer_blocks.</span></span><span class="sig-name descname"><span class="pre">dense_block</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">units</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_batchnorm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">relu_activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ReLU3.75'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/layer_blocks.html#dense_block"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.layer_blocks.dense_block" title="Permalink to this definition"></a></dt>
<dd><p>Adds a dense layer with optional layers in the following order:
batch normalization, activation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>tf.Tensor</em>) – Input tensor of shape <cite>(rows, cols, channels)</cite></p></li>
<li><p><strong>units</strong> (<em>int</em>) – dimensionality of the output space</p></li>
<li><p><strong>add_batchnorm</strong> (<em>bool</em><em>, </em><em>optional</em>) – add a BatchNormalization layer</p></li>
<li><p><strong>relu_activation</strong> (<em>str</em><em>, </em><em>optional</em>) – the ReLU activation to add to the layer in the form ‘ReLUx’
where ‘x’ is the max_value to use. Set to False to disable activation. Defaults to
‘ReLU3.75’.</p></li>
<li><p><strong>**kwargs</strong> – arguments passed to the Dense layer, such as
use_bias, kernel_initializer, weight_regularizer, etc.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>output tensor of the dense block.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.Tensor</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="transformers-blocks">
<h3>Transformers blocks<a class="headerlink" href="#transformers-blocks" title="Permalink to this headline"></a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="akida_models.layer_blocks.mlp_block">
<span class="sig-prename descclassname"><span class="pre">akida_models.layer_blocks.</span></span><span class="sig-name descname"><span class="pre">mlp_block</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mlp_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mlp_act</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'GeLU'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/layer_blocks.html#mlp_block"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.layer_blocks.mlp_block" title="Permalink to this definition"></a></dt>
<dd><p>MLP block definition.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>tf.Tensor</em>) – inputs</p></li>
<li><p><strong>mlp_dim</strong> (<em>int</em>) – number of units in the first dense layer</p></li>
<li><p><strong>dropout</strong> (<em>float</em>) – dropout rate</p></li>
<li><p><strong>name</strong> (<em>str</em>) – used as a base name for the layers in the block</p></li>
<li><p><strong>mlp_act</strong> (<em>str</em><em>, </em><em>optional</em>) – string that values in [‘GeLU’, ‘ReLUx’, ‘swish’] and that allows to
choose from GeLU, ReLUx or swish activation. Defaults to “GeLU”.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>MLP block outputs</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.layer_blocks.multi_head_attention">
<span class="sig-prename descclassname"><span class="pre">akida_models.layer_blocks.</span></span><span class="sig-name descname"><span class="pre">multi_head_attention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">softmax</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'softmax'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/layer_blocks.html#multi_head_attention"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.layer_blocks.multi_head_attention" title="Permalink to this definition"></a></dt>
<dd><p>Multi-head attention block definition.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>tf.Tensor</em>) – inputs</p></li>
<li><p><strong>num_heads</strong> (<em>int</em>) – the number of attention heads</p></li>
<li><p><strong>hidden_size</strong> (<em>int</em>) – query, key and value dense layers representation size (units)</p></li>
<li><p><strong>name</strong> (<em>str</em>) – used as a base name for the layers in the block</p></li>
<li><p><strong>softmax</strong> (<em>str</em><em>, </em><em>optional</em>) – string with values in [‘softmax’, ‘softmax2’] that allows to choose
between softmax and softmax2 activation. Defaults to ‘softmax’.</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>ValueError</strong> – if hidden_size is not a multiple of num_heads</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>block outputs and attention softmaxed scores</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>(tf.Tensor, tf.Tensor)</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.layer_blocks.transformer_block">
<span class="sig-prename descclassname"><span class="pre">akida_models.layer_blocks.</span></span><span class="sig-name descname"><span class="pre">transformer_block</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mlp_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'LN'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">softmax</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'softmax'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mlp_act</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'GeLU'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/layer_blocks.html#transformer_block"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.layer_blocks.transformer_block" title="Permalink to this definition"></a></dt>
<dd><p>Transformer block definition.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>tf.Tensor</em>) – inputs</p></li>
<li><p><strong>num_heads</strong> (<em>int</em>) – the number of attention heads</p></li>
<li><p><strong>hidden_size</strong> (<em>int</em>) – multi-head attention block internal size</p></li>
<li><p><strong>mlp_dim</strong> (<em>int</em>) – MLP block internal size</p></li>
<li><p><strong>dropout</strong> (<em>float</em>) – dropout rate</p></li>
<li><p><strong>name</strong> (<em>str</em>) – used as a base name for the layers in the block</p></li>
<li><p><strong>norm</strong> (<em>str</em><em>, </em><em>optional</em>) – string that values in [‘LN’, ‘GN1’, ‘BN’, ‘LMN’] and that allows to
choose from LayerNormalization, GroupNormalization(groups=1, …), BatchNormalization
or LayerMadNormalization layers respectively in the block. Defaults to ‘LN’.</p></li>
<li><p><strong>softmax</strong> (<em>str</em><em>, </em><em>optional</em>) – string with values in [‘softmax’, ‘softmax2’] that allows to choose
between softmax and softmax2 activation in attention. Defaults to ‘softmax’.</p></li>
<li><p><strong>mlp_act</strong> (<em>str</em><em>, </em><em>optional</em>) – string that values in [‘GeLU’, ‘ReLUx’, ‘swish’] and that allows to
choose from GeLU, ReLUx or swish activation in the MLP block. Defaults to “GeLU”.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>block outputs and (attention softmaxed scores, the
normalized sum of inputs and attention outputs)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(tf.Tensor, (tf.Tensor, tf.Tensor))</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="transposed-blocks">
<h3>Transposed blocks<a class="headerlink" href="#transposed-blocks" title="Permalink to this headline"></a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="akida_models.layer_blocks.conv_transpose_block">
<span class="sig-prename descclassname"><span class="pre">akida_models.layer_blocks.</span></span><span class="sig-name descname"><span class="pre">conv_transpose_block</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filters</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_batchnorm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">relu_activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ReLU8'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/layer_blocks.html#conv_transpose_block"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.layer_blocks.conv_transpose_block" title="Permalink to this definition"></a></dt>
<dd><p>Adds a transposed convolutional layer with optional layers in the following order:
batch normalization, activation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>tf.Tensor</em>) – input tensor of shape <cite>(rows, cols, channels)</cite></p></li>
<li><p><strong>filters</strong> (<em>int</em>) – the dimensionality of the output space (i.e. the number of output filters in
the convolution).</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em> or </em><em>tuple of 2 integers</em>) – specifying the height and width of the 2D
convolution kernel. Can be a single integer to specify the same value for all spatial
dimensions.</p></li>
<li><p><strong>add_batchnorm</strong> (<em>bool</em><em>, </em><em>optional</em>) – add a BatchNormalization layer. Defaults to False.</p></li>
<li><p><strong>relu_activation</strong> (<em>str</em><em>, </em><em>optional</em>) – the ReLU activation to add to the layer in the form ‘ReLUx’
where ‘x’ is the max_value to use. Set to False to disable activation. Defaults to
‘ReLU3.75’.</p></li>
<li><p><strong>**kwargs</strong> – arguments passed to the keras.Conv2DTranspose layer, such as strides, padding,
use_bias, weight_regularizer, etc.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>output tensor of transposed convolution block.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.layer_blocks.sepconv_transpose_block">
<span class="sig-prename descclassname"><span class="pre">akida_models.layer_blocks.</span></span><span class="sig-name descname"><span class="pre">sepconv_transpose_block</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filters</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strides</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'same'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_batchnorm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">relu_activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ReLU3.75'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'glorot_uniform'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pointwise_regularizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/layer_blocks.html#sepconv_transpose_block"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.layer_blocks.sepconv_transpose_block" title="Permalink to this definition"></a></dt>
<dd><p>Adds a transposed separable convolutional layer with optional layers in the following order:
batch normalization, activation.</p>
<p>The separable operation is made of a DepthwiseConv2DTranspose followed by a pointwise Conv2D.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>tf.Tensor</em>) – input tensor of shape <cite>(rows, cols, channels)</cite></p></li>
<li><p><strong>filters</strong> (<em>int</em>) – the dimensionality of the output space (i.e. the number of output filters in
the pointwise convolution).</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em> or </em><em>tuple of 2 integers</em>) – specifying the height and width of the depthwise
transpose kernel. Can be a single integer to specify the same value for all spatial
dimensions.</p></li>
<li><p><strong>strides</strong> (<em>int</em><em> or </em><em>tuple of 2 integers</em><em>, </em><em>optional</em>) – strides of the transposed depthwise.
Defaults to 2.</p></li>
<li><p><strong>padding</strong> (<em>str</em><em>, </em><em>optional</em>) – padding mode for the transposed depthwise. Defaults to ‘same’.</p></li>
<li><p><strong>use_bias</strong> (<em>bool</em><em>, </em><em>optional</em>) – whether the layer uses a bias vectors. Defaults to True.</p></li>
<li><p><strong>add_batchnorm</strong> (<em>bool</em><em>, </em><em>optional</em>) – add a BatchNormalization layer. Defaults to False.</p></li>
<li><p><strong>relu_activation</strong> (<em>str</em><em>, </em><em>optional</em>) – the ReLU activation to add to the layer in the form ‘ReLUx’
where ‘x’ is the max_value to use. Set to False to disable activation. Defaults to
‘ReLU3.75’.</p></li>
<li><p><strong>name</strong> (<em>str</em><em>, </em><em>optional</em>) – name of the layer. Defaults to None.</p></li>
<li><p><strong>kernel_initializer</strong> (<em>keras.initializer</em><em>, </em><em>optional</em>) – initializer for both kernels. Defaults to
‘glorot_uniform’.</p></li>
<li><p><strong>pointwise_regularizer</strong> (<em>keras.regularizers</em><em>, </em><em>optional</em>) – regularizer function applied to the
pointwise kernel matrix. Defaults to None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>output tensor of transposed separable convolution block.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.Tensor</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="detection-block">
<h3>Detection block<a class="headerlink" href="#detection-block" title="Permalink to this headline"></a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="akida_models.layer_blocks.yolo_head_block">
<span class="sig-prename descclassname"><span class="pre">akida_models.layer_blocks.</span></span><span class="sig-name descname"><span class="pre">yolo_head_block</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_boxes</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">classes</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filters</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1024</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/layer_blocks.html#yolo_head_block"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.layer_blocks.yolo_head_block" title="Permalink to this definition"></a></dt>
<dd><p>Adds the <a class="reference external" href="https://arxiv.org/pdf/1612.08242.pdf">YOLOv2 detection head</a>, at the output
of a model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code>) – input tensor of shape <cite>(rows, cols, channels)</cite>.</p></li>
<li><p><strong>num_boxes</strong> (<em>int</em>) – number of boxes.</p></li>
<li><p><strong>classes</strong> (<em>int</em>) – number of classes.</p></li>
<li><p><strong>filters</strong> (<em>int</em><em>, </em><em>optional</em>) – number of filters in hidden layers. Defaults to 1024.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>output tensor of yolo detection head block.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.Tensor</span></code></p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>This block replaces conv layers by separable_conv, to decrease the amount of parameters.</p>
</dd></dl>

</section>
</section>
<section id="helpers">
<h2>Helpers<a class="headerlink" href="#helpers" title="Permalink to this headline"></a></h2>
<section id="gamma-constraint">
<h3>Gamma constraint<a class="headerlink" href="#gamma-constraint" title="Permalink to this headline"></a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="akida_models.add_gamma_constraint">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">add_gamma_constraint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/gamma_constraint.html#add_gamma_constraint"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.add_gamma_constraint" title="Permalink to this definition"></a></dt>
<dd><p>Method helper to add a MinValueConstraint to an existing model so that
gamma values of its BatchNormalization layers are above a defined minimum.</p>
<p>This is typically used to help having a model that will be Akida compatible
after conversion. In some cases, the mapping on hardware will fail because
of huge values for <cite>threshold</cite> or <cite>act_step</cite> with a message indicating that
a value cannot fit in a 20 bit signed or unsigned integer.
In such a case, this helper can be called to apply a constraint that can fix
the issue.</p>
<p>Note that in order for the constraint to be applied to the actual weights,
some training must be done: for an already trained model, it can be on a few
batches, one epoch or more depending on the impact the constraint has on
accuracy. This helper can also be called to a new model that has not been
trained yet.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>model</strong> (<em>keras.Model</em>) – the model for which gamma constraints will be
added.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the same model with BatchNormalisation layers updated.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="unfusing-separableconvolutional">
<h3>Unfusing SeparableConvolutional<a class="headerlink" href="#unfusing-separableconvolutional" title="Permalink to this headline"></a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="akida_models.unfuse_sepconv2d">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">unfuse_sepconv2d</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/unfuse_sepconv_layers.html#unfuse_sepconv2d"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.unfuse_sepconv2d" title="Permalink to this definition"></a></dt>
<dd><p>Unfuse the SeparableConv2D layers of a model by replacing them with an equivalent
DepthwiseConv2D + (pointwise)Conv2D layers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>model</strong> (<em>keras.Model</em>) – the model to update</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the original model or a new model with unfused SeparableConv2D layers</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="extract-samples">
<h3>Extract samples<a class="headerlink" href="#extract-samples" title="Permalink to this headline"></a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="akida_models.extract_samples">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">extract_samples</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">out_file</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nb_samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1024</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/extract.html#extract_samples"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.extract_samples" title="Permalink to this definition"></a></dt>
<dd><p>Extracts samples from dataset and save them to a npz file.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>out_file</strong> (<em>str</em>) – name of output file</p></li>
<li><p><strong>dataset</strong> (<em>numpy.ndarray</em><em> or </em><em>tf.data.Dataset</em>) – dataset for extract samples</p></li>
<li><p><strong>nb_samples</strong> (<em>int</em><em>, </em><em>optional</em>) – number of samples. Defaults to 1024.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="knowledge-distillation">
<h2>Knowledge distillation<a class="headerlink" href="#knowledge-distillation" title="Permalink to this headline"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="akida_models.distiller.Distiller">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">akida_models.distiller.</span></span><span class="sig-name descname"><span class="pre">Distiller</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/distiller.html#Distiller"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.distiller.Distiller" title="Permalink to this definition"></a></dt>
<dd><p>The class that will be used to train the student model using the
distillation knowledge method.</p>
<p>Reference <a class="reference external" href="https://arxiv.org/abs/1503.02531">Hinton et al. (2015)</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>student</strong> (<em>keras.Model</em>) – the student model</p></li>
<li><p><strong>teacher</strong> (<em>keras.Model</em>) – the well trained teacher model</p></li>
<li><p><strong>alpha</strong> (<em>float</em><em>, </em><em>optional</em>) – weight to student_loss_fn and 1-alpha
to distillation_loss_fn. Defaults to 0.1</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="akida_models.distiller.DeitDistiller">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">akida_models.distiller.</span></span><span class="sig-name descname"><span class="pre">DeitDistiller</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/distiller.html#DeitDistiller"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.distiller.DeitDistiller" title="Permalink to this definition"></a></dt>
<dd><p>Distiller class to train the student model using the
Knowledge Distillation (KD) method, found on <a class="reference external" href="https://arxiv.org/pdf/2012.12877.pdf">https://arxiv.org/pdf/2012.12877.pdf</a></p>
<p>The main difference with the classic KD is that the student has to produce two potential
classification outputs. This type of training is based on the assumption that each output
has sufficiently interacted with the whole model, therefore the main architecture can be
trained through two different sources, as follows:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">output_kd</span> <span class="o">=</span> <span class="n">student</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output_tc</span> <span class="o">=</span> <span class="n">teacher</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">student_loss</span> <span class="o">=</span> <span class="n">student_loss_fn</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">distillation_loss</span> <span class="o">=</span> <span class="n">distillation_loss_fn</span><span class="p">(</span><span class="n">output_tc</span><span class="p">,</span> <span class="n">output_kd</span><span class="p">)</span>
</pre></div>
</div>
<p>This means we will expect to have different inputs for each loss, unlike classical KD,
where the student’s prediction is shared for both losses. However, given that each
classifier has interacted with the student model, the gradient of each loss will contribute
to the update of the model weights according to the alpha percentage.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>student</strong> (<em>keras.Model</em>) – the student model</p></li>
<li><p><strong>teacher</strong> (<em>keras.Model</em>) – the well trained teacher model</p></li>
<li><p><strong>alpha</strong> (<em>float</em><em>, </em><em>optional</em>) – weight to student_loss_fn and 1-alpha
to distillation_loss_fn. Defaults to 0.1</p></li>
<li><p><strong>temperature</strong> (<em>float</em><em>, </em><em>optional</em>) – if <code class="docutils literal notranslate"><span class="pre">distiller_type</span></code> when compile is equal to ‘soft’,
this value will be used as temperature parameter of KLDistillationLoss.
Defaults to 1.0.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.distiller.KLDistillationLoss">
<span class="sig-prename descclassname"><span class="pre">akida_models.distiller.</span></span><span class="sig-name descname"><span class="pre">KLDistillationLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">temperature</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/distiller.html#KLDistillationLoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.distiller.KLDistillationLoss" title="Permalink to this definition"></a></dt>
<dd><p>The <cite>KLDistillationLoss</cite> is a simple wrapper around the KLDivergence loss
that accepts raw predictions instead of probability distributions.</p>
<p>Before invoking the KLDivergence loss, it converts the inputs predictions to
probabilities by dividing them by a constant ‘temperature’ and applies a
softmax.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>temperature</strong> (<em>float</em>) – temperature for softening probability
distributions. Larger temperature gives softer distributions.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="macs">
<h2>MACS<a class="headerlink" href="#macs" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="akida_models.macs.get_flops">
<span class="sig-prename descclassname"><span class="pre">akida_models.macs.</span></span><span class="sig-name descname"><span class="pre">get_flops</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/macs.html#get_flops"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.macs.get_flops" title="Permalink to this definition"></a></dt>
<dd><p>Calculate FLOPS for a tf.keras.Model or tf.keras.Sequential model in inference mode.</p>
<p>It uses tf.compat.v1.profiler under the hood.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>model</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">keras.Model</span></code>) – the model to evaluate</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>object containing the FLOPS</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tf.compat.v1.profiler.GraphNodeProto</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.macs.display_macs">
<span class="sig-prename descclassname"><span class="pre">akida_models.macs.</span></span><span class="sig-name descname"><span class="pre">display_macs</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/macs.html#display_macs"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.macs.display_macs" title="Permalink to this definition"></a></dt>
<dd><p>Displays the MACs for a keras model</p>
<p>By default it displays only the total MACS.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">keras.Model</span></code>) – the model to evaluate</p></li>
<li><p><strong>verbose</strong> (<em>bool</em>) – display MACS for each operation</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="model-i-o">
<h2>Model I/O<a class="headerlink" href="#model-i-o" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="akida_models.model_io.load_model">
<span class="sig-prename descclassname"><span class="pre">akida_models.model_io.</span></span><span class="sig-name descname"><span class="pre">load_model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model_path</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/model_io.html#load_model"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.model_io.load_model" title="Permalink to this definition"></a></dt>
<dd><p>Combine the cnn2snn.load_quantized_model and quantizeml.load_model</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>model_path</strong> (<em>str</em>) – model path</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the load model</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.model_io.load_weights">
<span class="sig-prename descclassname"><span class="pre">akida_models.model_io.</span></span><span class="sig-name descname"><span class="pre">load_weights</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights_path</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/model_io.html#load_weights"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.model_io.load_weights" title="Permalink to this definition"></a></dt>
<dd><p>Loads weights from a npz file and apply it to a model.</p>
<p>Go through the dictionary of weights of the npz file, find the
corresponding variable in the model and partially load its weights.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>keras.Model</em>) – the model to update</p></li>
<li><p><strong>weights_path</strong> (<em>str</em>) – the path of the npz file to load</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.model_io.save_weights">
<span class="sig-prename descclassname"><span class="pre">akida_models.model_io.</span></span><span class="sig-name descname"><span class="pre">save_weights</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights_path</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/model_io.html#save_weights"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.model_io.save_weights" title="Permalink to this definition"></a></dt>
<dd><p>Save model weights on an npz file.</p>
<p>Takes a model and save the weights of all its layers into an npz file.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>keras.Model</em>) – the model to save its weights</p></li>
<li><p><strong>weights_path</strong> (<em>str</em>) – the path of the npz file to save</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.model_io.get_model_path">
<span class="sig-prename descclassname"><span class="pre">akida_models.model_io.</span></span><span class="sig-name descname"><span class="pre">get_model_path</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">subdir</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_name_v1</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">file_hash_v1</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_name_v2</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">file_hash_v2</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/model_io.html#get_model_path"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.model_io.get_model_path" title="Permalink to this definition"></a></dt>
<dd><p>Selects the model file on the server depending on the AkidaVersion.</p>
<p>The model path, model name and its hash depends on the Akida version context.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>subdir</strong> (<em>str</em><em>, </em><em>optional</em>) – the subdirectory where the model is on the data server.
Defaults to “”.</p></li>
<li><p><strong>model_name_v1</strong> (<em>str</em><em>, </em><em>optional</em>) – the model v1 name. Defaults to None.</p></li>
<li><p><strong>file_hash_v1</strong> (<em>str</em><em>, </em><em>optional</em>) – the model file v1 hash. Defaults to None.</p></li>
<li><p><strong>model_name_v2</strong> (<em>str</em><em>, </em><em>optional</em>) – the model v2 name. Defaults to None.</p></li>
<li><p><strong>file_hash_v2</strong> (<em>str</em><em>, </em><em>optional</em>) – the model file v2 hash. Defaults to None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the model path, model name and file hash.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>str, str, str</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="utils">
<h2>Utils<a class="headerlink" href="#utils" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="akida_models.utils.fetch_file">
<span class="sig-prename descclassname"><span class="pre">akida_models.utils.</span></span><span class="sig-name descname"><span class="pre">fetch_file</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">origin</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fname</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">file_hash</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache_subdir</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'datasets'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">extract</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache_dir</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/utils.html#fetch_file"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.utils.fetch_file" title="Permalink to this definition"></a></dt>
<dd><p>Downloads a file from a URL if it is not already in the cache.</p>
<p>Reimplements <cite>keras.utils.get_file</cite> without raising an error when detecting a file_hash
mismatch (it will just re-download the model).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>origin</strong> (<em>str</em>) – original URL of the file.</p></li>
<li><p><strong>fname</strong> (<em>str</em><em>, </em><em>optional</em>) – name of the file. If an absolute path <cite>/path/to/file.txt</cite> is
specified the file will be saved at that location. If <cite>None</cite>, the name of the file at
<cite>origin</cite> will be used. Defaults to None.</p></li>
<li><p><strong>file_hash</strong> (<em>str</em><em>, </em><em>optional</em>) – the expected hash string of the file after download. Defaults to
None.</p></li>
<li><p><strong>cache_subdir</strong> (<em>str</em><em>, </em><em>optional</em>) – subdirectory under the Keras cache dir where the file is
saved. If an absolute path <cite>/path/to/folder</cite> is specified the file will be saved at that
location. Defaults to ‘datasets’.</p></li>
<li><p><strong>extract</strong> (<em>bool</em><em>, </em><em>optional</em>) – True tries extracting the file as an Archive, like tar or zip.
Defaults to False.</p></li>
<li><p><strong>cache_dir</strong> (<em>str</em><em>, </em><em>optional</em>) – location to store cached files, when None it defaults to the
default directory <cite>~/.keras/</cite>. Defaults to None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>path to the downloaded file</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.utils.get_tensorboard_callback">
<span class="sig-prename descclassname"><span class="pre">akida_models.utils.</span></span><span class="sig-name descname"><span class="pre">get_tensorboard_callback</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">out_dir</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">histogram_freq</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/utils.html#get_tensorboard_callback"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.utils.get_tensorboard_callback" title="Permalink to this definition"></a></dt>
<dd><p>Build a Tensorboard call, pointing to the output directory</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>out_dir</strong> (<em>str</em>) – parent directory of the folder to create</p></li>
<li><p><strong>histogram_freq</strong> (<em>int</em><em>, </em><em>optional</em>) – frequency to export logs. Defaults to 1.</p></li>
<li><p><strong>prefix</strong> (<em>str</em><em>, </em><em>optional</em>) – prefix name. Defaults to ‘’.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.utils.get_params_by_version">
<span class="sig-prename descclassname"><span class="pre">akida_models.utils.</span></span><span class="sig-name descname"><span class="pre">get_params_by_version</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">relu_v2</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ReLU3.75'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/utils.html#get_params_by_version"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.utils.get_params_by_version" title="Permalink to this definition"></a></dt>
<dd><p>Provides the layer parameters depending on Akida version</p>
<p>With Akida v1, sepconv are fused, the ReLU max value is 6.
With Akida v2, sepconv are unfused, the ReLU max value is “relu_v2” and the ReLU is at the end
of the block with GAP.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>relu_v2</strong> (<em>str</em><em>, </em><em>optional</em>) – ReLUx string when targetting V2. Defaults to ReLU3.75.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>fused, post_relu_gap, relu_activation</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>bool, bool, str</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="model-zoo">
<h2>Model zoo<a class="headerlink" href="#model-zoo" title="Permalink to this headline"></a></h2>
<section id="akidanet">
<h3>AkidaNet<a class="headerlink" href="#akidanet" title="Permalink to this headline"></a></h3>
<section id="imagenet">
<h4>ImageNet<a class="headerlink" href="#imagenet" title="Permalink to this headline"></a></h4>
<dl class="py function">
<dt class="sig sig-object py" id="akida_models.akidanet_imagenet">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">akidanet_imagenet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">include_top</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pooling</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">classes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_scaling</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(128,</span> <span class="pre">-</span> <span class="pre">1)</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/imagenet/model_akidanet.html#akidanet_imagenet"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.akidanet_imagenet" title="Permalink to this definition"></a></dt>
<dd><p>Instantiates the AkidaNet architecture.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_shape</strong> (<em>tuple</em><em>, </em><em>optional</em>) – shape tuple. Defaults to None.</p></li>
<li><p><strong>alpha</strong> (<em>float</em><em>, </em><em>optional</em>) – <p>controls the width of the model.
Defaults to 1.0.</p>
<ul>
<li><p>If <cite>alpha</cite> &lt; 1.0, proportionally decreases the number of filters
in each layer.</p></li>
<li><p>If <cite>alpha</cite> &gt; 1.0, proportionally increases the number of filters
in each layer.</p></li>
<li><p>If <cite>alpha</cite> = 1, default number of filters from the paper are used
at each layer.</p></li>
</ul>
</p></li>
<li><p><strong>include_top</strong> (<em>bool</em><em>, </em><em>optional</em>) – whether to include the fully-connected
layer at the top of the model. Defaults to True.</p></li>
<li><p><strong>pooling</strong> (<em>str</em><em>, </em><em>optional</em>) – <p>optional pooling mode for feature extraction
when <cite>include_top</cite> is <cite>False</cite>.
Defaults to None.</p>
<ul>
<li><p><cite>None</cite> means that the output of the model will be the 4D tensor
output of the last convolutional block.</p></li>
<li><p><cite>avg</cite> means that global average pooling will be applied to the
output of the last convolutional block, and thus the output of the
model will be a 2D tensor.</p></li>
</ul>
</p></li>
<li><p><strong>classes</strong> (<em>int</em><em>, </em><em>optional</em>) – optional number of classes to classify images
into, only to be specified if <cite>include_top</cite> is <cite>True</cite>. Defaults to 1000.</p></li>
<li><p><strong>input_scaling</strong> (<em>tuple</em><em>, </em><em>optional</em>) – scale factor and offset to apply to
inputs. Defaults to (128, -1). Note that following Akida convention,
the scale factor is an integer used as a divider.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a Keras model for AkidaNet/ImageNet.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>keras.Model</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>ValueError</strong> – in case of invalid input shape.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.akidanet_imagenet_pretrained">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">akidanet_imagenet_pretrained</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/imagenet/model_akidanet.html#akidanet_imagenet_pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.akidanet_imagenet_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Helper method to retrieve an <cite>akidanet_imagenet</cite> model that was trained on
ImageNet dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>alpha</strong> (<em>float</em><em>, </em><em>optional</em>) – width of the model, allowed values in [0.25,
0.5, 1]. Defaults to 1.0.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a Keras Model instance.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.akidanet_edge_imagenet">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">akidanet_edge_imagenet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">base_model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">classes</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">base_layer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'classifier'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/imagenet/model_akidanet_edge.html#akidanet_edge_imagenet"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.akidanet_edge_imagenet" title="Permalink to this definition"></a></dt>
<dd><p>Instantiates an AkidaNet-edge architecture.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>base_model</strong> (<em>str/keras.Model</em>) – an akidanet_imagenet base model.</p></li>
<li><p><strong>classes</strong> (<em>int</em>) – the number of classes for the edge classifier.</p></li>
<li><p><strong>base_layer</strong> (<em>str</em><em>, </em><em>optional</em>) – the last base layer. Defaults to “classifier”.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a Keras Model instance.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.akidanet_edge_imagenet_pretrained">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">akidanet_edge_imagenet_pretrained</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/imagenet/model_akidanet_edge.html#akidanet_edge_imagenet_pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.akidanet_edge_imagenet_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Helper method to retrieve a <cite>akidanet_edge_imagenet</cite> model that was
trained on ImageNet dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a Keras Model instance.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.akidanet18_imagenet">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">akidanet18_imagenet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">include_top</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pooling</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">classes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">depths</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(4,</span> <span class="pre">4,</span> <span class="pre">4,</span> <span class="pre">4)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dimensions</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(64,</span> <span class="pre">128,</span> <span class="pre">256,</span> <span class="pre">512)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_scaling</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(128,</span> <span class="pre">-</span> <span class="pre">1)</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/imagenet/model_akidanet18.html#akidanet18_imagenet"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.akidanet18_imagenet" title="Permalink to this definition"></a></dt>
<dd><p>Instantiates the AkidaNet18 architecture.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_shape</strong> (<em>tuple</em><em>, </em><em>optional</em>) – shape tuple. Defaults to None.</p></li>
<li><p><strong>include_top</strong> (<em>bool</em><em>, </em><em>optional</em>) – whether to include the fully-connected
layer at the top of the model. Defaults to True.</p></li>
<li><p><strong>pooling</strong> (<em>str</em><em>, </em><em>optional</em>) – <p>optional pooling mode for feature extraction
when <cite>include_top</cite> is <cite>False</cite>.
Defaults to None.</p>
<ul>
<li><p><cite>None</cite> means that the output of the model will be the 4D tensor
output of the last convolutional block.</p></li>
<li><p><cite>avg</cite> means that global average pooling will be applied to the
output of the last convolutional block, and thus the output of the
model will be a 2D tensor.</p></li>
</ul>
</p></li>
<li><p><strong>classes</strong> (<em>int</em><em>, </em><em>optional</em>) – optional number of classes to classify images
into, only to be specified if <cite>include_top</cite> is <cite>True</cite>. Defaults to 1000.</p></li>
<li><p><strong>depth</strong> (<em>tuple</em><em>, </em><em>optional</em>) – number of layers in each stages of the model. The length of the
tuple defines the number of stages. Defaults to (4, 4, 4, 4).</p></li>
<li><p><strong>dimensions</strong> (<em>tuple</em><em>, </em><em>optional</em>) – number of filters in each stage on the model. The length of
the tuple must be equal to the length of the <cite>depth</cite> tuple. Defaults to
(64, 128, 256, 512).</p></li>
<li><p><strong>input_scaling</strong> (<em>tuple</em><em>, </em><em>optional</em>) – scale factor and offset to apply to
inputs. Defaults to (128, -1). Note that following Akida convention,
the scale factor is an integer used as a divider.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a Keras model for AkidaNet/ImageNet.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>keras.Model</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>ValueError</strong> – in case of invalid input shape or mismatching <cite>depth</cite> and <cite>dimensions</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.akidanet18_imagenet_pretrained">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">akidanet18_imagenet_pretrained</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/imagenet/model_akidanet18.html#akidanet18_imagenet_pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.akidanet18_imagenet_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Helper method to retrieve an <cite>akidanet18_imagenet</cite> model that was trained on ImageNet dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a Keras Model instance.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.akidanet_faceidentification_pretrained">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">akidanet_faceidentification_pretrained</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/imagenet/model_akidanet.html#akidanet_faceidentification_pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.akidanet_faceidentification_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Helper method to retrieve an <cite>akidanet_imagenet</cite> model that was trained on
CASIA Webface dataset and that performs face identification.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a Keras Model instance.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.akidanet_faceidentification_edge_pretrained">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">akidanet_faceidentification_edge_pretrained</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/imagenet/model_akidanet_edge.html#akidanet_faceidentification_edge_pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.akidanet_faceidentification_edge_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Helper method to retrieve an <cite>akidanet_edge_imagenet</cite> model that was trained
on CASIA Webface dataset and that performs face identification.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a Keras Model instance.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.akidanet_plantvillage_pretrained">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">akidanet_plantvillage_pretrained</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/imagenet/model_akidanet.html#akidanet_plantvillage_pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.akidanet_plantvillage_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Helper method to retrieve an <cite>akidanet_imagenet</cite> model that was trained on
PlantVillage dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a Keras Model instance.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.akidanet_vww_pretrained">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">akidanet_vww_pretrained</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/imagenet/model_akidanet.html#akidanet_vww_pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.akidanet_vww_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Helper method to retrieve an <cite>akidanet_imagenet</cite> model that was trained on
VWW dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a Keras Model instance.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<section id="preprocessing">
<h5>Preprocessing<a class="headerlink" href="#preprocessing" title="Permalink to this headline"></a></h5>
<dl class="py function">
<dt class="sig sig-object py" id="akida_models.imagenet.preprocessing.preprocess_image">
<span class="sig-prename descclassname"><span class="pre">akida_models.imagenet.preprocessing.</span></span><span class="sig-name descname"><span class="pre">preprocess_image</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">image_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data_aug</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/imagenet/preprocessing.html#preprocess_image"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.imagenet.preprocessing.preprocess_image" title="Permalink to this definition"></a></dt>
<dd><p>ImageNet data preprocessing.</p>
<p>Preprocessing includes cropping, and resizing for both training and
validation images. Training preprocessing introduces some random distortion
of the image to improve accuracy.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>image</strong> (<em>tf.Tensor</em>) – input image as a 3-D tensor</p></li>
<li><p><strong>image_size</strong> (<em>int</em>) – desired image size</p></li>
<li><p><strong>training</strong> (<em>bool</em><em>, </em><em>optional</em>) – True for training preprocessing, False for
validation and inference. Defaults to False.</p></li>
<li><p><strong>data_aug</strong> (<em>keras.Sequential</em><em>, </em><em>optional</em>) – data augmentation. Defaults to None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>preprocessed image</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tensorflow.Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.imagenet.preprocessing.index_to_label">
<span class="sig-prename descclassname"><span class="pre">akida_models.imagenet.preprocessing.</span></span><span class="sig-name descname"><span class="pre">index_to_label</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">index</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/imagenet/preprocessing.html#index_to_label"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.imagenet.preprocessing.index_to_label" title="Permalink to this definition"></a></dt>
<dd><p>Function to get an ImageNet label from an index.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>index</strong> – between 0 and 999</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a string of comma separated labels</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

</section>
</section>
</section>
<section id="mobilenet">
<h3>Mobilenet<a class="headerlink" href="#mobilenet" title="Permalink to this headline"></a></h3>
<section id="id1">
<h4>ImageNet<a class="headerlink" href="#id1" title="Permalink to this headline"></a></h4>
<dl class="py function">
<dt class="sig sig-object py" id="akida_models.mobilenet_imagenet">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">mobilenet_imagenet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">include_top</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pooling</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">classes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_stride2</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_scaling</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(128,</span> <span class="pre">-</span> <span class="pre">1)</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/imagenet/model_mobilenet.html#mobilenet_imagenet"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.mobilenet_imagenet" title="Permalink to this definition"></a></dt>
<dd><p>Instantiates the MobileNet architecture.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_shape</strong> (<em>tuple</em><em>, </em><em>optional</em>) – shape tuple. Defaults to None.</p></li>
<li><p><strong>alpha</strong> (<em>float</em><em>, </em><em>optional</em>) – <p>controls the width of the model.
Defaults to 1.0.</p>
<ul>
<li><p>If <cite>alpha</cite> &lt; 1.0, proportionally decreases the number of filters
in each layer.</p></li>
<li><p>If <cite>alpha</cite> &gt; 1.0, proportionally increases the number of filters
in each layer.</p></li>
<li><p>If <cite>alpha</cite> = 1, default number of filters from the paper are used
at each layer.</p></li>
</ul>
</p></li>
<li><p><strong>dropout</strong> (<em>float</em><em>, </em><em>optional</em>) – dropout rate. Defaults to 1e-3.</p></li>
<li><p><strong>include_top</strong> (<em>bool</em><em>, </em><em>optional</em>) – whether to include the fully-connected
layer at the top of the model. Defaults to True.</p></li>
<li><p><strong>pooling</strong> (<em>str</em><em>, </em><em>optional</em>) – <p>optional pooling mode for feature extraction
when <cite>include_top</cite> is <cite>False</cite>.
Defaults to None.</p>
<ul>
<li><p><cite>None</cite> means that the output of the model will be the 4D tensor
output of the last convolutional block.</p></li>
<li><p><cite>avg</cite> means that global average pooling will be applied to the
output of the last convolutional block, and thus the output of the
model will be a 2D tensor.</p></li>
</ul>
</p></li>
<li><p><strong>classes</strong> (<em>int</em><em>, </em><em>optional</em>) – optional number of classes to classify images
into, only to be specified if <cite>include_top</cite> is <cite>True</cite>. Defaults to 1000.</p></li>
<li><p><strong>use_stride2</strong> (<em>bool</em><em>, </em><em>optional</em>) – replace max pooling operations by stride 2
convolutions in layers separable 2, 4, 6 and 12. Defaults to True.</p></li>
<li><p><strong>input_scaling</strong> (<em>tuple</em><em>, </em><em>optional</em>) – scale factor and offset to apply to
inputs. Defaults to (128, -1). Note that following Akida convention,
the scale factor is an integer used as a divider.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a Keras model for MobileNet/ImageNet.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>keras.Model</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>ValueError</strong> – in case of invalid input shape.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.mobilenet_imagenet_pretrained">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">mobilenet_imagenet_pretrained</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/imagenet/model_mobilenet.html#mobilenet_imagenet_pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.mobilenet_imagenet_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Helper method to retrieve a <cite>mobilenet_imagenet</cite> model that was trained on
ImageNet dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>alpha</strong> (<em>float</em>) – width of the model.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a Keras Model instance.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="ds-cnn">
<h3>DS-CNN<a class="headerlink" href="#ds-cnn" title="Permalink to this headline"></a></h3>
<section id="kws">
<h4>KWS<a class="headerlink" href="#kws" title="Permalink to this headline"></a></h4>
<dl class="py function">
<dt class="sig sig-object py" id="akida_models.ds_cnn_kws">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">ds_cnn_kws</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(49,</span> <span class="pre">10,</span> <span class="pre">1)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">classes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">33</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">include_top</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_scaling</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(255,</span> <span class="pre">0)</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/kws/model_ds_cnn.html#ds_cnn_kws"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.ds_cnn_kws" title="Permalink to this definition"></a></dt>
<dd><p>Instantiates a MobileNet-like model for the “Keyword Spotting” example.</p>
<p>This model is based on the MobileNet architecture, mainly with fewer layers.
The weights and activations are quantized such that it can be converted into
an Akida model.</p>
<p>This architecture is originated from <a class="reference external" href="https://arxiv.org/pdf/1711.07128.pdf">https://arxiv.org/pdf/1711.07128.pdf</a>
and was created for the “Keyword Spotting” (KWS) or “Speech Commands”
dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_shape</strong> (<em>tuple</em><em>, </em><em>optional</em>) – input shape tuple of the model. Defaults to (49, 10, 1).</p></li>
<li><p><strong>classes</strong> (<em>int</em><em>, </em><em>optional</em>) – optional number of classes to classify words into, only
be specified if <cite>include_top</cite> is True. Defaults to 33.</p></li>
<li><p><strong>include_top</strong> (<em>bool</em><em>, </em><em>optional</em>) – whether to include the classification layer at the top of the
model. Defaults to True.</p></li>
<li><p><strong>input_scaling</strong> (<em>tuple</em><em>, </em><em>optional</em>) – scale factor and offset to apply to
inputs. Defaults to (255, 0). Note that following Akida convention,
the scale factor is an integer used as a divider.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a Keras model for MobileNet/KWS</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.ds_cnn_kws_pretrained">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">ds_cnn_kws_pretrained</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/kws/model_ds_cnn.html#ds_cnn_kws_pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.ds_cnn_kws_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Helper method to retrieve a <cite>ds_cnn_kws</cite> model that was trained on
KWS dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a Keras Model instance.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<section id="id2">
<h5>Preprocessing<a class="headerlink" href="#id2" title="Permalink to this headline"></a></h5>
<dl class="py function">
<dt class="sig sig-object py" id="akida_models.kws.preprocessing.prepare_model_settings">
<span class="sig-prename descclassname"><span class="pre">akida_models.kws.preprocessing.</span></span><span class="sig-name descname"><span class="pre">prepare_model_settings</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_rate</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clip_duration_ms</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">window_size_ms</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">window_stride_ms</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_bin_count</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/kws/preprocessing.html#prepare_model_settings"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.kws.preprocessing.prepare_model_settings" title="Permalink to this definition"></a></dt>
<dd><p>Calculates common settings needed for all models.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sample_rate</strong> – Number of audio samples per second.</p></li>
<li><p><strong>clip_duration_ms</strong> – Length of each audio clip to be analyzed.</p></li>
<li><p><strong>window_size_ms</strong> – Duration of frequency analysis window.</p></li>
<li><p><strong>window_stride_ms</strong> – How far to move in time between frequency windows.</p></li>
<li><p><strong>feature_bin_count</strong> – Number of frequency bins to use for analysis.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Dictionary containing common settings.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>ValueError</strong> – If the preprocessing mode isn’t recognized.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.kws.preprocessing.prepare_words_list">
<span class="sig-prename descclassname"><span class="pre">akida_models.kws.preprocessing.</span></span><span class="sig-name descname"><span class="pre">prepare_words_list</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">wanted_words</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/kws/preprocessing.html#prepare_words_list"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.kws.preprocessing.prepare_words_list" title="Permalink to this definition"></a></dt>
<dd><p>Prepends common tokens to the custom word list.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>wanted_words</strong> – List of strings containing the custom words.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>List with the standard silence and unknown tokens added.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.kws.preprocessing.which_set">
<span class="sig-prename descclassname"><span class="pre">akida_models.kws.preprocessing.</span></span><span class="sig-name descname"><span class="pre">which_set</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">filename</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation_percentage</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">testing_percentage</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/kws/preprocessing.html#which_set"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.kws.preprocessing.which_set" title="Permalink to this definition"></a></dt>
<dd><p>Determines which data partition the file should belong to.</p>
<p>We want to keep files in the same training, validation, or testing sets even
if new ones are added over time. This makes it less likely that testing
samples will accidentally be reused in training when long runs are restarted
for example. To keep this stability, a hash of the filename is taken and used
to determine which set it should belong to. This determination only depends on
the name and the set proportions, so it won’t change as other files are added.</p>
<p>It’s also useful to associate particular files as related (for example words
spoken by the same person), so anything after ‘_nohash_’ in a filename is
ignored for set determination. This ensures that ‘bobby_nohash_0.wav’ and
‘bobby_nohash_1.wav’ are always in the same set, for example.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>filename</strong> – File path of the data sample.</p></li>
<li><p><strong>validation_percentage</strong> – How much of the data set to use for validation.</p></li>
<li><p><strong>testing_percentage</strong> – How much of the data set to use for testing.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>String, one of ‘training’, ‘validation’, or ‘testing’.</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="akida_models.kws.preprocessing.AudioProcessor">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">akida_models.kws.preprocessing.</span></span><span class="sig-name descname"><span class="pre">AudioProcessor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_rate</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clip_duration_ms</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">window_size_ms</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">window_stride_ms</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_bin_count</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data_url</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data_dir</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">silence_percentage</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">unknown_percentage</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">wanted_words</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation_percentage</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">testing_percentage</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/kws/preprocessing.html#AudioProcessor"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.kws.preprocessing.AudioProcessor" title="Permalink to this definition"></a></dt>
<dd><p>Handles loading, partitioning, and preparing audio training data.</p>
<p><strong>Methods:</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#akida_models.kws.preprocessing.AudioProcessor.get_augmented_data_for_wav" title="akida_models.kws.preprocessing.AudioProcessor.get_augmented_data_for_wav"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_augmented_data_for_wav</span></code></a>(wav_filename, ...)</p></td>
<td><p>Applies the feature transformation process to a wav audio file, adding data augmentation (background noise and time shifting).</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#akida_models.kws.preprocessing.AudioProcessor.get_data" title="akida_models.kws.preprocessing.AudioProcessor.get_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_data</span></code></a>(how_many, offset, ...)</p></td>
<td><p>Gather samples from the data set, applying transformations as needed.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#akida_models.kws.preprocessing.AudioProcessor.get_features_for_wav" title="akida_models.kws.preprocessing.AudioProcessor.get_features_for_wav"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_features_for_wav</span></code></a>(wav_filename)</p></td>
<td><p>Applies the feature transformation process to the input_wav.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#akida_models.kws.preprocessing.AudioProcessor.maybe_download_and_extract_dataset" title="akida_models.kws.preprocessing.AudioProcessor.maybe_download_and_extract_dataset"><code class="xref py py-obj docutils literal notranslate"><span class="pre">maybe_download_and_extract_dataset</span></code></a>(data_url, ...)</p></td>
<td><p>Download and extract data set tar file.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#akida_models.kws.preprocessing.AudioProcessor.prepare_background_data" title="akida_models.kws.preprocessing.AudioProcessor.prepare_background_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">prepare_background_data</span></code></a>()</p></td>
<td><p>Searches a folder for background noise audio, and loads it into memory.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#akida_models.kws.preprocessing.AudioProcessor.prepare_data_index" title="akida_models.kws.preprocessing.AudioProcessor.prepare_data_index"><code class="xref py py-obj docutils literal notranslate"><span class="pre">prepare_data_index</span></code></a>(silence_percentage, ...)</p></td>
<td><p>Prepares a list of the samples organized by set and label.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#akida_models.kws.preprocessing.AudioProcessor.prepare_processing_graph" title="akida_models.kws.preprocessing.AudioProcessor.prepare_processing_graph"><code class="xref py py-obj docutils literal notranslate"><span class="pre">prepare_processing_graph</span></code></a>()</p></td>
<td><p>Builds a TensorFlow graph to apply the input distortions.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt class="sig sig-object py" id="akida_models.kws.preprocessing.AudioProcessor.get_augmented_data_for_wav">
<span class="sig-name descname"><span class="pre">get_augmented_data_for_wav</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">wav_filename</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">background_frequency</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">background_volume_range</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_shift</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_augmented_samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/kws/preprocessing.html#AudioProcessor.get_augmented_data_for_wav"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.kws.preprocessing.AudioProcessor.get_augmented_data_for_wav" title="Permalink to this definition"></a></dt>
<dd><p>Applies the feature transformation process to a wav audio file,
adding data augmentation (background noise and time shifting).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>wav_filename</strong> (<em>str</em>) – The path to the input audio file.</p></li>
<li><p><strong>background_frequency</strong> – How many clips will have background noise, 0.0 to
1.0.</p></li>
<li><p><strong>background_volume_range</strong> – How loud the background noise will be.</p></li>
<li><p><strong>time_shift</strong> – How much to randomly shift the clips by in time.</p></li>
<li><p><strong>num_augmented_samples</strong> – How many samples will be generated using data
augmentation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Numpy data array containing the generated features for every augmented
sample.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="akida_models.kws.preprocessing.AudioProcessor.get_data">
<span class="sig-name descname"><span class="pre">get_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">how_many</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">offset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">background_frequency</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">background_volume_range</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_shift</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/kws/preprocessing.html#AudioProcessor.get_data"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.kws.preprocessing.AudioProcessor.get_data" title="Permalink to this definition"></a></dt>
<dd><p>Gather samples from the data set, applying transformations as needed.</p>
<p>When the mode is ‘training’, a random selection of samples will be returned,
otherwise the first N clips in the partition will be used. This ensures that
validation always uses the same samples, reducing noise in the metrics.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>how_many</strong> – Desired number of samples to return. -1 means the entire
contents of this partition.</p></li>
<li><p><strong>offset</strong> – Where to start when fetching deterministically.</p></li>
<li><p><strong>background_frequency</strong> – How many clips will have background noise, 0.0 to
1.0.</p></li>
<li><p><strong>background_volume_range</strong> – How loud the background noise will be.</p></li>
<li><p><strong>time_shift</strong> – How much to randomly shift the clips by in time.</p></li>
<li><p><strong>mode</strong> – Which partition to use, must be ‘training’, ‘validation’, or
‘testing’.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>List of sample data for the transformed samples, and list of label indexes</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>ValueError</strong> – If background samples are too short.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="akida_models.kws.preprocessing.AudioProcessor.get_features_for_wav">
<span class="sig-name descname"><span class="pre">get_features_for_wav</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">wav_filename</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/kws/preprocessing.html#AudioProcessor.get_features_for_wav"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.kws.preprocessing.AudioProcessor.get_features_for_wav" title="Permalink to this definition"></a></dt>
<dd><p>Applies the feature transformation process to the input_wav.</p>
<p>Runs the feature generation process (generally producing a spectrogram from
the input samples) on the WAV file. This can be useful for testing and
verifying implementations being run on other platforms.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>wav_filename</strong> – The path to the input audio file.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Numpy data array containing the generated features.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="akida_models.kws.preprocessing.AudioProcessor.maybe_download_and_extract_dataset">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">maybe_download_and_extract_dataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data_url</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dest_directory</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/kws/preprocessing.html#AudioProcessor.maybe_download_and_extract_dataset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.kws.preprocessing.AudioProcessor.maybe_download_and_extract_dataset" title="Permalink to this definition"></a></dt>
<dd><p>Download and extract data set tar file.</p>
<p>If the data set we’re using doesn’t already exist, this function
downloads it from the TensorFlow.org website and unpacks it into a
directory.
If the data_url is none, don’t download anything and expect the data
directory to contain the correct files already.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data_url</strong> – Web location of the tar file containing the data set.</p></li>
<li><p><strong>dest_directory</strong> – File path to extract data to.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="akida_models.kws.preprocessing.AudioProcessor.prepare_background_data">
<span class="sig-name descname"><span class="pre">prepare_background_data</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/kws/preprocessing.html#AudioProcessor.prepare_background_data"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.kws.preprocessing.AudioProcessor.prepare_background_data" title="Permalink to this definition"></a></dt>
<dd><p>Searches a folder for background noise audio, and loads it into
memory.</p>
<p>It’s expected that the background audio samples will be in a subdirectory
named ‘_background_noise_’ inside the ‘data_dir’ folder, as .wavs that match
the sample rate of the training data, but can be much longer in duration.</p>
<p>If the ‘_background_noise_’ folder doesn’t exist at all, this isn’t an
error, it’s just taken to mean that no background noise augmentation should
be used. If the folder does exist, but it’s empty, that’s treated as an
error.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>List of raw PCM-encoded audio samples of background noise.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>Exception</strong> – If files aren’t found in the folder.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="akida_models.kws.preprocessing.AudioProcessor.prepare_data_index">
<span class="sig-name descname"><span class="pre">prepare_data_index</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">silence_percentage</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">unknown_percentage</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">wanted_words</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation_percentage</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">testing_percentage</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/kws/preprocessing.html#AudioProcessor.prepare_data_index"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.kws.preprocessing.AudioProcessor.prepare_data_index" title="Permalink to this definition"></a></dt>
<dd><p>Prepares a list of the samples organized by set and label.</p>
<p>The training loop needs a list of all the available data, organized by
which partition it should belong to, and with ground truth labels attached.
This function analyzes the folders below the <cite>data_dir</cite>, figures out the
right
labels for each file based on the name of the subdirectory it belongs to,
and uses a stable hash to assign it to a data set partition.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>silence_percentage</strong> – How much of the resulting data should be background.</p></li>
<li><p><strong>unknown_percentage</strong> – How much should be audio outside the wanted classes.</p></li>
<li><p><strong>wanted_words</strong> – Labels of the classes we want to be able to recognize.</p></li>
<li><p><strong>validation_percentage</strong> – How much of the data set to use for validation.</p></li>
<li><p><strong>testing_percentage</strong> – How much of the data set to use for testing.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Dictionary containing a list of file information for each set partition,
and a lookup map for each class to determine its numeric index.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>Exception</strong> – If expected files are not found.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="akida_models.kws.preprocessing.AudioProcessor.prepare_processing_graph">
<span class="sig-name descname"><span class="pre">prepare_processing_graph</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/kws/preprocessing.html#AudioProcessor.prepare_processing_graph"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.kws.preprocessing.AudioProcessor.prepare_processing_graph" title="Permalink to this definition"></a></dt>
<dd><p>Builds a TensorFlow graph to apply the input distortions.</p>
<p>Creates a graph that loads a WAVE file, decodes it, scales the volume,
shifts it in time, adds in background noise, calculates a spectrogram, and
then builds an MFCC fingerprint from that.</p>
</dd></dl>

</dd></dl>

</section>
</section>
</section>
<section id="vgg">
<h3>VGG<a class="headerlink" href="#vgg" title="Permalink to this headline"></a></h3>
<section id="utk-face">
<h4>UTK Face<a class="headerlink" href="#utk-face" title="Permalink to this headline"></a></h4>
<dl class="py function">
<dt class="sig sig-object py" id="akida_models.vgg_utk_face">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">vgg_utk_face</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(32,</span> <span class="pre">32,</span> <span class="pre">3)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_scaling</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(127,</span> <span class="pre">-</span> <span class="pre">1)</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/utk_face/model_vgg.html#vgg_utk_face"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.vgg_utk_face" title="Permalink to this definition"></a></dt>
<dd><p>Instantiates a VGG-like model for the regression example on age
estimation using UTKFace dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_shape</strong> (<em>tuple</em><em>, </em><em>optional</em>) – input shape tuple of the model. Defaults to (32, 32, 3).</p></li>
<li><p><strong>input_scaling</strong> (<em>tuple</em><em>, </em><em>optional</em>) – scale factor and offset to apply to
inputs. Defaults to (127, -1). Note that following Akida convention,
the scale factor is an integer used as a divider.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a Keras model for VGG/UTKFace</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.vgg_utk_face_pretrained">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">vgg_utk_face_pretrained</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/utk_face/model_vgg.html#vgg_utk_face_pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.vgg_utk_face_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Helper method to retrieve a <cite>vgg_utk_face</cite> model that was trained on
UTK Face dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a Keras Model instance.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<section id="id3">
<h5>Preprocessing<a class="headerlink" href="#id3" title="Permalink to this headline"></a></h5>
<dl class="py function">
<dt class="sig sig-object py" id="akida_models.utk_face.preprocessing.load_data">
<span class="sig-prename descclassname"><span class="pre">akida_models.utk_face.preprocessing.</span></span><span class="sig-name descname"><span class="pre">load_data</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/utk_face/preprocessing.html#load_data"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.utk_face.preprocessing.load_data" title="Permalink to this definition"></a></dt>
<dd><p>Loads the dataset from Brainchip data server.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>train set, train labels, test
set and test labels as numpy arrays</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>np.array, np.array, np.array, np.array</p>
</dd>
</dl>
</dd></dl>

</section>
</section>
</section>
<section id="yolo">
<h3>YOLO<a class="headerlink" href="#yolo" title="Permalink to this headline"></a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="akida_models.yolo_base">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">yolo_base</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(224,</span> <span class="pre">224,</span> <span class="pre">3)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">classes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nb_box</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_scaling</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(127.5,</span> <span class="pre">-</span> <span class="pre">1)</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/detection/model_yolo.html#yolo_base"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.yolo_base" title="Permalink to this definition"></a></dt>
<dd><p>Instantiates the YOLOv2 architecture.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_shape</strong> (<em>tuple</em><em>, </em><em>optional</em>) – input shape tuple. Defaults to (224, 224, 3).</p></li>
<li><p><strong>classes</strong> (<em>int</em><em>, </em><em>optional</em>) – number of classes to classify images into. Defaults to 1.</p></li>
<li><p><strong>nb_box</strong> (<em>int</em><em>, </em><em>optional</em>) – number of anchors boxes to use. Defaults to 5.</p></li>
<li><p><strong>alpha</strong> (<em>float</em><em>, </em><em>optional</em>) – controls the width of the model. Defaults to 1.0.</p></li>
<li><p><strong>input_scaling</strong> (<em>tuple</em><em>, </em><em>optional</em>) – scale factor and offset to apply to
inputs. Defaults to (127.5, -1). Note that following Akida
convention, the scale factor is a number used as a divider.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a Keras Model instance.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.yolo_widerface_pretrained">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">yolo_widerface_pretrained</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/detection/model_yolo.html#yolo_widerface_pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.yolo_widerface_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Helper method to retrieve a <cite>yolo_base</cite> model that was trained on WiderFace
dataset and the anchors that are needed to interpet the model output.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a Keras Model instance and a list of anchors.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>keras.Model, list</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.yolo_voc_pretrained">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">yolo_voc_pretrained</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/detection/model_yolo.html#yolo_voc_pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.yolo_voc_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Helper method to retrieve a <cite>yolo_base</cite> model that was trained on PASCAL
VOC2012 dataset for ‘person’ and ‘car’ classes only, and the anchors that
are needed to interpet the model output.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a Keras Model instance and a list of anchors.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>keras.Model, list</p>
</dd>
</dl>
</dd></dl>

<section id="yolo-toolkit">
<h4>YOLO Toolkit<a class="headerlink" href="#yolo-toolkit" title="Permalink to this headline"></a></h4>
<section id="processing">
<h5>Processing<a class="headerlink" href="#processing" title="Permalink to this headline"></a></h5>
<dl class="py function">
<dt class="sig sig-object py" id="akida_models.detection.processing.load_image">
<span class="sig-prename descclassname"><span class="pre">akida_models.detection.processing.</span></span><span class="sig-name descname"><span class="pre">load_image</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image_path</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/detection/processing.html#load_image"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.detection.processing.load_image" title="Permalink to this definition"></a></dt>
<dd><p>Loads an image from a path.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>image_path</strong> (<em>string</em>) – full path of the image to load</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a Tensorflow image Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.detection.processing.preprocess_image">
<span class="sig-prename descclassname"><span class="pre">akida_models.detection.processing.</span></span><span class="sig-name descname"><span class="pre">preprocess_image</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image_buffer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/detection/processing.html#preprocess_image"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.detection.processing.preprocess_image" title="Permalink to this definition"></a></dt>
<dd><p>Preprocess an image for YOLO inference.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>image_buffer</strong> (<em>tf.Tensor</em>) – image to preprocess</p></li>
<li><p><strong>output_size</strong> (<em>tuple</em>) – shape of the image after preprocessing</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A resized and normalized image as a Numpy array.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.detection.processing.decode_output">
<span class="sig-prename descclassname"><span class="pre">akida_models.detection.processing.</span></span><span class="sig-name descname"><span class="pre">decode_output</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">anchors</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nb_classes</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">obj_threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nms_threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/detection/processing.html#decode_output"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.detection.processing.decode_output" title="Permalink to this definition"></a></dt>
<dd><p>Decodes a YOLO model output.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output</strong> (<em>tf.Tensor</em>) – model output to decode</p></li>
<li><p><strong>anchors</strong> (<em>list</em>) – list of anchors boxes</p></li>
<li><p><strong>nb_classes</strong> (<em>int</em>) – number of classes</p></li>
<li><p><strong>obj_threshold</strong> (<em>float</em>) – confidence threshold for a box</p></li>
<li><p><strong>nms_threshold</strong> (<em>float</em>) – non-maximal supression threshold</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>List of <cite>BoundingBox</cite> objects</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.detection.processing.parse_voc_annotations">
<span class="sig-prename descclassname"><span class="pre">akida_models.detection.processing.</span></span><span class="sig-name descname"><span class="pre">parse_voc_annotations</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">gt_folder</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">image_folder</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">file_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/detection/processing.html#parse_voc_annotations"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.detection.processing.parse_voc_annotations" title="Permalink to this definition"></a></dt>
<dd><p>Loads PASCAL-VOC data.</p>
<p>Data is loaded using the groundtruth informations and stored in a
dictionary.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>gt_folder</strong> (<em>str</em>) – path to the folder containing ground truth files</p></li>
<li><p><strong>image_folder</strong> (<em>str</em>) – path to the folder containing the images</p></li>
<li><p><strong>file_path</strong> (<em>str</em>) – file containing the list of files to parse</p></li>
<li><p><strong>labels</strong> (<em>list</em>) – list of labels of interest</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a dictionnary containing all data present in the ground truth file</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.detection.processing.parse_widerface_annotations">
<span class="sig-prename descclassname"><span class="pre">akida_models.detection.processing.</span></span><span class="sig-name descname"><span class="pre">parse_widerface_annotations</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">gt_file</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">image_folder</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/detection/processing.html#parse_widerface_annotations"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.detection.processing.parse_widerface_annotations" title="Permalink to this definition"></a></dt>
<dd><p>Loads WiderFace data.</p>
<p>Data is loaded using the groundtruth informations and stored in a
dictionary.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>gt_file</strong> (<em>str</em>) – path to the ground truth file</p></li>
<li><p><strong>image_folder</strong> (<em>str</em>) – path to the directory containing the images</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a dictionnary containing all data present in the ground truth file</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="akida_models.detection.processing.BoundingBox">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">akida_models.detection.processing.</span></span><span class="sig-name descname"><span class="pre">BoundingBox</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">score</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">classes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/detection/processing.html#BoundingBox"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.detection.processing.BoundingBox" title="Permalink to this definition"></a></dt>
<dd><p>Utility class to represent a bounding box.</p>
<p>The box is defined by its top left corner (x1, y1), bottom right corner
(x2, y2), label, score and classes.</p>
<p><strong>Methods:</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#akida_models.detection.processing.BoundingBox.get_label" title="akida_models.detection.processing.BoundingBox.get_label"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_label</span></code></a>()</p></td>
<td><p>Returns the label for this bounding box.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#akida_models.detection.processing.BoundingBox.get_score" title="akida_models.detection.processing.BoundingBox.get_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_score</span></code></a>()</p></td>
<td><p>Returns the score for this bounding box.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#akida_models.detection.processing.BoundingBox.iou" title="akida_models.detection.processing.BoundingBox.iou"><code class="xref py py-obj docutils literal notranslate"><span class="pre">iou</span></code></a>(other)</p></td>
<td><p>Computes intersection over union ratio between this bounding box and another one.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt class="sig sig-object py" id="akida_models.detection.processing.BoundingBox.get_label">
<span class="sig-name descname"><span class="pre">get_label</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/detection/processing.html#BoundingBox.get_label"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.detection.processing.BoundingBox.get_label" title="Permalink to this definition"></a></dt>
<dd><p>Returns the label for this bounding box.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Index of the label as an integer.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="akida_models.detection.processing.BoundingBox.get_score">
<span class="sig-name descname"><span class="pre">get_score</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/detection/processing.html#BoundingBox.get_score"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.detection.processing.BoundingBox.get_score" title="Permalink to this definition"></a></dt>
<dd><p>Returns the score for this bounding box.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Confidence as a float.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="akida_models.detection.processing.BoundingBox.iou">
<span class="sig-name descname"><span class="pre">iou</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/detection/processing.html#BoundingBox.iou"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.detection.processing.BoundingBox.iou" title="Permalink to this definition"></a></dt>
<dd><p>Computes intersection over union ratio between this bounding box and
another one.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>other</strong> (<a class="reference internal" href="#akida_models.detection.processing.BoundingBox" title="akida_models.detection.processing.BoundingBox"><em>BoundingBox</em></a>) – the other bounding box for IOU computation</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>IOU value as a float</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="performance">
<h5>Performance<a class="headerlink" href="#performance" title="Permalink to this headline"></a></h5>
<dl class="py class">
<dt class="sig sig-object py" id="akida_models.detection.map_evaluation.MapEvaluation">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">akida_models.detection.map_evaluation.</span></span><span class="sig-name descname"><span class="pre">MapEvaluation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">model</span></em>, <em class="sig-param"><span class="pre">val_data</span></em>, <em class="sig-param"><span class="pre">labels</span></em>, <em class="sig-param"><span class="pre">anchors</span></em>, <em class="sig-param"><span class="pre">period=1</span></em>, <em class="sig-param"><span class="pre">obj_threshold=0.5</span></em>, <em class="sig-param"><span class="pre">nms_threshold=0.5</span></em>, <em class="sig-param"><span class="pre">max_box_per_image=10</span></em>, <em class="sig-param"><span class="pre">is_keras_model=True</span></em>, <em class="sig-param"><span class="pre">decode_output_fn=&lt;function</span> <span class="pre">decode_output&gt;</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/detection/map_evaluation.html#MapEvaluation"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.detection.map_evaluation.MapEvaluation" title="Permalink to this definition"></a></dt>
<dd><p>Evaluate a given dataset using a given model.
Code originally from <a class="reference external" href="https://github.com/fizyr/keras-retinanet">https://github.com/fizyr/keras-retinanet</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>keras.Model</em>) – model to evaluate.</p></li>
<li><p><strong>val_data</strong> (<em>dict</em>) – dictionary containing validation data as obtained
using <cite>preprocess_widerface.py</cite> module</p></li>
<li><p><strong>labels</strong> (<em>list</em>) – list of labels as strings</p></li>
<li><p><strong>anchors</strong> (<em>list</em>) – list of anchors boxes</p></li>
<li><p><strong>period</strong> (<em>int</em><em>, </em><em>optional</em>) – periodicity the precision is printed,
defaults to once per epoch.</p></li>
<li><p><strong>obj_threshold</strong> (<em>float</em><em>, </em><em>optional</em>) – confidence threshold for a box</p></li>
<li><p><strong>nms_threshold</strong> (<em>float</em><em>, </em><em>optional</em>) – non-maximal supression threshold</p></li>
<li><p><strong>max_box_per_image</strong> (<em>int</em><em>, </em><em>optional</em>) – maximum number of detections per
image</p></li>
<li><p><strong>is_keras_model</strong> (<em>bool</em><em>, </em><em>optional</em>) – indicated if the model is a Keras
model (True) or an Akida model (False)</p></li>
<li><p><strong>decode_output_fn</strong> (<em>Callable</em><em>, </em><em>optional</em>) – function to decode model’s outputs.
Defaults to <code class="xref py py-func docutils literal notranslate"><span class="pre">decode_output()</span></code> (yolo decode output function).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A dict mapping class names to mAP scores.</p>
</dd>
</dl>
<p><strong>Methods:</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#akida_models.detection.map_evaluation.MapEvaluation.evaluate_map" title="akida_models.detection.map_evaluation.MapEvaluation.evaluate_map"><code class="xref py py-obj docutils literal notranslate"><span class="pre">evaluate_map</span></code></a>()</p></td>
<td><p>Evaluates current mAP score on the model.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#akida_models.detection.map_evaluation.MapEvaluation.on_epoch_end" title="akida_models.detection.map_evaluation.MapEvaluation.on_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_epoch_end</span></code></a>(epoch[, logs])</p></td>
<td><p>Keras callback called at the end of an epoch.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt class="sig sig-object py" id="akida_models.detection.map_evaluation.MapEvaluation.evaluate_map">
<span class="sig-name descname"><span class="pre">evaluate_map</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/detection/map_evaluation.html#MapEvaluation.evaluate_map"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.detection.map_evaluation.MapEvaluation.evaluate_map" title="Permalink to this definition"></a></dt>
<dd><p>Evaluates current mAP score on the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>global mAP score and dictionnary of label and mAP for each
class.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>tuple</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="akida_models.detection.map_evaluation.MapEvaluation.on_epoch_end">
<span class="sig-name descname"><span class="pre">on_epoch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">epoch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/detection/map_evaluation.html#MapEvaluation.on_epoch_end"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.detection.map_evaluation.MapEvaluation.on_epoch_end" title="Permalink to this definition"></a></dt>
<dd><p>Keras callback called at the end of an epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>epoch</strong> (<em>int</em>) – index of epoch.</p></li>
<li><p><strong>logs</strong> (<em>dict</em><em>, </em><em>optional</em>) – metric results for this training epoch, and
for the validation epoch if validation is performed. Validation
result keys are prefixed with val. For training epoch, the
values of the Model’s metrics are returned.
Example: {‘loss’: 0.2, ‘acc’: 0.7}. Defaults to None.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="anchors">
<h5>Anchors<a class="headerlink" href="#anchors" title="Permalink to this headline"></a></h5>
<dl class="py function">
<dt class="sig sig-object py" id="akida_models.detection.generate_anchors.generate_anchors">
<span class="sig-prename descclassname"><span class="pre">akida_models.detection.generate_anchors.</span></span><span class="sig-name descname"><span class="pre">generate_anchors</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">annotations_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_anchors</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grid_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(7,</span> <span class="pre">7)</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/detection/generate_anchors.html#generate_anchors"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.detection.generate_anchors.generate_anchors" title="Permalink to this definition"></a></dt>
<dd><p>Creates anchors by clustering dimensions of the ground truth boxes
from the training dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>annotations_data</strong> (<em>dict</em>) – dictionnary of preprocessed VOC data</p></li>
<li><p><strong>num_anchors</strong> (<em>int</em><em>, </em><em>optional</em>) – number of anchors</p></li>
<li><p><strong>grid_size</strong> (<em>tuple</em><em>, </em><em>optional</em>) – size of the YOLO grid</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the computed anchors</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="batchgenerator">
<h5>BatchGenerator<a class="headerlink" href="#batchgenerator" title="Permalink to this headline"></a></h5>
<dl class="py function">
<dt class="sig sig-object py" id="akida_models.detection.batch_generator.BatchGenerator">
<span class="sig-prename descclassname"><span class="pre">akida_models.detection.batch_generator.</span></span><span class="sig-name descname"><span class="pre">BatchGenerator</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grid_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">jitter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/detection/batch_generator.html#BatchGenerator"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.detection.batch_generator.BatchGenerator" title="Permalink to this definition"></a></dt>
<dd><p>Data generator used for the training process.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.detection.batch_generator.BatchYoloGenerator">
<span class="sig-prename descclassname"><span class="pre">akida_models.detection.batch_generator.</span></span><span class="sig-name descname"><span class="pre">BatchYoloGenerator</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grid_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">anchors</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">jitter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/detection/batch_generator.html#BatchYoloGenerator"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.detection.batch_generator.BatchYoloGenerator" title="Permalink to this definition"></a></dt>
<dd><p>Data generator used for the training process.</p>
</dd></dl>

</section>
<section id="id4">
<h5>Utils<a class="headerlink" href="#id4" title="Permalink to this headline"></a></h5>
<dl class="py function">
<dt class="sig sig-object py" id="akida_models.detection.box_utils.xywh_to_xyxy">
<span class="sig-prename descclassname"><span class="pre">akida_models.detection.box_utils.</span></span><span class="sig-name descname"><span class="pre">xywh_to_xyxy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">boxes</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/detection/box_utils.html#xywh_to_xyxy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.detection.box_utils.xywh_to_xyxy" title="Permalink to this definition"></a></dt>
<dd><p>Convert a set of boxes from format xywh to xyxy, where each format represent:</p>
<blockquote>
<div><ul class="simple">
<li><p>‘xywh’: format of (‘cx’, ‘xy’, ‘w’, ‘h’), also called ‘centroids’ and</p></li>
<li><p>‘xyxy’: format of (‘x_min’, ‘y_min’, ‘x_max’, ‘y_max’), also called ‘corners’.</p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>boxes</strong> (<em>tf.Tensor</em><em> or </em><em>np.ndarray</em>) – tensor with shape (N, 4)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>tensor with new format</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.Tensor or np.ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.detection.box_utils.xyxy_to_xywh">
<span class="sig-prename descclassname"><span class="pre">akida_models.detection.box_utils.</span></span><span class="sig-name descname"><span class="pre">xyxy_to_xywh</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">boxes</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/detection/box_utils.html#xyxy_to_xywh"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.detection.box_utils.xyxy_to_xywh" title="Permalink to this definition"></a></dt>
<dd><p>Convert a set of boxes from format xyxy to xywh, where each format represent:</p>
<blockquote>
<div><ul class="simple">
<li><p>‘xyxy’: format of (‘x_min’, ‘y_min’, ‘x_max’, ‘y_max’), also called ‘corners’ and</p></li>
<li><p>‘xywh’: format of (‘cx’, ‘xy’, ‘w’, ‘h’), also called ‘centroids’.</p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>boxes</strong> (<em>tf.Tensor</em>) – tensor with shape (N, 4)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>tensor with new format</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.detection.box_utils.compute_overlap">
<span class="sig-prename descclassname"><span class="pre">akida_models.detection.box_utils.</span></span><span class="sig-name descname"><span class="pre">compute_overlap</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">a1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">a2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'element_wise'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">box_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'xywh'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/detection/box_utils.html#compute_overlap"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.detection.box_utils.compute_overlap" title="Permalink to this definition"></a></dt>
<dd><p>Calculate ious between a1, a2 in two different modes:</p>
<blockquote>
<div><ul class="simple">
<li><p>element_wise: compute iou element-by-element, returning 1D array tensor,</p></li>
<li><p>outer_product: compute cross iou with all possible combination between inputs.</p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>a1</strong> (<em>tf.Tensor</em><em> or </em><em>np.ndarray</em>) – set of boxes, with shape at least equal to (N, 4).</p></li>
<li><p><strong>a2</strong> (<em>tf.Tensor</em><em> or </em><em>np.ndarray</em>) – set of boxes, with compatible broadcast-shape
(in ‘element_wise’ mode) or shape at least equal to (N, 4) (in ‘outer_product’ mode).</p></li>
<li><p><strong>mode</strong> (<em>str</em><em>, </em><em>optional</em>) – the mode to use. ‘element_wise’ or ‘outer_product’. Defaults to
“element_wise”.</p></li>
<li><p><strong>box_format</strong> (<em>str</em><em>, </em><em>optional</em>) – format of both inputs. Defaults to ‘xywh’.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>IoU between inputs with shape (N,) in ‘element_wise’,
otherwise (N, M).</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.Tensor or np.ndarray</p>
</dd>
</dl>
</dd></dl>

</section>
</section>
</section>
<section id="pointnet">
<h3>PointNet++<a class="headerlink" href="#pointnet" title="Permalink to this headline"></a></h3>
<section id="modelnet40">
<h4>ModelNet40<a class="headerlink" href="#modelnet40" title="Permalink to this headline"></a></h4>
<dl class="py function">
<dt class="sig sig-object py" id="akida_models.pointnet_plus_modelnet40">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">pointnet_plus_modelnet40</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">selected_points</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">128</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">knn_points</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">classes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">40</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/modelnet40/model_pointnet_plus.html#pointnet_plus_modelnet40"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.pointnet_plus_modelnet40" title="Permalink to this definition"></a></dt>
<dd><p>Instantiates a PointNet++ model for the ModelNet40 classification.</p>
<p>This example implements the point cloud deep learning paper
<a class="reference external" href="https://arxiv.org/abs/1612.00593">PointNet (Qi et al., 2017)</a>. For a
detailed introduction on PointNet see <a class="reference external" href="https://medium.com/&#64;luis_gonzales/an-in-depth-look-at-pointnet-111d7efdaa1a">this blog post</a>.</p>
<p>PointNet++ is conceived as a repeated series of operations: sampling and
grouping of points, followed by the trainable convnet itself. Those
operations are then repeated at increased scale.
Each of the selected points is taken as the centroid of the K-nearest
neighbours. This defines a localized group.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>selected_points</strong> (<em>int</em><em>, </em><em>optional</em>) – the number of points to process per
sample. Default is 128.</p></li>
<li><p><strong>features</strong> (<em>int</em><em>, </em><em>optional</em>) – the number of features. Expected values are
1 or 3. Default is 3.</p></li>
<li><p><strong>knn_points</strong> (<em>int</em><em>, </em><em>optional</em>) – the number of points to include in each
localised group. Must be a power of 2, and ideally an integer square
(so 64, or 16 for a deliberately small network, or 256 for large).
Default is 64.</p></li>
<li><p><strong>classes</strong> (<em>int</em><em>, </em><em>optional</em>) – the number of classes for the classifier.
Default is 40.</p></li>
<li><p><strong>alpha</strong> (<em>float</em><em>, </em><em>optional</em>) – network filters multiplier. Default is 1.0.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a quantized Keras model for PointNet++/ModelNet40.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.pointnet_plus_modelnet40_pretrained">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">pointnet_plus_modelnet40_pretrained</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/modelnet40/model_pointnet_plus.html#pointnet_plus_modelnet40_pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.pointnet_plus_modelnet40_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Helper method to retrieve a <cite>pointnet_plus</cite> model that was trained on
ModelNet40 dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a Keras Model instance.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<section id="id5">
<h5>Processing<a class="headerlink" href="#id5" title="Permalink to this headline"></a></h5>
<dl class="py function">
<dt class="sig sig-object py" id="akida_models.modelnet40.preprocessing.get_modelnet_from_file">
<span class="sig-prename descclassname"><span class="pre">akida_models.modelnet40.preprocessing.</span></span><span class="sig-name descname"><span class="pre">get_modelnet_from_file</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_points</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filename</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ModelNet40.zip'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/modelnet40/preprocessing.html#get_modelnet_from_file"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.modelnet40.preprocessing.get_modelnet_from_file" title="Permalink to this definition"></a></dt>
<dd><p>Load the ModelNet data from file.</p>
<p>First parse through the ModelNet data folders. Each mesh is loaded and
sampled into a point cloud before being added to a standard python list and
converted to a <cite>numpy</cite> array. We also store the current enumerate index
value as the object label and use a dictionary to recall this later.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_points</strong> (<em>int</em>) – number of points with which mesh is sample.</p></li>
<li><p><strong>filename</strong> (<em>str</em>) – the dataset file to load if the npz file was not
generated yet. Defaults to “ModelNet40.zip”.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>train set, train labels,
test set, test labels as numpy arrays and dict containing class
folder name.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>np.array, np.array, np.array, np.array, dict</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.modelnet40.preprocessing.get_modelnet">
<span class="sig-prename descclassname"><span class="pre">akida_models.modelnet40.preprocessing.</span></span><span class="sig-name descname"><span class="pre">get_modelnet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_points</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_labels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_points</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_labels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">selected_points</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">128</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">knn_points</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/modelnet40/preprocessing.html#get_modelnet"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.modelnet40.preprocessing.get_modelnet" title="Permalink to this definition"></a></dt>
<dd><p>Obtains the ModelNet dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>train_points</strong> (<em>numpy.array</em>) – train set.</p></li>
<li><p><strong>train_labels</strong> (<em>numpy.array</em>) – train labels.</p></li>
<li><p><strong>test_points</strong> (<em>numpy.array</em>) – test set.</p></li>
<li><p><strong>test_labels</strong> (<em>numpy.array</em>) – test labels.</p></li>
<li><p><strong>batch_size</strong> (<em>int</em>) – size of the batch.</p></li>
<li><p><strong>selected_points</strong> (<em>int</em>) – num points to process per sample. Default is 512.</p></li>
<li><p><strong>knn_points</strong> (<em>int</em>) – number of points to include in each localised group.
Must be a power of 2, and ideally an integer square (so 64, or 16
for a deliberately small network, or 256 for large).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>train and test point with data
augmentation.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.data.Dataset, tf.data.Dataset</p>
</dd>
</dl>
</dd></dl>

</section>
</section>
</section>
<section id="gxnor">
<h3>GXNOR<a class="headerlink" href="#gxnor" title="Permalink to this headline"></a></h3>
<section id="mnist">
<h4>MNIST<a class="headerlink" href="#mnist" title="Permalink to this headline"></a></h4>
<dl class="py function">
<dt class="sig sig-object py" id="akida_models.gxnor_mnist">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">gxnor_mnist</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/mnist/model_gxnor.html#gxnor_mnist"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.gxnor_mnist" title="Permalink to this definition"></a></dt>
<dd><p>Instantiates a Keras GXNOR model with an additional dense layer to make
better classification.</p>
<p>The paper describing the original model can be found <a class="reference external" href="https://www.sciencedirect.com/science/article/pii/S0893608018300108">here</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a Keras model for GXNOR/MNIST</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.gxnor_mnist_pretrained">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">gxnor_mnist_pretrained</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/mnist/model_gxnor.html#gxnor_mnist_pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.gxnor_mnist_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Helper method to retrieve a <cite>gxnor_mnist</cite> model that was trained on MNIST dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a Keras Model instance.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="centernet">
<h3>CenterNet<a class="headerlink" href="#centernet" title="Permalink to this headline"></a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="akida_models.centernet_base">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">centernet_base</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(224,</span> <span class="pre">224,</span> <span class="pre">3)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">classes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_scaling</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(127,</span> <span class="pre">-</span> <span class="pre">1)</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/centernet/model_centernet.html#centernet_base"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.centernet_base" title="Permalink to this definition"></a></dt>
<dd><p>A Keras Model implementing the CenterNet architecture, on top of an AkidaNet backbone</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_shape</strong> (<em>tuple</em><em>, </em><em>optional</em>) – input shape. Defaults to (224, 224, 3).</p></li>
<li><p><strong>classes</strong> (<em>int</em><em>, </em><em>optional</em>) – number of output classes. Defaults to 2.</p></li>
<li><p><strong>alpha</strong> (<em>float</em><em>, </em><em>optional</em>) – controls the width of the model. Defaults to 0.5.</p></li>
<li><p><strong>input_scaling</strong> (<em>tuple</em><em>, </em><em>optional</em>) – input scaling. Defaults to (127, -1).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a Keras Model instance.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.centernet_voc_pretrained">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">centernet_voc_pretrained</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/centernet/model_centernet.html#centernet_voc_pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.centernet_voc_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Helper method to retrieve an <cite>centernet_base</cite> model that was trained on VOC detection dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a Keras Model instance.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.centernet.centernet_processing.decode_output">
<span class="sig-prename descclassname"><span class="pre">akida_models.centernet.centernet_processing.</span></span><span class="sig-name descname"><span class="pre">decode_output</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nb_classes</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">obj_threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_detections</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/centernet/centernet_processing.html#decode_output"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.centernet.centernet_processing.decode_output" title="Permalink to this definition"></a></dt>
<dd><p>Decodes a CenterNet model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output</strong> (<em>tf.Tensor</em>) – model output to decode.</p></li>
<li><p><strong>nb_classes</strong> (<em>int</em>) – number of classes.</p></li>
<li><p><strong>obj_threshold</strong> (<em>float</em><em>, </em><em>optional</em>) – confidence threshold for a box. Defaults to 0.1.</p></li>
<li><p><strong>max_detection</strong> (<em>int</em><em>, </em><em>optional</em>) – maximum number of boxes the model is allowed to produce.
Defaults to 100.</p></li>
<li><p><strong>kernel</strong> (<em>int</em><em>, </em><em>optional</em>) – max pool kernel size. Defaults to 5.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><cite>BoundingBox</cite> objects</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>List</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="akida_models.centernet.centernet_loss.CenternetLoss">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">akida_models.centernet.centernet_loss.</span></span><span class="sig-name descname"><span class="pre">CenternetLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-12</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">heatmap_loss_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">wh_loss_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">offset_loss_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/centernet/centernet_loss.html#CenternetLoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.centernet.centernet_loss.CenternetLoss" title="Permalink to this definition"></a></dt>
<dd><p>Computes CenterNet loss from a model raw output.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>alpha</strong> (<em>float</em><em>, </em><em>optional</em>) – alpha parameter in heatmap loss. Defaults to 2.0.</p></li>
<li><p><strong>gamma</strong> (<em>float</em><em>, </em><em>optional</em>) – gamma parameter in heatmap loss. Defaults to 4.0.</p></li>
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>optional</em>) – epsilon parameter in heatmap loss. Defaults to 1e-12.</p></li>
<li><p><strong>heatmap_loss_weight</strong> (<em>float</em><em>, </em><em>optional</em>) – heatmap loss weight. Defaults to 1.0.</p></li>
<li><p><strong>wh_loss_weight</strong> (<em>float</em><em>, </em><em>optional</em>) – location loss weight. Defaults to 0.1.</p></li>
<li><p><strong>offset_loss_weight</strong> (<em>float</em><em>, </em><em>optional</em>) – offset loss weight. Defaults to 1.0.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="akida_models.centernet.centernet_batch_generator.BatchCenternetGenerator">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">akida_models.centernet.centernet_batch_generator.</span></span><span class="sig-name descname"><span class="pre">BatchCenternetGenerator</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grid_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">jitter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/centernet/centernet_batch_generator.html#BatchCenternetGenerator"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.centernet.centernet_batch_generator.BatchCenternetGenerator" title="Permalink to this definition"></a></dt>
<dd><p>Data generator used for the training process</p>
</dd></dl>

</section>
<section id="akidaunet">
<h3>AkidaUNet<a class="headerlink" href="#akidaunet" title="Permalink to this headline"></a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="akida_models.akida_unet_portrait128">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">akida_unet_portrait128</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(128,</span> <span class="pre">128,</span> <span class="pre">3)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_scaling</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(128,</span> <span class="pre">-</span> <span class="pre">1)</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/portrait128/model_akida_unet.html#akida_unet_portrait128"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.akida_unet_portrait128" title="Permalink to this definition"></a></dt>
<dd><p>Instantiates an Akida U-Net architecture.</p>
<p>It is composed of an AkidaNet-ImageNet encoder followed by a succession of Conv2DTranspose
layers for the decoder part.
It does not contain any skip connection (concatenation) between the encoder and the decoder
branches.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_shape</strong> (<em>tuple</em><em>, </em><em>optional</em>) – input shape tuple. Defaults to (128, 128, 3).</p></li>
<li><p><strong>alpha</strong> (<em>float</em><em>, </em><em>optional</em>) – controls the width (number of filters) of the model. Defaults to
0.5.</p></li>
<li><p><strong>input_scaling</strong> (<em>tuple</em><em>, </em><em>optional</em>) – scale factor and offset to apply to inputs. Defaults to
(128, -1). Note that following Akida convention, the scale factor is a number used as a
divider.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a Keras Model instance.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.akida_unet_portrait128_pretrained">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">akida_unet_portrait128_pretrained</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/portrait128/model_akida_unet.html#akida_unet_portrait128_pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.akida_unet_portrait128_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Helper method to retrieve an <cite>akida_unet</cite> model that was trained on portrait128 dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a Keras Model instance.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="transformers">
<h3>Transformers<a class="headerlink" href="#transformers" title="Permalink to this headline"></a></h3>
<section id="vit">
<h4>ViT<a class="headerlink" href="#vit" title="Permalink to this headline"></a></h4>
<dl class="py function">
<dt class="sig sig-object py" id="akida_models.vit_imagenet">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">vit_imagenet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">patch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_blocks</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mlp_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">classes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">include_top</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'LN'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">last_norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'LN'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">softmax</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'softmax'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">act</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'GeLU'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/transformers/model_vit.html#vit_imagenet"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.vit_imagenet" title="Permalink to this definition"></a></dt>
<dd><p>Build a ViT model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_shape</strong> (<em>tuple</em>) – image shape tuple</p></li>
<li><p><strong>patch_size</strong> (<em>int</em>) – the size of each patch (must fit evenly in image size)</p></li>
<li><p><strong>num_blocks</strong> (<em>int</em>) – the number of transformer blocks to use.</p></li>
<li><p><strong>hidden_size</strong> (<em>int</em>) – the number of filters to use</p></li>
<li><p><strong>num_heads</strong> (<em>int</em>) – the number of transformer heads</p></li>
<li><p><strong>name</strong> (<em>str</em>) – the model name</p></li>
<li><p><strong>mlp_dim</strong> (<em>int</em>) – the number of dimensions for the MLP output in the transformers.</p></li>
<li><p><strong>classes</strong> (<em>int</em><em>, </em><em>optional</em>) – number of classes to classify images into, only to be specified if
<cite>include_top</cite> is True. Defaults to 1000.</p></li>
<li><p><strong>dropout</strong> (<em>float</em><em>, </em><em>optional</em>) – fraction of the units to drop for dense layers. Defaults to 0.1.</p></li>
<li><p><strong>include_top</strong> (<em>bool</em><em>, </em><em>optional</em>) – whether to include the final classifier head. If False,
the output will correspond to that of the transformer. Defaults to True.</p></li>
<li><p><strong>norm</strong> (<em>str</em><em>, </em><em>optional</em>) – string that values in [‘LN’, ‘GN1’, ‘BN’, ‘LMN’] and that allows to
choose from LayerNormalization, GroupNormalization(groups=1, …), BatchNormalization
or LayerMadNormalization layers respectively in the model. Defaults to ‘LN’.</p></li>
<li><p><strong>last_norm</strong> (<em>str</em><em>, </em><em>optional</em>) – string that values in [‘LN’, ‘BN’]
and that allows to choose from LayerNormalization or
BatchNormalization in the classifier network. Defaults to ‘LN’.</p></li>
<li><p><strong>softmax</strong> (<em>str</em><em>, </em><em>optional</em>) – string with values in [‘softmax’, ‘softmax2’]
that allows to choose between softmax and softmax2 in MHA. Defaults
to ‘softmax’.</p></li>
<li><p><strong>act</strong> (<em>str</em><em>, </em><em>optional</em>) – string that values in [‘GeLU’, ‘ReLUx’, ‘swish’] and that allows to
choose from GeLU, ReLUx or swish activation in MLP block. Defaults to ‘GeLU’.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.vit_ti16">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">vit_ti16</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(224,</span> <span class="pre">224,</span> <span class="pre">3)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">classes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'LN'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">last_norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'LN'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">softmax</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'softmax'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">act</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'GeLU'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">include_top</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/transformers/model_vit.html#vit_ti16"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.vit_ti16" title="Permalink to this definition"></a></dt>
<dd><p>Build ViT-Tiny. All arguments passed to vit_imagenet.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_shape</strong> (<em>tuple</em><em>, </em><em>optional</em>) – input shape. Defaults to (224, 224, 3).</p></li>
<li><p><strong>classes</strong> (<em>int</em><em>, </em><em>optional</em>) – number of classes. Defaults to 1000.</p></li>
<li><p><strong>norm</strong> (<em>str</em><em>, </em><em>optional</em>) – string that values in [‘LN’, ‘GN1’, ‘BN’, ‘LMN’] and that allows to
choose from LayerNormalization, GroupNormalization(groups=1, …), BatchNormalization
or LayerMadNormalization layers respectively in the model. Defaults to ‘LN’.</p></li>
<li><p><strong>last_norm</strong> (<em>str</em><em>, </em><em>optional</em>) – string that values in [‘LN’, ‘BN’]
and that allows to choose from LayerNormalization or
BatchNormalization in the classifier network. Defaults to ‘LN’.</p></li>
<li><p><strong>softmax</strong> (<em>str</em><em>, </em><em>optional</em>) – string with values in [‘softmax’, ‘softmax2’] that allows to choose
between softmax and softmax2 in attention block. Defaults to ‘softmax’.</p></li>
<li><p><strong>act</strong> (<em>str</em><em>, </em><em>optional</em>) – string that values in [‘GeLU’, ‘ReLUx’, ‘swish’] and that allows to
choose from GeLU, ReLUx or swish activation inside MLP. Defaults to ‘GeLU’.</p></li>
<li><p><strong>include_top</strong> (<em>bool</em><em>, </em><em>optional</em>) – whether to include the final classifier network.
Defaults to True.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the requested model</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.bc_vit_ti16">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">bc_vit_ti16</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(224,</span> <span class="pre">224,</span> <span class="pre">3)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">classes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">include_top</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/transformers/model_vit.html#bc_vit_ti16"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.bc_vit_ti16" title="Permalink to this definition"></a></dt>
<dd><p>Build ViT-Tiny, changing all LN by LMN, using softmax2 and ReLU8.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_shape</strong> (<em>tuple</em><em>, </em><em>optional</em>) – input shape. Defaults to (224, 224, 3).</p></li>
<li><p><strong>classes</strong> (<em>int</em><em>, </em><em>optional</em>) – number of classes. Defaults to 1000.</p></li>
<li><p><strong>include_top</strong> (<em>bool</em><em>, </em><em>optional</em>) – whether to include the final classifier network.
Defaults to True.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the requested model</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.bc_vit_ti16_imagenet_pretrained">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">bc_vit_ti16_imagenet_pretrained</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/transformers/model_vit.html#bc_vit_ti16_imagenet_pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.bc_vit_ti16_imagenet_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Helper method to retrieve a <cite>bc_vit_ti16</cite> model that was trained on ImageNet dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a Keras Model instance</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.vit_s16">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">vit_s16</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(224,</span> <span class="pre">224,</span> <span class="pre">3)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">classes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">include_top</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/transformers/model_vit.html#vit_s16"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.vit_s16" title="Permalink to this definition"></a></dt>
<dd><p>Build ViT-Small.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_shape</strong> (<em>tuple</em><em>, </em><em>optional</em>) – input shape. Defaults to (224, 224, 3).</p></li>
<li><p><strong>classes</strong> (<em>int</em><em>, </em><em>optional</em>) – number of classes. Defaults to 1000.</p></li>
<li><p><strong>include_top</strong> (<em>bool</em><em>, </em><em>optional</em>) – whether to include the final classifier network.
Defaults to True.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the requested model</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.vit_s32">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">vit_s32</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(224,</span> <span class="pre">224,</span> <span class="pre">3)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">classes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">include_top</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/transformers/model_vit.html#vit_s32"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.vit_s32" title="Permalink to this definition"></a></dt>
<dd><p>Build ViT-Small.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_shape</strong> (<em>tuple</em><em>, </em><em>optional</em>) – input shape. Defaults to (224, 224, 3).</p></li>
<li><p><strong>classes</strong> (<em>int</em><em>, </em><em>optional</em>) – number of classes. Defaults to 1000.</p></li>
<li><p><strong>include_top</strong> (<em>bool</em><em>, </em><em>optional</em>) – whether to include the final classifier network.
Defaults to True.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the requested model</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.vit_b16">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">vit_b16</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(224,</span> <span class="pre">224,</span> <span class="pre">3)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">classes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">include_top</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/transformers/model_vit.html#vit_b16"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.vit_b16" title="Permalink to this definition"></a></dt>
<dd><p>Build ViT-B16.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_shape</strong> (<em>tuple</em><em>, </em><em>optional</em>) – input shape. Defaults to (224, 224, 3).</p></li>
<li><p><strong>classes</strong> (<em>int</em><em>, </em><em>optional</em>) – number of classes. Defaults to 1000.</p></li>
<li><p><strong>include_top</strong> (<em>bool</em><em>, </em><em>optional</em>) – whether to include the final classifier network.
Defaults to True.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the requested model</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.vit_b32">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">vit_b32</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(224,</span> <span class="pre">224,</span> <span class="pre">3)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">classes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">include_top</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/transformers/model_vit.html#vit_b32"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.vit_b32" title="Permalink to this definition"></a></dt>
<dd><p>Build ViT-B32.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_shape</strong> (<em>tuple</em><em>, </em><em>optional</em>) – input shape. Defaults to (224, 224, 3).</p></li>
<li><p><strong>classes</strong> (<em>int</em><em>, </em><em>optional</em>) – number of classes. Defaults to 1000.</p></li>
<li><p><strong>include_top</strong> (<em>bool</em><em>, </em><em>optional</em>) – whether to include the final classifier network.
Defaults to True.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the requested model</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.vit_l16">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">vit_l16</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(384,</span> <span class="pre">384,</span> <span class="pre">3)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">classes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">include_top</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/transformers/model_vit.html#vit_l16"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.vit_l16" title="Permalink to this definition"></a></dt>
<dd><p>Build ViT-L16.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_shape</strong> (<em>tuple</em><em>, </em><em>optional</em>) – input shape. Defaults to (384, 384, 3).</p></li>
<li><p><strong>classes</strong> (<em>int</em><em>, </em><em>optional</em>) – number of classes. Defaults to 1000.</p></li>
<li><p><strong>include_top</strong> (<em>bool</em><em>, </em><em>optional</em>) – whether to include the final classifier network.
Defaults to True.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the requested model</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.vit_l32">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">vit_l32</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(384,</span> <span class="pre">384,</span> <span class="pre">3)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">classes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">include_top</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/transformers/model_vit.html#vit_l32"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.vit_l32" title="Permalink to this definition"></a></dt>
<dd><p>Build ViT-L32.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_shape</strong> (<em>tuple</em><em>, </em><em>optional</em>) – input shape. Defaults to (384, 384, 3).</p></li>
<li><p><strong>classes</strong> (<em>int</em><em>, </em><em>optional</em>) – number of classes. Defaults to 1000.</p></li>
<li><p><strong>include_top</strong> (<em>bool</em><em>, </em><em>optional</em>) – whether to include the final classifier network.
Defaults to True.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the requested model</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="deit">
<h4>DeiT<a class="headerlink" href="#deit" title="Permalink to this headline"></a></h4>
<dl class="py function">
<dt class="sig sig-object py" id="akida_models.deit_imagenet">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">deit_imagenet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_blocks</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mlp_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">patch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">classes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">include_top</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">distilled</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'LN'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">last_norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'LN'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">softmax</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'softmax'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">act</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'GeLU'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/transformers/model_deit.html#deit_imagenet"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.deit_imagenet" title="Permalink to this definition"></a></dt>
<dd><p>Build a DeiT model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_shape</strong> (<em>tuple</em>) – image shape tuple</p></li>
<li><p><strong>num_blocks</strong> (<em>int</em>) – the number of transformer blocks to use.</p></li>
<li><p><strong>hidden_size</strong> (<em>int</em>) – the number of filters to use</p></li>
<li><p><strong>num_heads</strong> (<em>int</em>) – the number of transformer heads</p></li>
<li><p><strong>name</strong> (<em>str</em>) – the model name</p></li>
<li><p><strong>mlp_dim</strong> (<em>int</em>) – the number of dimensions for the MLP output in the transformers.</p></li>
<li><p><strong>patch_size</strong> (<em>int</em><em>, </em><em>optional</em>) – the size of each patch (must fit evenly in image size). Defaults
to 16.</p></li>
<li><p><strong>classes</strong> (<em>int</em><em>, </em><em>optional</em>) – number of classes to classify images into, only to be specified if
<cite>include_top</cite> is True. Defaults to 1000.</p></li>
<li><p><strong>dropout</strong> (<em>float</em><em>, </em><em>optional</em>) – fraction of the units to drop for dense layers. Defaults to 0.1.</p></li>
<li><p><strong>include_top</strong> (<em>bool</em><em>, </em><em>optional</em>) – whether to include the final classifier head. If False,
the output will correspond to that of the transformer. Defaults to True.</p></li>
<li><p><strong>distilled</strong> (<em>bool</em><em>, </em><em>optional</em>) – Build model append a distilled token. Defaults to False.</p></li>
<li><p><strong>norm</strong> (<em>str</em><em>, </em><em>optional</em>) – string that values in [‘LN’, ‘GN1’, ‘BN’, ‘LMN’] and that allows to
choose from LayerNormalization, GroupNormalization(groups=1, …), BatchNormalization
or LayerMadNormalization layers respectively in the model. Defaults to ‘LN’.</p></li>
<li><p><strong>last_norm</strong> (<em>str</em><em>, </em><em>optional</em>) – string that values in [‘LN’, ‘BN’]
and that allows to choose from LayerNormalization or
BatchNormalization in the classifier network. Defaults to ‘LN’.</p></li>
<li><p><strong>softmax</strong> (<em>str</em><em>, </em><em>optional</em>) – string with values in [‘softmax’, ‘softmax2’]
that allows to choose between softmax and softmax2 in MHA. Defaults
to ‘softmax’.</p></li>
<li><p><strong>act</strong> (<em>str</em><em>, </em><em>optional</em>) – string that values in [‘GeLU’, ‘ReLUx’, ‘swish’] and that allows to
choose from GeLU, ReLUx or swish activation in MLP block. Defaults to ‘GeLU’.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.deit_ti16">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">deit_ti16</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(224,</span> <span class="pre">224,</span> <span class="pre">3)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">classes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">distilled</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'LN'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">last_norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'LN'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">softmax</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'softmax'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">act</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'GeLU'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">include_top</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/transformers/model_deit.html#deit_ti16"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.deit_ti16" title="Permalink to this definition"></a></dt>
<dd><p>Build DeiT-Tiny.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_shape</strong> (<em>tuple</em><em>, </em><em>optional</em>) – input shape. Defaults to (224, 224, 3).</p></li>
<li><p><strong>classes</strong> (<em>int</em><em>, </em><em>optional</em>) – number of classes. Defaults to 1000.</p></li>
<li><p><strong>distilled</strong> (<em>bool</em><em>, </em><em>optional</em>) – build model appending a distilled token. Defaults to False.</p></li>
<li><p><strong>norm</strong> (<em>str</em><em>, </em><em>optional</em>) – string that values in [‘LN’, ‘GN1’, ‘BN’, ‘LMN’] and that allows to
choose from LayerNormalization, GroupNormalization(groups=1, …), BatchNormalization
or LayerMadNormalization layers respectively in the model. Defaults to ‘LN’.</p></li>
<li><p><strong>last_norm</strong> (<em>str</em><em>, </em><em>optional</em>) – string that values in [‘LN’, ‘BN’]
and that allows to choose from LayerNormalization or
BatchNormalization in the classifier network. Defaults to ‘LN’.</p></li>
<li><p><strong>softmax</strong> (<em>str</em><em>, </em><em>optional</em>) – string with values in [‘softmax’, ‘softmax2’] that allows to choose
between softmax and softmax2 in attention block. Defaults to ‘softmax’.</p></li>
<li><p><strong>act</strong> (<em>str</em><em>, </em><em>optional</em>) – string that values in [‘GeLU’, ‘ReLUx’, ‘swish’] and that allows to
choose from GeLU, ReLUx or swish activation inside MLP. Defaults to ‘GeLU’.</p></li>
<li><p><strong>include_top</strong> (<em>bool</em><em>, </em><em>optional</em>) – whether to include the final classifier network.
Defaults to True.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the requested model</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.bc_deit_ti16">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">bc_deit_ti16</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(224,</span> <span class="pre">224,</span> <span class="pre">3)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">classes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">distilled</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">include_top</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_blocks</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">12</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/transformers/model_deit.html#bc_deit_ti16"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.bc_deit_ti16" title="Permalink to this definition"></a></dt>
<dd><p>Build DeiT-Tiny, changing all LN by LMN, using softmax2 and ReLU8.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_shape</strong> (<em>tuple</em><em>, </em><em>optional</em>) – input shape. Defaults to (224, 224, 3).</p></li>
<li><p><strong>classes</strong> (<em>int</em><em>, </em><em>optional</em>) – number of classes. Defaults to 1000.</p></li>
<li><p><strong>distilled</strong> (<em>bool</em><em>, </em><em>optional</em>) – build model appending a distilled token. Defaults to False.</p></li>
<li><p><strong>include_top</strong> (<em>bool</em><em>, </em><em>optional</em>) – whether to include the final classifier network.
Defaults to True.</p></li>
<li><p><strong>num_blocks</strong> (<em>int</em><em>, </em><em>optional</em>) – the number of transformer blocks to use. Defaults to 12.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the requested model</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.bc_deit_dist_ti16_imagenet_pretrained">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">bc_deit_dist_ti16_imagenet_pretrained</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/transformers/model_deit.html#bc_deit_dist_ti16_imagenet_pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.bc_deit_dist_ti16_imagenet_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Helper method to retrieve a <cite>bc_deit_dist_ti16</cite> model that was trained on ImageNet dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a Keras Model instance</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.deit_s16">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">deit_s16</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(224,</span> <span class="pre">224,</span> <span class="pre">3)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">classes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">distilled</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">include_top</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/transformers/model_deit.html#deit_s16"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.deit_s16" title="Permalink to this definition"></a></dt>
<dd><p>Build DeiT-Small.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_shape</strong> (<em>tuple</em><em>, </em><em>optional</em>) – input shape. Defaults to (224, 224, 3).</p></li>
<li><p><strong>classes</strong> (<em>int</em><em>, </em><em>optional</em>) – number of classes. Defaults to 1000.</p></li>
<li><p><strong>distilled</strong> (<em>bool</em><em>, </em><em>optional</em>) – build model appending a distilled token. Defaults to False.</p></li>
<li><p><strong>include_top</strong> (<em>bool</em><em>, </em><em>optional</em>) – whether to include the final classifier network.
Defaults to True.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the requested model</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.deit_b16">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">deit_b16</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(224,</span> <span class="pre">224,</span> <span class="pre">3)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">classes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">distilled</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">include_top</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/transformers/model_deit.html#deit_b16"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.deit_b16" title="Permalink to this definition"></a></dt>
<dd><p>Build DeiT-B16.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_shape</strong> (<em>tuple</em><em>, </em><em>optional</em>) – input shape. Defaults to (224, 224, 3).</p></li>
<li><p><strong>classes</strong> (<em>int</em><em>, </em><em>optional</em>) – number of classes. Defaults to 1000.</p></li>
<li><p><strong>distilled</strong> (<em>bool</em><em>, </em><em>optional</em>) – build model appending a distilled token. Defaults to False.</p></li>
<li><p><strong>include_top</strong> (<em>bool</em><em>, </em><em>optional</em>) – whether to include the final classifier network.
Defaults to True.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the requested model</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

</section>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="quantizeml_apis.html" class="btn btn-neutral float-left" title="QuantizeML API" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../examples/index.html" class="btn btn-neutral float-right" title="Akida examples" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, BrainChip Holdings Ltd. All Rights Reserved.</p>
  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>