<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Akida models API &mdash; Akida Examples  documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery-binder.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery-dataframe.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery-rendered-html.css" type="text/css" />
      <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Akida examples" href="../examples/index.html" />
    <link rel="prev" title="CNN2SNN Toolkit API" href="cnn2snn_apis.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #78b3ff" >
            <a href="../index.html">
            <img src="../_static/akida.svg" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                MetaTF 2.2.3
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../index.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../installation.html#requirements">Requirements</a></li>
<li class="toctree-l2"><a class="reference internal" href="../installation.html#quick-installation">Quick installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../installation.html#running-examples">Running examples</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../user_guide/user_guide.html">User guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../user_guide/getting_started.html">Getting started</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/getting_started.html#for-beginners">For beginners</a></li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/getting_started.html#for-users-familiar-with-deep-learning">For users familiar with deep-learning</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../user_guide/akida.html">Akida user guide</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/akida.html#introduction">Introduction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida.html#akida-layers">Akida layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida.html#input-format">Input Format</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida.html#a-versatile-machine-learning-framework">A versatile machine learning framework</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/akida.html#the-sequential-model">The Sequential model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida.html#specifying-the-model">Specifying the model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida.html#accessing-layer-parameters-and-weights">Accessing layer parameters and weights</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida.html#inference">Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida.html#saving-and-loading">Saving and loading</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida.html#input-layer-types">Input layer types</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida.html#data-processing-layer-types">Data-Processing layer types</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/akida.html#model-hardware-mapping">Model Hardware Mapping</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida.html#devices">Devices</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida.html#model-mapping">Model mapping</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida.html#advanced-mapping-details-and-hardware-devices-usage">Advanced Mapping Details and Hardware Devices Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida.html#performances-measurement">Performances measurement</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/akida.html#id1">Using Akida Edge learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida.html#learning-constraints">Learning constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida.html#compiling-a-layer">Compiling a layer</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../user_guide/cnn2snn.html">CNN2SNN toolkit</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/cnn2snn.html#overview">Overview</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#conversion-workflow">Conversion workflow</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#typical-training-scenario">Typical training scenario</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#design-compatibility-constraints">Design compatibility constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#quantization-compatibility-constraints">Quantization compatibility constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#command-line-interface">Command-line interface</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/cnn2snn.html#layers-considerations">Layers Considerations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#supported-layer-types">Supported layer types</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#cnn2snn-quantization-aware-layers">CNN2SNN Quantization-aware layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#training-only-layers">Training-Only Layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#first-layers">First Layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#id6">Final Layers</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/cnn2snn.html#tips-and-tricks">Tips and Tricks</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../user_guide/akida_models.html">Akida models zoo</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/akida_models.html#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/akida_models.html#command-line-interface-for-model-creation">Command-line interface for model creation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/akida_models.html#command-line-interface-for-model-training">Command-line interface for model training</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida_models.html#utk-face-training">UTK Face training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida_models.html#kws-training">KWS training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida_models.html#yolo-training">YOLO training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida_models.html#akidanet-training">AkidaNet training</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/akida_models.html#command-line-interface-for-model-evaluation">Command-line interface for model evaluation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/akida_models.html#id1">Layer Blocks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida_models.html#conv-block"><code class="docutils literal notranslate"><span class="pre">conv_block</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida_models.html#dense-block"><code class="docutils literal notranslate"><span class="pre">dense_block</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida_models.html#separable-conv-block"><code class="docutils literal notranslate"><span class="pre">separable_conv_block</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../user_guide/hw_constraints.html">Hardware constraints</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/hw_constraints.html#inputconvolutional">InputConvolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/hw_constraints.html#convolutional">Convolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/hw_constraints.html#separableconvolutional">SeparableConvolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/hw_constraints.html#fullyconnected">FullyConnected</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../user_guide/compatibility.html">Akida versions compatibility</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/compatibility.html#upgrading-models-with-legacy-quantizers">Upgrading models with legacy quantizers</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="api_reference.html">API reference</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="akida_apis.html">Akida runtime</a><ul>
<li class="toctree-l3"><a class="reference internal" href="akida_apis.html#model">Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="akida_apis.html#layer">Layer</a><ul>
<li class="toctree-l4"><a class="reference internal" href="akida_apis.html#id1">Layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida_apis.html#mapping">Mapping</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="akida_apis.html#inputdata">InputData</a></li>
<li class="toctree-l3"><a class="reference internal" href="akida_apis.html#inputconvolutional">InputConvolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="akida_apis.html#fullyconnected">FullyConnected</a></li>
<li class="toctree-l3"><a class="reference internal" href="akida_apis.html#convolutional">Convolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="akida_apis.html#separableconvolutional">SeparableConvolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="akida_apis.html#layer-parameters">Layer parameters</a><ul>
<li class="toctree-l4"><a class="reference internal" href="akida_apis.html#layertype">LayerType</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida_apis.html#padding">Padding</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida_apis.html#pooltype">PoolType</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="akida_apis.html#optimizers">Optimizers</a></li>
<li class="toctree-l3"><a class="reference internal" href="akida_apis.html#sequence">Sequence</a><ul>
<li class="toctree-l4"><a class="reference internal" href="akida_apis.html#id2">Sequence</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida_apis.html#backendtype">BackendType</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida_apis.html#pass">Pass</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="akida_apis.html#device">Device</a><ul>
<li class="toctree-l4"><a class="reference internal" href="akida_apis.html#id3">Device</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida_apis.html#hwversion">HwVersion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="akida_apis.html#hwdevice">HWDevice</a><ul>
<li class="toctree-l4"><a class="reference internal" href="akida_apis.html#id4">HWDevice</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida_apis.html#socdriver">SocDriver</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida_apis.html#clockmode">ClockMode</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="akida_apis.html#powermeter">PowerMeter</a></li>
<li class="toctree-l3"><a class="reference internal" href="akida_apis.html#np">NP</a></li>
<li class="toctree-l3"><a class="reference internal" href="akida_apis.html#tools">Tools</a><ul>
<li class="toctree-l4"><a class="reference internal" href="akida_apis.html#sparsity">Sparsity</a></li>
<li class="toctree-l4"><a class="reference internal" href="akida_apis.html#compatibility">Compatibility</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="cnn2snn_apis.html">CNN2SNN</a><ul>
<li class="toctree-l3"><a class="reference internal" href="cnn2snn_apis.html#tool-functions">Tool functions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn_apis.html#quantize">quantize</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn_apis.html#quantize-layer">quantize_layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn_apis.html#convert">convert</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn_apis.html#check-model-compatibility">check_model_compatibility</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn_apis.html#load-quantized-model">load_quantized_model</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn_apis.html#transforms">Transforms</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn_apis.html#calibration">Calibration</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cnn2snn_apis.html#quantizers">Quantizers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn_apis.html#weightquantizer">WeightQuantizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn_apis.html#linearweightquantizer">LinearWeightQuantizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn_apis.html#stdweightquantizer">StdWeightQuantizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn_apis.html#stdperaxisquantizer">StdPerAxisQuantizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn_apis.html#maxquantizer">MaxQuantizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn_apis.html#maxperaxisquantizer">MaxPerAxisQuantizer</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="cnn2snn_apis.html#quantized-layers">Quantized layers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn_apis.html#quantizedconv2d">QuantizedConv2D</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn_apis.html#quantizeddense">QuantizedDense</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn_apis.html#quantizedseparableconv2d">QuantizedSeparableConv2D</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn_apis.html#quantizedactivation">QuantizedActivation</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn_apis.html#activationdiscreterelu">ActivationDiscreteRelu</a></li>
<li class="toctree-l4"><a class="reference internal" href="cnn2snn_apis.html#quantizedrelu">QuantizedReLU</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Akida models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#layer-blocks">Layer blocks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#conv-block">conv_block</a></li>
<li class="toctree-l4"><a class="reference internal" href="#separable-conv-block">separable_conv_block</a></li>
<li class="toctree-l4"><a class="reference internal" href="#dense-block">dense_block</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#helpers">Helpers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#batchnormalization-gamma-constraint">BatchNormalization gamma constraint</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#knowledge-distillation">Knowledge distillation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#pruning">Pruning</a></li>
<li class="toctree-l3"><a class="reference internal" href="#training">Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="#model-zoo">Model zoo</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#akidanet">AkidaNet</a></li>
<li class="toctree-l4"><a class="reference internal" href="#mobilenet">Mobilenet</a></li>
<li class="toctree-l4"><a class="reference internal" href="#ds-cnn">DS-CNN</a></li>
<li class="toctree-l4"><a class="reference internal" href="#vgg">VGG</a></li>
<li class="toctree-l4"><a class="reference internal" href="#yolo">YOLO</a></li>
<li class="toctree-l4"><a class="reference internal" href="#convtiny">ConvTiny</a></li>
<li class="toctree-l4"><a class="reference internal" href="#pointnet">PointNet++</a></li>
<li class="toctree-l4"><a class="reference internal" href="#gxnor">GXNOR</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../examples/index.html">Examples</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../examples/index.html#general-examples">General examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_0_gxnor_mnist.html">GXNOR/MNIST inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_0_gxnor_mnist.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_0_gxnor_mnist.html#create-a-keras-gxnor-model">2. Create a Keras GXNOR model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_0_gxnor_mnist.html#conversion-to-akida">3. Conversion to Akida</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_1_akidanet_imagenet.html">AkidaNet/ImageNet inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_1_akidanet_imagenet.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_1_akidanet_imagenet.html#create-a-keras-akidanet-model">2. Create a Keras AkidaNet model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_1_akidanet_imagenet.html#quantized-model">3. Quantized model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_1_akidanet_imagenet.html#pretrained-quantized-model">4. Pretrained quantized model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_1_akidanet_imagenet.html#conversion-to-akida">5. Conversion to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_1_akidanet_imagenet.html#hardware-mapping-and-performance">6. Hardware mapping and performance</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_2_ds_cnn_kws.html">DS-CNN/KWS inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_2_ds_cnn_kws.html#load-the-preprocessed-dataset">1. Load the preprocessed dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_2_ds_cnn_kws.html#load-a-pre-trained-native-keras-model">2. Load a pre-trained native Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_2_ds_cnn_kws.html#load-a-pre-trained-quantized-keras-model-satisfying-akida-nsoc-requirements">3. Load a pre-trained quantized Keras model satisfying Akida NSoC requirements</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_2_ds_cnn_kws.html#conversion-to-akida">4. Conversion to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_2_ds_cnn_kws.html#confusion-matrix">5. Confusion matrix</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_3_regression.html">Regression tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_3_regression.html#load-the-dataset">1. Load the dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_3_regression.html#load-a-pre-trained-native-keras-model">2. Load a pre-trained native Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_3_regression.html#load-a-pre-trained-quantized-keras-model-satisfying-akida-nsoc-requirements">3. Load a pre-trained quantized Keras model satisfying Akida NSoC requirements</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_3_regression.html#conversion-to-akida">4. Conversion to Akida</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_3_regression.html#estimate-age-on-a-single-image">5. Estimate age on a single image</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_4_transfer_learning.html">Transfer learning with AkidaNet for PlantVillage</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_transfer_learning.html#transfer-learning-process">Transfer learning process</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_transfer_learning.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_transfer_learning.html#get-a-trained-akidanet-base-model">2. Get a trained AkidaNet base model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_transfer_learning.html#add-a-float-classification-head-to-the-model">3. Add a float classification head to the model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_transfer_learning.html#freeze-the-base-model">4. Freeze the base model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_transfer_learning.html#train-for-a-few-epochs">5. Train for a few epochs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_transfer_learning.html#quantize-the-classification-head">6. Quantize the classification head</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_4_transfer_learning.html#compute-accuracy">7. Compute accuracy</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/general/plot_5_voc_yolo_detection.html">YOLO/PASCAL-VOC detection tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_5_voc_yolo_detection.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_5_voc_yolo_detection.html#preprocessing-tools">2. Preprocessing tools</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_5_voc_yolo_detection.html#model-architecture">3. Model architecture</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_5_voc_yolo_detection.html#training">4. Training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_5_voc_yolo_detection.html#performance">5. Performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/general/plot_5_voc_yolo_detection.html#conversion-to-akida">6. Conversion to Akida</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/index.html#cnn2snn-tutorials">CNN2SNN tutorials</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/cnn2snn/plot_0_cnn_flow.html">CNN conversion flow tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_0_cnn_flow.html#load-and-reshape-mnist-dataset">1. Load and reshape MNIST dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_0_cnn_flow.html#model-definition">2. Model definition</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_0_cnn_flow.html#model-training">3. Model training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_0_cnn_flow.html#model-quantization">4. Model quantization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_0_cnn_flow.html#model-fine-tuning-quantization-aware-training">5. Model fine tuning (quantization-aware training)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_0_cnn_flow.html#model-conversion">6. Model conversion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/cnn2snn/plot_1_advanced_cnn2snn.html">Advanced CNN2SNN tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_1_advanced_cnn2snn.html#design-a-cnn2snn-quantized-model">1. Design a CNN2SNN quantized model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_1_advanced_cnn2snn.html#weight-quantizer-details">2. Weight Quantizer Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_1_advanced_cnn2snn.html#understanding-quantized-activation">3. Understanding quantized activation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/cnn2snn/plot_1_advanced_cnn2snn.html#how-to-deal-with-too-high-scale-factors">4. How to deal with too high scale factors</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/index.html#edge-examples">Edge examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/edge/plot_0_edge_learning_vision.html">Akida vision edge learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_0_edge_learning_vision.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_0_edge_learning_vision.html#prepare-akida-model-for-learning">2. Prepare Akida model for learning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_0_edge_learning_vision.html#edge-learning-with-akida">3. Edge learning with Akida</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/edge/plot_1_edge_learning_kws.html">Akida edge learning for keyword spotting</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_1_edge_learning_kws.html#edge-learning-process">1. Edge learning process</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_1_edge_learning_kws.html#dataset-preparation">2. Dataset preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_1_edge_learning_kws.html#prepare-akida-model-for-learning">3. Prepare Akida model for learning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_1_edge_learning_kws.html#learn-with-akida-using-the-training-set">4. Learn with Akida using the training set</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_1_edge_learning_kws.html#edge-learning">5. Edge learning</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/edge/plot_2_edge_learning_parameters.html">Tips to set Akida learning parameters</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_2_edge_learning_parameters.html#akida-learning-parameters">1. Akida learning parameters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_2_edge_learning_parameters.html#create-akida-model">2. Create Akida model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_2_edge_learning_parameters.html#estimate-the-required-number-of-weights-of-the-trainable-layer">3. Estimate the required number of weights of the trainable layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/edge/plot_2_edge_learning_parameters.html#estimate-the-number-of-neurons-per-class">4. Estimate the number of neurons per class</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../zoo_performances.html">Model zoo performances</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../zoo_performances.html#image-icon-ref-image-domain"> Image domain</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../zoo_performances.html#classification">Classification</a></li>
<li class="toctree-l3"><a class="reference internal" href="../zoo_performances.html#object-detection">Object detection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../zoo_performances.html#regression">Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="../zoo_performances.html#face-recognition">Face recognition</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../zoo_performances.html#audio-icon-ref-audio-domain"> Audio domain</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../zoo_performances.html#keyword-spotting">Keyword spotting</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../zoo_performances.html#time-icon-ref-time-domain"> Time domain</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../zoo_performances.html#fault-detection">Fault detection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../zoo_performances.html#id1">Classification</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../zoo_performances.html#pointcloud-icon-ref-point-cloud"> Point cloud</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../zoo_performances.html#id2">Classification</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/Brainchip-Inc/akida_examples/releases">Changelog</a></li>
<li class="toctree-l1"><a class="reference external" href="https://support.brainchip.com/portal/home">Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #78b3ff" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Akida Examples</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="api_reference.html">API reference</a> &raquo;</li>
      <li>Akida models API</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="module-akida_models">
<span id="akida-models-api"></span><h1>Akida models API<a class="headerlink" href="#module-akida_models" title="Permalink to this headline"></a></h1>
<p>Imports models.</p>
<section id="layer-blocks">
<h2>Layer blocks<a class="headerlink" href="#layer-blocks" title="Permalink to this headline"></a></h2>
<section id="conv-block">
<h3>conv_block<a class="headerlink" href="#conv-block" title="Permalink to this headline"></a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="akida_models.layer_blocks.conv_block">
<span class="sig-prename descclassname"><span class="pre">akida_models.layer_blocks.</span></span><span class="sig-name descname"><span class="pre">conv_block</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filters</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pooling</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pool_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(2,</span> <span class="pre">2)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_batchnorm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/layer_blocks.html#conv_block"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.layer_blocks.conv_block" title="Permalink to this definition"></a></dt>
<dd><p>Adds a convolutional layer with optional layers in the following order:
max pooling, batch normalization, activation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>tf.Tensor</em>) – input tensor of shape <cite>(rows, cols, channels)</cite></p></li>
<li><p><strong>filters</strong> (<em>int</em>) – the dimensionality of the output space
(i.e. the number of output filters in the convolution).</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em> or </em><em>tuple of 2 integers</em>) – specifying the
height and width of the 2D convolution kernel.
Can be a single integer to specify the same value for
all spatial dimensions.</p></li>
<li><p><strong>pooling</strong> (<em>str</em>) – add a pooling layer of type ‘pooling’ among the
values ‘max’, ‘avg’, ‘global_max’ or ‘global_avg’, with pooling
size set to pool_size. If ‘None’, no pooling will be added.</p></li>
<li><p><strong>pool_size</strong> (<em>int</em><em> or </em><em>tuple of 2 integers</em>) – factors by which to
downscale (vertical, horizontal). (2, 2) will halve the input in
both spatial dimension. If only one integer is specified, the same
window length will be used for both dimensions.</p></li>
<li><p><strong>add_batchnorm</strong> (<em>bool</em>) – add a BatchNormalization layer</p></li>
<li><p><strong>add_activation</strong> (<em>bool</em>) – add a ReLU layer</p></li>
<li><p><strong>**kwargs</strong> – arguments passed to the keras.Conv2D layer, such as
strides, padding, use_bias, weight_regularizer, etc.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>output tensor of conv2D block.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.Tensor</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="separable-conv-block">
<h3>separable_conv_block<a class="headerlink" href="#separable-conv-block" title="Permalink to this headline"></a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="akida_models.layer_blocks.separable_conv_block">
<span class="sig-prename descclassname"><span class="pre">akida_models.layer_blocks.</span></span><span class="sig-name descname"><span class="pre">separable_conv_block</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filters</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pooling</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pool_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(2,</span> <span class="pre">2)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_batchnorm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/layer_blocks.html#separable_conv_block"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.layer_blocks.separable_conv_block" title="Permalink to this definition"></a></dt>
<dd><p>Adds a separable convolutional layer with optional layers in the
following order: global average pooling, max pooling, batch normalization,
activation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>tf.Tensor</em>) – input tensor of shape <cite>(height, width, channels)</cite></p></li>
<li><p><strong>filters</strong> (<em>int</em>) – the dimensionality of the output space
(i.e. the number of output filters in the pointwise convolution).</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em> or </em><em>tuple of 2 integers</em>) – specifying the
height and width of the 2D convolution window. Can be a single
integer to specify the same value for all spatial dimensions.</p></li>
<li><p><strong>pooling</strong> (<em>str</em>) – add a pooling layer of type ‘pooling’ among the
values ‘max’, ‘avg’, ‘global_max’ or ‘global_avg’, with pooling
size set to pool_size. If ‘None’, no pooling will be added.</p></li>
<li><p><strong>pool_size</strong> (<em>int</em><em> or </em><em>tuple of 2 integers</em>) – factors by which to
downscale (vertical, horizontal). (2, 2) will halve the input in
both spatial dimension. If only one integer is specified, the same
window length will be used for both dimensions.</p></li>
<li><p><strong>add_batchnorm</strong> (<em>bool</em>) – add a BatchNormalization layer</p></li>
<li><p><strong>add_activation</strong> (<em>bool</em>) – add a ReLU layer</p></li>
<li><p><strong>**kwargs</strong> – arguments passed to the keras.SeparableConv2D layer,
such as strides, padding, use_bias, etc.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>output tensor of separable conv block.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.Tensor</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="dense-block">
<h3>dense_block<a class="headerlink" href="#dense-block" title="Permalink to this headline"></a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="akida_models.layer_blocks.dense_block">
<span class="sig-prename descclassname"><span class="pre">akida_models.layer_blocks.</span></span><span class="sig-name descname"><span class="pre">dense_block</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">units</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_batchnorm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/layer_blocks.html#dense_block"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.layer_blocks.dense_block" title="Permalink to this definition"></a></dt>
<dd><p>Adds a dense layer with optional layers in the following order:
batch normalization, activation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>tf.Tensor</em>) – Input tensor of shape <cite>(rows, cols, channels)</cite></p></li>
<li><p><strong>units</strong> (<em>int</em>) – dimensionality of the output space</p></li>
<li><p><strong>add_batchnorm</strong> (<em>bool</em>) – add a BatchNormalization layer</p></li>
<li><p><strong>add_activation</strong> (<em>bool</em>) – add a ReLU layer</p></li>
<li><p><strong>**kwargs</strong> – arguments passed to the Dense layer, such as
use_bias, kernel_initializer, weight_regularizer, etc.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>output tensor of the dense block.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.Tensor</p>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="helpers">
<h2>Helpers<a class="headerlink" href="#helpers" title="Permalink to this headline"></a></h2>
<section id="batchnormalization-gamma-constraint">
<h3>BatchNormalization gamma constraint<a class="headerlink" href="#batchnormalization-gamma-constraint" title="Permalink to this headline"></a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="akida_models.add_gamma_constraint">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">add_gamma_constraint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/gamma_constraint.html#add_gamma_constraint"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.add_gamma_constraint" title="Permalink to this definition"></a></dt>
<dd><p>Method helper to add a MinValueConstraint to an existing model so that
gamma values of its BatchNormalization layers are above a defined minimum.</p>
<p>This is typically used to help having a model that will be Akida compatible
after conversion. In some cases, the mapping on hardware will fail because
of huge values for <cite>threshold</cite> or <cite>act_step</cite> with a message indicating that
a value cannot fit in a 20 bit signed or unsigned integer.
In such a case, this helper can be called to apply a constraint that can fix
the issue.</p>
<p>Note that in order for the constraint to be applied to the actual weights,
some training must be done: for an already trained model, it can be on a few
batches, one epoch or more depending on the impact the constraint has on
accuracy. This helper can also be called to a new model that has not been
trained yet.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>model</strong> (<em>keras.Model</em>) – the model for which gamma constraints will be
added.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the same model with BatchNormalisation layers updated.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="knowledge-distillation">
<h2>Knowledge distillation<a class="headerlink" href="#knowledge-distillation" title="Permalink to this headline"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="akida_models.distiller.Distiller">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">akida_models.distiller.</span></span><span class="sig-name descname"><span class="pre">Distiller</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/distiller.html#Distiller"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.distiller.Distiller" title="Permalink to this definition"></a></dt>
<dd><p>The class that will be used to train the student model using the
distillation knowledge method.</p>
<p>Reference <a class="reference external" href="https://arxiv.org/abs/1503.02531">Hinton et al. (2015)</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>student</strong> (<em>keras.Model</em>) – the student model</p></li>
<li><p><strong>teacher</strong> (<em>keras.Model</em>) – the well trained teacher model</p></li>
</ul>
</dd>
</dl>
<p><strong>Methods:</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#akida_models.distiller.Distiller.compile" title="akida_models.distiller.Distiller.compile"><code class="xref py py-obj docutils literal notranslate"><span class="pre">compile</span></code></a>(optimizer, metrics, student_loss_fn, ...)</p></td>
<td><p>Configure the distiller.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#akida_models.distiller.Distiller.test_step" title="akida_models.distiller.Distiller.test_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">test_step</span></code></a>(data)</p></td>
<td><p>The logic for one evaluation step.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#akida_models.distiller.Distiller.train_step" title="akida_models.distiller.Distiller.train_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">train_step</span></code></a>(data)</p></td>
<td><p>The logic for one training step.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt class="sig sig-object py" id="akida_models.distiller.Distiller.compile">
<span class="sig-name descname"><span class="pre">compile</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metrics</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">student_loss_fn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">distillation_loss_fn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/distiller.html#Distiller.compile"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.distiller.Distiller.compile" title="Permalink to this definition"></a></dt>
<dd><p>Configure the distiller.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong> (<em>keras.optimizers.Optimizer</em>) – Keras optimizer
for the student weights</p></li>
<li><p><strong>metrics</strong> (<em>keras.metrics.Metric</em>) – Keras metrics for evaluation</p></li>
<li><p><strong>student_loss_fn</strong> (<em>keras.losses.Loss</em>) – loss function of difference
between student predictions and ground-truth</p></li>
<li><p><strong>distillation_loss_fn</strong> (<em>keras.losses.Loss</em>) – loss function of
difference between student predictions and teacher predictions</p></li>
<li><p><strong>alpha</strong> (<em>float</em>) – weight to student_loss_fn and 1-alpha to
distillation_loss_fn</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="akida_models.distiller.Distiller.test_step">
<span class="sig-name descname"><span class="pre">test_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/distiller.html#Distiller.test_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.distiller.Distiller.test_step" title="Permalink to this definition"></a></dt>
<dd><p>The logic for one evaluation step.</p>
<p>This method can be overridden to support custom evaluation logic.
This method is called by <cite>Model.make_test_function</cite>.</p>
<p>This function should contain the mathematical logic for one step of
evaluation.
This typically includes the forward pass, loss calculation, and metrics
updates.</p>
<p>Configuration details for <em>how</em> this logic is run (e.g. <cite>tf.function</cite> and
<cite>tf.distribute.Strategy</cite> settings), should be left to
<cite>Model.make_test_function</cite>, which can also be overridden.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>data</strong> – A nested structure of <a href="#id1"><span class="problematic" id="id2">`</span></a>Tensor`s.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>dict</cite> containing values that will be passed to
<cite>tf.keras.callbacks.CallbackList.on_train_batch_end</cite>. Typically, the
values of the <cite>Model</cite>’s metrics are returned.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="akida_models.distiller.Distiller.train_step">
<span class="sig-name descname"><span class="pre">train_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/distiller.html#Distiller.train_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.distiller.Distiller.train_step" title="Permalink to this definition"></a></dt>
<dd><p>The logic for one training step.</p>
<p>This method can be overridden to support custom training logic.
For concrete examples of how to override this method see
[Customizing what happends in fit](<a class="reference external" href="https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit">https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit</a>).
This method is called by <cite>Model.make_train_function</cite>.</p>
<p>This method should contain the mathematical logic for one step of training.
This typically includes the forward pass, loss calculation, backpropagation,
and metric updates.</p>
<p>Configuration details for <em>how</em> this logic is run (e.g. <cite>tf.function</cite> and
<cite>tf.distribute.Strategy</cite> settings), should be left to
<cite>Model.make_train_function</cite>, which can also be overridden.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>data</strong> – A nested structure of <a href="#id3"><span class="problematic" id="id4">`</span></a>Tensor`s.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <cite>dict</cite> containing values that will be passed to
<cite>tf.keras.callbacks.CallbackList.on_train_batch_end</cite>. Typically, the
values of the <cite>Model</cite>’s metrics are returned. Example:
<cite>{‘loss’: 0.2, ‘accuracy’: 0.7}</cite>.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.distiller.KLDistillationLoss">
<span class="sig-prename descclassname"><span class="pre">akida_models.distiller.</span></span><span class="sig-name descname"><span class="pre">KLDistillationLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">temperature</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/distiller.html#KLDistillationLoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.distiller.KLDistillationLoss" title="Permalink to this definition"></a></dt>
<dd><p>The <cite>KLDistillationLoss</cite> is a simple wrapper around the KLDivergence loss
that accepts raw predictions instead of probability distributions.</p>
<p>Before invoking the KLDivergence loss, it converts the inputs predictions to
probabilities by dividing them by a constant ‘temperature’ and applies a
softmax.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>temperature</strong> (<em>float</em>) – temperature for softening probability
distributions. Larger temperature gives softer distributions.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="pruning">
<h2>Pruning<a class="headerlink" href="#pruning" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="akida_models.prune_model">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">prune_model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">model</span></em>, <em class="sig-param"><span class="pre">acceptance_function</span></em>, <em class="sig-param"><span class="pre">pruning_rates=None</span></em>, <em class="sig-param"><span class="pre">prunable_layers_policy=&lt;function</span> <span class="pre">neural_layers&gt;</span></em>, <em class="sig-param"><span class="pre">prunable_filters_policy=&lt;function</span> <span class="pre">smallest_filters&gt;</span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/filter_pruning.html#prune_model"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.prune_model" title="Permalink to this definition"></a></dt>
<dd><p>Prune model automatically based on an acceptance function.</p>
<p>The algorithm for filter pruning is as follows:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Select the first prunable layer (according to the
<code class="docutils literal notranslate"><span class="pre">prunable_layers_policy</span></code> function).</p></li>
<li><p>As long as the <code class="docutils literal notranslate"><span class="pre">acceptance_function</span></code> returns True, prune
successively the layer with different pruning rates (according to
<code class="docutils literal notranslate"><span class="pre">pruning_rates</span></code> and <code class="docutils literal notranslate"><span class="pre">prunable_filters_policy</span></code>).</p></li>
<li><p>When the current pruned model is not acceptable, the last valid
pruning rate is selected for the final pruned model.</p></li>
<li><p>Repeat steps 1, 2 and 3 for the next prunable layers.</p></li>
</ol>
</div></blockquote>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">acceptable_drop</span> <span class="o">=</span> <span class="mf">0.05</span>

<span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">accuracy</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">accuracy</span>

<span class="n">ref_accuracy</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">base_model</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">acceptance_function</span><span class="p">(</span><span class="n">pruned_model</span><span class="p">):</span>
    <span class="c1"># This function returns True if the pruned_model is acceptable.</span>
    <span class="c1"># Here, the pruned model is acceptable if the accuracy drops</span>
    <span class="c1"># less than 5% from the base model.</span>

    <span class="k">return</span> <span class="n">ref_accuracy</span> <span class="o">-</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">pruned_model</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">acceptable_drop</span>

<span class="c1"># Prune model</span>
<span class="n">pruned_model</span><span class="p">,</span> <span class="n">pruning_rates</span> <span class="o">=</span> <span class="n">prune_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">acceptance_function</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>keras.Model</em>) – a keras model to prune</p></li>
<li><p><strong>acceptance_function</strong> (<em>function</em>) – a criterion function that returns True
if the pruned model is acceptable. The signature must be
<cite>function(model)</cite>.</p></li>
<li><p><strong>pruning_rates</strong> (<em>list</em><em>, </em><em>optional</em>) – a list of pruning rates to test. Default
is [0.1, 0.2, …, 0.9].</p></li>
<li><p><strong>prunable_layers_policy</strong> (<em>function</em><em>, </em><em>optional</em>) – a function returning a list
of layers to prune in the model. The signature must be
<cite>function(model)</cite>, and must return a list of prunable layer names.
By default, all neural layers (Conv2D/SeparableConv2D/Dense/
QuantizedConv2D/QuantizedSeparableConv2D/QuantizedDense) are
candidates for pruning.</p></li>
<li><p><strong>prunable_filters_policy</strong> (<em>function</em><em>, </em><em>optional</em>) – a function that returns
the filters to prune in a given layer for a specific pruning rate.
The signature must be <cite>function(layer, pruning_rate)</cite> and returns a
list of indices to prune. By default, filters with the lowest
magnitude are pruned.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the pruned model and the pruning rates.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tuple</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.delete_filters">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">delete_filters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_to_prune</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filters_to_prune</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/filter_pruning.html#delete_filters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.delete_filters" title="Permalink to this definition"></a></dt>
<dd><p>Deletes filters in the given layer and updates weights in it and its
subsequent layers.</p>
<p>A pruned model is returned. Only linear models are supported.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>keras.Model</em>) – the model to prune.</p></li>
<li><p><strong>layer_to_prune</strong> (<em>str</em>) – the name of the neural layer where filters will be
deleted.</p></li>
<li><p><strong>filters_to_prune</strong> (<em>list</em>) – indices of filters to delete in the given
layer.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the pruned model</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>keras.Sequential</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="training">
<h2>Training<a class="headerlink" href="#training" title="Permalink to this headline"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="akida_models.training.freeze_model_before">
<span class="sig-prename descclassname"><span class="pre">akida_models.training.</span></span><span class="sig-name descname"><span class="pre">freeze_model_before</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">freeze_before</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/training.html#freeze_model_before"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.training.freeze_model_before" title="Permalink to this definition"></a></dt>
<dd><p>Freezes the model before the given layer name.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>keras.Model</em>) – the model to freeze</p></li>
<li><p><strong>freeze_before</strong> (<em>str</em>) – name of the layer from which the model will not be
frozen</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>ValueError</strong> – if the provided layer name was not found in the model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.training.evaluate_model">
<span class="sig-prename descclassname"><span class="pre">akida_models.training.</span></span><span class="sig-name descname"><span class="pre">evaluate_model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">steps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">print_history</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/training.html#evaluate_model"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.training.evaluate_model" title="Permalink to this definition"></a></dt>
<dd><p>Evaluates model performances.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>keras.Model</em>) – the model to evaluate</p></li>
<li><p><strong>x</strong> (<em>tf.Dataset</em><em>, </em><em>np.array</em><em> or </em><em>generator</em>) – evaluation input data</p></li>
<li><p><strong>y</strong> (<em>tf.Dataset</em><em>, </em><em>np.array</em><em> or </em><em>generator</em><em>, </em><em>optional</em>) – evaluation target
data. Defaults to None.</p></li>
<li><p><strong>batch_size</strong> (<em>int</em><em>, </em><em>optional</em>) – the batch size. Defaults to None.</p></li>
<li><p><strong>steps</strong> (<em>int</em><em>, </em><em>optional</em>) – total number of steps before declaring the
evaluation round finished. Defaults to None.</p></li>
<li><p><strong>print_history</strong> (<em>bool</em><em>, </em><em>optional</em>) – either to print all history or just
accuracy. Defaults to False.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.training.evaluate_akida_model">
<span class="sig-prename descclassname"><span class="pre">akida_models.training.</span></span><span class="sig-name descname"><span class="pre">evaluate_akida_model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/training.html#evaluate_akida_model"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.training.evaluate_akida_model" title="Permalink to this definition"></a></dt>
<dd><p>Evaluates Akida model and return predictions and labels to compute
accuracy.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<a class="reference internal" href="akida_apis.html#akida.Model" title="akida.Model"><em>akida.Model</em></a>) – the model to evaluate</p></li>
<li><p><strong>x</strong> (<em>tf.Dataset</em><em>, </em><em>np.array</em><em> or </em><em>generator</em>) – evaluation input data</p></li>
<li><p><strong>activation</strong> (<em>str</em>) – activation function to apply to potentials</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>predictions and labels</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>np.array, np.array</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.training.compile_model">
<span class="sig-prename descclassname"><span class="pre">akida_models.training.</span></span><span class="sig-name descname"><span class="pre">compile_model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'categorical_crossentropy'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metrics</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/training.html#compile_model"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.training.compile_model" title="Permalink to this definition"></a></dt>
<dd><p>Compiles the model using Adam optimizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>keras.Model</em>) – the model to compile</p></li>
<li><p><strong>learning_rate</strong> (<em>float</em><em>, </em><em>optional</em>) – the learning rate. Defaults to 1e-3.</p></li>
<li><p><strong>loss</strong> (<em>str</em><em> or </em><em>function</em><em>, </em><em>optional</em>) – the loss function. Defaults to
‘categorical_crossentropy’.</p></li>
<li><p><strong>metrics</strong> (<em>list</em><em>, </em><em>optional</em>) – list of metrics to be evaluated during
training and testing. Defaults to None.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="model-zoo">
<h2>Model zoo<a class="headerlink" href="#model-zoo" title="Permalink to this headline"></a></h2>
<section id="akidanet">
<h3>AkidaNet<a class="headerlink" href="#akidanet" title="Permalink to this headline"></a></h3>
<section id="imagenet">
<h4>ImageNet<a class="headerlink" href="#imagenet" title="Permalink to this headline"></a></h4>
<dl class="py function">
<dt class="sig sig-object py" id="akida_models.akidanet_imagenet">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">akidanet_imagenet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">include_top</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pooling</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">classes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_quantization</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activ_quantization</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_weight_quantization</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_scaling</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(128,</span> <span class="pre">-</span> <span class="pre">1)</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/imagenet/model_akidanet.html#akidanet_imagenet"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.akidanet_imagenet" title="Permalink to this definition"></a></dt>
<dd><p>Instantiates the AkidaNet architecture.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_shape</strong> (<em>tuple</em>) – optional shape tuple.</p></li>
<li><p><strong>alpha</strong> (<em>float</em>) – <p>controls the width of the model.</p>
<ul>
<li><p>If <cite>alpha</cite> &lt; 1.0, proportionally decreases the number of filters
in each layer.</p></li>
<li><p>If <cite>alpha</cite> &gt; 1.0, proportionally increases the number of filters
in each layer.</p></li>
<li><p>If <cite>alpha</cite> = 1, default number of filters from the paper are used
at each layer.</p></li>
</ul>
</p></li>
<li><p><strong>include_top</strong> (<em>bool</em>) – whether to include the fully-connected
layer at the top of the model.</p></li>
<li><p><strong>pooling</strong> (<em>str</em>) – <p>optional pooling mode for feature extraction
when <cite>include_top</cite> is <cite>False</cite>.</p>
<ul>
<li><p><cite>None</cite> means that the output of the model will be the 4D tensor
output of the last convolutional block.</p></li>
<li><p><cite>avg</cite> means that global average pooling will be applied to the
output of the last convolutional block, and thus the output of the
model will be a 2D tensor.</p></li>
</ul>
</p></li>
<li><p><strong>classes</strong> (<em>int</em>) – optional number of classes to classify images
into, only to be specified if <cite>include_top</cite> is <cite>True</cite>.</p></li>
<li><p><strong>weight_quantization</strong> (<em>int</em>) – <p>sets all weights in the model to have
a particular quantization bitwidth except for the weights in the
first layer.</p>
<ul>
<li><p>’0’ implements floating point 32-bit weights.</p></li>
<li><p>’2’ through ‘8’ implements n-bit weights where n is from 2-8 bits.</p></li>
</ul>
</p></li>
<li><p><strong>activ_quantization</strong> – <p>sets all activations in the model to have a
particular activation quantization bitwidth.</p>
<ul>
<li><p>’0’ implements floating point 32-bit activations.</p></li>
<li><p>’2’ through ‘8’ implements n-bit weights where n is from 2-8 bits.</p></li>
</ul>
</p></li>
<li><p><strong>input_weight_quantization</strong> – <p>sets weight quantization in the first layer.
Defaults to weight_quantization value.</p>
<ul>
<li><p>’0’ implements floating point 32-bit weights.</p></li>
<li><p>’2’ through ‘8’ implements n-bit weights where n is from 2-8 bits.</p></li>
</ul>
</p></li>
<li><p><strong>input_scaling</strong> (<em>tuple</em><em>, </em><em>optional</em>) – scale factor and offset to apply to
inputs. Defaults to (128, -1). Note that following Akida convention,
the scale factor is an integer used as a divider.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a Keras model for AkidaNet/ImageNet.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>keras.Model</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>ValueError</strong> – in case of invalid input shape.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.akidanet_imagenet_pretrained">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">akidanet_imagenet_pretrained</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/imagenet/model_akidanet.html#akidanet_imagenet_pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.akidanet_imagenet_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Helper method to retrieve an <cite>akidanet_imagenet</cite> model that was trained on
ImageNet dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>alpha</strong> (<em>float</em><em>, </em><em>optional</em>) – width of the model, allowed values in [0.25,
0.5, 1]. Defaults to 1.0.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a Keras Model instance.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.akidanet_edge_imagenet">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">akidanet_edge_imagenet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">base_model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">classes</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/imagenet/model_akidanet_edge.html#akidanet_edge_imagenet"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.akidanet_edge_imagenet" title="Permalink to this definition"></a></dt>
<dd><p>Instantiates an AkidaNet-edge architecture.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>base_model</strong> (<em>str/keras.Model</em>) – an akidanet_imagenet quantized model.</p></li>
<li><p><strong>classes</strong> (<em>int</em>) – the number of classes for the edge classifier.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a Keras Model instance.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.akidanet_edge_imagenet_pretrained">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">akidanet_edge_imagenet_pretrained</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/imagenet/model_akidanet_edge.html#akidanet_edge_imagenet_pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.akidanet_edge_imagenet_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Helper method to retrieve a <cite>akidanet_edge_imagenet</cite> model that was
trained on ImageNet dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a Keras Model instance.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.akidanet_imagenette_pretrained">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">akidanet_imagenette_pretrained</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/imagenet/model_akidanet.html#akidanet_imagenette_pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.akidanet_imagenette_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Helper method to retrieve a <cite>akidanet_imagenet</cite> model that was trained on
Imagenette dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>alpha</strong> (<em>float</em><em>, </em><em>optional</em>) – width of the model, allowed values in [0.25,
0.5, 1]. Defaults to 1.0.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a Keras Model instance.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.akidanet_cats_vs_dogs_pretrained">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">akidanet_cats_vs_dogs_pretrained</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/imagenet/model_akidanet.html#akidanet_cats_vs_dogs_pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.akidanet_cats_vs_dogs_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Helper method to retrieve an <cite>akidanet_imagenet</cite> model that was trained on
Cats vs.Dogs dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a Keras Model instance.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.akidanet_faceidentification_pretrained">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">akidanet_faceidentification_pretrained</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/imagenet/model_akidanet.html#akidanet_faceidentification_pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.akidanet_faceidentification_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Helper method to retrieve an <cite>akidanet_imagenet</cite> model that was trained on
CASIA Webface dataset and that performs face identification.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a Keras Model instance.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.akidanet_faceidentification_edge_pretrained">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">akidanet_faceidentification_edge_pretrained</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/imagenet/model_akidanet_edge.html#akidanet_faceidentification_edge_pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.akidanet_faceidentification_edge_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Helper method to retrieve an <cite>akidanet_edge_imagenet</cite> model that was trained
on CASIA Webface dataset and that performs face identification.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a Keras Model instance.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.akidanet_faceverification_pretrained">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">akidanet_faceverification_pretrained</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/imagenet/model_akidanet.html#akidanet_faceverification_pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.akidanet_faceverification_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Helper method to retrieve an <cite>akidanet_imagenet</cite> model that was trained on
CASIA Webface dataset and optimized with ArcFace that can perform face
verification on LFW.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a Keras Model instance.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.akidanet_melanoma_pretrained">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">akidanet_melanoma_pretrained</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/imagenet/model_akidanet.html#akidanet_melanoma_pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.akidanet_melanoma_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Helper method to retrieve an <cite>akidanet_imagenet</cite> model that was trained on
SIIM-ISIC Melanoma Classification dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a Keras Model instance.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.akidanet_odir5k_pretrained">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">akidanet_odir5k_pretrained</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/imagenet/model_akidanet.html#akidanet_odir5k_pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.akidanet_odir5k_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Helper method to retrieve an <cite>akidanet_imagenet</cite> model that was trained on
ODIR-5K dataset.</p>
<p>The model focuses on the following classes that are a part of the original
dataset: normal, cataract, AMD (age related macular degeneration) and
pathological myopia.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a Keras Model instance.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.akidanet_retinal_oct_pretrained">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">akidanet_retinal_oct_pretrained</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/imagenet/model_akidanet.html#akidanet_retinal_oct_pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.akidanet_retinal_oct_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Helper method to retrieve an <cite>akidanet_imagenet</cite> model that was trained on
retinal OCT dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a Keras Model instance.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.akidanet_ecg_pretrained">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">akidanet_ecg_pretrained</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/imagenet/model_akidanet.html#akidanet_ecg_pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.akidanet_ecg_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Helper method to retrieve an <cite>akidanet_imagenet</cite> model that was trained on
ECG classification Physionet2017 dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a Keras Model instance.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.akidanet_plantvillage_pretrained">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">akidanet_plantvillage_pretrained</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/imagenet/model_akidanet.html#akidanet_plantvillage_pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.akidanet_plantvillage_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Helper method to retrieve an <cite>akidanet_imagenet</cite> model that was trained on
PlantVillage dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a Keras Model instance.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.akidanet_cifar10_pretrained">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">akidanet_cifar10_pretrained</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/imagenet/model_akidanet.html#akidanet_cifar10_pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.akidanet_cifar10_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Helper method to retrieve an <cite>akidanet_imagenet</cite> model that was trained on
CIFAR-10 dataset. Since CIFAR-10 images have a 32x32 size, they need to be
resized to match akidanet input layer. This can be done by calling the
‘resize_image’ function available under akida_models.cifar10.preprocessing.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a Keras Model instance.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.akidanet_vww_pretrained">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">akidanet_vww_pretrained</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/imagenet/model_akidanet.html#akidanet_vww_pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.akidanet_vww_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Helper method to retrieve an <cite>akidanet_imagenet</cite> model that was trained on
VWW dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a Keras Model instance.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<section id="preprocessing">
<h5>Preprocessing<a class="headerlink" href="#preprocessing" title="Permalink to this headline"></a></h5>
<dl class="py function">
<dt class="sig sig-object py" id="akida_models.imagenet.preprocessing.preprocess_image">
<span class="sig-prename descclassname"><span class="pre">akida_models.imagenet.preprocessing.</span></span><span class="sig-name descname"><span class="pre">preprocess_image</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">image_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data_aug</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/imagenet/preprocessing.html#preprocess_image"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.imagenet.preprocessing.preprocess_image" title="Permalink to this definition"></a></dt>
<dd><p>ImageNet data preprocessing.</p>
<p>Preprocessing includes cropping, and resizing for both training and
validation images. Training preprocessing introduces some random distortion
of the image to improve accuracy.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>image</strong> (<em>tf.Tensor</em>) – input image as a 3-D tensor</p></li>
<li><p><strong>image_size</strong> (<em>int</em>) – desired image size</p></li>
<li><p><strong>training</strong> (<em>bool</em><em>, </em><em>optional</em>) – True for training preprocessing, False for
validation and inference. Defaults to False.</p></li>
<li><p><strong>data_aug</strong> (<em>keras.Sequential</em><em>, </em><em>optional</em>) – data augmentation. Defaults to None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>preprocessed image</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tensorflow.Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.imagenet.preprocessing.index_to_label">
<span class="sig-prename descclassname"><span class="pre">akida_models.imagenet.preprocessing.</span></span><span class="sig-name descname"><span class="pre">index_to_label</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">index</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/imagenet/preprocessing.html#index_to_label"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.imagenet.preprocessing.index_to_label" title="Permalink to this definition"></a></dt>
<dd><p>Function to get an ImageNet label from an index.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>index</strong> – between 0 and 999</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a string of comma separated labels</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

</section>
</section>
</section>
<section id="mobilenet">
<h3>Mobilenet<a class="headerlink" href="#mobilenet" title="Permalink to this headline"></a></h3>
<section id="id5">
<h4>ImageNet<a class="headerlink" href="#id5" title="Permalink to this headline"></a></h4>
<dl class="py function">
<dt class="sig sig-object py" id="akida_models.mobilenet_imagenet">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">mobilenet_imagenet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">include_top</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pooling</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">classes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_stride2</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_quantization</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activ_quantization</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_weight_quantization</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_scaling</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(128,</span> <span class="pre">-</span> <span class="pre">1)</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/imagenet/model_mobilenet.html#mobilenet_imagenet"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.mobilenet_imagenet" title="Permalink to this definition"></a></dt>
<dd><p>Instantiates the MobileNet architecture.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_shape</strong> (<em>tuple</em>) – optional shape tuple.</p></li>
<li><p><strong>alpha</strong> (<em>float</em>) – <p>controls the width of the model.</p>
<ul>
<li><p>If <cite>alpha</cite> &lt; 1.0, proportionally decreases the number of filters
in each layer.</p></li>
<li><p>If <cite>alpha</cite> &gt; 1.0, proportionally increases the number of filters
in each layer.</p></li>
<li><p>If <cite>alpha</cite> = 1, default number of filters from the paper are used
at each layer.</p></li>
</ul>
</p></li>
<li><p><strong>dropout</strong> (<em>float</em>) – dropout rate</p></li>
<li><p><strong>include_top</strong> (<em>bool</em>) – whether to include the fully-connected
layer at the top of the model.</p></li>
<li><p><strong>pooling</strong> (<em>str</em>) – <p>Optional pooling mode for feature extraction
when <cite>include_top</cite> is <cite>False</cite>.</p>
<ul>
<li><p><cite>None</cite> means that the output of the model will be the 4D tensor
output of the last convolutional block.</p></li>
<li><p><cite>avg</cite> means that global average pooling will be applied to the
output of the last convolutional block, and thus the output of the
model will be a 2D tensor.</p></li>
</ul>
</p></li>
<li><p><strong>classes</strong> (<em>int</em>) – optional number of classes to classify images
into, only to be specified if <cite>include_top</cite> is True.</p></li>
<li><p><strong>use_stride2</strong> (<em>bool</em>) – optional, replace max pooling operations by stride 2
convolutions in layers separable 2, 4, 6 and 12.</p></li>
<li><p><strong>weight_quantization</strong> (<em>int</em>) – <p>sets all weights in the model to have
a particular quantization bitwidth except for the weights in the
first layer.</p>
<ul>
<li><p>’0’ implements floating point 32-bit weights.</p></li>
<li><p>’2’ through ‘8’ implements n-bit weights where n is from 2-8 bits.</p></li>
</ul>
</p></li>
<li><p><strong>activ_quantization</strong> – <p>sets all activations in the model to have a
particular activation quantization bitwidth.</p>
<ul>
<li><p>’0’ implements floating point 32-bit activations.</p></li>
<li><p>’2’ through ‘8’ implements n-bit weights where n is from 2-8 bits.</p></li>
</ul>
</p></li>
<li><p><strong>input_weight_quantization</strong> – <p>sets weight quantization in the first layer.
Defaults to weight_quantization value.</p>
<ul>
<li><p>’0’ implements floating point 32-bit weights.</p></li>
<li><p>’2’ through ‘8’ implements n-bit weights where n is from 2-8 bits.</p></li>
</ul>
</p></li>
<li><p><strong>input_scaling</strong> (<em>tuple</em><em>, </em><em>optional</em>) – scale factor and offset to apply to
inputs. Defaults to (128, -1). Note that following Akida convention,
the scale factor is an integer used as a divider.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a Keras model for MobileNet/ImageNet.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>keras.Model</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>ValueError</strong> – in case of invalid input shape.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.mobilenet_imagenet_pretrained">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">mobilenet_imagenet_pretrained</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/imagenet/model_mobilenet.html#mobilenet_imagenet_pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.mobilenet_imagenet_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Helper method to retrieve a <cite>mobilenet_imagenet</cite> model that was trained on
ImageNet dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>alpha</strong> (<em>float</em>) – width of the model.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a Keras Model instance.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.mobilenet_edge_imagenet">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">mobilenet_edge_imagenet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">base_model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">classes</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/imagenet/model_mobilenet_edge.html#mobilenet_edge_imagenet"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.mobilenet_edge_imagenet" title="Permalink to this definition"></a></dt>
<dd><p>Instantiates a MobileNet-edge architecture.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>base_model</strong> (<em>str/keras.Model</em>) – a mobilenet_imagenet quantized model.</p></li>
<li><p><strong>classes</strong> (<em>int</em>) – the number of classes for the edge classifier.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a Keras Model instance.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.mobilenet_edge_imagenet_pretrained">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">mobilenet_edge_imagenet_pretrained</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/imagenet/model_mobilenet_edge.html#mobilenet_edge_imagenet_pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.mobilenet_edge_imagenet_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Helper method to retrieve a <cite>mobilenet_edge_imagenet</cite> model that was
trained on ImageNet dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a Keras Model instance.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="ds-cnn">
<h3>DS-CNN<a class="headerlink" href="#ds-cnn" title="Permalink to this headline"></a></h3>
<section id="kws">
<h4>KWS<a class="headerlink" href="#kws" title="Permalink to this headline"></a></h4>
<dl class="py function">
<dt class="sig sig-object py" id="akida_models.ds_cnn_kws">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">ds_cnn_kws</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(49,</span> <span class="pre">10,</span> <span class="pre">1)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">classes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">33</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">include_top</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_quantization</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activ_quantization</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_weight_quantization</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_scaling</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(255,</span> <span class="pre">0)</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/kws/model_ds_cnn.html#ds_cnn_kws"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.ds_cnn_kws" title="Permalink to this definition"></a></dt>
<dd><p>Instantiates a MobileNet-like model for the “Keyword Spotting” example.</p>
<p>This model is based on the MobileNet architecture, mainly with fewer layers.
The weights and activations are quantized such that it can be converted into
an Akida model.</p>
<p>This architecture is originated from <a class="reference external" href="https://arxiv.org/pdf/1711.07128.pdf">https://arxiv.org/pdf/1711.07128.pdf</a>
and was created for the “Keyword Spotting” (KWS) or “Speech Commands”
dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_shape</strong> (<em>tuple</em>) – input shape tuple of the model</p></li>
<li><p><strong>classes</strong> (<em>int</em>) – optional number of classes to classify words into, only
be specified if <cite>include_top</cite> is True.</p></li>
<li><p><strong>include_top</strong> (<em>bool</em>) – whether to include the fully-connected
layer at the top of the model.</p></li>
<li><p><strong>weight_quantization</strong> (<em>int</em>) – <p>sets all weights in the model to have
a particular quantization bitwidth except for the weights in the
first layer.</p>
<ul>
<li><p>’0’ implements floating point 32-bit weights.</p></li>
<li><p>’2’ through ‘8’ implements n-bit weights where n is from 2-8 bits.</p></li>
</ul>
</p></li>
<li><p><strong>activ_quantization</strong> (<em>int</em>) – <p>sets all activations in the model to have a
particular activation quantization bitwidth.</p>
<ul>
<li><p>’0’ implements floating point 32-bit activations.</p></li>
<li><p>’1’ through ‘8’ implements n-bit weights where n is from 2-8 bits.</p></li>
</ul>
</p></li>
<li><p><strong>input_weight_quantization</strong> (<em>int</em>) – <p>sets weight quantization in the first
layer. Defaults to weight_quantization value.</p>
<ul>
<li><p>’None’ implements the same bitwidth as the other weights.</p></li>
<li><p>’0’ implements floating point 32-bit weights.</p></li>
<li><p>’2’ through ‘8’ implements n-bit weights where n is from 2-8 bits.</p></li>
</ul>
</p></li>
<li><p><strong>input_scaling</strong> (<em>tuple</em><em>, </em><em>optional</em>) – scale factor and offset to apply to
inputs. Defaults to (255, 0). Note that following Akida convention,
the scale factor is an integer used as a divider.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a Keras model for MobileNet/KWS</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.ds_cnn_kws_pretrained">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">ds_cnn_kws_pretrained</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/kws/model_ds_cnn.html#ds_cnn_kws_pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.ds_cnn_kws_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Helper method to retrieve a <cite>ds_cnn_kws</cite> model that was trained on
KWS dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a Keras Model instance.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<section id="id6">
<h5>Preprocessing<a class="headerlink" href="#id6" title="Permalink to this headline"></a></h5>
<dl class="py function">
<dt class="sig sig-object py" id="akida_models.kws.preprocessing.prepare_model_settings">
<span class="sig-prename descclassname"><span class="pre">akida_models.kws.preprocessing.</span></span><span class="sig-name descname"><span class="pre">prepare_model_settings</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_rate</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clip_duration_ms</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">window_size_ms</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">window_stride_ms</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_bin_count</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/kws/preprocessing.html#prepare_model_settings"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.kws.preprocessing.prepare_model_settings" title="Permalink to this definition"></a></dt>
<dd><p>Calculates common settings needed for all models.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sample_rate</strong> – Number of audio samples per second.</p></li>
<li><p><strong>clip_duration_ms</strong> – Length of each audio clip to be analyzed.</p></li>
<li><p><strong>window_size_ms</strong> – Duration of frequency analysis window.</p></li>
<li><p><strong>window_stride_ms</strong> – How far to move in time between frequency windows.</p></li>
<li><p><strong>feature_bin_count</strong> – Number of frequency bins to use for analysis.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Dictionary containing common settings.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>ValueError</strong> – If the preprocessing mode isn’t recognized.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.kws.preprocessing.prepare_words_list">
<span class="sig-prename descclassname"><span class="pre">akida_models.kws.preprocessing.</span></span><span class="sig-name descname"><span class="pre">prepare_words_list</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">wanted_words</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/kws/preprocessing.html#prepare_words_list"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.kws.preprocessing.prepare_words_list" title="Permalink to this definition"></a></dt>
<dd><p>Prepends common tokens to the custom word list.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>wanted_words</strong> – List of strings containing the custom words.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>List with the standard silence and unknown tokens added.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.kws.preprocessing.which_set">
<span class="sig-prename descclassname"><span class="pre">akida_models.kws.preprocessing.</span></span><span class="sig-name descname"><span class="pre">which_set</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">filename</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation_percentage</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">testing_percentage</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/kws/preprocessing.html#which_set"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.kws.preprocessing.which_set" title="Permalink to this definition"></a></dt>
<dd><p>Determines which data partition the file should belong to.</p>
<p>We want to keep files in the same training, validation, or testing sets even
if new ones are added over time. This makes it less likely that testing
samples will accidentally be reused in training when long runs are restarted
for example. To keep this stability, a hash of the filename is taken and used
to determine which set it should belong to. This determination only depends on
the name and the set proportions, so it won’t change as other files are added.</p>
<p>It’s also useful to associate particular files as related (for example words
spoken by the same person), so anything after ‘_nohash_’ in a filename is
ignored for set determination. This ensures that ‘bobby_nohash_0.wav’ and
‘bobby_nohash_1.wav’ are always in the same set, for example.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>filename</strong> – File path of the data sample.</p></li>
<li><p><strong>validation_percentage</strong> – How much of the data set to use for validation.</p></li>
<li><p><strong>testing_percentage</strong> – How much of the data set to use for testing.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>String, one of ‘training’, ‘validation’, or ‘testing’.</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="akida_models.kws.preprocessing.AudioProcessor">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">akida_models.kws.preprocessing.</span></span><span class="sig-name descname"><span class="pre">AudioProcessor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_rate</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clip_duration_ms</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">window_size_ms</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">window_stride_ms</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_bin_count</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data_url</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data_dir</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">silence_percentage</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">unknown_percentage</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">wanted_words</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation_percentage</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">testing_percentage</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/kws/preprocessing.html#AudioProcessor"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.kws.preprocessing.AudioProcessor" title="Permalink to this definition"></a></dt>
<dd><p>Handles loading, partitioning, and preparing audio training data.</p>
<p><strong>Methods:</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#akida_models.kws.preprocessing.AudioProcessor.get_augmented_data_for_wav" title="akida_models.kws.preprocessing.AudioProcessor.get_augmented_data_for_wav"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_augmented_data_for_wav</span></code></a>(wav_filename, ...)</p></td>
<td><p>Applies the feature transformation process to a wav audio file,</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#akida_models.kws.preprocessing.AudioProcessor.get_data" title="akida_models.kws.preprocessing.AudioProcessor.get_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_data</span></code></a>(how_many, offset, ...)</p></td>
<td><p>Gather samples from the data set, applying transformations as needed.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#akida_models.kws.preprocessing.AudioProcessor.get_features_for_wav" title="akida_models.kws.preprocessing.AudioProcessor.get_features_for_wav"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_features_for_wav</span></code></a>(wav_filename)</p></td>
<td><p>Applies the feature transformation process to the input_wav.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#akida_models.kws.preprocessing.AudioProcessor.maybe_download_and_extract_dataset" title="akida_models.kws.preprocessing.AudioProcessor.maybe_download_and_extract_dataset"><code class="xref py py-obj docutils literal notranslate"><span class="pre">maybe_download_and_extract_dataset</span></code></a>(data_url, ...)</p></td>
<td><p>Download and extract data set tar file.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#akida_models.kws.preprocessing.AudioProcessor.prepare_background_data" title="akida_models.kws.preprocessing.AudioProcessor.prepare_background_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">prepare_background_data</span></code></a>()</p></td>
<td><p>Searches a folder for background noise audio, and loads it into</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#akida_models.kws.preprocessing.AudioProcessor.prepare_data_index" title="akida_models.kws.preprocessing.AudioProcessor.prepare_data_index"><code class="xref py py-obj docutils literal notranslate"><span class="pre">prepare_data_index</span></code></a>(silence_percentage, ...)</p></td>
<td><p>Prepares a list of the samples organized by set and label.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#akida_models.kws.preprocessing.AudioProcessor.prepare_processing_graph" title="akida_models.kws.preprocessing.AudioProcessor.prepare_processing_graph"><code class="xref py py-obj docutils literal notranslate"><span class="pre">prepare_processing_graph</span></code></a>()</p></td>
<td><p>Builds a TensorFlow graph to apply the input distortions.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt class="sig sig-object py" id="akida_models.kws.preprocessing.AudioProcessor.get_augmented_data_for_wav">
<span class="sig-name descname"><span class="pre">get_augmented_data_for_wav</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">wav_filename</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">background_frequency</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">background_volume_range</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_shift</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_augmented_samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/kws/preprocessing.html#AudioProcessor.get_augmented_data_for_wav"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.kws.preprocessing.AudioProcessor.get_augmented_data_for_wav" title="Permalink to this definition"></a></dt>
<dd><dl class="simple">
<dt>Applies the feature transformation process to a wav audio file,</dt><dd><p>adding data augmentation (background noise and time shifting).</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>wav_filename</strong> (<em>str</em>) – The path to the input audio file.</p></li>
<li><p><strong>background_frequency</strong> – How many clips will have background noise, 0.0 to
1.0.</p></li>
<li><p><strong>background_volume_range</strong> – How loud the background noise will be.</p></li>
<li><p><strong>time_shift</strong> – How much to randomly shift the clips by in time.</p></li>
<li><p><strong>num_augmented_samples</strong> – How many samples will be generated using data
augmentation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>Numpy data array containing the generated features for every augmented</dt><dd><p>sample.</p>
</dd>
</dl>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="akida_models.kws.preprocessing.AudioProcessor.get_data">
<span class="sig-name descname"><span class="pre">get_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">how_many</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">offset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">background_frequency</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">background_volume_range</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_shift</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/kws/preprocessing.html#AudioProcessor.get_data"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.kws.preprocessing.AudioProcessor.get_data" title="Permalink to this definition"></a></dt>
<dd><p>Gather samples from the data set, applying transformations as needed.</p>
<p>When the mode is ‘training’, a random selection of samples will be returned,
otherwise the first N clips in the partition will be used. This ensures that
validation always uses the same samples, reducing noise in the metrics.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>how_many</strong> – Desired number of samples to return. -1 means the entire
contents of this partition.</p></li>
<li><p><strong>offset</strong> – Where to start when fetching deterministically.</p></li>
<li><p><strong>background_frequency</strong> – How many clips will have background noise, 0.0 to
1.0.</p></li>
<li><p><strong>background_volume_range</strong> – How loud the background noise will be.</p></li>
<li><p><strong>time_shift</strong> – How much to randomly shift the clips by in time.</p></li>
<li><p><strong>mode</strong> – Which partition to use, must be ‘training’, ‘validation’, or
‘testing’.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>List of sample data for the transformed samples, and list of label indexes</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>ValueError</strong> – If background samples are too short.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="akida_models.kws.preprocessing.AudioProcessor.get_features_for_wav">
<span class="sig-name descname"><span class="pre">get_features_for_wav</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">wav_filename</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/kws/preprocessing.html#AudioProcessor.get_features_for_wav"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.kws.preprocessing.AudioProcessor.get_features_for_wav" title="Permalink to this definition"></a></dt>
<dd><p>Applies the feature transformation process to the input_wav.</p>
<p>Runs the feature generation process (generally producing a spectrogram from
the input samples) on the WAV file. This can be useful for testing and
verifying implementations being run on other platforms.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>wav_filename</strong> – The path to the input audio file.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Numpy data array containing the generated features.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="akida_models.kws.preprocessing.AudioProcessor.maybe_download_and_extract_dataset">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">maybe_download_and_extract_dataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data_url</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dest_directory</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/kws/preprocessing.html#AudioProcessor.maybe_download_and_extract_dataset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.kws.preprocessing.AudioProcessor.maybe_download_and_extract_dataset" title="Permalink to this definition"></a></dt>
<dd><p>Download and extract data set tar file.</p>
<p>If the data set we’re using doesn’t already exist, this function
downloads it from the TensorFlow.org website and unpacks it into a
directory.
If the data_url is none, don’t download anything and expect the data
directory to contain the correct files already.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data_url</strong> – Web location of the tar file containing the data set.</p></li>
<li><p><strong>dest_directory</strong> – File path to extract data to.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="akida_models.kws.preprocessing.AudioProcessor.prepare_background_data">
<span class="sig-name descname"><span class="pre">prepare_background_data</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/kws/preprocessing.html#AudioProcessor.prepare_background_data"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.kws.preprocessing.AudioProcessor.prepare_background_data" title="Permalink to this definition"></a></dt>
<dd><dl class="simple">
<dt>Searches a folder for background noise audio, and loads it into</dt><dd><p>memory.</p>
</dd>
</dl>
<p>It’s expected that the background audio samples will be in a subdirectory
named ‘_background_noise_’ inside the ‘data_dir’ folder, as .wavs that match
the sample rate of the training data, but can be much longer in duration.</p>
<p>If the ‘_background_noise_’ folder doesn’t exist at all, this isn’t an
error, it’s just taken to mean that no background noise augmentation should
be used. If the folder does exist, but it’s empty, that’s treated as an
error.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>List of raw PCM-encoded audio samples of background noise.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>Exception</strong> – If files aren’t found in the folder.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="akida_models.kws.preprocessing.AudioProcessor.prepare_data_index">
<span class="sig-name descname"><span class="pre">prepare_data_index</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">silence_percentage</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">unknown_percentage</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">wanted_words</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation_percentage</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">testing_percentage</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/kws/preprocessing.html#AudioProcessor.prepare_data_index"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.kws.preprocessing.AudioProcessor.prepare_data_index" title="Permalink to this definition"></a></dt>
<dd><p>Prepares a list of the samples organized by set and label.</p>
<p>The training loop needs a list of all the available data, organized by
which partition it should belong to, and with ground truth labels attached.
This function analyzes the folders below the <cite>data_dir</cite>, figures out the
right
labels for each file based on the name of the subdirectory it belongs to,
and uses a stable hash to assign it to a data set partition.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>silence_percentage</strong> – How much of the resulting data should be background.</p></li>
<li><p><strong>unknown_percentage</strong> – How much should be audio outside the wanted classes.</p></li>
<li><p><strong>wanted_words</strong> – Labels of the classes we want to be able to recognize.</p></li>
<li><p><strong>validation_percentage</strong> – How much of the data set to use for validation.</p></li>
<li><p><strong>testing_percentage</strong> – How much of the data set to use for testing.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Dictionary containing a list of file information for each set partition,
and a lookup map for each class to determine its numeric index.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>Exception</strong> – If expected files are not found.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="akida_models.kws.preprocessing.AudioProcessor.prepare_processing_graph">
<span class="sig-name descname"><span class="pre">prepare_processing_graph</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/kws/preprocessing.html#AudioProcessor.prepare_processing_graph"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.kws.preprocessing.AudioProcessor.prepare_processing_graph" title="Permalink to this definition"></a></dt>
<dd><p>Builds a TensorFlow graph to apply the input distortions.</p>
<p>Creates a graph that loads a WAVE file, decodes it, scales the volume,
shifts it in time, adds in background noise, calculates a spectrogram, and
then builds an MFCC fingerprint from that.</p>
</dd></dl>

</dd></dl>

</section>
</section>
</section>
<section id="vgg">
<h3>VGG<a class="headerlink" href="#vgg" title="Permalink to this headline"></a></h3>
<section id="id7">
<h4>ImageNet<a class="headerlink" href="#id7" title="Permalink to this headline"></a></h4>
<dl class="py function">
<dt class="sig sig-object py" id="akida_models.vgg_imagenet">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">vgg_imagenet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(224,</span> <span class="pre">224,</span> <span class="pre">3)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">classes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">include_top</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pooling</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_quantization</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activ_quantization</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_weight_quantization</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_scaling</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(128,</span> <span class="pre">-</span> <span class="pre">1)</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/imagenet/model_vgg.html#vgg_imagenet"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.vgg_imagenet" title="Permalink to this definition"></a></dt>
<dd><p>Instantiates a VGG11 architecture with reduced number of filters in
convolutional layers (i.e. a quarter of the filters of the original
implementation of <a class="reference external" href="https://arxiv.org/pdf/1409.1556.pdf">https://arxiv.org/pdf/1409.1556.pdf</a>).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_shape</strong> (<em>tuple</em><em>, </em><em>optional</em>) – input shape tuple. Defaults to (224, 224,
3).</p></li>
<li><p><strong>classes</strong> (<em>int</em><em>, </em><em>optional</em>) – optional number of classes to classify images
into. Defaults to 1000.</p></li>
<li><p><strong>include_top</strong> (<em>bool</em><em>, </em><em>optional</em>) – whether to include the classification
layers at the top of the model. Defaults to True.</p></li>
<li><p><strong>pooling</strong> (<em>str</em><em>, </em><em>optional</em>) – <p>Optional pooling mode for feature extraction
when <cite>include_top</cite> is <cite>False</cite>. Defaults to None.</p>
<ul>
<li><p><cite>None</cite> means that the output of the model will be the 4D tensor
output of the last convolutional block.</p></li>
<li><p><cite>avg</cite> means that global average pooling will be applied to the
output of the last convolutional block, and thus the output of the
model will be a 2D tensor.</p></li>
</ul>
</p></li>
<li><p><strong>weight_quantization</strong> (<em>int</em><em>, </em><em>optional</em>) – <p>sets all weights in the model to
have a particular quantization bitwidth except for the weights in
the first layer. Defaults to 0.</p>
<ul>
<li><p>’0’ implements floating point 32-bit weights.</p></li>
<li><p>’2’ through ‘8’ implements n-bit weights where n is from 2-8 bits.</p></li>
</ul>
</p></li>
<li><p><strong>activ_quantization</strong> (<em>int</em><em>, </em><em>optional</em>) – <p>sets all activations in the model to
have a particular activation quantization bitwidth. Defaults to 0.</p>
<ul>
<li><p>’0’ implements floating point 32-bit activations.</p></li>
<li><p>’2’ through ‘8’ implements n-bit weights where n is from 2-8 bits.</p></li>
</ul>
</p></li>
<li><p><strong>input_weight_quantization</strong> (<em>int</em><em>, </em><em>optional</em>) – <p>sets weight quantization in
the first layer. Defaults to weight_quantization value.</p>
<ul>
<li><p>’0’ implements floating point 32-bit weights.</p></li>
<li><p>’2’ through ‘8’ implements n-bit weights where n is from 2-8 bits.</p></li>
</ul>
</p></li>
<li><p><strong>input_scaling</strong> (<em>tuple</em><em>, </em><em>optional</em>) – scale factor and offset to apply to
inputs. Defaults to (128, -1). Note that following Akida convention,
the scale factor is an integer used as a divider.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a Keras model for VGG/ImageNet</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.vgg_imagenet_pretrained">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">vgg_imagenet_pretrained</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/imagenet/model_vgg.html#vgg_imagenet_pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.vgg_imagenet_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Helper method to retrieve a <cite>vgg_imagenet</cite> model that was trained on
ImageNet dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a Keras Model instance.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="utk-face">
<h4>UTK Face<a class="headerlink" href="#utk-face" title="Permalink to this headline"></a></h4>
<dl class="py function">
<dt class="sig sig-object py" id="akida_models.vgg_utk_face">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">vgg_utk_face</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(32,</span> <span class="pre">32,</span> <span class="pre">3)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_quantization</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activ_quantization</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_weight_quantization</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_scaling</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(127,</span> <span class="pre">-</span> <span class="pre">1)</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/utk_face/model_vgg.html#vgg_utk_face"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.vgg_utk_face" title="Permalink to this definition"></a></dt>
<dd><p>Instantiates a VGG-like model for the regression example on age
estimation using UTKFace dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_shape</strong> (<em>tuple</em>) – input shape tuple of the model</p></li>
<li><p><strong>weight_quantization</strong> (<em>int</em>) – <p>sets all weights in the model to have
a particular quantization bitwidth except for the weights in the
first layer.</p>
<ul>
<li><p>’0’ implements floating point 32-bit weights.</p></li>
<li><p>’2’ through ‘8’ implements n-bit weights where n is from 2-8 bits.</p></li>
</ul>
</p></li>
<li><p><strong>activ_quantization</strong> (<em>int</em>) – <p>sets all activations in the model to have a
particular activation quantization bitwidth.</p>
<ul>
<li><p>’0’ implements floating point 32-bit activations.</p></li>
<li><p>’1’ through ‘8’ implements n-bit weights where n is from 1-8 bits.</p></li>
</ul>
</p></li>
<li><p><strong>input_weight_quantization</strong> (<em>int</em>) – <p>sets weight quantization in the first
layer. Defaults to weight_quantization value.</p>
<ul>
<li><p>’None’ implements the same bitwidth as the other weights.</p></li>
<li><p>’0’ implements floating point 32-bit weights.</p></li>
<li><p>’2’ through ‘8’ implements n-bit weights where n is from 2-8 bits.</p></li>
</ul>
</p></li>
<li><p><strong>input_scaling</strong> (<em>tuple</em><em>, </em><em>optional</em>) – scale factor and offset to apply to
inputs. Defaults to (127, -1). Note that following Akida convention,
the scale factor is an integer used as a divider.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a Keras model for VGG/UTKFace</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.vgg_utk_face_pretrained">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">vgg_utk_face_pretrained</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/utk_face/model_vgg.html#vgg_utk_face_pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.vgg_utk_face_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Helper method to retrieve a <cite>vgg_utk_face</cite> model that was trained on
UTK Face dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a Keras Model instance.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<section id="id8">
<h5>Preprocessing<a class="headerlink" href="#id8" title="Permalink to this headline"></a></h5>
<dl class="py function">
<dt class="sig sig-object py" id="akida_models.utk_face.preprocessing.load_data">
<span class="sig-prename descclassname"><span class="pre">akida_models.utk_face.preprocessing.</span></span><span class="sig-name descname"><span class="pre">load_data</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/utk_face/preprocessing.html#load_data"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.utk_face.preprocessing.load_data" title="Permalink to this definition"></a></dt>
<dd><p>Loads the dataset from Brainchip data server.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>train set, train labels, test
set and test labels as numpy arrays</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>np.array, np.array, np.array, np.array</p>
</dd>
</dl>
</dd></dl>

</section>
</section>
</section>
<section id="yolo">
<h3>YOLO<a class="headerlink" href="#yolo" title="Permalink to this headline"></a></h3>
<dl class="py function">
<dt class="sig sig-object py" id="akida_models.yolo_base">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">yolo_base</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(224,</span> <span class="pre">224,</span> <span class="pre">3)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">classes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nb_box</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_quantization</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activ_quantization</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_weight_quantization</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_scaling</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(127.5,</span> <span class="pre">-</span> <span class="pre">1)</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/detection/model_yolo.html#yolo_base"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.yolo_base" title="Permalink to this definition"></a></dt>
<dd><p>Instantiates the YOLOv2 architecture.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_shape</strong> (<em>tuple</em>) – input shape tuple</p></li>
<li><p><strong>classes</strong> (<em>int</em>) – number of classes to classify images into</p></li>
<li><p><strong>nb_box</strong> (<em>int</em>) – number of anchors boxes to use</p></li>
<li><p><strong>alpha</strong> (<em>float</em>) – controls the width of the model</p></li>
<li><p><strong>weight_quantization</strong> (<em>int</em>) – <p>sets all weights in the model to have
a particular quantization bitwidth except for the weights in the
first layer.</p>
<ul>
<li><p>’0’ implements floating point 32-bit weights.</p></li>
<li><p>’2’ through ‘8’ implements n-bit weights where n is from 2-8 bits.</p></li>
</ul>
</p></li>
<li><p><strong>activ_quantization</strong> – <p>sets all activations in the model to have a
particular activation quantization bitwidth.</p>
<ul>
<li><p>’0’ implements floating point 32-bit activations.</p></li>
<li><p>’2’ through ‘8’ implements n-bit weights where n is from 2-8 bits.</p></li>
</ul>
</p></li>
<li><p><strong>input_weight_quantization</strong> – <p>sets weight quantization in the first layer.
Defaults to weight_quantization value.</p>
<ul>
<li><p>’0’ implements floating point 32-bit weights.</p></li>
<li><p>’2’ through ‘8’ implements n-bit weights where n is from 2-8 bits.</p></li>
</ul>
</p></li>
<li><p><strong>input_scaling</strong> (<em>tuple</em><em>, </em><em>optional</em>) – scale factor and offset to apply to
inputs. Defaults to (127.5, -1). Note that following Akida
convention, the scale factor is a number used as a divider.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a Keras Model instance.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.yolo_widerface_pretrained">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">yolo_widerface_pretrained</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/detection/model_yolo.html#yolo_widerface_pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.yolo_widerface_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Helper method to retrieve a <cite>yolo_base</cite> model that was trained on WiderFace
dataset and the anchors that are needed to interpet the model output.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a Keras Model instance and a list of anchors.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>keras.Model, list</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.yolo_voc_pretrained">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">yolo_voc_pretrained</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/detection/model_yolo.html#yolo_voc_pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.yolo_voc_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Helper method to retrieve a <cite>yolo_base</cite> model that was trained on PASCAL
VOC2012 dataset for ‘person’ and ‘car’ classes only, and the anchors that
are needed to interpet the model output.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a Keras Model instance and a list of anchors.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>keras.Model, list</p>
</dd>
</dl>
</dd></dl>

<section id="yolo-toolkit">
<h4>YOLO Toolkit<a class="headerlink" href="#yolo-toolkit" title="Permalink to this headline"></a></h4>
<section id="processing">
<h5>Processing<a class="headerlink" href="#processing" title="Permalink to this headline"></a></h5>
<dl class="py function">
<dt class="sig sig-object py" id="akida_models.detection.processing.load_image">
<span class="sig-prename descclassname"><span class="pre">akida_models.detection.processing.</span></span><span class="sig-name descname"><span class="pre">load_image</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image_path</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/detection/processing.html#load_image"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.detection.processing.load_image" title="Permalink to this definition"></a></dt>
<dd><p>Loads an image from a path.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>image_path</strong> (<em>string</em>) – full path of the image to load</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a Tensorflow image Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.detection.processing.preprocess_image">
<span class="sig-prename descclassname"><span class="pre">akida_models.detection.processing.</span></span><span class="sig-name descname"><span class="pre">preprocess_image</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">image_buffer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/detection/processing.html#preprocess_image"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.detection.processing.preprocess_image" title="Permalink to this definition"></a></dt>
<dd><p>Preprocess an image for YOLO inference.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>image_buffer</strong> (<em>tf.Tensor</em>) – image to preprocess</p></li>
<li><p><strong>output_size</strong> (<em>tuple</em>) – shape of the image after preprocessing</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A resized and normalized image as a Numpy array.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.detection.processing.decode_output">
<span class="sig-prename descclassname"><span class="pre">akida_models.detection.processing.</span></span><span class="sig-name descname"><span class="pre">decode_output</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">anchors</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nb_classes</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">obj_threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nms_threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/detection/processing.html#decode_output"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.detection.processing.decode_output" title="Permalink to this definition"></a></dt>
<dd><p>Decodes a YOLO model output.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output</strong> (<em>tf.Tensor</em>) – model output to decode</p></li>
<li><p><strong>anchors</strong> (<em>list</em>) – list of anchors boxes</p></li>
<li><p><strong>nb_classes</strong> (<em>int</em>) – number of classes</p></li>
<li><p><strong>obj_threshold</strong> (<em>float</em>) – confidence threshold for a box</p></li>
<li><p><strong>nms_threshold</strong> (<em>float</em>) – non-maximal supression threshold</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>List of <cite>BoundingBox</cite> objects</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.detection.processing.parse_voc_annotations">
<span class="sig-prename descclassname"><span class="pre">akida_models.detection.processing.</span></span><span class="sig-name descname"><span class="pre">parse_voc_annotations</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">gt_folder</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">image_folder</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">file_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/detection/processing.html#parse_voc_annotations"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.detection.processing.parse_voc_annotations" title="Permalink to this definition"></a></dt>
<dd><p>Loads PASCAL-VOC data.</p>
<p>Data is loaded using the groundtruth informations and stored in a
dictionary.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>gt_folder</strong> (<em>str</em>) – path to the folder containing ground truth files</p></li>
<li><p><strong>image_folder</strong> (<em>str</em>) – path to the folder containing the images</p></li>
<li><p><strong>file_path</strong> (<em>str</em>) – file containing the list of files to parse</p></li>
<li><p><strong>labels</strong> (<em>list</em>) – list of labels of interest</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a dictionnary containing all data present in the ground truth file</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.detection.processing.parse_widerface_annotations">
<span class="sig-prename descclassname"><span class="pre">akida_models.detection.processing.</span></span><span class="sig-name descname"><span class="pre">parse_widerface_annotations</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">gt_file</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">image_folder</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/detection/processing.html#parse_widerface_annotations"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.detection.processing.parse_widerface_annotations" title="Permalink to this definition"></a></dt>
<dd><p>Loads WiderFace data.</p>
<p>Data is loaded using the groundtruth informations and stored in a
dictionary.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>gt_file</strong> (<em>str</em>) – path to the ground truth file</p></li>
<li><p><strong>image_folder</strong> (<em>str</em>) – path to the directory containing the images</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a dictionnary containing all data present in the ground truth file</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="akida_models.detection.processing.BoundingBox">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">akida_models.detection.processing.</span></span><span class="sig-name descname"><span class="pre">BoundingBox</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">score</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">classes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/detection/processing.html#BoundingBox"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.detection.processing.BoundingBox" title="Permalink to this definition"></a></dt>
<dd><p>Utility class to represent a bounding box.</p>
<p>The box is defined by its top left corner (x1, y1), bottom right corner
(x2, y2), label, score and classes.</p>
<p><strong>Methods:</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#akida_models.detection.processing.BoundingBox.get_label" title="akida_models.detection.processing.BoundingBox.get_label"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_label</span></code></a>()</p></td>
<td><p>Returns the label for this bounding box.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#akida_models.detection.processing.BoundingBox.get_score" title="akida_models.detection.processing.BoundingBox.get_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_score</span></code></a>()</p></td>
<td><p>Returns the score for this bounding box.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#akida_models.detection.processing.BoundingBox.iou" title="akida_models.detection.processing.BoundingBox.iou"><code class="xref py py-obj docutils literal notranslate"><span class="pre">iou</span></code></a>(other)</p></td>
<td><p>Computes intersection over union ratio between this bounding box and another one.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt class="sig sig-object py" id="akida_models.detection.processing.BoundingBox.get_label">
<span class="sig-name descname"><span class="pre">get_label</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/detection/processing.html#BoundingBox.get_label"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.detection.processing.BoundingBox.get_label" title="Permalink to this definition"></a></dt>
<dd><p>Returns the label for this bounding box.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Index of the label as an integer.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="akida_models.detection.processing.BoundingBox.get_score">
<span class="sig-name descname"><span class="pre">get_score</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/detection/processing.html#BoundingBox.get_score"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.detection.processing.BoundingBox.get_score" title="Permalink to this definition"></a></dt>
<dd><p>Returns the score for this bounding box.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Confidence as a float.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="akida_models.detection.processing.BoundingBox.iou">
<span class="sig-name descname"><span class="pre">iou</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/detection/processing.html#BoundingBox.iou"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.detection.processing.BoundingBox.iou" title="Permalink to this definition"></a></dt>
<dd><p>Computes intersection over union ratio between this bounding box and
another one.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>other</strong> (<a class="reference internal" href="#akida_models.detection.processing.BoundingBox" title="akida_models.detection.processing.BoundingBox"><em>BoundingBox</em></a>) – the other bounding box for IOU computation</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>IOU value as a float</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="performances">
<h5>Performances<a class="headerlink" href="#performances" title="Permalink to this headline"></a></h5>
<dl class="py class">
<dt class="sig sig-object py" id="akida_models.detection.map_evaluation.MapEvaluation">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">akida_models.detection.map_evaluation.</span></span><span class="sig-name descname"><span class="pre">MapEvaluation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">val_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">anchors</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">period</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">obj_threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nms_threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_box_per_image</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">is_keras_model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/detection/map_evaluation.html#MapEvaluation"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.detection.map_evaluation.MapEvaluation" title="Permalink to this definition"></a></dt>
<dd><p>Evaluate a given dataset using a given model.
Code originally from <a class="reference external" href="https://github.com/fizyr/keras-retinanet">https://github.com/fizyr/keras-retinanet</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>keras.Model</em>) – model to evaluate.</p></li>
<li><p><strong>val_data</strong> (<em>dict</em>) – dictionary containing validation data as obtained
using <cite>preprocess_widerface.py</cite> module</p></li>
<li><p><strong>labels</strong> (<em>list</em>) – list of labels as strings</p></li>
<li><p><strong>anchors</strong> (<em>list</em>) – list of anchors boxes</p></li>
<li><p><strong>period</strong> (<em>int</em><em>, </em><em>optional</em>) – periodicity the precision is printed,
defaults to once per epoch.</p></li>
<li><p><strong>obj_threshold</strong> (<em>float</em><em>, </em><em>optional</em>) – confidence threshold for a box</p></li>
<li><p><strong>nms_threshold</strong> (<em>float</em><em>, </em><em>optional</em>) – non-maximal supression threshold</p></li>
<li><p><strong>max_box_per_image</strong> (<em>int</em><em>, </em><em>optional</em>) – maximum number of detections per
image</p></li>
<li><p><strong>is_keras_model</strong> (<em>bool</em><em>, </em><em>optional</em>) – indicated if the model is a Keras
model (True) or an Akida model (False)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A dict mapping class names to mAP scores.</p>
</dd>
</dl>
<p><strong>Methods:</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#akida_models.detection.map_evaluation.MapEvaluation.evaluate_map" title="akida_models.detection.map_evaluation.MapEvaluation.evaluate_map"><code class="xref py py-obj docutils literal notranslate"><span class="pre">evaluate_map</span></code></a>()</p></td>
<td><p>Evaluates current mAP score on the model.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#akida_models.detection.map_evaluation.MapEvaluation.on_epoch_end" title="akida_models.detection.map_evaluation.MapEvaluation.on_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_epoch_end</span></code></a>(epoch[, logs])</p></td>
<td><p>Keras callback called at the end of an epoch.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt class="sig sig-object py" id="akida_models.detection.map_evaluation.MapEvaluation.evaluate_map">
<span class="sig-name descname"><span class="pre">evaluate_map</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/detection/map_evaluation.html#MapEvaluation.evaluate_map"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.detection.map_evaluation.MapEvaluation.evaluate_map" title="Permalink to this definition"></a></dt>
<dd><p>Evaluates current mAP score on the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>global mAP score and dictionnary of label and mAP for each
class.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>tuple</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="akida_models.detection.map_evaluation.MapEvaluation.on_epoch_end">
<span class="sig-name descname"><span class="pre">on_epoch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">epoch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/detection/map_evaluation.html#MapEvaluation.on_epoch_end"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.detection.map_evaluation.MapEvaluation.on_epoch_end" title="Permalink to this definition"></a></dt>
<dd><p>Keras callback called at the end of an epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>epoch</strong> (<em>int</em>) – index of epoch.</p></li>
<li><p><strong>logs</strong> (<em>dict</em><em>, </em><em>optional</em>) – metric results for this training epoch, and
for the validation epoch if validation is performed. Validation
result keys are prefixed with val. For training epoch, the
values of the Model’s metrics are returned.
Example: {‘loss’: 0.2, ‘acc’: 0.7}. Defaults to None.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="anchors">
<h5>Anchors<a class="headerlink" href="#anchors" title="Permalink to this headline"></a></h5>
<dl class="py function">
<dt class="sig sig-object py" id="akida_models.detection.generate_anchors.generate_anchors">
<span class="sig-prename descclassname"><span class="pre">akida_models.detection.generate_anchors.</span></span><span class="sig-name descname"><span class="pre">generate_anchors</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">annotations_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_anchors</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grid_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(7,</span> <span class="pre">7)</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/detection/generate_anchors.html#generate_anchors"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.detection.generate_anchors.generate_anchors" title="Permalink to this definition"></a></dt>
<dd><p>Creates anchors by clustering dimensions of the ground truth boxes
from the training dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>annotations_data</strong> (<em>dict</em>) – dictionnary of preprocessed VOC data</p></li>
<li><p><strong>num_anchors</strong> (<em>int</em><em>, </em><em>optional</em>) – number of anchors</p></li>
<li><p><strong>grid_size</strong> (<em>tuple</em><em>, </em><em>optional</em>) – size of the YOLO grid</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the computed anchors</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

</section>
</section>
</section>
<section id="convtiny">
<h3>ConvTiny<a class="headerlink" href="#convtiny" title="Permalink to this headline"></a></h3>
<section id="cwru">
<h4>CWRU<a class="headerlink" href="#cwru" title="Permalink to this headline"></a></h4>
<dl class="py function">
<dt class="sig sig-object py" id="akida_models.convtiny_cwru">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">convtiny_cwru</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/cwru/model_convtiny.html#convtiny_cwru"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.convtiny_cwru" title="Permalink to this definition"></a></dt>
<dd><p>Instantiates a CNN for CWRU classification with input shape (32, 32, 1)
and 10 classes.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a Keras model for Convtiny/CWRU</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.convtiny_cwru_pretrained">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">convtiny_cwru_pretrained</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/cwru/model_convtiny.html#convtiny_cwru_pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.convtiny_cwru_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Helper method to retrieve a <cite>convtiny_cwru</cite> model that was trained on
CWRU dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a Keras Model instance.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

</section>
</section>
<section id="pointnet">
<h3>PointNet++<a class="headerlink" href="#pointnet" title="Permalink to this headline"></a></h3>
<section id="modelnet40">
<h4>ModelNet40<a class="headerlink" href="#modelnet40" title="Permalink to this headline"></a></h4>
<dl class="py function">
<dt class="sig sig-object py" id="akida_models.pointnet_plus_modelnet40">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">pointnet_plus_modelnet40</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">selected_points</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">128</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">knn_points</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">classes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">40</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_quantization</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activ_quantization</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/modelnet40/model_pointnet_plus.html#pointnet_plus_modelnet40"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.pointnet_plus_modelnet40" title="Permalink to this definition"></a></dt>
<dd><p>Instantiates a PointNet++ model for the ModelNet40 classification.</p>
<p>This example implements the point cloud deep learning paper
<a class="reference external" href="https://arxiv.org/abs/1612.00593">PointNet (Qi et al., 2017)</a>. For a
detailed introduction on PointNet see <a class="reference external" href="https://medium.com/&#64;luis_gonzales/an-in-depth-look-at-pointnet-111d7efdaa1a">this blog post</a>.</p>
<p>PointNet++ is conceived as a repeated series of operations: sampling and
grouping of points, followed by the trainable convnet itself. Those
operations are then repeated at increased scale.
Each of the selected points is taken as the centroid of the K-nearest
neighbours. This defines a localized group.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>selected_points</strong> (<em>int</em><em>, </em><em>optional</em>) – the number of points to process per
sample. Default is 128.</p></li>
<li><p><strong>features</strong> (<em>int</em><em>, </em><em>optional</em>) – the number of features. Expected values are
1 or 3. Default is 3.</p></li>
<li><p><strong>knn_points</strong> (<em>int</em><em>, </em><em>optional</em>) – the number of points to include in each
localised group. Must be a power of 2, and ideally an integer square
(so 64, or 16 for a deliberately small network, or 256 for large).
Default is 64.</p></li>
<li><p><strong>classes</strong> (<em>int</em><em>, </em><em>optional</em>) – the number of classes for the classifier.
Default is 40.</p></li>
<li><p><strong>alpha</strong> (<em>float</em><em>, </em><em>optional</em>) – network filters multiplier. Default is 1.0.</p></li>
<li><p><strong>weight_quantization</strong> (<em>int</em><em>, </em><em>optional</em>) – <p>sets all weights in the model to
have a particular quantization bitwidth except for the weights in
the first layer. Defaults to 0.</p>
<ul>
<li><p>’0’ implements floating point 32-bit weights.</p></li>
<li><p>’2’ through ‘8’ implements n-bit weights where n is from 2-8 bits.</p></li>
</ul>
</p></li>
<li><p><strong>activ_quantization</strong> (<em>int</em><em>, </em><em>optional</em>) – <p>sets all activations in the model to
have a particular activation quantization bitwidth. Defaults to 0.</p>
<ul>
<li><p>’0’ implements floating point 32-bit activations.</p></li>
<li><p>’2’ through ‘8’ implements n-bit weights where n is from 2-8 bits.</p></li>
</ul>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a quantized Keras model for PointNet++/ModelNet40.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.pointnet_plus_modelnet40_pretrained">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">pointnet_plus_modelnet40_pretrained</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/modelnet40/model_pointnet_plus.html#pointnet_plus_modelnet40_pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.pointnet_plus_modelnet40_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Helper method to retrieve a <cite>pointnet_plus</cite> model that was trained on
ModelNet40 dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a Keras Model instance.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<section id="id9">
<h5>Processing<a class="headerlink" href="#id9" title="Permalink to this headline"></a></h5>
<dl class="py function">
<dt class="sig sig-object py" id="akida_models.modelnet40.preprocessing.get_modelnet_from_file">
<span class="sig-prename descclassname"><span class="pre">akida_models.modelnet40.preprocessing.</span></span><span class="sig-name descname"><span class="pre">get_modelnet_from_file</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_points</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filename</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ModelNet40.zip'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/modelnet40/preprocessing.html#get_modelnet_from_file"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.modelnet40.preprocessing.get_modelnet_from_file" title="Permalink to this definition"></a></dt>
<dd><p>Load the ModelNet data from file.</p>
<p>First parse through the ModelNet data folders. Each mesh is loaded and
sampled into a point cloud before being added to a standard python list and
converted to a <cite>numpy</cite> array. We also store the current enumerate index
value as the object label and use a dictionary to recall this later.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_points</strong> (<em>int</em>) – number of points with which mesh is sample.</p></li>
<li><p><strong>filename</strong> (<em>str</em>) – the dataset file to load if the npz file was not
generated yet. Defaults to “ModelNet40.zip”.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>train set, train labels,</dt><dd><p>test set, test labels as numpy arrays and dict containing class
folder name.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>np.array, np.array, np.array, np.array, dict</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.modelnet40.preprocessing.get_modelnet">
<span class="sig-prename descclassname"><span class="pre">akida_models.modelnet40.preprocessing.</span></span><span class="sig-name descname"><span class="pre">get_modelnet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_points</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_labels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_points</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_labels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">selected_points</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">128</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">knn_points</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/modelnet40/preprocessing.html#get_modelnet"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.modelnet40.preprocessing.get_modelnet" title="Permalink to this definition"></a></dt>
<dd><p>Obtains the ModelNet dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>train_points</strong> (<em>numpy.array</em>) – train set.</p></li>
<li><p><strong>train_labels</strong> (<em>numpy.array</em>) – train labels.</p></li>
<li><p><strong>test_points</strong> (<em>numpy.array</em>) – test set.</p></li>
<li><p><strong>test_labels</strong> (<em>numpy.array</em>) – test labels.</p></li>
<li><p><strong>batch_size</strong> (<em>int</em>) – size of the batch.</p></li>
<li><p><strong>selected_points</strong> (<em>int</em>) – num points to process per sample. Default is 512.</p></li>
<li><p><strong>knn_points</strong> (<em>int</em>) – number of points to include in each localised group.
Must be a power of 2, and ideally an integer square (so 64, or 16
for a deliberately small network, or 256 for large).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>train and test point with data</dt><dd><p>augmentation.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.data.Dataset, tf.data.Dataset</p>
</dd>
</dl>
</dd></dl>

</section>
</section>
</section>
<section id="gxnor">
<h3>GXNOR<a class="headerlink" href="#gxnor" title="Permalink to this headline"></a></h3>
<section id="mnist">
<h4>MNIST<a class="headerlink" href="#mnist" title="Permalink to this headline"></a></h4>
<dl class="py function">
<dt class="sig sig-object py" id="akida_models.gxnor_mnist">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">gxnor_mnist</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">weight_quantization</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activ_quantization</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_weight_quantization</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/mnist/model_gxnor.html#gxnor_mnist"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.gxnor_mnist" title="Permalink to this definition"></a></dt>
<dd><p>Instantiates a Keras GXNOR model with an additional dense layer to make
better classification.</p>
<p>The paper describing the original model can be found <a class="reference external" href="https://www.sciencedirect.com/science/article/pii/S0893608018300108">here</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>weight_quantization</strong> (<em>int</em><em>, </em><em>optional</em>) – <p>sets all weights in the model to
have a particular quantization bitwidth except for the weights in
the first layer. Defaults to 0.</p>
<ul>
<li><p>’0’ implements floating point 32-bit weights.</p></li>
<li><p>’2’ through ‘8’ implements n-bit weights where n is from 2-8 bits.</p></li>
</ul>
</p></li>
<li><p><strong>activ_quantization</strong> (<em>int</em><em>, </em><em>optional</em>) – <p>sets all activations in the model to
have a particular activation quantization bitwidth. Defaults to 0.</p>
<ul>
<li><p>’0’ implements floating point 32-bit activations.</p></li>
<li><p>’2’ through ‘8’ implements n-bit weights where n is from 2-8 bits.</p></li>
</ul>
</p></li>
<li><p><strong>input_weight_quantization</strong> (<em>int</em><em>, </em><em>optional</em>) – <p>sets weight quantization in
the first layer. Defaults to weight_quantization value.</p>
<ul>
<li><p>’0’ implements floating point 32-bit weights.</p></li>
<li><p>’2’ through ‘8’ implements n-bit weights where n is from 2-8 bits.</p></li>
</ul>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a Keras model for GXNOR/MNIST</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="akida_models.gxnor_mnist_pretrained">
<span class="sig-prename descclassname"><span class="pre">akida_models.</span></span><span class="sig-name descname"><span class="pre">gxnor_mnist_pretrained</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/akida_models/mnist/model_gxnor.html#gxnor_mnist_pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#akida_models.gxnor_mnist_pretrained" title="Permalink to this definition"></a></dt>
<dd><p>Helper method to retrieve a <cite>gxnor_mnist</cite> model that was trained on
MNIST dataset.</p>
<p>This model was trained with the distillation knowledge method, using
the EfficientNet model from <a class="reference external" href="https://github.com/EscVM/Efficient-CapsNet">this repository</a> and the <cite>Distiller</cite> class from
the knowledge distillation toolkit (<cite>akida_models.distiller</cite>).</p>
<p>The float training was done for 30 epochs with a learning rate of 1e-4
After that we gradually quantize the model from:
8-4-4 –&gt; 4-4-4 –&gt; 4-4-2 –&gt; 2-2-2 –&gt; 2-2-1 tuning the model at each step
with the same distillation training method for 5 epochs and a learning rate
of 5e-5.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a Keras Model instance.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>keras.Model</p>
</dd>
</dl>
</dd></dl>

</section>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="cnn2snn_apis.html" class="btn btn-neutral float-left" title="CNN2SNN Toolkit API" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../examples/index.html" class="btn btn-neutral float-right" title="Akida examples" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, BrainChip Holdings Ltd. All Rights Reserved.</p>
  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>