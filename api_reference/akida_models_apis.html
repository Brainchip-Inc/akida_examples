

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Akida models API &mdash; Akida Examples  documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery-dataframe.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Akida examples" href="../examples/index.html" />
    <link rel="prev" title="CNN2SNN Toolkit API" href="cnn2snn_apis.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #3f51b5" >
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/akida.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                Akida 1.8.8
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../index.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../installation.html#requirements">Requirements</a></li>
<li class="toctree-l2"><a class="reference internal" href="../installation.html#quick-installation">Quick installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../installation.html#running-examples">Running examples</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../user_guide/user_guide.html">User guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../user_guide/getting_started.html">Getting started</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/getting_started.html#for-beginners">For beginners</a></li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/getting_started.html#for-users-familiar-with-deep-learning">For users familiar with deep-learning</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../user_guide/aee.html">Akida user guide</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/aee.html#the-akida-execution-engine">The Akida Execution Engine</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#id1">1. The Spiking Neural Network model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#id2">2. Input data format</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#id3">3. Determine training mode</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#id4">4. Interpreting outputs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/aee.html#neural-network-model">Neural Network model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#specifying-the-neural-network-model">Specifying the Neural Network model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#saving-and-loading">Saving and loading</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#input-layer-types">Input layer types</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#data-processing-layer-types">Data-Processing layer types</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/aee.html#id5">Using Akida Unsupervised Learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#learning-constraints">Learning constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#compiling-a-layer">Compiling a layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/aee.html#id6">Learning parameters</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../user_guide/cnn2snn.html">CNN2SNN toolkit</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/cnn2snn.html#overview">Overview</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#conversion-workflow">Conversion workflow</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#typical-training-scenario">Typical training scenario</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#design-compatibility-constraints">Design compatibility constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#quantization-compatibility-constraints">Quantization compatibility constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#command-line-interface">Command-line interface</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/cnn2snn.html#layers-considerations">Layers Considerations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#supported-layer-types">Supported layer types</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#cnn2snn-quantization-aware-layers">CNN2SNN Quantization-aware layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#training-only-layers">Training-Only Layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#first-layers">First Layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/cnn2snn.html#id6">Final Layers</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/cnn2snn.html#tips-and-tricks">Tips and Tricks</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../user_guide/akida_models.html">Akida models zoo</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/akida_models.html#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/akida_models.html#command-line-interface-for-model-creation">Command-line interface for model creation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/akida_models.html#command-line-interface-for-model-training">Command-line interface for model training</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida_models.html#cifar10-training-and-tuning">CIFAR10 training and tuning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida_models.html#utk-face-training">UTK Face training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida_models.html#kws-training">KWS training</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/akida_models.html#id1">Layer Blocks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida_models.html#conv-block"><code class="docutils literal notranslate"><span class="pre">conv_block</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida_models.html#dense-block"><code class="docutils literal notranslate"><span class="pre">dense_block</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/akida_models.html#separable-conv-block"><code class="docutils literal notranslate"><span class="pre">separable_conv_block</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../user_guide/hw_constraints.html">Hardware constraints</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/hw_constraints.html#input-layer">Input layer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../user_guide/hw_constraints.html#data-processing-layers">Data-Processing layers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/hw_constraints.html#convolutional-layer">Convolutional layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../user_guide/hw_constraints.html#fully-connected-layer">Fully connected layer</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="api_reference.html">API reference</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="aee_apis.html">Akida Execution Engine</a><ul>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#model">Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#layer">Layer</a></li>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#layerstatistics">LayerStatistics</a></li>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#observer">Observer</a></li>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#inputdata">InputData</a></li>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#inputconvolutional">InputConvolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#fullyconnected">FullyConnected</a></li>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#convolutional">Convolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#separableconvolutional">SeparableConvolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#dense">Dense</a></li>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#sparse">Sparse</a></li>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#coords-to-sparse">coords_to_sparse</a></li>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#dense-to-sparse">dense_to_sparse</a></li>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#packetize">packetize</a></li>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#backend">Backend</a></li>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#convolutionmode">ConvolutionMode</a></li>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#poolingtype">PoolingType</a></li>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#learningtype">LearningType</a></li>
<li class="toctree-l3"><a class="reference internal" href="aee_apis.html#compatibility">Compatibility</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="cnn2snn_apis.html">CNN2SNN</a><ul>
<li class="toctree-l3"><a class="reference internal" href="cnn2snn_apis.html#quantize">quantize</a></li>
<li class="toctree-l3"><a class="reference internal" href="cnn2snn_apis.html#convert">convert</a></li>
<li class="toctree-l3"><a class="reference internal" href="cnn2snn_apis.html#check-model-compatibility">check_model_compatibility</a></li>
<li class="toctree-l3"><a class="reference internal" href="cnn2snn_apis.html#weightquantizer">WeightQuantizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="cnn2snn_apis.html#trainableweightquantizer">TrainableWeightQuantizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="cnn2snn_apis.html#maxquantizer">MaxQuantizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="cnn2snn_apis.html#maxperaxisquantizer">MaxPerAxisQuantizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="cnn2snn_apis.html#weightfloat">WeightFloat</a></li>
<li class="toctree-l3"><a class="reference internal" href="cnn2snn_apis.html#quantizedconv2d">QuantizedConv2D</a></li>
<li class="toctree-l3"><a class="reference internal" href="cnn2snn_apis.html#quantizeddepthwiseconv2d">QuantizedDepthwiseConv2D</a></li>
<li class="toctree-l3"><a class="reference internal" href="cnn2snn_apis.html#quantizeddense">QuantizedDense</a></li>
<li class="toctree-l3"><a class="reference internal" href="cnn2snn_apis.html#quantizedseparableconv2d">QuantizedSeparableConv2D</a></li>
<li class="toctree-l3"><a class="reference internal" href="cnn2snn_apis.html#activationdiscreterelu">ActivationDiscreteRelu</a></li>
<li class="toctree-l3"><a class="reference internal" href="cnn2snn_apis.html#quantizedrelu">QuantizedReLU</a></li>
</ul>
</li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Akida models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#quantization-blocks">Quantization blocks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#conv-block">conv_block</a></li>
<li class="toctree-l4"><a class="reference internal" href="#separable-conv-block">separable_conv_block</a></li>
<li class="toctree-l4"><a class="reference internal" href="#dense-block">dense_block</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#layer-blocks">Layer blocks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id1">conv_block</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id2">separable_conv_block</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id3">dense_block</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#model-zoo">Model zoo</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#mobilenet">Mobilenet</a></li>
<li class="toctree-l4"><a class="reference internal" href="#ds-cnn">DS-CNN</a></li>
<li class="toctree-l4"><a class="reference internal" href="#vgg">VGG</a></li>
<li class="toctree-l4"><a class="reference internal" href="#yolo">YOLO</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../examples/index.html">Examples</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../examples/plot_gxnor_mnist.html">GXNOR/MNIST inference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_gxnor_mnist.html#loading-the-mnist-dataset">1. Loading the MNIST dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_gxnor_mnist.html#look-at-some-images-from-the-test-dataset">2. Look at some images from the test dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_gxnor_mnist.html#load-the-pre-trained-akida-model">3. Load the pre-trained Akida model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_gxnor_mnist.html#classify-a-single-image">4. Classify a single image</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_gxnor_mnist.html#check-performance-across-a-number-of-samples">5. Check performance across a number of samples</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/plot_regression.html">Regression tutorial</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_regression.html#load-the-dataset">1. Load the dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_regression.html#load-a-pre-trained-native-keras-model">2. Load a pre-trained native Keras model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_regression.html#load-a-pre-trained-quantized-keras-model-satisfying-akida-nsoc-requirements">3. Load a pre-trained quantized Keras model satisfying Akida NSoC requirements</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_regression.html#conversion-to-akida">4. Conversion to Akida</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_regression.html#estimate-age-on-a-single-image">5. Estimate age on a single image</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/plot_cnn_flow.html">CNN conversion flow tutorial</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_cnn_flow.html#load-and-reshape-mnist-dataset">1. Load and reshape MNIST dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_cnn_flow.html#model-definition">2. Model definition</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_cnn_flow.html#model-training">3. Model training</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_cnn_flow.html#model-quantization">4. Model quantization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_cnn_flow.html#model-fine-tuning-quantization-aware-training">5. Model fine tuning (quantization-aware training)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_cnn_flow.html#model-conversion">6. Model conversion</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/plot_ds_cnn_kws.html">DS-CNN/KWS inference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_ds_cnn_kws.html#load-the-preprocessed-dataset">1. Load the preprocessed dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_ds_cnn_kws.html#load-a-pre-trained-native-keras-model">2. Load a pre-trained native Keras model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_ds_cnn_kws.html#load-a-pre-trained-quantized-keras-model-satisfying-akida-nsoc-requirements">3. Load a pre-trained quantized Keras model satisfying Akida NSoC requirements</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_ds_cnn_kws.html#conversion-to-akida">4. Conversion to Akida</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_ds_cnn_kws.html#confusion-matrix">5. Confusion matrix</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/plot_ds_cnn_cifar10.html">DS-CNN CIFAR10 inference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_ds_cnn_cifar10.html#dataset-preparation">1. Dataset preparation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_ds_cnn_cifar10.html#create-a-keras-ds-cnn-model">2. Create a Keras DS-CNN model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_ds_cnn_cifar10.html#quantized-model">3. Quantized model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_ds_cnn_cifar10.html#pretrained-quantized-model">4. Pretrained quantized model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_ds_cnn_cifar10.html#conversion-to-akida">5. Conversion to Akida</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_ds_cnn_cifar10.html#convert-to-akida-model">5.1 Convert to Akida model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_ds_cnn_cifar10.html#check-hardware-compliancy">5.2 Check hardware compliancy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_ds_cnn_cifar10.html#check-performance">5.3 Check performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_ds_cnn_cifar10.html#show-predictions-for-a-random-image">5.4 Show predictions for a random image</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/plot_voc_yolo_detection.html">YOLO/PASCAL-VOC detection tutorial</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_voc_yolo_detection.html#introduction">1. Introduction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_voc_yolo_detection.html#object-detection">1.1 Object detection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_voc_yolo_detection.html#yolo-key-concepts">1.2 YOLO key concepts</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_voc_yolo_detection.html#preprocessing-tools">2. Preprocessing tools</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_voc_yolo_detection.html#model-architecture">3. Model architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_voc_yolo_detection.html#training">4. Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_voc_yolo_detection.html#performance">5. Performance</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_voc_yolo_detection.html#conversion-to-akida">6. Conversion to Akida</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_voc_yolo_detection.html#convert-to-akida-model">6.1 Convert to Akida model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_voc_yolo_detection.html#check-performance">6.1 Check performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_voc_yolo_detection.html#show-predictions-for-a-random-image">6.2 Show predictions for a random image</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/plot_transfer_learning.html">Transfer learning with MobileNet for cats vs. dogs</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_transfer_learning.html#transfer-learning-process">1. Transfer learning process</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_transfer_learning.html#load-and-preprocess-data">2. Load and preprocess data</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_transfer_learning.html#a-load-and-split-data">2.A - Load and split data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_transfer_learning.html#b-preprocess-the-test-set">2.B - Preprocess the test set</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_transfer_learning.html#c-get-labels">2.C - Get labels</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_transfer_learning.html#convert-a-quantized-keras-model-to-akida">3. Convert a quantized Keras model to Akida</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_transfer_learning.html#a-instantiate-a-keras-base-model">3.A - Instantiate a Keras base model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_transfer_learning.html#b-modify-the-network-and-load-pre-trained-weights">3.B - Modify the network and load pre-trained weights</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_transfer_learning.html#c-convert-to-akida">3.C - Convert to Akida</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_transfer_learning.html#classify-test-images">4. Classify test images</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_transfer_learning.html#a-classify-test-images">4.A Classify test images</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_transfer_learning.html#b-compare-results">4.B Compare results</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/plot_mobilenet_imagenet.html">MobileNet/ImageNet inference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_mobilenet_imagenet.html#load-cnn2snn-tool-dependencies">1. Load CNN2SNN tool dependencies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_mobilenet_imagenet.html#load-test-images-from-imagenet">2. Load test images from ImageNet</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_mobilenet_imagenet.html#load-test-images-and-preprocess-test-images">2.1 Load test images and preprocess test images</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_mobilenet_imagenet.html#load-labels">2.2 Load labels</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_mobilenet_imagenet.html#create-a-quantized-keras-model">3. Create a quantized Keras model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_mobilenet_imagenet.html#instantiate-keras-model">3.1 Instantiate Keras model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_mobilenet_imagenet.html#check-performance-of-the-keras-model">3.2 Check performance of the Keras model</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../examples/plot_mobilenet_imagenet.html#convert-keras-model-for-akida-nsoc">4. Convert Keras model for Akida NSoC</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_mobilenet_imagenet.html#convert-keras-model-to-an-akida-compatible-model">4.1 Convert Keras model to an Akida compatible model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_mobilenet_imagenet.html#test-performance-of-the-akida-model">4.2 Test performance of the Akida model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/plot_mobilenet_imagenet.html#show-predictions-for-a-random-test-image">4.3 Show predictions for a random test image</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/Brainchip-Inc/akida_examples/releases">Changelog</a></li>
<li class="toctree-l1"><a class="reference external" href="https://support.brainchip.com/portal/home">Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Akida Examples</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="api_reference.html">API reference</a> &raquo;</li>
        
      <li>Akida models API</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="module-akida_models">
<span id="akida-models-api"></span><h1>Akida models API<a class="headerlink" href="#module-akida_models" title="Permalink to this headline">¶</a></h1>
<div class="section" id="quantization-blocks">
<h2>Quantization blocks<a class="headerlink" href="#quantization-blocks" title="Permalink to this headline">¶</a></h2>
<div class="section" id="conv-block">
<h3>conv_block<a class="headerlink" href="#conv-block" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt id="akida_models.quantization_blocks.conv_block">
<code class="sig-prename descclassname">akida_models.quantization_blocks.</code><code class="sig-name descname">conv_block</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inputs</span></em>, <em class="sig-param"><span class="n">filters</span></em>, <em class="sig-param"><span class="n">kernel_size</span></em>, <em class="sig-param"><span class="n">weight_quantization</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">activ_quantization</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">pooling</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">pool_size</span><span class="o">=</span><span class="default_value">2, 2</span></em>, <em class="sig-param"><span class="n">add_batchnorm</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#akida_models.quantization_blocks.conv_block" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a quantized convolutional layer with optional layers in the
following order: max pooling, batch normalization, quantized activation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>tf.Tensor</em>) – input tensor of shape <cite>(rows, cols, channels)</cite></p></li>
<li><p><strong>filters</strong> (<em>int</em>) – the dimensionality of the output space
(i.e. the number of output filters in the convolution).</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em> or </em><em>tuple of 2 integers</em>) – specifying the
height and width of the 2D convolution kernel.
Can be a single integer to specify the same value for
all spatial dimensions.</p></li>
<li><p><strong>weight_quantization</strong> (<em>int</em>) – quantization bitwidth for weights
(usually 2, 4 or 8). For float weights (no quantization), set the
value to zero.</p></li>
<li><p><strong>activ_quantization</strong> (<em>int</em>) – (usually 1, 2 or 4). For a float activation (ReLU 6), set the
value to zero. For no activation, set it to None.</p></li>
<li><p><strong>pooling</strong> (<em>str</em>) – add a pooling layer of type ‘pooling’ among the
values ‘max’, ‘avg’, ‘global_max’ or ‘global_avg’, with pooling
size set to pool_size. If ‘None’, no pooling will be added.</p></li>
<li><p><strong>pool_size</strong> (<em>int</em><em> or </em><em>tuple of 2 integers</em>) – factors by which to
downscale (vertical, horizontal). (2, 2) will halve the input in
both spatial dimension. If only one integer is specified, the same
window length will be used for both dimensions.</p></li>
<li><p><strong>add_batchnorm</strong> (<em>bool</em>) – add a tf.keras.BatchNormalization layer</p></li>
<li><p><strong>**kwargs</strong> – arguments passed to the tf.keras.Conv2D layer, such as
strides, padding, use_bias, weight_regularizer, etc.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>output tensor of conv2D block.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.Tensor</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="separable-conv-block">
<h3>separable_conv_block<a class="headerlink" href="#separable-conv-block" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt id="akida_models.quantization_blocks.separable_conv_block">
<code class="sig-prename descclassname">akida_models.quantization_blocks.</code><code class="sig-name descname">separable_conv_block</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inputs</span></em>, <em class="sig-param"><span class="n">filters</span></em>, <em class="sig-param"><span class="n">kernel_size</span></em>, <em class="sig-param"><span class="n">weight_quantization</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">activ_quantization</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">pooling</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">pool_size</span><span class="o">=</span><span class="default_value">2, 2</span></em>, <em class="sig-param"><span class="n">add_batchnorm</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#akida_models.quantization_blocks.separable_conv_block" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a quantized separable convolutional layer with optional layers in
the following order: global average pooling, max pooling, batch
normalization, quantized activation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>tf.Tensor</em>) – input tensor of shape <cite>(height, width, channels)</cite></p></li>
<li><p><strong>filters</strong> (<em>int</em>) – the dimensionality of the output space
(i.e. the number of output filters in the pointwise convolution).</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em> or </em><em>tuple of 2 integers</em>) – specifying the
height and width of the 2D convolution window. Can be a single
integer to specify the same value for all spatial dimensions.</p></li>
<li><p><strong>weight_quantization</strong> (<em>int</em>) – quantization bitwidth for weights
(usually 2, 4 or 8). For float weights (no quantization), set the
value to zero.</p></li>
<li><p><strong>activ_quantization</strong> (<em>int</em>) – quantization bitwidth for activation
(usually 1, 2 or 4). For a float activation (ReLU 6), set the
value to zero. For no activation, set it to None.</p></li>
<li><p><strong>pooling</strong> (<em>str</em>) – add a pooling layer of type ‘pooling’ among the
values ‘max’, ‘avg’, ‘global_max’ or ‘global_avg’, with pooling
size set to pool_size. If ‘None’, no pooling will be added.</p></li>
<li><p><strong>pool_size</strong> (<em>int</em><em> or </em><em>tuple of 2 integers</em>) – factors by which to
downscale (vertical, horizontal). (2, 2) will halve the input in
both spatial dimension. If only one integer is specified, the same
window length will be used for both dimensions.</p></li>
<li><p><strong>add_batchnorm</strong> (<em>bool</em>) – add a tf.keras.BatchNormalization layer</p></li>
<li><p><strong>**kwargs</strong> – arguments passed to the tf.keras.SeparableConv2D layer,
such as strides, padding, use_bias, etc.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>output tensor of separable conv block.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.Tensor</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="dense-block">
<h3>dense_block<a class="headerlink" href="#dense-block" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt id="akida_models.quantization_blocks.dense_block">
<code class="sig-prename descclassname">akida_models.quantization_blocks.</code><code class="sig-name descname">dense_block</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inputs</span></em>, <em class="sig-param"><span class="n">units</span></em>, <em class="sig-param"><span class="n">weight_quantization</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">activ_quantization</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">add_batchnorm</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#akida_models.quantization_blocks.dense_block" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a quantized dense layer with optional layers in the following
order: batch normalization, quantized activation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>tf.Tensor</em>) – Input tensor of shape <cite>(rows, cols, channels)</cite></p></li>
<li><p><strong>units</strong> (<em>int</em>) – dimensionality of the output space</p></li>
<li><p><strong>weight_quantization</strong> (<em>int</em>) – quantization bitwidth for weights
(usually 2, 4 or 8). For float weights (no quantization), set the
value to zero.</p></li>
<li><p><strong>activ_quantization</strong> (<em>int</em>) – quantization bitwidth for activation
(usually 1, 2 or 4). For a float activation (ReLU 6), set the
value to zero. For no activation, set it to None.</p></li>
<li><p><strong>add_batchnorm</strong> (<em>bool</em>) – add a tf.keras.BatchNormalization layer</p></li>
<li><p><strong>**kwargs</strong> – arguments passed to the tf.keras.Dense layer, such as
use_bias, kernel_initializer, weight_regularizer, etc.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>output tensor of the dense block.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.Tensor</p>
</dd>
</dl>
</dd></dl>

</div>
</div>
<div class="section" id="layer-blocks">
<h2>Layer blocks<a class="headerlink" href="#layer-blocks" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id1">
<h3>conv_block<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt id="akida_models.layer_blocks.conv_block">
<code class="sig-prename descclassname">akida_models.layer_blocks.</code><code class="sig-name descname">conv_block</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inputs</span></em>, <em class="sig-param"><span class="n">filters</span></em>, <em class="sig-param"><span class="n">kernel_size</span></em>, <em class="sig-param"><span class="n">pooling</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">pool_size</span><span class="o">=</span><span class="default_value">2, 2</span></em>, <em class="sig-param"><span class="n">add_batchnorm</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">add_activation</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#akida_models.layer_blocks.conv_block" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a convolutional layer with optional layers in the following order:
max pooling, batch normalization, activation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>tf.Tensor</em>) – input tensor of shape <cite>(rows, cols, channels)</cite></p></li>
<li><p><strong>filters</strong> (<em>int</em>) – the dimensionality of the output space
(i.e. the number of output filters in the convolution).</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em> or </em><em>tuple of 2 integers</em>) – specifying the
height and width of the 2D convolution kernel.
Can be a single integer to specify the same value for
all spatial dimensions.</p></li>
<li><p><strong>pooling</strong> (<em>str</em>) – add a pooling layer of type ‘pooling’ among the
values ‘max’, ‘avg’, ‘global_max’ or ‘global_avg’, with pooling
size set to pool_size. If ‘None’, no pooling will be added.</p></li>
<li><p><strong>pool_size</strong> (<em>int</em><em> or </em><em>tuple of 2 integers</em>) – factors by which to
downscale (vertical, horizontal). (2, 2) will halve the input in
both spatial dimension. If only one integer is specified, the same
window length will be used for both dimensions.</p></li>
<li><p><strong>add_batchnorm</strong> (<em>bool</em>) – add a BatchNormalization layer</p></li>
<li><p><strong>add_activation</strong> (<em>bool</em>) – add a ReLU layer</p></li>
<li><p><strong>**kwargs</strong> – arguments passed to the tf.keras.Conv2D layer, such as
strides, padding, use_bias, weight_regularizer, etc.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>output tensor of conv2D block.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.Tensor</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="id2">
<h3>separable_conv_block<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt id="akida_models.layer_blocks.separable_conv_block">
<code class="sig-prename descclassname">akida_models.layer_blocks.</code><code class="sig-name descname">separable_conv_block</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inputs</span></em>, <em class="sig-param"><span class="n">filters</span></em>, <em class="sig-param"><span class="n">kernel_size</span></em>, <em class="sig-param"><span class="n">pooling</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">pool_size</span><span class="o">=</span><span class="default_value">2, 2</span></em>, <em class="sig-param"><span class="n">add_batchnorm</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">add_activation</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#akida_models.layer_blocks.separable_conv_block" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a separable convolutional layer with optional layers in the
following order: global average pooling, max pooling, batch normalization,
activation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>tf.Tensor</em>) – input tensor of shape <cite>(height, width, channels)</cite></p></li>
<li><p><strong>filters</strong> (<em>int</em>) – the dimensionality of the output space
(i.e. the number of output filters in the pointwise convolution).</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em><em> or </em><em>tuple of 2 integers</em>) – specifying the
height and width of the 2D convolution window. Can be a single
integer to specify the same value for all spatial dimensions.</p></li>
<li><p><strong>pooling</strong> (<em>str</em>) – add a pooling layer of type ‘pooling’ among the
values ‘max’, ‘avg’, ‘global_max’ or ‘global_avg’, with pooling
size set to pool_size. If ‘None’, no pooling will be added.</p></li>
<li><p><strong>pool_size</strong> (<em>int</em><em> or </em><em>tuple of 2 integers</em>) – factors by which to
downscale (vertical, horizontal). (2, 2) will halve the input in
both spatial dimension. If only one integer is specified, the same
window length will be used for both dimensions.</p></li>
<li><p><strong>add_batchnorm</strong> (<em>bool</em>) – add a BatchNormalization layer</p></li>
<li><p><strong>add_activation</strong> (<em>bool</em>) – add a ReLU layer</p></li>
<li><p><strong>**kwargs</strong> – arguments passed to the tf.keras.SeparableConv2D layer,
such as strides, padding, use_bias, etc.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>output tensor of separable conv block.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.Tensor</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="id3">
<h3>dense_block<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt id="akida_models.layer_blocks.dense_block">
<code class="sig-prename descclassname">akida_models.layer_blocks.</code><code class="sig-name descname">dense_block</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">inputs</span></em>, <em class="sig-param"><span class="n">units</span></em>, <em class="sig-param"><span class="n">add_batchnorm</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">add_activation</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#akida_models.layer_blocks.dense_block" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a dense layer with optional layers in the following order:
batch normalization, activation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>tf.Tensor</em>) – Input tensor of shape <cite>(rows, cols, channels)</cite></p></li>
<li><p><strong>units</strong> (<em>int</em>) – dimensionality of the output space</p></li>
<li><p><strong>add_batchnorm</strong> (<em>bool</em>) – add a BatchNormalization layer</p></li>
<li><p><strong>add_activation</strong> (<em>bool</em>) – add a ReLU layer</p></li>
<li><p><strong>**kwargs</strong> – arguments passed to the Dense layer, such as
use_bias, kernel_initializer, weight_regularizer, etc.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>output tensor of the dense block.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.Tensor</p>
</dd>
</dl>
</dd></dl>

</div>
</div>
<div class="section" id="model-zoo">
<h2>Model zoo<a class="headerlink" href="#model-zoo" title="Permalink to this headline">¶</a></h2>
<div class="section" id="mobilenet">
<h3>Mobilenet<a class="headerlink" href="#mobilenet" title="Permalink to this headline">¶</a></h3>
<div class="section" id="imagenet">
<h4>ImageNet<a class="headerlink" href="#imagenet" title="Permalink to this headline">¶</a></h4>
<dl class="py function">
<dt id="akida_models.mobilenet_imagenet">
<code class="sig-prename descclassname">akida_models.</code><code class="sig-name descname">mobilenet_imagenet</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_shape</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">alpha</span><span class="o">=</span><span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">dropout</span><span class="o">=</span><span class="default_value">0.001</span></em>, <em class="sig-param"><span class="n">include_top</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">weights</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">pooling</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">classes</span><span class="o">=</span><span class="default_value">1000</span></em>, <em class="sig-param"><span class="n">weight_quantization</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">activ_quantization</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">input_weight_quantization</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#akida_models.mobilenet_imagenet" title="Permalink to this definition">¶</a></dt>
<dd><p>Instantiates the MobileNet architecture.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_shape</strong> (<em>tuple</em>) – optional shape tuple.</p></li>
<li><p><strong>alpha</strong> (<em>float</em>) – <p>controls the width of the model.</p>
<ul>
<li><p>If <cite>alpha</cite> &lt; 1.0, proportionally decreases the number of filters
in each layer.</p></li>
<li><p>If <cite>alpha</cite> &gt; 1.0, proportionally increases the number of filters
in each layer.</p></li>
<li><p>If <cite>alpha</cite> = 1, default number of filters from the paper are used
at each layer.</p></li>
</ul>
</p></li>
<li><p><strong>dropout</strong> (<em>float</em>) – dropout rate</p></li>
<li><p><strong>include_top</strong> (<em>bool</em>) – whether to include the fully-connected
layer at the top of the model.</p></li>
<li><p><strong>weights</strong> (<em>str</em>) – the path to the weights file to be loaded.</p></li>
<li><p><strong>pooling</strong> (<em>str</em>) – <p>Optional pooling mode for feature extraction
when <cite>include_top</cite> is <cite>False</cite>.</p>
<ul>
<li><p><cite>None</cite> means that the output of the model will be the 4D tensor
output of the last convolutional block.</p></li>
<li><p><cite>avg</cite> means that global average pooling will be applied to the
output of the last convolutional block, and thus the output of the
model will be a 2D tensor.</p></li>
</ul>
</p></li>
<li><p><strong>classes</strong> (<em>int</em>) – optional number of classes to classify images
into, only to be specified if <cite>include_top</cite> is True.</p></li>
<li><p><strong>weight_quantization</strong> (<em>int</em>) – <p>sets all weights in the model to have
a particular quantization bitwidth except for the weights in the
first layer.</p>
<ul>
<li><p>’0’ implements floating point 32-bit weights.</p></li>
<li><p>’2’ through ‘8’ implements n-bit weights where n is from 2-8 bits.</p></li>
</ul>
</p></li>
<li><p><strong>activ_quantization</strong> – <p>sets all activations in the model to have a
particular activation quantization bitwidth.</p>
<ul>
<li><p>’0’ implements floating point 32-bit activations.</p></li>
<li><p>’2’ through ‘8’ implements n-bit weights where n is from 2-8 bits.</p></li>
</ul>
</p></li>
<li><p><strong>input_weight_quantization</strong> – <p>sets weight quantization in the first layer.
Defaults to weight_quantization value.</p>
<ul>
<li><p>’0’ implements floating point 32-bit weights.</p></li>
<li><p>’2’ through ‘8’ implements n-bit weights where n is from 2-8 bits.</p></li>
</ul>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A Keras model instance.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>ValueError</strong> – in case of invalid argument for <cite>weights</cite>,
    or invalid input shape.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="akida_models.mobilenet_imagenet_pretrained">
<code class="sig-prename descclassname">akida_models.</code><code class="sig-name descname">mobilenet_imagenet_pretrained</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#akida_models.mobilenet_imagenet_pretrained" title="Permalink to this definition">¶</a></dt>
<dd><p>Helper method to retrieve a <cite>mobilenet_imagenet</cite> model that was trained on
ImageNet dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a Keras Model instance.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>tf.keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="akida_models.mobilenet_cats_vs_dogs_pretrained">
<code class="sig-prename descclassname">akida_models.</code><code class="sig-name descname">mobilenet_cats_vs_dogs_pretrained</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#akida_models.mobilenet_cats_vs_dogs_pretrained" title="Permalink to this definition">¶</a></dt>
<dd><p>Helper method to retrieve a <cite>mobilenet_imagenet</cite> model that was trained on
Cats vs.Dogs dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a Keras Model instance.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>tf.keras.Model</p>
</dd>
</dl>
</dd></dl>

<div class="section" id="preprocessing">
<h5>Preprocessing<a class="headerlink" href="#preprocessing" title="Permalink to this headline">¶</a></h5>
<dl class="py function">
<dt id="akida_models.imagenet.preprocessing.process_record_dataset">
<code class="sig-prename descclassname">akida_models.imagenet.preprocessing.</code><code class="sig-name descname">process_record_dataset</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">dataset</span></em>, <em class="sig-param"><span class="n">is_training</span></em>, <em class="sig-param"><span class="n">batch_size</span></em>, <em class="sig-param"><span class="n">im_size</span></em>, <em class="sig-param"><span class="n">shuffle_buffer</span></em>, <em class="sig-param"><span class="n">parse_record_fn</span></em>, <em class="sig-param"><span class="n">num_epochs</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">dtype</span><span class="o">=</span><span class="default_value">tf.float32</span></em>, <em class="sig-param"><span class="n">datasets_num_private_threads</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">drop_remainder</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">tf_data_experimental_slack</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#akida_models.imagenet.preprocessing.process_record_dataset" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a Dataset with raw records, return an iterator over the records.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dataset</strong> – A Dataset representing raw records</p></li>
<li><p><strong>is_training</strong> – A boolean denoting whether the input is for training.</p></li>
<li><p><strong>batch_size</strong> – The number of samples per batch.</p></li>
<li><p><strong>shuffle_buffer</strong> – The buffer size to use when shuffling records. A larger
value results in better randomness, but smaller values reduce startup
time and use less memory.</p></li>
<li><p><strong>parse_record_fn</strong> – A function that takes a raw record and returns the
corresponding (image, label) pair.</p></li>
<li><p><strong>num_epochs</strong> – The number of epochs to repeat the dataset.</p></li>
<li><p><strong>dtype</strong> – Data type to use for images/features.</p></li>
<li><p><strong>datasets_num_private_threads</strong> – Number of threads for a private
threadpool created for all datasets computation.</p></li>
<li><p><strong>drop_remainder</strong> – A boolean indicates whether to drop the remainder of the
batches. If True, the batch dimension will be static.</p></li>
<li><p><strong>tf_data_experimental_slack</strong> – Whether to enable tf.data’s
<cite>experimental_slack</cite> option.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Dataset of (image, label) pairs ready for iteration.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="akida_models.imagenet.preprocessing.get_filenames">
<code class="sig-prename descclassname">akida_models.imagenet.preprocessing.</code><code class="sig-name descname">get_filenames</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">is_training</span></em>, <em class="sig-param"><span class="n">data_dir</span></em><span class="sig-paren">)</span><a class="headerlink" href="#akida_models.imagenet.preprocessing.get_filenames" title="Permalink to this definition">¶</a></dt>
<dd><p>Return filenames for dataset.</p>
</dd></dl>

<dl class="py function">
<dt id="akida_models.imagenet.preprocessing.parse_record">
<code class="sig-prename descclassname">akida_models.imagenet.preprocessing.</code><code class="sig-name descname">parse_record</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">raw_record</span></em>, <em class="sig-param"><span class="n">im_size</span></em>, <em class="sig-param"><span class="n">is_training</span></em>, <em class="sig-param"><span class="n">dtype</span></em><span class="sig-paren">)</span><a class="headerlink" href="#akida_models.imagenet.preprocessing.parse_record" title="Permalink to this definition">¶</a></dt>
<dd><p>Parses a record containing a training example of an image.</p>
<p>The input record is parsed into a label and image, and the image is passed
through preprocessing steps (cropping, flipping, and so on).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>raw_record</strong> – scalar Tensor tf.string containing a serialized
Example protocol buffer.</p></li>
<li><p><strong>is_training</strong> – A boolean denoting whether the input is for training.</p></li>
<li><p><strong>dtype</strong> – data type to use for images/features.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tuple with processed image tensor and one-hot-encoded label tensor.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="akida_models.imagenet.preprocessing.input_fn">
<code class="sig-prename descclassname">akida_models.imagenet.preprocessing.</code><code class="sig-name descname">input_fn</code><span class="sig-paren">(</span><em class="sig-param">is_training</em>, <em class="sig-param">data_dir</em>, <em class="sig-param">batch_size</em>, <em class="sig-param">im_size</em>, <em class="sig-param">num_epochs=1</em>, <em class="sig-param">dtype=tf.float32</em>, <em class="sig-param">datasets_num_private_threads=None</em>, <em class="sig-param">parse_record_fn=&lt;function parse_record&gt;</em>, <em class="sig-param">input_context=None</em>, <em class="sig-param">drop_remainder=False</em>, <em class="sig-param">tf_data_experimental_slack=False</em>, <em class="sig-param">training_dataset_cache=False</em><span class="sig-paren">)</span><a class="headerlink" href="#akida_models.imagenet.preprocessing.input_fn" title="Permalink to this definition">¶</a></dt>
<dd><p>Input function which provides batches for train or eval.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>is_training</strong> – A boolean denoting whether the input is for training.</p></li>
<li><p><strong>data_dir</strong> – The directory containing the input data.</p></li>
<li><p><strong>batch_size</strong> – The number of samples per batch.</p></li>
<li><p><strong>num_epochs</strong> – The number of epochs to repeat the dataset.</p></li>
<li><p><strong>dtype</strong> – Data type to use for images/features</p></li>
<li><p><strong>datasets_num_private_threads</strong> – Number of private threads for tf.data.</p></li>
<li><p><strong>parse_record_fn</strong> – Function to use for parsing the records.</p></li>
<li><p><strong>input_context</strong> – A <cite>tf.distribute.InputContext</cite> object passed in by
<cite>tf.distribute.Strategy</cite>.</p></li>
<li><p><strong>drop_remainder</strong> – A boolean indicates whether to drop the remainder of the
batches. If True, the batch dimension will be static.</p></li>
<li><p><strong>tf_data_experimental_slack</strong> – Whether to enable tf.data’s
<cite>experimental_slack</cite> option.</p></li>
<li><p><strong>training_dataset_cache</strong> – Whether to cache the training dataset on workers.
Typically used to improve training performance when training data is in
remote storage and can fit into worker memory.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A dataset that can be used for iteration.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="akida_models.imagenet.preprocessing.preprocess_image">
<code class="sig-prename descclassname">akida_models.imagenet.preprocessing.</code><code class="sig-name descname">preprocess_image</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">image_buffer</span></em>, <em class="sig-param"><span class="n">bbox</span></em>, <em class="sig-param"><span class="n">output_height</span></em>, <em class="sig-param"><span class="n">output_width</span></em>, <em class="sig-param"><span class="n">num_channels</span></em>, <em class="sig-param"><span class="n">is_training</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">alpha</span><span class="o">=</span><span class="default_value">128.0</span></em>, <em class="sig-param"><span class="n">beta</span><span class="o">=</span><span class="default_value">128.0</span></em><span class="sig-paren">)</span><a class="headerlink" href="#akida_models.imagenet.preprocessing.preprocess_image" title="Permalink to this definition">¶</a></dt>
<dd><p>Preprocesses the given image.</p>
<p>Preprocessing includes decoding, cropping, and resizing for both training
and eval images. Training preprocessing, however, introduces some random
distortion of the image to improve accuracy.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>image_buffer</strong> – scalar string Tensor representing the raw JPEG image buffer.</p></li>
<li><p><strong>bbox</strong> – 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]
where each coordinate is [0, 1) and the coordinates are arranged as
[ymin, xmin, ymax, xmax].</p></li>
<li><p><strong>output_height</strong> – The height of the image after preprocessing.</p></li>
<li><p><strong>output_width</strong> – The width of the image after preprocessing.</p></li>
<li><p><strong>num_channels</strong> – Integer depth of the image buffer for decoding.</p></li>
<li><p><strong>is_training</strong> – <cite>True</cite> if we’re preprocessing the image for training and
<cite>False</cite> otherwise.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A preprocessed image.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="akida_models.imagenet.preprocessing.index_to_label">
<code class="sig-prename descclassname">akida_models.imagenet.preprocessing.</code><code class="sig-name descname">index_to_label</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">index</span></em><span class="sig-paren">)</span><a class="headerlink" href="#akida_models.imagenet.preprocessing.index_to_label" title="Permalink to this definition">¶</a></dt>
<dd><p>Function to get an ImageNet label from an index.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>index</strong> – between 0 and 999</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a string of coma separated labels</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="akida_models.imagenet.preprocessing.resize_and_crop">
<code class="sig-prename descclassname">akida_models.imagenet.preprocessing.</code><code class="sig-name descname">resize_and_crop</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">image_buffer</span></em>, <em class="sig-param"><span class="n">output_height</span></em>, <em class="sig-param"><span class="n">output_width</span></em>, <em class="sig-param"><span class="n">num_channels</span></em><span class="sig-paren">)</span><a class="headerlink" href="#akida_models.imagenet.preprocessing.resize_and_crop" title="Permalink to this definition">¶</a></dt>
<dd><p>Resize and crop the given image.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>image_buffer</strong> – scalar string Tensor representing the raw JPEG image buffer.</p></li>
<li><p><strong>output_height</strong> – The height of the image after preprocessing.</p></li>
<li><p><strong>output_width</strong> – The width of the image after preprocessing.</p></li>
<li><p><strong>num_channels</strong> – Integer depth of the image buffer for decoding.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A resized and cropped image as a numpy array in uint8.</p>
</dd>
</dl>
</dd></dl>

</div>
</div>
</div>
<div class="section" id="ds-cnn">
<h3>DS-CNN<a class="headerlink" href="#ds-cnn" title="Permalink to this headline">¶</a></h3>
<div class="section" id="cifar-10">
<h4>CIFAR-10<a class="headerlink" href="#cifar-10" title="Permalink to this headline">¶</a></h4>
<dl class="py function">
<dt id="akida_models.ds_cnn_cifar10">
<code class="sig-prename descclassname">akida_models.</code><code class="sig-name descname">ds_cnn_cifar10</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_shape</span><span class="o">=</span><span class="default_value">32, 32, 3</span></em>, <em class="sig-param"><span class="n">classes</span><span class="o">=</span><span class="default_value">10</span></em>, <em class="sig-param"><span class="n">weights</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">weight_quantization</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">activ_quantization</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">input_weight_quantization</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#akida_models.ds_cnn_cifar10" title="Permalink to this definition">¶</a></dt>
<dd><p>Instantiates a MobileNet-like model for the “Cifar-10” example.
This model is based on the MobileNet architecture, mainly with fewer layers.
The weights and activations are quantized such that it can be converted into
an Akida model.</p>
<p>This architecture is originated from <a class="reference external" href="https://arxiv.org/abs/1704.04861">https://arxiv.org/abs/1704.04861</a> and
inspired from <a class="reference external" href="https://arxiv.org/pdf/1711.07128.pdf">https://arxiv.org/pdf/1711.07128.pdf</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_shape</strong> (<em>tuple</em>) – input shape tuple of the model</p></li>
<li><p><strong>classes</strong> (<em>int</em>) – number of classes to classify images into</p></li>
<li><p><strong>weights</strong> (<em>str</em>) – one of <cite>None</cite> (random initialization) or the path to the
weights file to be loaded.</p></li>
<li><p><strong>weight_quantization</strong> (<em>int</em>) – <p>sets all weights in the model to have
a particular quantization bitwidth except for the weights in the
first layer.</p>
<ul>
<li><p>’0’ implements floating point 32-bit weights</p></li>
<li><p>’2’ through ‘8’ implements n-bit weights where n is from 2-8 bits.</p></li>
</ul>
</p></li>
<li><p><strong>activ_quantization</strong> (<em>int</em>) – <p>sets all activations in the model to have a.
particular activation quantization bitwidth.</p>
<ul>
<li><p>’0’ implements floating point 32-bit activations.</p></li>
<li><p>’2’ through ‘8’ implements n-bit weights where n is from 2-8 bits.</p></li>
</ul>
</p></li>
<li><p><strong>input_weight_quantization</strong> (<em>int</em>) – <p>sets weight quantization in the first
layer. Defaults to weight_quantization value.</p>
<ul>
<li><p>’None’ implements the same bitwidth as the other weights.</p></li>
<li><p>’0’ implements floating point 32-bit weights.</p></li>
<li><p>’2’ through ‘8’ implements n-bit weights where n is from 2-8 bits.</p></li>
</ul>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a quantized Keras model for DS-CNN/cifar10</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="akida_models.ds_cnn_cifar10_pretrained">
<code class="sig-prename descclassname">akida_models.</code><code class="sig-name descname">ds_cnn_cifar10_pretrained</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#akida_models.ds_cnn_cifar10_pretrained" title="Permalink to this definition">¶</a></dt>
<dd><p>Helper method to retrieve a <cite>ds_cnn_cifar10</cite> model that was trained on
CIFAR10 dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a Keras Model instance.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>tf.keras.Model</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="kws">
<h4>KWS<a class="headerlink" href="#kws" title="Permalink to this headline">¶</a></h4>
<dl class="py function">
<dt id="akida_models.ds_cnn_kws">
<code class="sig-prename descclassname">akida_models.</code><code class="sig-name descname">ds_cnn_kws</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_shape</span><span class="o">=</span><span class="default_value">49, 10, 1</span></em>, <em class="sig-param"><span class="n">classes</span><span class="o">=</span><span class="default_value">33</span></em>, <em class="sig-param"><span class="n">include_top</span><span class="o">=</span><span class="default_value">True</span></em>, <em class="sig-param"><span class="n">weights</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">weight_quantization</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">activ_quantization</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">input_weight_quantization</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">last_layer_activ_quantization</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#akida_models.ds_cnn_kws" title="Permalink to this definition">¶</a></dt>
<dd><p>Instantiates a MobileNet-like model for the “Keyword Spotting” example.</p>
<p>This model is based on the MobileNet architecture, mainly with fewer layers.
The weights and activations are quantized such that it can be converted into
an Akida model.</p>
<p>This architecture is originated from <a class="reference external" href="https://arxiv.org/pdf/1711.07128.pdf">https://arxiv.org/pdf/1711.07128.pdf</a>
and was created for the “Keyword Spotting” (KWS) or “Speech Commands”
dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_shape</strong> (<em>tuple</em>) – input shape tuple of the model</p></li>
<li><p><strong>classes</strong> (<em>int</em>) – optional number of classes to classify words into, only
be specified if <cite>include_top</cite> is True.</p></li>
<li><p><strong>include_top</strong> (<em>bool</em>) – whether to include the fully-connected
layer at the top of the model.</p></li>
<li><p><strong>weights</strong> (<em>str</em>) – one of <cite>None</cite> (random initialization) or the path to the
weights file to be loaded.</p></li>
<li><p><strong>weight_quantization</strong> (<em>int</em>) – <p>sets all weights in the model to have
a particular quantization bitwidth except for the weights in the
first layer.</p>
<ul>
<li><p>’0’ implements floating point 32-bit weights.</p></li>
<li><p>’2’ through ‘8’ implements n-bit weights where n is from 2-8 bits.</p></li>
</ul>
</p></li>
<li><p><strong>activ_quantization</strong> (<em>int</em>) – <p>sets all activations in the model to have a
particular activation quantization bitwidth.</p>
<ul>
<li><p>’0’ implements floating point 32-bit activations.</p></li>
<li><p>’1’ through ‘8’ implements n-bit weights where n is from 2-8 bits.</p></li>
</ul>
</p></li>
<li><p><strong>input_weight_quantization</strong> (<em>int</em>) – <p>sets weight quantization in the first
layer. Defaults to weight_quantization value.</p>
<ul>
<li><p>’None’ implements the same bitwidth as the other weights.</p></li>
<li><p>’0’ implements floating point 32-bit weights.</p></li>
<li><p>’2’ through ‘8’ implements n-bit weights where n is from 2-8 bits.</p></li>
</ul>
</p></li>
<li><p><strong>last_layer_activ_quantization</strong> (<em>int</em>) – <p>sets activation quantization in the
layer before the last. Defaults to activ_quantization value.</p>
<ul>
<li><p>’None’ implements the same bitwidth as the other activations.</p></li>
<li><p>’0’ implements floating point 32-bit activations.</p></li>
<li><p>’1’ through ‘8’ implements n-bit weights where n is from 2-8 bits.</p></li>
</ul>
</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Returns</dt><dd><p>tf.keras.Model: a quantized Keras model for MobileNet/KWS</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="akida_models.ds_cnn_kws_pretrained">
<code class="sig-prename descclassname">akida_models.</code><code class="sig-name descname">ds_cnn_kws_pretrained</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#akida_models.ds_cnn_kws_pretrained" title="Permalink to this definition">¶</a></dt>
<dd><p>Helper method to retrieve a <cite>ds_cnn_kws</cite> model that was trained on
KWS dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a Keras Model instance.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>tf.keras.Model</p>
</dd>
</dl>
</dd></dl>

<div class="section" id="id4">
<h5>Preprocessing<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h5>
<dl class="py function">
<dt id="akida_models.kws.preprocessing.prepare_model_settings">
<code class="sig-prename descclassname">akida_models.kws.preprocessing.</code><code class="sig-name descname">prepare_model_settings</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">sample_rate</span></em>, <em class="sig-param"><span class="n">clip_duration_ms</span></em>, <em class="sig-param"><span class="n">window_size_ms</span></em>, <em class="sig-param"><span class="n">window_stride_ms</span></em>, <em class="sig-param"><span class="n">feature_bin_count</span></em><span class="sig-paren">)</span><a class="headerlink" href="#akida_models.kws.preprocessing.prepare_model_settings" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates common settings needed for all models.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sample_rate</strong> – Number of audio samples per second.</p></li>
<li><p><strong>clip_duration_ms</strong> – Length of each audio clip to be analyzed.</p></li>
<li><p><strong>window_size_ms</strong> – Duration of frequency analysis window.</p></li>
<li><p><strong>window_stride_ms</strong> – How far to move in time between frequency windows.</p></li>
<li><p><strong>feature_bin_count</strong> – Number of frequency bins to use for analysis.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Dictionary containing common settings.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>ValueError</strong> – If the preprocessing mode isn’t recognized.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="akida_models.kws.preprocessing.prepare_words_list">
<code class="sig-prename descclassname">akida_models.kws.preprocessing.</code><code class="sig-name descname">prepare_words_list</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">wanted_words</span></em><span class="sig-paren">)</span><a class="headerlink" href="#akida_models.kws.preprocessing.prepare_words_list" title="Permalink to this definition">¶</a></dt>
<dd><p>Prepends common tokens to the custom word list.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>wanted_words</strong> – List of strings containing the custom words.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>List with the standard silence and unknown tokens added.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="akida_models.kws.preprocessing.which_set">
<code class="sig-prename descclassname">akida_models.kws.preprocessing.</code><code class="sig-name descname">which_set</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">filename</span></em>, <em class="sig-param"><span class="n">validation_percentage</span></em>, <em class="sig-param"><span class="n">testing_percentage</span></em><span class="sig-paren">)</span><a class="headerlink" href="#akida_models.kws.preprocessing.which_set" title="Permalink to this definition">¶</a></dt>
<dd><p>Determines which data partition the file should belong to.</p>
<p>We want to keep files in the same training, validation, or testing sets even
if new ones are added over time. This makes it less likely that testing
samples will accidentally be reused in training when long runs are restarted
for example. To keep this stability, a hash of the filename is taken and used
to determine which set it should belong to. This determination only depends on
the name and the set proportions, so it won’t change as other files are added.</p>
<p>It’s also useful to associate particular files as related (for example words
spoken by the same person), so anything after ‘_nohash_’ in a filename is
ignored for set determination. This ensures that ‘bobby_nohash_0.wav’ and
‘bobby_nohash_1.wav’ are always in the same set, for example.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>filename</strong> – File path of the data sample.</p></li>
<li><p><strong>validation_percentage</strong> – How much of the data set to use for validation.</p></li>
<li><p><strong>testing_percentage</strong> – How much of the data set to use for testing.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>String, one of ‘training’, ‘validation’, or ‘testing’.</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt id="akida_models.kws.preprocessing.AudioProcessor">
<em class="property">class </em><code class="sig-prename descclassname">akida_models.kws.preprocessing.</code><code class="sig-name descname">AudioProcessor</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">sample_rate</span></em>, <em class="sig-param"><span class="n">clip_duration_ms</span></em>, <em class="sig-param"><span class="n">window_size_ms</span></em>, <em class="sig-param"><span class="n">window_stride_ms</span></em>, <em class="sig-param"><span class="n">feature_bin_count</span></em>, <em class="sig-param"><span class="n">data_url</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">data_dir</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">silence_percentage</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">unknown_percentage</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">wanted_words</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">validation_percentage</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">testing_percentage</span><span class="o">=</span><span class="default_value">0</span></em><span class="sig-paren">)</span><a class="headerlink" href="#akida_models.kws.preprocessing.AudioProcessor" title="Permalink to this definition">¶</a></dt>
<dd><p>Handles loading, partitioning, and preparing audio training data.</p>
<p><strong>Methods</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#akida_models.kws.preprocessing.AudioProcessor.get_augmented_data_for_wav" title="akida_models.kws.preprocessing.AudioProcessor.get_augmented_data_for_wav"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_augmented_data_for_wav</span></code></a>(wav_filename, …)</p></td>
<td><p>Applies the feature transformation process to a wav audio file, adding data augmentation (background noise and time shifting).</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#akida_models.kws.preprocessing.AudioProcessor.get_data" title="akida_models.kws.preprocessing.AudioProcessor.get_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_data</span></code></a>(how_many, offset, …)</p></td>
<td><p>Gather samples from the data set, applying transformations as needed.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#akida_models.kws.preprocessing.AudioProcessor.get_features_for_wav" title="akida_models.kws.preprocessing.AudioProcessor.get_features_for_wav"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_features_for_wav</span></code></a>(wav_filename)</p></td>
<td><p>Applies the feature transformation process to the input_wav.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#akida_models.kws.preprocessing.AudioProcessor.maybe_download_and_extract_dataset" title="akida_models.kws.preprocessing.AudioProcessor.maybe_download_and_extract_dataset"><code class="xref py py-obj docutils literal notranslate"><span class="pre">maybe_download_and_extract_dataset</span></code></a>(data_url, …)</p></td>
<td><p>Download and extract data set tar file.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#akida_models.kws.preprocessing.AudioProcessor.prepare_background_data" title="akida_models.kws.preprocessing.AudioProcessor.prepare_background_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">prepare_background_data</span></code></a>()</p></td>
<td><p>Searches a folder for background noise audio, and loads it into memory.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#akida_models.kws.preprocessing.AudioProcessor.prepare_data_index" title="akida_models.kws.preprocessing.AudioProcessor.prepare_data_index"><code class="xref py py-obj docutils literal notranslate"><span class="pre">prepare_data_index</span></code></a>(silence_percentage, …)</p></td>
<td><p>Prepares a list of the samples organized by set and label.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#akida_models.kws.preprocessing.AudioProcessor.prepare_processing_graph" title="akida_models.kws.preprocessing.AudioProcessor.prepare_processing_graph"><code class="xref py py-obj docutils literal notranslate"><span class="pre">prepare_processing_graph</span></code></a>()</p></td>
<td><p>Builds a TensorFlow graph to apply the input distortions.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="akida_models.kws.preprocessing.AudioProcessor.get_augmented_data_for_wav">
<code class="sig-name descname">get_augmented_data_for_wav</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">wav_filename</span></em>, <em class="sig-param"><span class="n">background_frequency</span></em>, <em class="sig-param"><span class="n">background_volume_range</span></em>, <em class="sig-param"><span class="n">time_shift</span></em>, <em class="sig-param"><span class="n">num_augmented_samples</span><span class="o">=</span><span class="default_value">1</span></em><span class="sig-paren">)</span><a class="headerlink" href="#akida_models.kws.preprocessing.AudioProcessor.get_augmented_data_for_wav" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the feature transformation process to a wav audio file, adding
data augmentation (background noise and time shifting).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>wav_filename</strong> (<em>str</em>) – The path to the input audio file.</p></li>
<li><p><strong>background_frequency</strong> – How many clips will have background noise, 0.0 to
1.0.</p></li>
<li><p><strong>background_volume_range</strong> – How loud the background noise will be.</p></li>
<li><p><strong>time_shift</strong> – How much to randomly shift the clips by in time.</p></li>
<li><p><strong>num_augmented_samples</strong> – How many samples will be generated using data
augmentation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>Numpy data array containing the generated features for every augmented</dt><dd><p>sample.</p>
</dd>
</dl>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="akida_models.kws.preprocessing.AudioProcessor.get_data">
<code class="sig-name descname">get_data</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">how_many</span></em>, <em class="sig-param"><span class="n">offset</span></em>, <em class="sig-param"><span class="n">background_frequency</span></em>, <em class="sig-param"><span class="n">background_volume_range</span></em>, <em class="sig-param"><span class="n">time_shift</span></em>, <em class="sig-param"><span class="n">mode</span></em><span class="sig-paren">)</span><a class="headerlink" href="#akida_models.kws.preprocessing.AudioProcessor.get_data" title="Permalink to this definition">¶</a></dt>
<dd><p>Gather samples from the data set, applying transformations as needed.</p>
<p>When the mode is ‘training’, a random selection of samples will be returned,
otherwise the first N clips in the partition will be used. This ensures that
validation always uses the same samples, reducing noise in the metrics.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>how_many</strong> – Desired number of samples to return. -1 means the entire
contents of this partition.</p></li>
<li><p><strong>offset</strong> – Where to start when fetching deterministically.</p></li>
<li><p><strong>background_frequency</strong> – How many clips will have background noise, 0.0 to
1.0.</p></li>
<li><p><strong>background_volume_range</strong> – How loud the background noise will be.</p></li>
<li><p><strong>time_shift</strong> – How much to randomly shift the clips by in time.</p></li>
<li><p><strong>mode</strong> – Which partition to use, must be ‘training’, ‘validation’, or
‘testing’.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>List of sample data for the transformed samples, and list of label indexes</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>ValueError</strong> – If background samples are too short.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="akida_models.kws.preprocessing.AudioProcessor.get_features_for_wav">
<code class="sig-name descname">get_features_for_wav</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">wav_filename</span></em><span class="sig-paren">)</span><a class="headerlink" href="#akida_models.kws.preprocessing.AudioProcessor.get_features_for_wav" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies the feature transformation process to the input_wav.</p>
<p>Runs the feature generation process (generally producing a spectrogram from
the input samples) on the WAV file. This can be useful for testing and
verifying implementations being run on other platforms.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>wav_filename</strong> – The path to the input audio file.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Numpy data array containing the generated features.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="akida_models.kws.preprocessing.AudioProcessor.maybe_download_and_extract_dataset">
<code class="sig-name descname">maybe_download_and_extract_dataset</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">data_url</span></em>, <em class="sig-param"><span class="n">dest_directory</span></em><span class="sig-paren">)</span><a class="headerlink" href="#akida_models.kws.preprocessing.AudioProcessor.maybe_download_and_extract_dataset" title="Permalink to this definition">¶</a></dt>
<dd><p>Download and extract data set tar file.</p>
<p>If the data set we’re using doesn’t already exist, this function
downloads it from the TensorFlow.org website and unpacks it into a
directory.
If the data_url is none, don’t download anything and expect the data
directory to contain the correct files already.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data_url</strong> – Web location of the tar file containing the data set.</p></li>
<li><p><strong>dest_directory</strong> – File path to extract data to.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="akida_models.kws.preprocessing.AudioProcessor.prepare_background_data">
<code class="sig-name descname">prepare_background_data</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#akida_models.kws.preprocessing.AudioProcessor.prepare_background_data" title="Permalink to this definition">¶</a></dt>
<dd><p>Searches a folder for background noise audio, and loads it into memory.</p>
<p>It’s expected that the background audio samples will be in a subdirectory
named ‘_background_noise_’ inside the ‘data_dir’ folder, as .wavs that match
the sample rate of the training data, but can be much longer in duration.</p>
<p>If the ‘_background_noise_’ folder doesn’t exist at all, this isn’t an
error, it’s just taken to mean that no background noise augmentation should
be used. If the folder does exist, but it’s empty, that’s treated as an
error.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>List of raw PCM-encoded audio samples of background noise.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>Exception</strong> – If files aren’t found in the folder.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="akida_models.kws.preprocessing.AudioProcessor.prepare_data_index">
<code class="sig-name descname">prepare_data_index</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">silence_percentage</span></em>, <em class="sig-param"><span class="n">unknown_percentage</span></em>, <em class="sig-param"><span class="n">wanted_words</span></em>, <em class="sig-param"><span class="n">validation_percentage</span></em>, <em class="sig-param"><span class="n">testing_percentage</span></em><span class="sig-paren">)</span><a class="headerlink" href="#akida_models.kws.preprocessing.AudioProcessor.prepare_data_index" title="Permalink to this definition">¶</a></dt>
<dd><p>Prepares a list of the samples organized by set and label.</p>
<p>The training loop needs a list of all the available data, organized by
which partition it should belong to, and with ground truth labels attached.
This function analyzes the folders below the <cite>data_dir</cite>, figures out the
right
labels for each file based on the name of the subdirectory it belongs to,
and uses a stable hash to assign it to a data set partition.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>silence_percentage</strong> – How much of the resulting data should be background.</p></li>
<li><p><strong>unknown_percentage</strong> – How much should be audio outside the wanted classes.</p></li>
<li><p><strong>wanted_words</strong> – Labels of the classes we want to be able to recognize.</p></li>
<li><p><strong>validation_percentage</strong> – How much of the data set to use for validation.</p></li>
<li><p><strong>testing_percentage</strong> – How much of the data set to use for testing.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Dictionary containing a list of file information for each set partition,
and a lookup map for each class to determine its numeric index.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>Exception</strong> – If expected files are not found.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="akida_models.kws.preprocessing.AudioProcessor.prepare_processing_graph">
<code class="sig-name descname">prepare_processing_graph</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#akida_models.kws.preprocessing.AudioProcessor.prepare_processing_graph" title="Permalink to this definition">¶</a></dt>
<dd><p>Builds a TensorFlow graph to apply the input distortions.</p>
<p>Creates a graph that loads a WAVE file, decodes it, scales the volume,
shifts it in time, adds in background noise, calculates a spectrogram, and
then builds an MFCC fingerprint from that.</p>
</dd></dl>

</dd></dl>

</div>
</div>
</div>
<div class="section" id="vgg">
<h3>VGG<a class="headerlink" href="#vgg" title="Permalink to this headline">¶</a></h3>
<div class="section" id="id5">
<h4>CIFAR-10<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h4>
<dl class="py function">
<dt id="akida_models.vgg_cifar10">
<code class="sig-prename descclassname">akida_models.</code><code class="sig-name descname">vgg_cifar10</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_shape</span><span class="o">=</span><span class="default_value">32, 32, 3</span></em>, <em class="sig-param"><span class="n">classes</span><span class="o">=</span><span class="default_value">10</span></em>, <em class="sig-param"><span class="n">weights</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">weight_quantization</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">activ_quantization</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">input_weight_quantization</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#akida_models.vgg_cifar10" title="Permalink to this definition">¶</a></dt>
<dd><p>Instantiates a vgg-like model for the “Cifar-10” example.
This model is based on the vgg architecture, mainly with fewer layers.
The weights and activations are quantized such that it can be converted into
an Akida model.
This architecture is inspired by vgg.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_shape</strong> (<em>tuple</em>) – input shape tuple of the model</p></li>
<li><p><strong>classes</strong> (<em>int</em>) – number of classes to classify images into</p></li>
<li><p><strong>weights</strong> (<em>str</em>) – one of <cite>None</cite> (random initialization) or the path to the
weights file to be loaded.</p></li>
<li><p><strong>weight_quantization</strong> (<em>int</em>) – <p>sets all weights in the model to have
a particular quantization bitwidth except for the weights in the
first layer.</p>
<ul>
<li><p>’0’ implements floating point 32-bit weights</p></li>
<li><p>’2’ through ‘8’ implements n-bit weights where n is from 2-8 bits.</p></li>
</ul>
</p></li>
<li><p><strong>activ_quantization</strong> (<em>int</em>) – <p>sets all activations in the model to have a.
particular activation quantization bitwidth.</p>
<ul>
<li><p>’0’ implements floating point 32-bit activations.</p></li>
<li><p>’2’ through ‘8’ implements n-bit weights where n is from 2-8 bits.</p></li>
</ul>
</p></li>
<li><p><strong>input_weight_quantization</strong> (<em>int</em>) – <p>sets weight quantization in the first
layer. Defaults to weight_quantization value.</p>
<ul>
<li><p>’None’ implements the same bitwidth as the other weights.</p></li>
<li><p>’0’ implements floating point 32-bit weights.</p></li>
<li><p>’2’ through ‘8’ implements n-bit weights where n is from 2-8 bits.</p></li>
</ul>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a quantized Keras model for vgg/cifar10</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="akida_models.vgg_cifar10_pretrained">
<code class="sig-prename descclassname">akida_models.</code><code class="sig-name descname">vgg_cifar10_pretrained</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#akida_models.vgg_cifar10_pretrained" title="Permalink to this definition">¶</a></dt>
<dd><p>Helper method to retrieve a <cite>vgg_cifar10</cite> model that was trained on
CIFAR10 dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a Keras Model instance.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>tf.keras.Model</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="utk-face">
<h4>UTK Face<a class="headerlink" href="#utk-face" title="Permalink to this headline">¶</a></h4>
<dl class="py function">
<dt id="akida_models.vgg_utk_face">
<code class="sig-prename descclassname">akida_models.</code><code class="sig-name descname">vgg_utk_face</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_shape</span><span class="o">=</span><span class="default_value">32, 32, 3</span></em>, <em class="sig-param"><span class="n">weights</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">weight_quantization</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">activ_quantization</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">input_weight_quantization</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#akida_models.vgg_utk_face" title="Permalink to this definition">¶</a></dt>
<dd><p>Instantiates a VGG-like model for the regression example on age
estimation using UTKFace dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_shape</strong> (<em>tuple</em>) – input shape tuple of the model</p></li>
<li><p><strong>weights</strong> (<em>str</em>) – one of <cite>None</cite> (random initialization), ‘utkface’ for
pretrained weights or the path to the weights file to be loaded.</p></li>
<li><p><strong>weight_quantization</strong> (<em>int</em>) – <p>sets all weights in the model to have
a particular quantization bitwidth except for the weights in the
first layer.</p>
<ul>
<li><p>’0’ implements floating point 32-bit weights.</p></li>
<li><p>’2’ through ‘8’ implements n-bit weights where n is from 2-8 bits.</p></li>
</ul>
</p></li>
<li><p><strong>activ_quantization</strong> (<em>int</em>) – <p>sets all activations in the model to have a
particular activation quantization bitwidth.</p>
<ul>
<li><p>’0’ implements floating point 32-bit activations.</p></li>
<li><p>’1’ through ‘8’ implements n-bit weights where n is from 1-8 bits.</p></li>
</ul>
</p></li>
<li><p><strong>input_weight_quantization</strong> (<em>int</em>) – <p>sets weight quantization in the first
layer. Defaults to weight_quantization value.</p>
<ul>
<li><p>’None’ implements the same bitwidth as the other weights.</p></li>
<li><p>’0’ implements floating point 32-bit weights.</p></li>
<li><p>’2’ through ‘8’ implements n-bit weights where n is from 2-8 bits.</p></li>
</ul>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a quantized Keras model for VGG/UTKFace</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="akida_models.vgg_utk_face_pretrained">
<code class="sig-prename descclassname">akida_models.</code><code class="sig-name descname">vgg_utk_face_pretrained</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#akida_models.vgg_utk_face_pretrained" title="Permalink to this definition">¶</a></dt>
<dd><p>Helper method to retrieve a <cite>vgg_utk_face</cite> model that was trained on
UTK Face dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a Keras Model instance.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>tf.keras.Model</p>
</dd>
</dl>
</dd></dl>

<div class="section" id="id6">
<h5>Preprocessing<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h5>
<dl class="py function">
<dt id="akida_models.utk_face.preprocessing.load_data">
<code class="sig-prename descclassname">akida_models.utk_face.preprocessing.</code><code class="sig-name descname">load_data</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#akida_models.utk_face.preprocessing.load_data" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads the dataset from Brainchip data server.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>train set, train labels, test
set and test labels as numpy arrays</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>np.array, np.array, np.array, np.array</p>
</dd>
</dl>
</dd></dl>

</div>
</div>
</div>
<div class="section" id="yolo">
<h3>YOLO<a class="headerlink" href="#yolo" title="Permalink to this headline">¶</a></h3>
<dl class="py function">
<dt id="akida_models.yolo_base">
<code class="sig-prename descclassname">akida_models.</code><code class="sig-name descname">yolo_base</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input_shape</span></em>, <em class="sig-param"><span class="n">classes</span></em>, <em class="sig-param"><span class="n">nb_box</span><span class="o">=</span><span class="default_value">5</span></em>, <em class="sig-param"><span class="n">grid_size</span><span class="o">=</span><span class="default_value">7, 7</span></em>, <em class="sig-param"><span class="n">alpha</span><span class="o">=</span><span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">dropout</span><span class="o">=</span><span class="default_value">0.001</span></em>, <em class="sig-param"><span class="n">weights</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">weight_quantization</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">activ_quantization</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">input_weight_quantization</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#akida_models.yolo_base" title="Permalink to this definition">¶</a></dt>
<dd><p>Instantiates the YOLOv2 architecture.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_shape</strong> (<em>tuple</em>) – input shape tuple</p></li>
<li><p><strong>classes</strong> (<em>int</em>) – number of classes to classify images into</p></li>
<li><p><strong>nb_box</strong> (<em>int</em>) – number of anchors boxes to use</p></li>
<li><p><strong>grid_size</strong> (<em>tuple</em>) – YOLO grid size tuple</p></li>
<li><p><strong>alpha</strong> (<em>float</em>) – controls the width of the model</p></li>
<li><p><strong>dropout</strong> (<em>float</em>) – dropout rate</p></li>
<li><p><strong>weights</strong> (<em>str</em>) – <cite>None</cite> (random initialization) or the path to the weights
file to be loaded.</p></li>
<li><p><strong>weight_quantization</strong> (<em>int</em>) – <p>sets all weights in the model to have
a particular quantization bitwidth except for the weights in the
first layer.</p>
<ul>
<li><p>’0’ implements floating point 32-bit weights.</p></li>
<li><p>’2’ through ‘8’ implements n-bit weights where n is from 2-8 bits.</p></li>
</ul>
</p></li>
<li><p><strong>activ_quantization</strong> – <p>sets all activations in the model to have a
particular activation quantization bitwidth.</p>
<ul>
<li><p>’0’ implements floating point 32-bit activations.</p></li>
<li><p>’2’ through ‘8’ implements n-bit weights where n is from 2-8 bits.</p></li>
</ul>
</p></li>
<li><p><strong>input_weight_quantization</strong> – <p>sets weight quantization in the first layer.
Defaults to weight_quantization value.</p>
<ul>
<li><p>’0’ implements floating point 32-bit weights.</p></li>
<li><p>’2’ through ‘8’ implements n-bit weights where n is from 2-8 bits.</p></li>
</ul>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a Keras Model instance.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tf.keras.Model</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="akida_models.yolo_widerface_pretrained">
<code class="sig-prename descclassname">akida_models.</code><code class="sig-name descname">yolo_widerface_pretrained</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#akida_models.yolo_widerface_pretrained" title="Permalink to this definition">¶</a></dt>
<dd><p>Helper method to retrieve a <cite>yolo_base</cite> model that was trained on WiderFace
dataset and the anchors that are needed to interpet the model output.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a Keras Model instance and a list of anchors.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>tf.keras.Model, list</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="akida_models.yolo_voc_pretrained">
<code class="sig-prename descclassname">akida_models.</code><code class="sig-name descname">yolo_voc_pretrained</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#akida_models.yolo_voc_pretrained" title="Permalink to this definition">¶</a></dt>
<dd><p>Helper method to retrieve a <cite>yolo_base</cite> model that was trained on PASCAL
VOC2012 dataset for ‘person’ and ‘car’ classes only, and the anchors that
are needed to interpet the model output.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a Keras Model instance and a list of anchors.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>tf.keras.Model, list</p>
</dd>
</dl>
</dd></dl>

<div class="section" id="yolo-toolkit">
<h4>YOLO Toolkit<a class="headerlink" href="#yolo-toolkit" title="Permalink to this headline">¶</a></h4>
<div class="section" id="processing">
<h5>Processing<a class="headerlink" href="#processing" title="Permalink to this headline">¶</a></h5>
<dl class="py function">
<dt id="akida_models.detection.processing.load_image">
<code class="sig-prename descclassname">akida_models.detection.processing.</code><code class="sig-name descname">load_image</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">image_path</span></em><span class="sig-paren">)</span><a class="headerlink" href="#akida_models.detection.processing.load_image" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads an image from a path.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>image_path</strong> (<em>string</em>) – full path of the image to load</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a Tensorflow image Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="akida_models.detection.processing.preprocess_image">
<code class="sig-prename descclassname">akida_models.detection.processing.</code><code class="sig-name descname">preprocess_image</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">image_buffer</span></em>, <em class="sig-param"><span class="n">output_size</span></em>, <em class="sig-param"><span class="n">normalize</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="headerlink" href="#akida_models.detection.processing.preprocess_image" title="Permalink to this definition">¶</a></dt>
<dd><p>Preprocess an image for YOLO inference.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>image_buffer</strong> (<em>tf.Tensor</em>) – image to preprocess</p></li>
<li><p><strong>output_size</strong> (<em>tuple</em>) – shape of the image after preprocessing</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A resized and normalized image as a Numpy array.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="akida_models.detection.processing.decode_output">
<code class="sig-prename descclassname">akida_models.detection.processing.</code><code class="sig-name descname">decode_output</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">output</span></em>, <em class="sig-param"><span class="n">anchors</span></em>, <em class="sig-param"><span class="n">nb_classes</span></em>, <em class="sig-param"><span class="n">obj_threshold</span><span class="o">=</span><span class="default_value">0.5</span></em>, <em class="sig-param"><span class="n">nms_threshold</span><span class="o">=</span><span class="default_value">0.5</span></em><span class="sig-paren">)</span><a class="headerlink" href="#akida_models.detection.processing.decode_output" title="Permalink to this definition">¶</a></dt>
<dd><p>Decodes a YOLO model output.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>output</strong> (<em>tf.Tensor</em>) – model output to decode</p></li>
<li><p><strong>anchors</strong> (<em>list</em>) – list of anchors boxes</p></li>
<li><p><strong>nb_classes</strong> (<em>int</em>) – number of classes</p></li>
<li><p><strong>obj_threshold</strong> (<em>float</em>) – confidence threshold for a box</p></li>
<li><p><strong>nms_threshold</strong> (<em>float</em>) – non-maximal supression threshold</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>List of <cite>BoundingBox</cite> objects</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="akida_models.detection.processing.parse_voc_annotations">
<code class="sig-prename descclassname">akida_models.detection.processing.</code><code class="sig-name descname">parse_voc_annotations</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">gt_folder</span></em>, <em class="sig-param"><span class="n">image_folder</span></em>, <em class="sig-param"><span class="n">file_path</span></em>, <em class="sig-param"><span class="n">labels</span></em><span class="sig-paren">)</span><a class="headerlink" href="#akida_models.detection.processing.parse_voc_annotations" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads PASCAL-VOC data.</p>
<p>Data is loaded using the groundtruth informations and stored in a
dictionary.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>gt_folder</strong> (<em>str</em>) – path to the folder containing ground truth files</p></li>
<li><p><strong>image_folder</strong> (<em>str</em>) – path to the folder containing the images</p></li>
<li><p><strong>file_path</strong> (<em>str</em>) – file containing the list of files to parse</p></li>
<li><p><strong>labels</strong> (<em>list</em>) – list of labels of interest</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a dictionnary containing all data present in the ground truth file</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="akida_models.detection.processing.parse_widerface_annotations">
<code class="sig-prename descclassname">akida_models.detection.processing.</code><code class="sig-name descname">parse_widerface_annotations</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">gt_file</span></em>, <em class="sig-param"><span class="n">image_folder</span></em><span class="sig-paren">)</span><a class="headerlink" href="#akida_models.detection.processing.parse_widerface_annotations" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads WiderFace data.</p>
<p>Data is loaded using the groundtruth informations and stored in a
dictionary.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>gt_file</strong> (<em>str</em>) – path to the ground truth file</p></li>
<li><p><strong>image_folder</strong> (<em>str</em>) – path to the directory containing the images</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a dictionnary containing all data present in the ground truth file</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt id="akida_models.detection.processing.BoundingBox">
<em class="property">class </em><code class="sig-prename descclassname">akida_models.detection.processing.</code><code class="sig-name descname">BoundingBox</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">x1</span></em>, <em class="sig-param"><span class="n">y1</span></em>, <em class="sig-param"><span class="n">x2</span></em>, <em class="sig-param"><span class="n">y2</span></em>, <em class="sig-param"><span class="n">score</span><span class="o">=</span><span class="default_value">- 1</span></em>, <em class="sig-param"><span class="n">classes</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#akida_models.detection.processing.BoundingBox" title="Permalink to this definition">¶</a></dt>
<dd><p>Utility class to represent a bounding box.</p>
<p>The box is defined by its top left corner (x1, y1), bottom right corner
(x2, y2), label, score and classes.</p>
<p><strong>Methods</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#akida_models.detection.processing.BoundingBox.get_label" title="akida_models.detection.processing.BoundingBox.get_label"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_label</span></code></a>()</p></td>
<td><p>Returns the label for this bounding box.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#akida_models.detection.processing.BoundingBox.get_score" title="akida_models.detection.processing.BoundingBox.get_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_score</span></code></a>()</p></td>
<td><p>Returns the score for this bounding box.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#akida_models.detection.processing.BoundingBox.iou" title="akida_models.detection.processing.BoundingBox.iou"><code class="xref py py-obj docutils literal notranslate"><span class="pre">iou</span></code></a>(other)</p></td>
<td><p>Computes intersection over union ratio between this bounding box and another one.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="akida_models.detection.processing.BoundingBox.get_label">
<code class="sig-name descname">get_label</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#akida_models.detection.processing.BoundingBox.get_label" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the label for this bounding box.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Index of the label as an integer.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="akida_models.detection.processing.BoundingBox.get_score">
<code class="sig-name descname">get_score</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#akida_models.detection.processing.BoundingBox.get_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the score for this bounding box.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Confidence as a float.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="akida_models.detection.processing.BoundingBox.iou">
<code class="sig-name descname">iou</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">other</span></em><span class="sig-paren">)</span><a class="headerlink" href="#akida_models.detection.processing.BoundingBox.iou" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes intersection over union ratio between this bounding box and
another one.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>other</strong> (<a class="reference internal" href="#akida_models.detection.processing.BoundingBox" title="akida_models.detection.processing.BoundingBox"><em>BoundingBox</em></a>) – the other bounding box for IOU computation</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>IOU value as a float</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="performances">
<h5>Performances<a class="headerlink" href="#performances" title="Permalink to this headline">¶</a></h5>
<dl class="py class">
<dt id="akida_models.detection.map_evaluation.MapEvaluation">
<em class="property">class </em><code class="sig-prename descclassname">akida_models.detection.map_evaluation.</code><code class="sig-name descname">MapEvaluation</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">model</span></em>, <em class="sig-param"><span class="n">val_data</span></em>, <em class="sig-param"><span class="n">labels</span></em>, <em class="sig-param"><span class="n">anchors</span></em>, <em class="sig-param"><span class="n">period</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">obj_threshold</span><span class="o">=</span><span class="default_value">0.5</span></em>, <em class="sig-param"><span class="n">nms_threshold</span><span class="o">=</span><span class="default_value">0.5</span></em>, <em class="sig-param"><span class="n">max_box_per_image</span><span class="o">=</span><span class="default_value">10</span></em>, <em class="sig-param"><span class="n">is_keras_model</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="headerlink" href="#akida_models.detection.map_evaluation.MapEvaluation" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluate a given dataset using a given model.
Code originally from <a class="reference external" href="https://github.com/fizyr/keras-retinanet">https://github.com/fizyr/keras-retinanet</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>tf.Keras.Model</em>) – model to evaluate.</p></li>
<li><p><strong>val_data</strong> (<em>dict</em>) – dictionary containing validation data as obtained
using <cite>preprocess_widerface.py</cite> module</p></li>
<li><p><strong>labels</strong> (<em>list</em>) – list of labels as strings</p></li>
<li><p><strong>anchors</strong> (<em>list</em>) – list of anchors boxes</p></li>
<li><p><strong>period</strong> (<em>int</em><em>, </em><em>optional</em>) – periodicity the precision is printed,
defaults to once per epoch.</p></li>
<li><p><strong>obj_threshold</strong> (<em>float</em><em>, </em><em>optional</em>) – confidence threshold for a box</p></li>
<li><p><strong>nms_threshold</strong> (<em>float</em><em>, </em><em>optional</em>) – non-maximal supression threshold</p></li>
<li><p><strong>max_box_per_image</strong> (<em>int</em><em>, </em><em>optional</em>) – maximum number of detections per
image</p></li>
<li><p><strong>is_keras_model</strong> (<em>bool</em><em>, </em><em>optional</em>) – indicated if the model is a Keras
model (True) or an Akida model (False)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A dict mapping class names to mAP scores.</p>
</dd>
</dl>
<p><strong>Methods</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#akida_models.detection.map_evaluation.MapEvaluation.evaluate_map" title="akida_models.detection.map_evaluation.MapEvaluation.evaluate_map"><code class="xref py py-obj docutils literal notranslate"><span class="pre">evaluate_map</span></code></a>()</p></td>
<td><p>Evaluates current mAP score on the model.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#akida_models.detection.map_evaluation.MapEvaluation.on_epoch_end" title="akida_models.detection.map_evaluation.MapEvaluation.on_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_epoch_end</span></code></a>(epoch[, logs])</p></td>
<td><p>Called at the end of an epoch.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt id="akida_models.detection.map_evaluation.MapEvaluation.evaluate_map">
<code class="sig-name descname">evaluate_map</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#akida_models.detection.map_evaluation.MapEvaluation.evaluate_map" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluates current mAP score on the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>global mAP score and dictionnary of label and mAP for each
class.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>tuple</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="akida_models.detection.map_evaluation.MapEvaluation.on_epoch_end">
<code class="sig-name descname">on_epoch_end</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">epoch</span></em>, <em class="sig-param"><span class="n">logs</span><span class="o">=</span><span class="default_value">{}</span></em><span class="sig-paren">)</span><a class="headerlink" href="#akida_models.detection.map_evaluation.MapEvaluation.on_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of an epoch.</p>
<p>Subclasses should override for any actions to run. This function should only
be called during TRAIN mode.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>epoch</strong> – integer, index of epoch.</p></li>
<li><p><strong>logs</strong> – dict, metric results for this training epoch, and for the
validation epoch if validation is performed. Validation result keys
are prefixed with <cite>val_</cite>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="anchors">
<h5>Anchors<a class="headerlink" href="#anchors" title="Permalink to this headline">¶</a></h5>
<dl class="py function">
<dt id="akida_models.detection.generate_anchors.generate_anchors">
<code class="sig-prename descclassname">akida_models.detection.generate_anchors.</code><code class="sig-name descname">generate_anchors</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">annotations_data</span></em>, <em class="sig-param"><span class="n">num_anchors</span><span class="o">=</span><span class="default_value">5</span></em>, <em class="sig-param"><span class="n">grid_size</span><span class="o">=</span><span class="default_value">7, 7</span></em><span class="sig-paren">)</span><a class="headerlink" href="#akida_models.detection.generate_anchors.generate_anchors" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates anchors by clustering dimensions of the ground truth boxes
from the training dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>annotations_data</strong> (<em>dict</em>) – dictionnary of preprocessed VOC data</p></li>
<li><p><strong>num_anchors</strong> (<em>int</em><em>, </em><em>optional</em>) – number of anchors</p></li>
<li><p><strong>grid_size</strong> (<em>tuple</em><em>, </em><em>optional</em>) – size of the YOLO grid</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the computed anchors</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

</div>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../examples/index.html" class="btn btn-neutral float-right" title="Akida examples" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="cnn2snn_apis.html" class="btn btn-neutral float-left" title="CNN2SNN Toolkit API" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright Copyright 2020, BrainChip Holdings Ltd. All Rights Reserved.

    </p>
  </div> 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>