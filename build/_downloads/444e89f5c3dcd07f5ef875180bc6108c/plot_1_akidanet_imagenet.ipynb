{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# AkidaNet/ImageNet inference\n\nThis tutorial presents how to convert, map, and capture performance from AKD1000 Hardware using an\nAkidaNet model.\n\nAkidaNet architecture is a [MobileNet v1-inspired](https://arxiv.org/abs/1704.04861)_ architecture\noptimized for implementation on Akida 1.0: it exploits the richer expressive power of standard\nconvolutions in early layers, but uses separable convolutions in later layers where filter memory is\nlimiting.\n\nAs [ImageNet](https://www.image-net.org/)_ images are not publicly available, performance is\nassessed using a set of 10 copyright free images that were found on Google using ImageNet class\nnames.\n\n.. Note::\n    This tutorial uses an Akida 1.0 architecture to show AKD1000 mapping and performance. See the\n    [dedicated tutorial](../quantization/plot_1_upgrading_to_2.0.html)_ for 1.0 and 2.0\n    differences.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Dataset preparation\n\nTest images all have at least 256 pixels in the smallest dimension. They must\nbe preprocessed to fit in the model. The ``imagenet.preprocessing.get_preprocessed_samples``\nfunction loads and preprocesses (decodes, crops and extracts a square\n224x224x3 patch from an input image) a set of 10 ImageNet-like images.\n\n.. Note:: Input size is here set to 224x224x3 as this is what is used by the\n          model presented in the next section.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import akida\nimport numpy as np\nfrom akida_models.imagenet import get_preprocessed_samples\n\n# Model specification and hyperparameters\nNUM_CHANNELS = 3\nIMAGE_SIZE = 224\n\n# Load the preprocessed images and their corresponding labels for the test set\nx_test, labels_test = get_preprocessed_samples(IMAGE_SIZE, NUM_CHANNELS)\nprint(f'{x_test.shape[0]} images and their labels are loaded and preprocessed.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Pretrained quantized model\n\nThe Akida model zoo contains a [pretrained quantized helper](../../api_reference/akida_models_apis.html#akida_models.akidanet_imagenet_pretrained).\n\nThe quantization scheme for this model is the following:\n\n * the first layer has 8-bit weights,\n * all other layers have 4-bit weights,\n * all activations are 4-bit.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from cnn2snn import set_akida_version, AkidaVersion\nfrom akida_models import akidanet_imagenet_pretrained\n\n# Use a quantized model with pretrained quantized weights\nwith set_akida_version(AkidaVersion.v1):\n    model_keras_quantized_pretrained = akidanet_imagenet_pretrained(0.5)\nmodel_keras_quantized_pretrained.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check model performance on the 10 images set.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from timeit import default_timer as timer\n\nnum_images = len(x_test)\n\nstart = timer()\npotentials_keras = model_keras_quantized_pretrained.predict(x_test, batch_size=100)\nend = timer()\nprint(f'TF-Keras inference on {num_images} images took {end-start:.2f} s.\\n')\n\npreds_keras = np.squeeze(np.argmax(potentials_keras, 1))\naccuracy_keras = np.sum(np.equal(preds_keras, labels_test)) / num_images\nprint(f\"TF-Keras accuracy: {accuracy_keras*num_images:.0f}/{num_images}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Conversion to Akida\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Convert to Akida model\n\nHere, the TF-Keras quantized model is converted into a suitable version for\nthe Akida accelerator. The\n[cnn2snn.convert](../../api_reference/cnn2snn_apis.html#cnn2snn.convert)_ function only needs\nthe TF-Keras model as argument.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from cnn2snn import convert\n\nmodel_akida = convert(model_keras_quantized_pretrained)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The [Model.summary](../../api_reference/akida_apis.html#akida.Model.summary)_\nmethod provides a detailed description of the Model layers.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_akida.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Check performance\n\nThe following will only compute accuracy for the 10 images set.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Check Model performance\nstart = timer()\naccuracy_akida = model_akida.evaluate(x_test, labels_test)\nend = timer()\nprint(f'Inference on {num_images} images took {end-start:.2f} s.\\n')\nprint(f\"Accuracy: {accuracy_akida*num_images:.0f}/{num_images}.\")\n\n# For non-regression purposes\nassert accuracy_akida >= 0.8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Show predictions for a random image\n\nLabels for test images are stored in the akida_models package. The matching\nbetween names (*string*) and labels (*integer*) is given through the\n``imagenet.preprocessing.index_to_label`` method.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport matplotlib.lines as lines\nfrom akida_models.imagenet import preprocessing\n\n\n# Functions used to display the top5 results\ndef get_top5(potentials, true_label):\n    \"\"\"\n    Returns the top 5 classes from the output potentials\n    \"\"\"\n    tmp_pots = potentials.copy()\n    top5 = []\n    min_val = np.min(tmp_pots)\n    for ii in range(5):\n        best = np.argmax(tmp_pots)\n        top5.append(best)\n        tmp_pots[best] = min_val\n\n    vals = np.zeros((6,))\n    vals[:5] = potentials[top5]\n    if true_label not in top5:\n        vals[5] = potentials[true_label]\n    else:\n        vals[5] = 0\n    vals /= np.max(vals)\n\n    class_name = []\n    for ii in range(5):\n        class_name.append(preprocessing.index_to_label(top5[ii]).split(',')[0])\n    if true_label in top5:\n        class_name.append('')\n    else:\n        class_name.append(\n            preprocessing.index_to_label(true_label).split(',')[0])\n\n    return top5, vals, class_name\n\n\ndef adjust_spines(ax, spines):\n    for loc, spine in ax.spines.items():\n        if loc in spines:\n            spine.set_position(('outward', 10))  # outward by 10 points\n        else:\n            spine.set_color('none')  # don't draw spine\n    # turn off ticks where there is no spine\n    if 'left' in spines:\n        ax.yaxis.set_ticks_position('left')\n    else:\n        # no yaxis ticks\n        ax.yaxis.set_ticks([])\n    if 'bottom' in spines:\n        ax.xaxis.set_ticks_position('bottom')\n    else:\n        # no xaxis ticks\n        ax.xaxis.set_ticks([])\n\n\ndef prepare_plots():\n    fig = plt.figure(figsize=(8, 4))\n    # Image subplot\n    ax0 = plt.subplot(1, 3, 1)\n    imgobj = ax0.imshow(np.zeros((IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS), dtype=np.uint8))\n    ax0.set_axis_off()\n    # Top 5 results subplot\n    ax1 = plt.subplot(1, 2, 2)\n    bar_positions = (0, 1, 2, 3, 4, 6)\n    rects = ax1.barh(bar_positions, np.zeros((6,)), align='center', height=0.5)\n    plt.xlim(-0.2, 1.01)\n    ax1.set(xlim=(-0.2, 1.15), ylim=(-1.5, 12))\n    ax1.set_yticks(bar_positions)\n    ax1.invert_yaxis()\n    ax1.yaxis.set_ticks_position('left')\n    ax1.xaxis.set_ticks([])\n    adjust_spines(ax1, 'left')\n    ax1.add_line(lines.Line2D((0, 0), (-0.5, 6.5), color=(0.0, 0.0, 0.0)))\n    # Adjust Plot Positions\n    ax0.set_position([0.05, 0.055, 0.3, 0.9])\n    l1, b1, w1, h1 = ax1.get_position().bounds\n    ax1.set_position([l1 * 1.05, b1 + 0.09 * h1, w1, 0.8 * h1])\n    # Add title box\n    plt.figtext(0.5,\n                0.9,\n                \"Imagenet Classification by Akida\",\n                size=20,\n                ha=\"center\",\n                va=\"center\",\n                bbox=dict(boxstyle=\"round\",\n                          ec=(0.5, 0.5, 0.5),\n                          fc=(0.9, 0.9, 1.0)))\n\n    return fig, imgobj, ax1, rects\n\n\ndef update_bars_chart(rects, vals, true_label):\n    counter = 0\n    for rect, h in zip(rects, yvals):\n        rect.set_width(h)\n        if counter < 5:\n            if top5[counter] == true_label:\n                if counter == 0:\n                    rect.set_facecolor((0.0, 1.0, 0.0))\n                else:\n                    rect.set_facecolor((0.0, 0.5, 0.0))\n            else:\n                rect.set_facecolor('gray')\n        elif counter == 5:\n            rect.set_facecolor('red')\n        counter += 1\n\n\n# Prepare plots\nfig, imgobj, ax1, rects = prepare_plots()\n\n# Get a random image\nrng = np.random.default_rng()\nimg = rng.integers(0, num_images)\n\n# Predict image class\noutputs_akida = model_akida.predict(np.expand_dims(x_test[img], axis=0)).squeeze()\n\n# Get top 5 prediction labels and associated names\ntrue_label = labels_test[img]\ntop5, yvals, class_name = get_top5(outputs_akida, true_label)\n\n# Draw Plots\nimgobj.set_data(x_test[img])\nax1.set_yticklabels(class_name, rotation='horizontal', size=9)\nupdate_bars_chart(rects, yvals, true_label)\nfig.canvas.draw()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Hardware mapping and performance\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1. Map on hardware\n\nList available Akida devices and check that an NSoC V2, Akida 1.0 production chip is available.\n\nIf a device is installed but not detected, reinstalling the driver might help, see the [driver\nsetup helper](https://github.com/Brainchip-Inc/akida_dw_edma/blob/master/README.md)_.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "devices = akida.devices()\nprint(f'Available devices: {[dev.desc for dev in devices]}')\nassert len(devices), \"No device found, this example needs an Akida NSoC_v2 device.\"\ndevice = devices[0]\nassert device.version == akida.NSoC_v2, \"Wrong device found, this example needs an Akida NSoC_v2.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Map the model on the device\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_akida.map(device)\n\n# Check model mapping: NP allocation and binary size\nmodel_akida.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2. Performance measurement\n\nPower measurement must be enabled on the device' soc (disabled by default).\nAfter sending data for inference, performance measurements are available in\nthe [model statistics](../../api_reference/akida_apis.html#akida.Model.statistics)_.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Enable power measurement\ndevice.soc.power_measurement_enabled = True\n\n# Send data for inference\n_ = model_akida.forward(x_test)\n\n# Display floor current\nfloor_power = device.soc.power_meter.floor\nprint(f'Floor power: {floor_power:.2f} mW')\n\n# Retrieve statistics\nprint(model_akida.statistics)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}