{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nInference on ImageNet with MobileNet\n====================================\n\n.. Note:: Please refer to `CNN2SNN Conversion Tutorial (MNIST)\n          <../../examples/cnn2snn/mnist_cnn2akida_demo.html>`__ notebook\n          and/or the `CNN2SNN documentation\n          <../../user_guide/cnn2snn.html>`__ for flow and steps details of\n          the CNN2SNN conversion.\n\nThis CNN2SNN tutorial presents how to convert a MobileNet pre-trained\nmodel into Akida. As ImageNet images are not publicly available, performances\nare assessed using a set of 10 copyright free images that were found on Google\nusing ImageNet class names.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Load CNN2SNN tool dependencies\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# System imports\nimport os\nimport numpy as np\nimport pickle\nimport csv\nimport imageio\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport matplotlib.lines as lines\nimport tensorflow as tf\n\nfrom timeit import default_timer as timer\n\n# ImageNet tutorial imports\nfrom akida_models import mobilenet_imagenet\nfrom akida_models.mobilenet.imagenet import imagenet_preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Load test images from ImageNet\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe inputs in the Keras MobileNet model must respect two requirements:\n\n* the input image size must be 224x224x3,\n* the input image values must be between -1 and 1.\n\nThis section goes as follows:\n\n* **Load and preprocess images.** The test images all have at least 256 pixels\n  in the smallest dimension. They must be preprocessed to fit in the model.\n  The ``imagenet_preprocessing.preprocess_image`` function decodes, crops and\n  extracts a square 224x224x3 patch from an input image.\n* **Load corresponding labels.** The labels for test images are stored in the\n  akida_models package. The matching between names (*string*) and labels\n  (*integer*) is given through ``imagenet_preprocessing.index_to_label``\n  method.\n\n.. Note:: Akida Execution Engine is configured to take 8-bit inputs\n          without rescaling. For conversion, rescaling values used for\n          training the Keras model are needed.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2.1 Load test images and preprocess test images\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Model specification and hyperparameters\nNUM_CHANNELS = 3\nIMAGE_SIZE = 224\nNUM_CLASSES = 1000\n\nnum_images = 10\n\nfile_path = tf.keras.utils.get_file(\"imagenet_like.zip\",\n                                    \"http://data.brainchip.com/dataset-mirror/imagenet_like/imagenet_like.zip\",\n                                    cache_subdir='datasets/imagenet_like',\n                                    extract=True)\ndata_folder = os.path.dirname(file_path)\n\n# Load images for test set\nx_test_files = []\nx_test = np.zeros((num_images, 224, 224, 3)).astype('uint8')\nfor id in range(num_images):\n    test_file = 'image_' + str(id+1).zfill(2) + '.jpg'\n    x_test_files.append(test_file)\n    img_path = os.path.join(data_folder, test_file)\n    base_image = tf.io.read_file(img_path)\n    image = imagenet_preprocessing.preprocess_image(\n        image_buffer=base_image,\n        bbox=None,\n        output_width=IMAGE_SIZE,\n        output_height=IMAGE_SIZE,\n        num_channels=NUM_CHANNELS,\n        alpha=1.,\n        beta=0.)\n    x_test[id, :, :, :] = np.expand_dims(image.numpy(), axis=0)\n\n# Rescale images for Keras model (normalization between -1 and 1)\n# Assume rescaling format of (x - b)/a\na = 127.5\nb = 127.5\ninput_scaling = (a, b)\nx_test_preprocess = (x_test.astype('float32') - b) / a\n\nprint(f'{num_images} images loaded and preprocessed.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2.2 Load labels\n^^^^^^^^^^^^^^^\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fname = os.path.join(data_folder, 'labels_validation.txt')\nvalidation_labels = dict()\nwith open(fname, newline='') as csvfile:\n    reader = csv.reader(csvfile, delimiter=' ')\n    for row in reader:\n        validation_labels[row[0]] = row[1]\n\n# Get labels for the test set by index\nlabels_test = np.zeros(num_images)\nfor i in range(num_images):\n    labels_test[i] = int(validation_labels[x_test_files[i]])\n\nprint('Labels loaded.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Create a quantized Keras model\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nA Keras model based on a MobileNet model is instantiated with quantized\nweights and activations. This model satisfies the Akida NSoC\nrequirements:\n\n* all the convolutional layers have 4-bit weights, except for the first\n  layer,\n* the first layer has 8-bit weights,\n* all the convolutional layers have 4-bit activations.\n\nThis section goes as follows:\n\n* **Instantiate a quantized Keras model** according to above specifications.\n* **Load pre-trained weights** that performs a 65 % accuracy on the test\n  dataset.\n* **Check performance** on the test set. According to the number of test\n  images, the inference could last for several minutes.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3.1 Instantiate Keras model\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe CNN2SNN module offers a way to easily instantiate a MobileNet model\nbased on Keras with quantized weights and activations. Our ``MobileNet``\nfunction returns a Keras model with custom quantized layers (see\n``quantization_layers.py`` in the CNN2SNN module).\n\n.. Note:: The pre-trained weights which are loaded correspond to the\n   parameters in the next cell. If you want to modify some of these\n   parameters, you must re-train the model and save the weights.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Instantiating MobileNet...\")\n\ninput_shape = (IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)\nmodel_keras = mobilenet_imagenet(input_shape=input_shape,\n                  classes=NUM_CLASSES,\n                  weights='imagenet',\n                  weights_quantization=4,\n                  activ_quantization=4,\n                  input_weights_quantization=8)\n\nprint(\"...done.\")\n\nmodel_keras.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3.2 Check performance of the Keras model\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f'Predicting with Keras model on {num_images} images ...')\n\nstart = timer()\npotentials_keras = model_keras.predict(x_test_preprocess, batch_size=100)\nend = timer()\nprint(f'Keras inference on {num_images} images took {end-start:.2f} s.\\n')\n\npreds_keras = np.squeeze(np.argmax(potentials_keras, 1))\naccuracy_keras = np.sum(np.equal(preds_keras, labels_test)) / num_images\n\nprint(f\"Keras accuracy: {accuracy_keras*100:.2f} %\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. Convert Keras model for Akida NSoC\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nHere, the Keras quantized model is converted into a suitable version for\nthe Akida NSoC. The `cnn2snn.convert <../../api_reference/cnn2snn_apis.html#convert>`__\nfunction needs as arguments the Keras model and the input scaling parameters.\nThe Akida model is then saved in a YAML file with the corresponding weights\nbinary files.\n\nThis section goes as follows:\n\n* **Convert the Keras MobileNet model** to an Akida model compatible for\n  Akida NSoC. Print a summary of the model.\n* **Test performance** of the Akida model (this can take minutes).\n* **Show predictions** for some test images.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.1 Convert Keras model to an Akida compatible model\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Convert to Akida and save model\nfrom cnn2snn import convert\n\nprint(\"Converting Keras model for Akida NSoC...\")\nmodel_akida = convert(model_keras, input_scaling=input_scaling)\nmodel_akida.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.2 Test performance of the Akida model\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f'Predicting with Akida model on {num_images} images ...')\n\nstart = timer()\npreds_akida = model_akida.predict(x_test)\nend = timer()\nprint(f'Inference on {num_images} images took {end-start:.2f} s.\\n')\n\naccuracy_akida = np.sum(np.equal(preds_akida, labels_test)) / num_images\n\nprint(f\"Accuracy: {accuracy_akida*100:.2f} %\")\n\n# For non-regression purpose\nassert accuracy_akida >= 0.9\n\n# Print model statistics\nprint(\"Model statistics\")\nstats = model_akida.get_statistics()\nmodel_akida.predict(x_test[:20])\nfor _, stat in stats.items():\n    print(stat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.3 Show predictions for a random test image\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nFor a random test image, we predict the top 5 classes and display the\nresults on a bar chart.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Functions used to display the top5 results\ndef get_top5(potentials, true_label):\n    \"\"\"\n    Returns the top 5 classes from the output potentials\n    \"\"\"\n    tmp_pots = potentials.copy()\n    top5 = []\n    min_val = np.min(tmp_pots)\n    for ii in range(5):\n        best = np.argmax(tmp_pots)\n        top5.append(best)\n        tmp_pots[best] = min_val\n\n    vals = np.zeros((6, ))\n    vals[:5] = potentials[top5]\n    if true_label not in top5:\n        vals[5] = potentials[true_label]\n    else:\n        vals[5] = 0\n    vals /= np.max(vals)\n\n    class_name = []\n    for ii in range(5):\n        class_name.append(imagenet_preprocessing.index_to_label(top5[ii]).split(',')[0])\n    if true_label in top5:\n        class_name.append('')\n    else:\n        class_name.append(imagenet_preprocessing.index_to_label(true_label).split(',')[0])\n\n    return top5, vals, class_name\n\ndef adjust_spines(ax,spines):\n    for loc, spine in ax.spines.items():\n        if loc in spines:\n            spine.set_position(('outward', 10))  # outward by 10 points\n        else:\n            spine.set_color('none')  # don't draw spine\n    # turn off ticks where there is no spine\n    if 'left' in spines:\n        ax.yaxis.set_ticks_position('left')\n    else:\n        # no yaxis ticks\n        ax.yaxis.set_ticks([])\n    if 'bottom' in spines:\n        ax.xaxis.set_ticks_position('bottom')\n    else:\n        # no xaxis ticks\n        ax.xaxis.set_ticks([])\n\ndef prepare_plots():\n    fig = plt.figure(figsize=(8, 4))\n    # Image subplot\n    ax0 = plt.subplot(1, 3, 1)\n    imgobj = ax0.imshow(np.zeros((224, 224, 3), dtype=np.uint8))\n    ax0.set_axis_off()\n    # Top 5 results subplot\n    ax1 = plt.subplot(1, 2, 2)\n    bar_positions = (0, 1, 2, 3, 4, 6)\n    rects = ax1.barh(bar_positions, np.zeros((6,)), align='center', height=0.5)\n    plt.xlim(-0.2, 1.01)\n    ax1.set(xlim=(-0.2, 1.15), ylim=(-1.5, 12))\n    ax1.set_yticks(bar_positions)\n    ax1.invert_yaxis()\n    ax1.yaxis.set_ticks_position('left')\n    ax1.xaxis.set_ticks([])\n    adjust_spines(ax1, 'left')\n    ax1.add_line(lines.Line2D((0, 0), (-0.5, 6.5), color=(0.0, 0.0, 0.0)))\n    txt_axlbl = ax1.text(-1, -1, 'Top 5 Predictions:', size=12)\n    # Adjust Plot Positions\n    ax0.set_position([0.05, 0.055, 0.3, 0.9])\n    l1, b1, w1, h1 = ax1.get_position().bounds\n    ax1.set_position([l1*1.05, b1 + 0.09*h1, w1, 0.8*h1])\n    # Add title box\n    plt.figtext(0.5, 0.9, \"Imagenet Classification by Akida\", size=20, ha=\"center\", va=\"center\",\n                bbox=dict(boxstyle=\"round\", ec=(0.5, 0.5, 0.5), fc=(0.9, 0.9, 1.0)))\n\n    return fig, imgobj, ax1, rects\n\ndef update_bars_chart(rects, vals, true_label):\n    counter = 0\n    for rect, h in zip(rects, yvals):\n        rect.set_width(h)\n        if counter<5:\n            if top5[counter] == true_label:\n                if counter==0:\n                    rect.set_facecolor((0.0, 1.0, 0.0))\n                else:\n                    rect.set_facecolor((0.0, 0.5, 0.0))\n            else:\n                rect.set_facecolor('gray')\n        elif counter == 5:\n            rect.set_facecolor('red')\n        counter+=1\n\n# Prepare plots\nfig, imgobj, ax1, rects = prepare_plots()\n\n# Get a random image\nimg = np.random.randint(num_images)\n\n# Predict image class\npotentials_akida = model_akida.evaluate(np.expand_dims(x_test[img], axis=0)).squeeze()\n\n# Get top 5 prediction labels and associated names\ntrue_label = int(validation_labels[x_test_files[img]])\ntop5, yvals, class_name = get_top5(potentials_akida, true_label)\n\n# Draw Plots\nimgobj.set_data(x_test[img])\nax1.set_yticklabels(class_name, rotation='horizontal', size=9)\nupdate_bars_chart(rects, yvals, true_label)\nfig.canvas.draw()\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}