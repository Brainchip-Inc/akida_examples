
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "examples/quantization/plot_1_upgrading_to_2.0.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_examples_quantization_plot_1_upgrading_to_2.0.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_examples_quantization_plot_1_upgrading_to_2.0.py:


Upgrading to Akida 2.0
======================

This tutorial targets Akida 1.0 users that are looking for advice on how to migrate their Akida 1.0
model towards Akida 2.0. It also lists the major differences in model architecture compatibilities
between 1.0 and 2.0.

.. GENERATED FROM PYTHON SOURCE LINES 12-32

1. Workflow differences
---------------------------------

.. figure:: ../../img/1.0vs2.0_flow.png
   :target: ../../_images/1.0vs2.0_flow.png
   :alt: 1.0 vs. 2.0 flow
   :scale: 25 %
   :align: center

   Akida 1.0 and 2.0 workflows

As shown in the figure above, the main difference between 1.0 and 2.0 workflows is the
quantization step that was based on CNN2SNN and that is now based on QuantizeML.

Providing your model architecture is 2.0 compatible (`next section
<./plot_1_upgrading_to_2.0.html#models-architecture-differences>`__ lists differences), upgrading to
2.0 is limited to moving from a `cnn2snn.quantize` call to a `quantizeml.models.quantize
<../../api_reference/quantizeml_apis.html#quantizeml.models.quantize>`__ call. The code snippets
below show the two different calls.


.. GENERATED FROM PYTHON SOURCE LINES 32-46

.. code-block:: Python


    import tf_keras as keras

    # Build a simple model that is cross-compatible
    input = keras.layers.Input((32, 32, 3))
    x = keras.layers.Conv2D(kernel_size=3, filters=32, strides=2, padding='same')(input)
    x = keras.layers.BatchNormalization()(x)
    x = keras.layers.ReLU(max_value=6.0)(x)
    x = keras.layers.Flatten()(x)
    x = keras.layers.Dense(units=10)(x)

    model = keras.Model(input, x)
    model.summary()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Model: "model_4"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     input_5 (InputLayer)        [(None, 32, 32, 3)]       0         
                                                                 
     conv2d (Conv2D)             (None, 16, 16, 32)        896       
                                                                 
     batch_normalization (Batch  (None, 16, 16, 32)        128       
     Normalization)                                                  
                                                                 
     re_lu_5 (ReLU)              (None, 16, 16, 32)        0         
                                                                 
     flatten_3 (Flatten)         (None, 8192)              0         
                                                                 
     dense (Dense)               (None, 10)                81930     
                                                                 
    =================================================================
    Total params: 82954 (324.04 KB)
    Trainable params: 82890 (323.79 KB)
    Non-trainable params: 64 (256.00 Byte)
    _________________________________________________________________




.. GENERATED FROM PYTHON SOURCE LINES 47-56

.. code-block:: Python


    import cnn2snn

    # Akida 1.0 flow
    quantized_model_1_0 = cnn2snn.quantize(model, input_weight_quantization=8, weight_quantization=4,
                                           activ_quantization=4)
    akida_model_1_0 = cnn2snn.convert(quantized_model_1_0)
    akida_model_1_0.summary()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

                    Model Summary                 
    ______________________________________________
    Input shape  Output shape  Sequences  Layers
    ==============================================
    [32, 32, 3]  [1, 1, 10]    1          2     
    ______________________________________________

    _____________________________________________________
    Layer (type)         Output shape  Kernel shape    

    ============= SW/conv2d-dense (Software) ============

    conv2d (InputConv.)  [16, 16, 32]  (3, 3, 3, 32)   
    _____________________________________________________
    dense (Fully.)       [1, 1, 10]    (1, 1, 8192, 10)
    _____________________________________________________




.. GENERATED FROM PYTHON SOURCE LINES 57-66

.. code-block:: Python


    import quantizeml

    # Akida 2.0 flow
    qparams = quantizeml.models.QuantizationParams(input_weight_bits=8, weight_bits=4,
                                                   activation_bits=4)
    quantized_model_2_0 = quantizeml.models.quantize(model, qparams=qparams)
    akida_model_2_0 = cnn2snn.convert(quantized_model_2_0)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /usr/local/lib/python3.11/dist-packages/quantizeml/models/quantize.py:577: UserWarning: Quantizing per-axis with random calibration samples is not accurate. Set QuantizationParams.per_tensor_activations=True when calibrating with random samples. Continuing execution.
      warnings.warn("Quantizing per-axis with random calibration samples is not accurate. "
       1/1024 [..............................] - ETA: 1:49      57/1024 [>.............................] - ETA: 0s       113/1024 [==>...........................] - ETA: 0s     168/1024 [===>..........................] - ETA: 0s     224/1024 [=====>........................] - ETA: 0s     280/1024 [=======>......................] - ETA: 0s     337/1024 [========>.....................] - ETA: 0s     395/1024 [==========>...................] - ETA: 0s     452/1024 [============>.................] - ETA: 0s     508/1024 [=============>................] - ETA: 0s     562/1024 [===============>..............] - ETA: 0s     618/1024 [=================>............] - ETA: 0s     674/1024 [==================>...........] - ETA: 0s     730/1024 [====================>.........] - ETA: 0s     786/1024 [======================>.......] - ETA: 0s     842/1024 [=======================>......] - ETA: 0s     898/1024 [=========================>....] - ETA: 0s     954/1024 [==========================>...] - ETA: 0s    1010/1024 [============================>.] - ETA: 0s    1024/1024 [==============================] - 1s 901us/step




.. GENERATED FROM PYTHON SOURCE LINES 67-71

.. code-block:: Python


    akida_model_2_0.summary()






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

                    Model Summary                 
    ______________________________________________
    Input shape  Output shape  Sequences  Layers
    ==============================================
    [32, 32, 3]  [1, 1, 10]    1          4     
    ______________________________________________

    ____________________________________________________________
    Layer (type)                   Output shape  Kernel shape 

    ======= SW/input_quantizer_2-dequantizer_5 (Software) ======

    input_quantizer_2 (Quantizer)  [32, 32, 3]   N/A          
    ____________________________________________________________
    conv2d (InputConv2D)           [16, 16, 32]  (3, 3, 3, 32)
    ____________________________________________________________
    dense (Dense1D)                [1, 1, 10]    (8192, 10)   
    ____________________________________________________________
    dequantizer_5 (Dequantizer)    [1, 1, 10]    N/A          
    ____________________________________________________________




.. GENERATED FROM PYTHON SOURCE LINES 72-79

.. note:: Here we use 8/4/4 quantization to match the CNN2SNN version above, but most users will
          typically use the default 8-bit quantization that comes with QuantizeML.

QuantizeML quantization API is close to the legacy CNN2SNN quantization API and further details on
how to use it are given in the `global workflow tutorial
<../general/plot_0_global_workflow.html>`__ and the `advanced QuantizeML tutorial
<./plot_0_advanced_quantizeml.html>`__.

.. GENERATED FROM PYTHON SOURCE LINES 81-143

2. Models architecture differences
----------------------------------

2.1. Separable convolution
^^^^^^^^^^^^^^^^^^^^^^^^^^

In Akida 1.0, a TF-Keras `SeparableConv2D
<https://www.tensorflow.org/api_docs/python/tf/keras/layers/SeparableConv2D>`__ used to be
quantized as a `QuantizedSeparableConv2D` and converted to an Akida `SeparableConvolutional
<../../api_reference/akida_apis.html#akida.SeparableConvolutional>`__ layer. These 3 layers each
perform a "fused" operation where the depthwise and pointwise operations are grouped together in a
single layer.

In Akida 2.0, the fused separable layer support has been dropped in favor of a more commonly used
unfused operation where the depthwise and the pointwise operations are computed in independent
layers. The akida_models package offers a `separable_conv_block
<../../api_reference/akida_models_apis.html#akida_models.layer_blocks.separable_conv_block>`__
with a ``fused=False`` parameter that will create the `DepthwiseConv2D
<https://www.tensorflow.org/api_docs/python/tf/keras/layers/DepthwiseConv2D>`__ and the pointwise
`Conv2D <https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D>`__ layers under the
hood. This block will then be quantized towards a `QuantizedDepthwiseConv2D
<../../api_reference/quantizeml_apis.html#quantizeml.layers.QuantizedDepthwiseConv2D>`__ and a
pointwise `QuantizedConv2D
<../../api_reference/quantizeml_apis.html#quantizeml.layers.QuantizedConv2D>`__ before
conversion into `DepthwiseConv2D <../../api_reference/akida_apis.html#akida.DepthwiseConv2D>`__
and pointwise `Conv2D <../../api_reference/akida_apis.html#akida.Conv2D>`__ respectively.

Note that while the resulting model topography is slightly different, the fused and unfused
mathematical operations are strictly equivalent.

In order to ease 1.0 to 2.0 migration of existing models, akida_models offers an
`unfuse_sepconv2d <../../api_reference/akida_models_apis.html#akida_models.unfuse_sepconv2d>`__
API that takes a model with fused layers and transforms it into an unfused equivalent version. For
convenience, an ``unfuse`` CLI action is also provided.

.. code-block:: bash

   akida_models unfuse -m model.h5

2.2. Global average pooling operation
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The supported position of the `GlobalAveragePooling2D
<https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling2D>`__ operation
has changed in Akida 2.0 as it now must come after the ReLU activation (when there is one). In
other words, in Akida 1.0 the layers were organized as follows:

* ... > Neural layer > GlobalAveragePooling > (BatchNormalization) > ReLU >  Neural layer > ...

In Akida 2.0 the supported sequence of layer is:

* ... > Neural layer > (BatchNormalization) > (ReLU) > GlobalAveragePooling >  Neural layer > ...

This can also be configured using the ``post_relu_gap`` parameter of akida_models `layer_blocks
<../../api_reference/akida_models_apis.html#layer-blocks>`__.

To migrate an existing model from 1.0 to 2.0, it is possible to load 1.0 weights into a 2.0
oriented architecture using `TF-Keras save and load APIs
<https://www.tensorflow.org/tutorials/keras/save_and_load>`__ because the global average pooling
position does not have an effect on model weights. However, the sequences between 1.0 and 2.0 are
not mathematically equivalent so it might be required to tune or even retrain the model.


.. GENERATED FROM PYTHON SOURCE LINES 145-151

3. Using ``AkidaVersion``
-------------------------

It is still possible to build, quantize and convert models towards a 1.0 target using the
`AkidaVersion API <../../api_reference/cnn2snn_apis.html#akida-version>`__.


.. GENERATED FROM PYTHON SOURCE LINES 151-157

.. code-block:: Python


    # Reusing the previously defined 2.0 model but converting to a 1.0 target this time
    with cnn2snn.set_akida_version(cnn2snn.AkidaVersion.v1):
        akida_model = cnn2snn.convert(quantized_model_2_0)
    akida_model.summary()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

                    Model Summary                 
    ______________________________________________
    Input shape  Output shape  Sequences  Layers
    ==============================================
    [32, 32, 3]  [1, 1, 10]    1          3     
    ______________________________________________

    _______________________________________________________________
    Layer (type)                   Output shape  Kernel shape    

    ============ SW/input_quantizer_2-dense (Software) ============

    input_quantizer_2 (Quantizer)  [32, 32, 3]   N/A             
    _______________________________________________________________
    conv2d (InputConv.)            [16, 16, 32]  (3, 3, 3, 32)   
    _______________________________________________________________
    dense (Fully.)                 [1, 1, 10]    (1, 1, 8192, 10)
    _______________________________________________________________




.. GENERATED FROM PYTHON SOURCE LINES 158-160

One will notice the different Akida layers types as detailed in `Akida user guide
<../../user_guide/akida.html#akida-layers>`__.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 4.939 seconds)


.. _sphx_glr_download_examples_quantization_plot_1_upgrading_to_2.0.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_1_upgrading_to_2.0.ipynb <plot_1_upgrading_to_2.0.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_1_upgrading_to_2.0.py <plot_1_upgrading_to_2.0.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_1_upgrading_to_2.0.zip <plot_1_upgrading_to_2.0.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
