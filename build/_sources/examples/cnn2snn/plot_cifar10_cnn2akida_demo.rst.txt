.. note::
    :class: sphx-glr-download-link-note

    Click :ref:`here <sphx_glr_download_examples_cnn2snn_plot_cifar10_cnn2akida_demo.py>` to download the full example code
.. rst-class:: sphx-glr-example-title

.. _sphx_glr_examples_cnn2snn_plot_cifar10_cnn2akida_demo.py:


Inference on CIFAR10 with VGG and MobileNet
===========================================

The CIFAR-10 dataset consists of 60000 32x32 color images in 10 classes,
with 6000 images per class. There are 50000 training images and 10000
test images.

This tutorial uses this dataset for a classic object classification task
(airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck).

We start from state-of-the-art CNN models, illustrating how these models
can be quantized, then converted to an Akida model with an equivalent
accuracy.

The neural networks used in this tutorial are inspired from the
`VGG <https://arxiv.org/abs/1409.1556>`__ and
`MobileNets <https://arxiv.org/abs/1704.04861>`__ architecture
respectively.

The VGG architecture uses Convolutional and Dense layers: these layers
must therefore be quantized with at most 2 bits of precision to be
compatible with the Akida NSoC. This causes a 2 % drop in accuracy.

The MobileNet architecture uses Separable Convolutional layers that can
be quantized using 4 bits of precision, allowing to preserve the base
Keras model accuracy.

+---------------------------+-------------+
| Model                     | Accuracy    |
+===========================+=============+
| VGG Keras                 | 93.15 %     |
+---------------------------+-------------+
| VGG Keras quantized       | 91.30 %     |
+---------------------------+-------------+
| VGG Akida                 | **91.59 %** |
+---------------------------+-------------+
| MobileNet Keras           | 93.49 %     |
+---------------------------+-------------+
| MobileNet Keras quantized | 93.07 %     |
+---------------------------+-------------+
| MobileNet Akida           | **93.22 %** |
+---------------------------+-------------+

1. Load CNN2SNN tool dependencies
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


.. code-block:: default


    # System imports
    import os
    import sys
    import numpy as np
    from sklearn.metrics import accuracy_score
    from timeit import default_timer as timer

    # TensorFlow imports
    from tensorflow.keras.datasets import cifar10

    # Akida models imports
    from akida_models import mobilenet_cifar10, vgg_cifar10

    # CNN2SNN
    from cnn2snn import convert









2. Load and reshape CIFAR10 dataset
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


.. code-block:: default


    # Load CIFAR10 dataset
    (x_train, y_train), (x_test, y_test) = cifar10.load_data()

    # Reshape x-data
    x_train = x_train.reshape(50000, 32, 32, 3)
    x_test = x_test.reshape(10000, 32, 32, 3)
    input_shape = (32, 32, 3)

    # Set aside raw test data for use with Akida Execution Engine later
    raw_x_test = x_test.astype('uint8')

    # Rescale x-data
    a = 255
    b = 0
    input_scaling = (a, b)
    x_train = x_train.astype('float32')
    x_test = x_test.astype('float32')
    x_train = (x_train - b)/a
    x_test = (x_test - b)/a






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz
         8192/170498071 [..............................] - ETA: 34:35        40960/170498071 [..............................] - ETA: 13:51        90112/170498071 [..............................] - ETA: 9:26        204800/170498071 [..............................] - ETA: 5:32       417792/170498071 [..............................] - ETA: 3:23       860160/170498071 [..............................] - ETA: 1:58      1728512/170498071 [..............................] - ETA: 1:08      2777088/170498071 [..............................] - ETA: 49s       4513792/170498071 [..............................] - ETA: 33s      5971968/170498071 [>.............................] - ETA: 27s      7872512/170498071 [>.............................] - ETA: 21s      7938048/170498071 [>.............................] - ETA: 22s      9879552/170498071 [>.............................] - ETA: 18s      9920512/170498071 [>.............................] - ETA: 19s     11771904/170498071 [=>............................] - ETA: 16s     11919360/170498071 [=>............................] - ETA: 17s     13410304/170498071 [=>............................] - ETA: 15s     13950976/170498071 [=>............................] - ETA: 15s     15458304/170498071 [=>............................] - ETA: 14s     15982592/170498071 [=>............................] - ETA: 14s     17539072/170498071 [==>...........................] - ETA: 13s     18030592/170498071 [==>...........................] - ETA: 13s     19652608/170498071 [==>...........................] - ETA: 12s     20127744/170498071 [==>...........................] - ETA: 12s     21782528/170498071 [==>...........................] - ETA: 11s     22224896/170498071 [==>...........................] - ETA: 12s     23945216/170498071 [===>..........................] - ETA: 11s     24354816/170498071 [===>..........................] - ETA: 11s     26091520/170498071 [===>..........................] - ETA: 10s     26501120/170498071 [===>..........................] - ETA: 10s     28237824/170498071 [===>..........................] - ETA: 10s     28663808/170498071 [====>.........................] - ETA: 10s     30416896/170498071 [====>.........................] - ETA: 9s      30842880/170498071 [====>.........................] - ETA: 9s     32628736/170498071 [====>.........................] - ETA: 9s     33054720/170498071 [====>.........................] - ETA: 9s     34856960/170498071 [=====>........................] - ETA: 9s     35299328/170498071 [=====>........................] - ETA: 9s     37134336/170498071 [=====>........................] - ETA: 8s     37593088/170498071 [=====>........................] - ETA: 8s     39510016/170498071 [=====>........................] - ETA: 8s     39854080/170498071 [======>.......................] - ETA: 8s     41787392/170498071 [======>.......................] - ETA: 8s     42131456/170498071 [======>.......................] - ETA: 8s     44081152/170498071 [======>.......................] - ETA: 7s     44408832/170498071 [======>.......................] - ETA: 7s     46383104/170498071 [=======>......................] - ETA: 7s     46718976/170498071 [=======>......................] - ETA: 7s     48652288/170498071 [=======>......................] - ETA: 7s     49045504/170498071 [=======>......................] - ETA: 7s     50995200/170498071 [=======>......................] - ETA: 7s     51388416/170498071 [========>.....................] - ETA: 7s     53403648/170498071 [========>.....................] - ETA: 7s     55074816/170498071 [========>.....................] - ETA: 6s     55762944/170498071 [========>.....................] - ETA: 6s     57466880/170498071 [=========>....................] - ETA: 6s     58138624/170498071 [=========>....................] - ETA: 6s     59891712/170498071 [=========>....................] - ETA: 6s     60530688/170498071 [=========>....................] - ETA: 6s     62300160/170498071 [=========>....................] - ETA: 6s     62955520/170498071 [==========>...................] - ETA: 6s     64708608/170498071 [==========>...................] - ETA: 5s     65380352/170498071 [==========>...................] - ETA: 5s     67166208/170498071 [==========>...................] - ETA: 5s     67837952/170498071 [==========>...................] - ETA: 5s     69656576/170498071 [===========>..................] - ETA: 5s     70287360/170498071 [===========>..................] - ETA: 5s     72114176/170498071 [===========>..................] - ETA: 5s     72720384/170498071 [===========>..................] - ETA: 5s     74555392/170498071 [============>.................] - ETA: 5s     75177984/170498071 [============>.................] - ETA: 5s     77062144/170498071 [============>.................] - ETA: 5s     77660160/170498071 [============>.................] - ETA: 5s     79536128/170498071 [============>.................] - ETA: 4s     80134144/170498071 [=============>................] - ETA: 4s     82026496/170498071 [=============>................] - ETA: 4s     82681856/170498071 [=============>................] - ETA: 4s     84557824/170498071 [=============>................] - ETA: 4s     85106688/170498071 [=============>................] - ETA: 4s     87072768/170498071 [==============>...............] - ETA: 4s     87646208/170498071 [==============>...............] - ETA: 4s     89563136/170498071 [==============>...............] - ETA: 4s     90193920/170498071 [==============>...............] - ETA: 4s     92086272/170498071 [===============>..............] - ETA: 4s     92741632/170498071 [===============>..............] - ETA: 4s     94642176/170498071 [===============>..............] - ETA: 3s     95313920/170498071 [===============>..............] - ETA: 3s     97214464/170498071 [================>.............] - ETA: 3s     97886208/170498071 [================>.............] - ETA: 3s     99901440/170498071 [================>.............] - ETA: 3s    100491264/170498071 [================>.............] - ETA: 3s    102473728/170498071 [=================>............] - ETA: 3s    103047168/170498071 [=================>............] - ETA: 3s    105062400/170498071 [=================>............] - ETA: 3s    105619456/170498071 [=================>............] - ETA: 3s    107651072/170498071 [=================>............] - ETA: 3s    108224512/170498071 [==================>...........] - ETA: 3s    110256128/170498071 [==================>...........] - ETA: 2s    110845952/170498071 [==================>...........] - ETA: 2s    112885760/170498071 [==================>...........] - ETA: 2s    113467392/170498071 [==================>...........] - ETA: 2s    115482624/170498071 [===================>..........] - ETA: 2s    116097024/170498071 [===================>..........] - ETA: 2s    118087680/170498071 [===================>..........] - ETA: 2s    118743040/170498071 [===================>..........] - ETA: 2s    120774656/170498071 [====================>.........] - ETA: 2s    121364480/170498071 [====================>.........] - ETA: 2s    123404288/170498071 [====================>.........] - ETA: 2s    124002304/170498071 [====================>.........] - ETA: 2s    126083072/170498071 [=====================>........] - ETA: 2s    127623168/170498071 [=====================>........] - ETA: 2s    128745472/170498071 [=====================>........] - ETA: 2s    130293760/170498071 [=====================>........] - ETA: 1s    131407872/170498071 [======================>.......] - ETA: 1s    133029888/170498071 [======================>.......] - ETA: 1s    134078464/170498071 [======================>.......] - ETA: 1s    135741440/170498071 [======================>.......] - ETA: 1s    136765440/170498071 [=======================>......] - ETA: 1s    138469376/170498071 [=======================>......] - ETA: 1s    139436032/170498071 [=======================>......] - ETA: 1s    141205504/170498071 [=======================>......] - ETA: 1s    142221312/170498071 [========================>.....] - ETA: 1s    143974400/170498071 [========================>.....] - ETA: 1s    144924672/170498071 [========================>.....] - ETA: 1s    146677760/170498071 [========================>.....] - ETA: 1s    147660800/170498071 [========================>.....] - ETA: 1s    149397504/170498071 [=========================>....] - ETA: 0s    150372352/170498071 [=========================>....] - ETA: 0s    152100864/170498071 [=========================>....] - ETA: 0s    153051136/170498071 [=========================>....] - ETA: 0s    155328512/170498071 [==========================>...] - ETA: 0s    155770880/170498071 [==========================>...] - ETA: 0s    158048256/170498071 [==========================>...] - ETA: 0s    158457856/170498071 [==========================>...] - ETA: 0s    160784384/170498071 [===========================>..] - ETA: 0s    161202176/170498071 [===========================>..] - ETA: 0s    163487744/170498071 [===========================>..] - ETA: 0s    163913728/170498071 [===========================>..] - ETA: 0s    166273024/170498071 [============================>.] - ETA: 0s    167813120/170498071 [============================>.] - ETA: 0s    169033728/170498071 [============================>.] - ETA: 0s    170500096/170498071 [==============================] - 8s 0us/step




3. Create a quantized Keras VGG model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A Keras model based on the `VGG <https://arxiv.org/abs/1409.1556>`__
architecture is instantiated with quantized weights and activations.

This model relies only on FullyConnected and Convolutional layers:

  * all the layers have 2-bit weights,
  * all the layers have 2-bit activations.

This model therefore satisfies the Akida NSoC requirements.

This section goes as follows:

  * **3.A - Instantiate a quantized Keras VGG model** according to above
    specifications and load pre-trained weights** that performs 91 % accuracy
    on the test dataset.
  * **3.B - Check performance** on the test set.

3.A Instantiate Keras model
^^^^^^^^^^^^^^^^^^^^^^^^^^^

The ``vgg_cifar10`` function returns a VGG Keras model with custom
quantized layers (see ``quantization_layers.py`` in the CNN2SNN module).

.. Note:: The pre-trained weights which are loaded in the section 3.B
          corresponds to the quantization parameters in the next cell. If you
          want to modify some of these parameters, you must re-train the model
          and save the weights.

Pre-trained weights were obtained after a series of training episodes,
starting from unconstrained float weights and activations and ending
with quantized 2-bits weights and activations.

For the first training episode, we train the model with unconstrained
float weights and activations for 1000 epochs.

For the subsequent training episodes, we start from the weights trained
in the previous episode, progressively reducing the bitwidth of
activations, then weights. We also stop the episode when the training
loss has stopped decreasing for 20 epochs.

The table below summarizes the results obtained when preparing the
weights stored under `<http://data.brainchip.com/models/vgg/>`__:

+---------+----------------+---------------+----------+--------+
| Episode | Weights Quant. | Activ. Quant. | Accuracy | Epochs |
+=========+================+===============+==========+========+
| 1       | N/A            | N/A           | 93.15 %  | 1000   |
+---------+----------------+---------------+----------+--------+
| 2       | 4 bits         | 4 bits        | 93.24 %  | 30     |
+---------+----------------+---------------+----------+--------+
| 3       | 3 bits         | 4 bits        | 92.91 %  | 50     |
+---------+----------------+---------------+----------+--------+
| 4       | 3 bits         | 3 bits        | 92.38 %  | 64     |
+---------+----------------+---------------+----------+--------+
| 5       | 2 bits         | 3 bits        | 91.48 %  | 82     |
+---------+----------------+---------------+----------+--------+
| 6       | 2 bits         | 2 bits        | 91.31 %  | 74     |
+---------+----------------+---------------+----------+--------+

Please refer to `mnist_cnn2akida_demo example <mnist_cnn2akida_demo.html>`__
and/or the `CNN2SNN toolkit <../../api_reference/cnn2snn_apis.html>`__
documentation for flow and training steps details.


.. code-block:: default


    # Instantiate the quantized model
    model_keras = vgg_cifar10(input_shape,
                              weights='cifar10',
                              weights_quantization=2,
                              activ_quantization=2,
                              input_weights_quantization=2)
    model_keras.summary()





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Downloading data from http://data.brainchip.com/models/vgg/vgg_cifar10_wq2_aq2.hdf5
        8192/56218264 [..............................] - ETA: 1:52       81920/56218264 [..............................] - ETA: 45s       622592/56218264 [..............................] - ETA: 10s     2375680/56218264 [>.............................] - ETA: 4s      4603904/56218264 [=>............................] - ETA: 5s     4833280/56218264 [=>............................] - ETA: 6s     4849664/56218264 [=>............................] - ETA: 6s     4882432/56218264 [=>............................] - ETA: 7s     4923392/56218264 [=>............................] - ETA: 7s     4972544/56218264 [=>............................] - ETA: 7s     5029888/56218264 [=>............................] - ETA: 8s     5095424/56218264 [=>............................] - ETA: 8s     5177344/56218264 [=>............................] - ETA: 9s     5259264/56218264 [=>............................] - ETA: 9s     5357568/56218264 [=>............................] - ETA: 9s     5464064/56218264 [=>............................] - ETA: 9s     5578752/56218264 [=>............................] - ETA: 10s     5693440/56218264 [==>...........................] - ETA: 10s     5824512/56218264 [==>...........................] - ETA: 10s     5963776/56218264 [==>...........................] - ETA: 10s     6103040/56218264 [==>...........................] - ETA: 10s     6258688/56218264 [==>...........................] - ETA: 10s     6422528/56218264 [==>...........................] - ETA: 11s     6602752/56218264 [==>...........................] - ETA: 11s     6791168/56218264 [==>...........................] - ETA: 11s     7004160/56218264 [==>...........................] - ETA: 11s     7233536/56218264 [==>...........................] - ETA: 11s     7479296/56218264 [==>...........................] - ETA: 10s     7741440/56218264 [===>..........................] - ETA: 10s     8028160/56218264 [===>..........................] - ETA: 10s     8339456/56218264 [===>..........................] - ETA: 10s     8675328/56218264 [===>..........................] - ETA: 10s     9043968/56218264 [===>..........................] - ETA: 10s     9453568/56218264 [====>.........................] - ETA: 9s      9904128/56218264 [====>.........................] - ETA: 9s    10403840/56218264 [====>.........................] - ETA: 9s    10944512/56218264 [====>.........................] - ETA: 8s    11542528/56218264 [=====>........................] - ETA: 8s    12181504/56218264 [=====>........................] - ETA: 8s    12869632/56218264 [=====>........................] - ETA: 7s    13598720/56218264 [======>.......................] - ETA: 7s    14376960/56218264 [======>.......................] - ETA: 6s    15179776/56218264 [=======>......................] - ETA: 6s    16048128/56218264 [=======>......................] - ETA: 6s    16982016/56218264 [========>.....................] - ETA: 5s    17981440/56218264 [========>.....................] - ETA: 5s    19054592/56218264 [=========>....................] - ETA: 5s    20176896/56218264 [=========>....................] - ETA: 4s    21348352/56218264 [==========>...................] - ETA: 4s    22593536/56218264 [===========>..................] - ETA: 4s    23863296/56218264 [===========>..................] - ETA: 3s    25157632/56218264 [============>.................] - ETA: 3s    26517504/56218264 [=============>................] - ETA: 3s    27983872/56218264 [=============>................] - ETA: 3s    29499392/56218264 [==============>...............] - ETA: 2s    31080448/56218264 [===============>..............] - ETA: 2s    32759808/56218264 [================>.............] - ETA: 2s    34562048/56218264 [=================>............] - ETA: 1s    36364288/56218264 [==================>...........] - ETA: 1s    38125568/56218264 [===================>..........] - ETA: 1s    39837696/56218264 [====================>.........] - ETA: 1s    41549824/56218264 [=====================>........] - ETA: 1s    43196416/56218264 [======================>.......] - ETA: 1s    44793856/56218264 [======================>.......] - ETA: 0s    46424064/56218264 [=======================>......] - ETA: 0s    48111616/56218264 [========================>.....] - ETA: 0s    49848320/56218264 [=========================>....] - ETA: 0s    51617792/56218264 [==========================>...] - ETA: 0s    53452800/56218264 [===========================>..] - ETA: 0s    55296000/56218264 [============================>.] - ETA: 0s    56221696/56218264 [==============================] - 4s 0us/step
    Model: "vgg_cifar10"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    input_2 (InputLayer)         [(None, 32, 32, 3)]       0         
    _________________________________________________________________
    conv_0 (QuantizedConv2D)     (None, 32, 32, 128)       3456      
    _________________________________________________________________
    conv_0_BN (BatchNormalizatio (None, 32, 32, 128)       512       
    _________________________________________________________________
    conv_0_relu (ActivationDiscr (None, 32, 32, 128)       0         
    _________________________________________________________________
    conv_1 (QuantizedConv2D)     (None, 32, 32, 128)       147456    
    _________________________________________________________________
    conv_1_maxpool (MaxPooling2D (None, 16, 16, 128)       0         
    _________________________________________________________________
    conv_1_BN (BatchNormalizatio (None, 16, 16, 128)       512       
    _________________________________________________________________
    conv_1_relu (ActivationDiscr (None, 16, 16, 128)       0         
    _________________________________________________________________
    conv_2 (QuantizedConv2D)     (None, 16, 16, 256)       294912    
    _________________________________________________________________
    conv_2_BN (BatchNormalizatio (None, 16, 16, 256)       1024      
    _________________________________________________________________
    conv_2_relu (ActivationDiscr (None, 16, 16, 256)       0         
    _________________________________________________________________
    conv_3 (QuantizedConv2D)     (None, 16, 16, 256)       589824    
    _________________________________________________________________
    conv_3_maxpool (MaxPooling2D (None, 8, 8, 256)         0         
    _________________________________________________________________
    conv_3_BN (BatchNormalizatio (None, 8, 8, 256)         1024      
    _________________________________________________________________
    conv_3_relu (ActivationDiscr (None, 8, 8, 256)         0         
    _________________________________________________________________
    conv_4 (QuantizedConv2D)     (None, 8, 8, 512)         1179648   
    _________________________________________________________________
    conv_4_BN (BatchNormalizatio (None, 8, 8, 512)         2048      
    _________________________________________________________________
    conv_4_relu (ActivationDiscr (None, 8, 8, 512)         0         
    _________________________________________________________________
    conv_5 (QuantizedConv2D)     (None, 8, 8, 512)         2359296   
    _________________________________________________________________
    conv_5_maxpool (MaxPooling2D (None, 4, 4, 512)         0         
    _________________________________________________________________
    conv_5_BN (BatchNormalizatio (None, 4, 4, 512)         2048      
    _________________________________________________________________
    conv_5_relu (ActivationDiscr (None, 4, 4, 512)         0         
    _________________________________________________________________
    flatten (Flatten)            (None, 8192)              0         
    _________________________________________________________________
    dense_6 (QuantizedDense)     (None, 1024)              8388608   
    _________________________________________________________________
    dense_6_BN (BatchNormalizati (None, 1024)              4096      
    _________________________________________________________________
    dense_6_relu (ActivationDisc (None, 1024)              0         
    _________________________________________________________________
    dense_7 (QuantizedDense)     (None, 1024)              1048576   
    _________________________________________________________________
    dense_7_BN (BatchNormalizati (None, 1024)              4096      
    _________________________________________________________________
    dense_7_relu (ActivationDisc (None, 1024)              0         
    _________________________________________________________________
    dense_8 (QuantizedDense)     (None, 10)                10240     
    =================================================================
    Total params: 14,037,376
    Trainable params: 14,029,696
    Non-trainable params: 7,680
    _________________________________________________________________




3.B Check performance
^^^^^^^^^^^^^^^^^^^^^

We check the Keras model accuracy on the first *n* images of the test
set.

The table below summarizes the expected results:

+---------+----------+
| #Images | Accuracy |
+=========+==========+
| 100     | 94.00 %  |
+---------+----------+
| 1000    | 90.80 %  |
+---------+----------+
| 10000   | 91.30 %  |
+---------+----------+

.. Note:: Depending on your hardware setup, the processing time may vary
          greatly.


.. code-block:: default


    num_images = 1000

    # Check Model performance
    start = timer()
    potentials_keras = model_keras.predict(x_test[:num_images])
    preds_keras = np.squeeze(np.argmax(potentials_keras, 1))

    accuracy = accuracy_score(y_test[:num_images], preds_keras)
    print("Accuracy: "+"{0:.2f}".format(100*accuracy)+"%")
    end = timer()
    print(f'Keras inference on {num_images} images took {end-start:.2f} s.\n')






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Accuracy: 91.20%
    Keras inference on 1000 images took 2.41 s.





4. Conversion to Akida
~~~~~~~~~~~~~~~~~~~~~~

4.A Convert to Akida model
^^^^^^^^^^^^^^^^^^^^^^^^^^

When converting to an Akida model, we just need to pass the Keras model
and the input scaling that was used during training.


.. code-block:: default


    # Convert the model
    model_akida = convert(model_keras, input_scaling=input_scaling)









4.B Check hardware compliancy
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The `Model.summary() <../../api_reference/aee_apis.html#akida.Model.summary>`__
method provides a detailed description of the Model layers.

It also indicates it they are hardware-compatible (see the ``HW`` third
column).


.. code-block:: default


    model_akida.summary()






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    -------------------------------------------------------------------------------------------------------------------------
    Layer (type)           HW  Input shape   Output shape  Kernel shape  Learning (#classes)       #InConn/#Weights/ThFire   
    =========================================================================================================================
    conv_0 (InputConvoluti yes [32, 32, 3]   [32, 32, 128] (3 x 3 x 3)   N/A                       27 / 16 / 0               
    -------------------------------------------------------------------------------------------------------------------------
    conv_1 (Convolutional) yes [32, 32, 128] [16, 16, 128] (3 x 3 x 128) N/A                       1152 / 626 / 0            
    -------------------------------------------------------------------------------------------------------------------------
    conv_2 (Convolutional) yes [16, 16, 128] [16, 16, 256] (3 x 3 x 128) N/A                       1152 / 669 / 0            
    -------------------------------------------------------------------------------------------------------------------------
    conv_3 (Convolutional) yes [16, 16, 256] [8, 8, 256]   (3 x 3 x 256) N/A                       2304 / 1371 / 0           
    -------------------------------------------------------------------------------------------------------------------------
    conv_4 (Convolutional) yes [8, 8, 256]   [8, 8, 512]   (3 x 3 x 256) N/A                       2304 / 1391 / 0           
    -------------------------------------------------------------------------------------------------------------------------
    conv_5 (Convolutional) yes [8, 8, 512]   [4, 4, 512]   (3 x 3 x 512) N/A                       4608 / 2826 / 0           
    -------------------------------------------------------------------------------------------------------------------------
    dense_6 (FullyConnecte yes [4, 4, 512]   [1, 1, 1024]  N/A           N/A                       8192 / 4977 / 0           
    -------------------------------------------------------------------------------------------------------------------------
    dense_7 (FullyConnecte yes [1, 1, 1024]  [1, 1, 1024]  N/A           N/A                       1024 / 604 / 0            
    -------------------------------------------------------------------------------------------------------------------------
    dense_8 (FullyConnecte yes [1, 1, 1024]  [1, 1, 10]    N/A           N/A                       1024 / 500 / 0            
    -------------------------------------------------------------------------------------------------------------------------




4.C Check performance
^^^^^^^^^^^^^^^^^^^^^

We check the Akida model accuracy on the first *n* images of the test
set.

The table below summarizes the expected results:

+---------+----------+
| #Images | Accuracy |
+=========+==========+
| 100     | 95.00 %  |
+---------+----------+
| 1000    | 91.90 %  |
+---------+----------+
| 10000   | 91.59 %  |
+---------+----------+

Due to the conversion process, the predictions may be slightly different
between the original Keras model and Akida on some specific images.

This explains why when testing on a limited number of images the
accuracy numbers between Keras and Akida may be quite different. On the
full test set however, the two models accuracies are almost identical.

 .. Note:: Depending on your hardware setup, the processing time may vary
           greatly.


.. code-block:: default


    num_images = 1000

    # Check Model performance
    start = timer()
    results = model_akida.predict(raw_x_test[:num_images])
    accuracy = accuracy_score(y_test[:num_images], results)

    print("Accuracy: "+"{0:.2f}".format(100*accuracy)+"%")
    end = timer()
    print(f'Akida inference on {num_images} images took {end-start:.2f} s.\n')

    # For non-regression purpose
    if num_images == 1000:
        assert accuracy == 0.919





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Accuracy: 91.90%
    Akida inference on 1000 images took 3.49 s.






.. code-block:: default


    # Print model statistics
    print("Model statistics")
    stats = model_akida.get_statistics()
    model_akida.predict(raw_x_test[:20])
    for _, stat in stats.items():
        print(stat)






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Model statistics
    Layer (type)                  output sparsity     
    conv_0 (InputConvolutional)   0.75                
    Layer (type)                  input sparsity      output sparsity     ops                 
    conv_1 (Convolutional)        0.75                0.75                37861690            
    Layer (type)                  input sparsity      output sparsity     ops                 
    conv_2 (Convolutional)        0.75                0.79                19215590            
    Layer (type)                  input sparsity      output sparsity     ops                 
    conv_3 (Convolutional)        0.79                0.83                31943232            
    Layer (type)                  input sparsity      output sparsity     ops                 
    conv_4 (Convolutional)        0.83                0.89                12716237            
    Layer (type)                  input sparsity      output sparsity     ops                 
    conv_5 (Convolutional)        0.89                0.89                17111347            
    Layer (type)                  input sparsity      output sparsity     ops                 
    dense_6 (FullyConnected)      0.89                0.87                960154              
    Layer (type)                  input sparsity      output sparsity     ops                 
    dense_7 (FullyConnected)      0.87                0.87                139622              
    Layer (type)                  input sparsity      output sparsity     ops                 
    dense_8 (FullyConnected)      0.87                0.00                1345                




5. Create a quantized Keras MobileNet model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A Keras model based on the
`MobileNets <https://arxiv.org/abs/1704.04861>`__ architecture is
instantiated with quantized weights and activations.

This model relies on a first Convolutional layer followed by several
Separable Convolutional layers:

  * all the layers have 4-bit weights,
  * all the layers have 4-bit activations.

This model therefore satisfies the Akida NSoC requirements.

This section goes as follows:

  * **5.A - Instantiate a quantized Keras model** according to above
    specifications
  * **5.B - Check performance** on the test set.

5.A Instantiate Keras MobileNet model
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The ``mobilenet_cifar10`` function returns a MobileNet Keras model with
custom quantized layers (see ``quantization_layers.py`` in the CNN2SNN
module).

 .. Note:: The pre-trained weights which are loaded in the section 3.B
           corresponds to the quantization parameters in the next cell. If you
           want to modify some of these parameters, you must re-train the
           model and save the weights.

Pre-trained weights were obtained after two training episodes:

  * first, we train the model with unconstrained float weights and
    activations for 1000 epochs,
  * then, we tune the model with quantized weights initialized from those
    trained in the previous episode.

We stop the second training episode when the training loss has stopped
decreasing for 20 epochs.

The table below summarizes the results obtained when preparing the
weights stored under ``http://data.brainchip.com/models/mobilenet/``:

+---------+----------------+---------------+----------+--------+
| Episode | Weights Quant. | Activ. Quant. | Accuracy | Epochs |
+=========+================+===============+==========+========+
| 1       | N/A            | N/A           | 93.49 %  | 1000   |
+---------+----------------+---------------+----------+--------+
| 2       | 4 bits         | 4 bits        | 93.07 %  | 44     |
+---------+----------------+---------------+----------+--------+

Please refer to `mnist_cnn2akida_demo example <mnist_cnn2akida_demo.html>`__
and/or the `CNN2SNN toolkit <../../api_reference/cnn2snn_apis.html>`__
documentation for flow and training steps details.


.. code-block:: default


    # Use a quantized model with pretrained quantized weights (93.07% accuracy)
    model_keras = mobilenet_cifar10(input_shape,
                                    weights='cifar10',
                                    weights_quantization=4,
                                    activ_quantization=4,
                                    input_weights_quantization=8)
    model_keras.summary()






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Downloading data from http://data.brainchip.com/models/mobilenet/mobilenet_cifar10_wq4_aq4.hdf5
        8192/10815304 [..............................] - ETA: 21s       81920/10815304 [..............................] - ETA: 8s       655360/10815304 [>.............................] - ETA: 1s     2195456/10815304 [=====>........................] - ETA: 0s     5226496/10815304 [=============>................] - ETA: 0s     7479296/10815304 [===================>..........] - ETA: 0s     9125888/10815304 [========================>.....] - ETA: 0s    10821632/10815304 [==============================] - 0s 0us/step
    Model: "mobilenet_cifar10"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    input_3 (InputLayer)         [(None, 32, 32, 3)]       0         
    _________________________________________________________________
    conv_0 (QuantizedConv2D)     (None, 32, 32, 128)       3456      
    _________________________________________________________________
    conv_0_BN (BatchNormalizatio (None, 32, 32, 128)       512       
    _________________________________________________________________
    conv_0_relu (ActivationDiscr (None, 32, 32, 128)       0         
    _________________________________________________________________
    separable_1 (QuantizedSepara (None, 32, 32, 128)       17536     
    _________________________________________________________________
    separable_1_BN (BatchNormali (None, 32, 32, 128)       512       
    _________________________________________________________________
    separable_1_relu (Activation (None, 32, 32, 128)       0         
    _________________________________________________________________
    separable_2 (QuantizedSepara (None, 32, 32, 256)       33920     
    _________________________________________________________________
    separable_2_BN (BatchNormali (None, 32, 32, 256)       1024      
    _________________________________________________________________
    separable_2_relu (Activation (None, 32, 32, 256)       0         
    _________________________________________________________________
    separable_3 (QuantizedSepara (None, 32, 32, 256)       67840     
    _________________________________________________________________
    separable_3_maxpool (MaxPool (None, 16, 16, 256)       0         
    _________________________________________________________________
    separable_3_BN (BatchNormali (None, 16, 16, 256)       1024      
    _________________________________________________________________
    separable_3_relu (Activation (None, 16, 16, 256)       0         
    _________________________________________________________________
    separable_4 (QuantizedSepara (None, 16, 16, 512)       133376    
    _________________________________________________________________
    separable_4_BN (BatchNormali (None, 16, 16, 512)       2048      
    _________________________________________________________________
    separable_4_relu (Activation (None, 16, 16, 512)       0         
    _________________________________________________________________
    separable_5 (QuantizedSepara (None, 16, 16, 512)       266752    
    _________________________________________________________________
    separable_5_maxpool (MaxPool (None, 8, 8, 512)         0         
    _________________________________________________________________
    separable_5_BN (BatchNormali (None, 8, 8, 512)         2048      
    _________________________________________________________________
    separable_5_relu (Activation (None, 8, 8, 512)         0         
    _________________________________________________________________
    separable_6 (QuantizedSepara (None, 8, 8, 512)         266752    
    _________________________________________________________________
    separable_6_BN (BatchNormali (None, 8, 8, 512)         2048      
    _________________________________________________________________
    separable_6_relu (Activation (None, 8, 8, 512)         0         
    _________________________________________________________________
    separable_7 (QuantizedSepara (None, 8, 8, 512)         266752    
    _________________________________________________________________
    separable_7_maxpool (MaxPool (None, 4, 4, 512)         0         
    _________________________________________________________________
    separable_7_BN (BatchNormali (None, 4, 4, 512)         2048      
    _________________________________________________________________
    separable_7_relu (Activation (None, 4, 4, 512)         0         
    _________________________________________________________________
    separable_8 (QuantizedSepara (None, 4, 4, 1024)        528896    
    _________________________________________________________________
    separable_8_BN (BatchNormali (None, 4, 4, 1024)        4096      
    _________________________________________________________________
    separable_8_relu (Activation (None, 4, 4, 1024)        0         
    _________________________________________________________________
    separable_9 (QuantizedSepara (None, 4, 4, 1024)        1057792   
    _________________________________________________________________
    separable_9_BN (BatchNormali (None, 4, 4, 1024)        4096      
    _________________________________________________________________
    separable_9_relu (Activation (None, 4, 4, 1024)        0         
    _________________________________________________________________
    separable_10 (QuantizedSepar (None, 4, 4, 10)          19456     
    _________________________________________________________________
    separable_10_global_avg (Glo (None, 10)                0         
    =================================================================
    Total params: 2,681,984
    Trainable params: 2,672,256
    Non-trainable params: 9,728
    _________________________________________________________________




5.B Check performance
^^^^^^^^^^^^^^^^^^^^^

We check the Keras model accuracy on the first *n* images of the test
set.

The table below summarizes the expected results:

+---------+----------+
| #Images | Accuracy |
+=========+==========+
| 100     | 95.00 %  |
+---------+----------+
| 1000    | 93.10 %  |
+---------+----------+
| 10000   | 93.07 %  |
+---------+----------+

.. Note:: Depending on your hardware setup, the processing time may vary
          greatly.


.. code-block:: default


    num_images = 1000

    # Check Model performance
    start = timer()
    potentials_keras = model_keras.predict(x_test[:num_images])
    preds_keras = np.squeeze(np.argmax(potentials_keras, 1))

    accuracy = accuracy_score(y_test[:num_images], preds_keras)
    print("Accuracy: "+"{0:.2f}".format(100*accuracy)+"%")
    end = timer()
    print(f'Keras inference on {num_images} images took {end-start:.2f} s.\n')






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Accuracy: 93.00%
    Keras inference on 1000 images took 2.91 s.





6. Conversion to Akida
~~~~~~~~~~~~~~~~~~~~~~

6.A Convert to Akida model
^^^^^^^^^^^^^^^^^^^^^^^^^^

When converting to an Akida model, we just need to pass the Keras model
and the input scaling that was used during training.


.. code-block:: default


    model_akida = convert(model_keras, input_scaling=input_scaling)









6.B Check hardware compliancy
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The `Model.summary() <../../api_reference/aee_apis.html#akida.Model.summary>`__
method provides a detailed description of the Model layers.

It also indicates it they are hardware-compatible (see the ``HW`` third
column).


.. code-block:: default


    model_akida.summary()






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    -------------------------------------------------------------------------------------------------------------------------
    Layer (type)           HW  Input shape   Output shape  Kernel shape  Learning (#classes)       #InConn/#Weights/ThFire   
    =========================================================================================================================
    conv_0 (InputConvoluti yes [32, 32, 3]   [32, 32, 128] (3 x 3 x 3)   N/A                       27 / 26 / 0               
    -------------------------------------------------------------------------------------------------------------------------
    separable_1 (Separable yes [32, 32, 128] [32, 32, 128] (3 x 3 x 128) N/A                       1152 / 55 / 0             
    -------------------------------------------------------------------------------------------------------------------------
    separable_2 (Separable yes [32, 32, 128] [32, 32, 256] (3 x 3 x 128) N/A                       1152 / 73 / 0             
    -------------------------------------------------------------------------------------------------------------------------
    separable_3 (Separable yes [32, 32, 256] [16, 16, 256] (3 x 3 x 256) N/A                       2304 / 108 / 0            
    -------------------------------------------------------------------------------------------------------------------------
    separable_4 (Separable yes [16, 16, 256] [16, 16, 512] (3 x 3 x 256) N/A                       2304 / 143 / 0            
    -------------------------------------------------------------------------------------------------------------------------
    separable_5 (Separable yes [16, 16, 512] [8, 8, 512]   (3 x 3 x 512) N/A                       4608 / 215 / 0            
    -------------------------------------------------------------------------------------------------------------------------
    separable_6 (Separable yes [8, 8, 512]   [8, 8, 512]   (3 x 3 x 512) N/A                       4608 / 216 / 0            
    -------------------------------------------------------------------------------------------------------------------------
    separable_7 (Separable yes [8, 8, 512]   [4, 4, 512]   (3 x 3 x 512) N/A                       4608 / 216 / 0            
    -------------------------------------------------------------------------------------------------------------------------
    separable_8 (Separable yes [4, 4, 512]   [4, 4, 1024]  (3 x 3 x 512) N/A                       4608 / 285 / 0            
    -------------------------------------------------------------------------------------------------------------------------
    separable_9 (Separable yes [4, 4, 1024]  [4, 4, 1024]  (3 x 3 x 1024 N/A                       9216 / 419 / 0            
    -------------------------------------------------------------------------------------------------------------------------
    separable_10 (Separabl yes [4, 4, 1024]  [1, 1, 10]    (3 x 3 x 1024 N/A                       9216 / 14 / 0             
    -------------------------------------------------------------------------------------------------------------------------




6.C Check performance
^^^^^^^^^^^^^^^^^^^^^

We check the Akida model accuracy on the first *n* images of the test
set.

The table below summarizes the expected results:

+---------+----------+
| #Images | Accuracy |
+=========+==========+
| 100     | 95.00 %  |
+---------+----------+
| 1000    | 93.10 %  |
+---------+----------+
| 10000   | 93.22 %  |
+---------+----------+

Due to the conversion process, the predictions may be slightly different
between the original Keras model and Akida on some specific images.

This explains why when testing on a limited number of images the
accuracy numbers between Keras and Akida may be quite different. On the
full test set however, the two models accuracies are almost identical.

 .. Note:: Depending on your hardware setup, the processing time may vary
           greatly.


.. code-block:: default


    num_images = 1000

    # Check Model performance
    start = timer()
    results = model_akida.predict(raw_x_test[:num_images])
    accuracy = accuracy_score(y_test[:num_images], results)

    print("Accuracy: "+"{0:.2f}".format(100*accuracy)+"%")
    end = timer()
    print(f'Akida inference on {num_images} images took {end-start:.2f} s.\n')

    # For non-regression purpose
    if num_images == 1000:
        assert accuracy == 0.931





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Accuracy: 93.10%
    Akida inference on 1000 images took 6.79 s.






.. code-block:: default


    # Print model statistics
    print("Model statistics")
    stats = model_akida.get_statistics()
    model_akida.predict(raw_x_test[:20])
    for _, stat in stats.items():
        print(stat)






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Model statistics
    Layer (type)                  output sparsity     
    conv_0 (InputConvolutional)   0.59                
    Layer (type)                  input sparsity      output sparsity     ops                 
    separable_1 (SeparableConvolu 0.59                0.53                62663175            
    Layer (type)                  input sparsity      output sparsity     ops                 
    separable_2 (SeparableConvolu 0.53                0.54                143484989           
    Layer (type)                  input sparsity      output sparsity     ops                 
    separable_3 (SeparableConvolu 0.54                0.61                279008748           
    Layer (type)                  input sparsity      output sparsity     ops                 
    separable_4 (SeparableConvolu 0.61                0.65                118130331           
    Layer (type)                  input sparsity      output sparsity     ops                 
    separable_5 (SeparableConvolu 0.65                0.70                214518748           
    Layer (type)                  input sparsity      output sparsity     ops                 
    separable_6 (SeparableConvolu 0.70                0.68                44972119            
    Layer (type)                  input sparsity      output sparsity     ops                 
    separable_7 (SeparableConvolu 0.68                0.75                48254114            
    Layer (type)                  input sparsity      output sparsity     ops                 
    separable_8 (SeparableConvolu 0.75                0.84                18696769            
    Layer (type)                  input sparsity      output sparsity     ops                 
    separable_9 (SeparableConvolu 0.84                0.84                24647816            
    Layer (type)                  input sparsity      output sparsity     ops                 
    separable_10 (SeparableConvol 0.84                0.00                260459              




6D. Show predictions for a random image
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^



.. code-block:: default


    import matplotlib.pyplot as plt
    import matplotlib.lines as lines
    import matplotlib.patches as patches

    label_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']

    # prepare plot
    barWidth = 0.75
    pause_time = 1

    fig = plt.figure(num='CIFAR10 Classification by Akida Execution Engine', figsize=(8, 4))
    ax0 = plt.subplot(1, 3, 1)
    imgobj = ax0.imshow(np.zeros((32, 32, 3), dtype=np.uint8))
    ax0.set_axis_off()
    # Results subplots
    ax1 = plt.subplot(1, 2, 2)
    ax1.xaxis.set_visible(False)
    ax0.text(0, 34, 'Actual class:')
    actual_class = ax0.text(16, 34, 'None')
    ax0.text(0, 37, 'Predicted class:')
    predicted_class = ax0.text(20, 37, 'None')

    # Take a random test image
    i = np.random.randint(y_test.shape[0])

    true_idx = int(y_test[i])
    pot =  model_akida.evaluate(np.expand_dims(raw_x_test[i], axis=0)).squeeze()

    rpot = np.arange(len(pot))
    ax1.barh(rpot, pot, height=barWidth)
    ax1.set_yticks(rpot - 0.07*barWidth)
    ax1.set_yticklabels(label_names)
    predicted_idx = pot.argmax()
    imgobj.set_data(raw_x_test[i])
    if predicted_idx == true_idx:
        ax1.get_children()[predicted_idx].set_color('g')
    else:
        ax1.get_children()[predicted_idx].set_color('r')
    actual_class.set_text(label_names[true_idx])
    predicted_class.set_text(label_names[predicted_idx])
    ax1.set_title('Akida\'s predictions')
    plt.show()



.. image:: /examples/cnn2snn/images/sphx_glr_plot_cifar10_cnn2akida_demo_001.png
    :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  33.642 seconds)


.. _sphx_glr_download_examples_cnn2snn_plot_cifar10_cnn2akida_demo.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download

     :download:`Download Python source code: plot_cifar10_cnn2akida_demo.py <plot_cifar10_cnn2akida_demo.py>`



  .. container:: sphx-glr-download

     :download:`Download Jupyter notebook: plot_cifar10_cnn2akida_demo.ipynb <plot_cifar10_cnn2akida_demo.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
