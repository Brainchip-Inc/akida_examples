
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "examples/general/plot_7_global_pytorch_workflow.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_examples_general_plot_7_global_pytorch_workflow.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_examples_general_plot_7_global_pytorch_workflow.py:


PyTorch to Akida workflow
=========================

The `Global Akida workflow <../general/plot_0_global_workflow.html>`__ guide
describes the steps to prepare a model for Akida starting from a TF-Keras model.
Here we will instead describe a workflow to go from a model trained in PyTorch.

.. Note::
   | This example targets users who already have a PyTorch training pipeline
     in place, and a trained model: this workflow will allow you to rapidly convert
     your model to Akida 2.0.
   | Note however that this pathway offers slightly less flexibility than our default,
     TensorFlow-based pathway - specifically, fine tuning of the quantized model is
     not possible when starting from PyTorch.
   | In most cases, that won't matter, there should be almost no performance drop when
     quantizing to 8-bit anyway.
   | However, advanced users interested in further optimization of the original model
     (going to 4-bit quantization for example) or users who don't yet have a
     training pipeline in place may prefer the extra options afforded by our default,
     TensorFlow-based `Global Akida workflow <../general/plot_0_global_workflow.html>`__.


QuantizeML natively allows the quantization and fine-tuning of TensorFlow models. While
it does not support PyTorch quantization natively, it allows to quantize float models
stored in the `Open Neural Network eXchange (ONNX) <https://onnx.ai>`__ format. Export
from PyTorch to ONNX is well supported, and so this provides a straightforward pathway to
prepare your PyTorch model for Akida.

As a concrete example, we will prepare a PyTorch model on a simple classification task
(MNIST). This model will then be exported to ONNX and quantized to 8-bit using QuantizeML.
The quantized model is then converted to Akida, and performance evaluated to show that
there has been no loss in accuracy.

Please refer to the `Akida user guide <../../user_guide/akida.html>`__ for further information.

.. Note::
   | This example is loosely based on the PyTorch `Training a Classifier
     <https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html>`__ tutorial and
     does not aim to describe PyTorch training in detail. We assume that if you are following
     this example, it's because you already have a trained PyTorch model.
   | `PyTorch 2.7.0 <https://github.com/pytorch/pytorch/releases/tag/v2.7.0>`__ is used
     for this example.

     .. code-block::

        pip install torch==2.7.0 torchvision

.. Warning::
   | The MNIST example below is light enough to train on the CPU only.
   | However, where GPU acceleration is desirable for the PyTorch training step, you may find
     it simpler to use separate virtual environments for the PyTorch-dependent sections
     (`1. Create and train`_ and `2. Export`_) vs the TensorFlow-dependent sections
     (`3. Quantize`_ and `4. Convert`_).


.. figure:: ../../img/overall_onnx_flow.png
   :target: ../../_images/overall_onnx_flow.png
   :alt: Overall pytorch flow
   :scale: 60 %
   :align: center

   PyTorch Akida workflow

.. GENERATED FROM PYTHON SOURCE LINES 67-70

1. Create and train
~~~~~~~~~~~~~~~~~~~


.. GENERATED FROM PYTHON SOURCE LINES 72-75

1.1. Load and normalize MNIST dataset
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^


.. GENERATED FROM PYTHON SOURCE LINES 75-114

.. code-block:: Python


    import torch
    import torchvision
    import torchvision.transforms as transforms

    import matplotlib.pyplot as plt

    batch_size = 128


    def get_dataloader(train, batch_size, num_workers=2):
        transform = transforms.Compose([transforms.ToTensor(),
                                        transforms.Normalize(0.5, 0.5)])
        dataset = torchvision.datasets.MNIST(root='datasets/mnist',
                                             train=train,
                                             download=True,
                                             transform=transform)
        return torch.utils.data.DataLoader(dataset,
                                           batch_size=batch_size,
                                           shuffle=train,
                                           num_workers=num_workers)


    # Load MNIST dataset and normalize between [-1, 1]
    trainloader = get_dataloader(train=True, batch_size=batch_size)
    testloader = get_dataloader(train=False, batch_size=batch_size)


    def imshow(img):
        # Unnormalize
        img = img / 2 + 0.5
        npimg = img.numpy()
        plt.imshow(npimg.transpose((1, 2, 0)))
        plt.show()


    # Get some random training images
    images, labels = next(iter(trainloader))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0.00/9.91M [00:00<?, ?B/s]      1%|          | 65.5k/9.91M [00:00<00:27, 358kB/s]      3%|▎         | 262k/9.91M [00:00<00:12, 783kB/s]       7%|▋         | 721k/9.91M [00:00<00:04, 1.98MB/s]     16%|█▌        | 1.57M/9.91M [00:00<00:02, 4.00MB/s]     22%|██▏       | 2.16M/9.91M [00:00<00:02, 3.80MB/s]     54%|█████▎    | 5.31M/9.91M [00:00<00:00, 11.2MB/s]     71%|███████   | 7.05M/9.91M [00:00<00:00, 12.9MB/s]     89%|████████▉ | 8.81M/9.91M [00:01<00:00, 14.1MB/s]    100%|██████████| 9.91M/9.91M [00:01<00:00, 8.89MB/s]
      0%|          | 0.00/28.9k [00:00<?, ?B/s]    100%|██████████| 28.9k/28.9k [00:00<00:00, 316kB/s]
      0%|          | 0.00/1.65M [00:00<?, ?B/s]      6%|▌         | 98.3k/1.65M [00:00<00:02, 534kB/s]     24%|██▍       | 393k/1.65M [00:00<00:01, 1.18MB/s]     60%|█████▉    | 983k/1.65M [00:00<00:00, 2.67MB/s]    100%|██████████| 1.65M/1.65M [00:00<00:00, 3.01MB/s]
      0%|          | 0.00/4.54k [00:00<?, ?B/s]    100%|██████████| 4.54k/4.54k [00:00<00:00, 6.62MB/s]




.. GENERATED FROM PYTHON SOURCE LINES 115-120

.. code-block:: Python


    # Show images and labels
    imshow(torchvision.utils.make_grid(images, nrow=8))
    print("Labels:\n", labels.reshape((-1, 8)))




.. image-sg:: /examples/general/images/sphx_glr_plot_7_global_pytorch_workflow_001.png
   :alt: plot 7 global pytorch workflow
   :srcset: /examples/general/images/sphx_glr_plot_7_global_pytorch_workflow_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Labels:
     tensor([[4, 7, 9, 4, 7, 2, 4, 7],
            [1, 4, 1, 1, 7, 2, 3, 7],
            [3, 0, 5, 8, 6, 6, 7, 4],
            [0, 9, 6, 0, 5, 9, 3, 2],
            [9, 6, 1, 7, 9, 0, 9, 6],
            [1, 9, 1, 7, 7, 5, 7, 2],
            [4, 0, 5, 8, 3, 9, 1, 0],
            [0, 6, 4, 7, 8, 5, 9, 5],
            [4, 2, 7, 1, 7, 1, 6, 6],
            [3, 6, 6, 1, 4, 5, 8, 2],
            [2, 4, 5, 8, 9, 4, 1, 4],
            [3, 4, 6, 1, 1, 0, 5, 4],
            [2, 0, 9, 0, 1, 5, 5, 5],
            [2, 6, 0, 4, 6, 9, 1, 3],
            [8, 3, 2, 3, 9, 6, 1, 8],
            [1, 5, 3, 2, 8, 1, 5, 1]])




.. GENERATED FROM PYTHON SOURCE LINES 121-129

1.2. Model definition
^^^^^^^^^^^^^^^^^^^^^

Note that at this stage, there is nothing specific to the Akida IP.
The model constructed below uses the `torch.nn.Sequential
<https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html#nn-sequential>`__
module to define a standard CNN.


.. GENERATED FROM PYTHON SOURCE LINES 129-143

.. code-block:: Python


    model_torch = torch.nn.Sequential(torch.nn.Conv2d(1, 32, 5, padding=(2, 2)),
                                      torch.nn.ReLU6(),
                                      torch.nn.MaxPool2d(kernel_size=2),
                                      torch.nn.Conv2d(32, 64, 3, stride=2),
                                      torch.nn.ReLU(),
                                      torch.nn.Dropout(0.25),
                                      torch.nn.Flatten(),
                                      torch.nn.Linear(2304, 512),
                                      torch.nn.ReLU(),
                                      torch.nn.Dropout(0.5),
                                      torch.nn.Linear(512, 10))
    print(model_torch)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Sequential(
      (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
      (1): ReLU6()
      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2))
      (4): ReLU()
      (5): Dropout(p=0.25, inplace=False)
      (6): Flatten(start_dim=1, end_dim=-1)
      (7): Linear(in_features=2304, out_features=512, bias=True)
      (8): ReLU()
      (9): Dropout(p=0.5, inplace=False)
      (10): Linear(in_features=512, out_features=10, bias=True)
    )




.. GENERATED FROM PYTHON SOURCE LINES 144-147

1.3. Model training
^^^^^^^^^^^^^^^^^^^


.. GENERATED FROM PYTHON SOURCE LINES 147-176

.. code-block:: Python


    # Define training rules
    optimizer = torch.optim.Adam(model_torch.parameters(), lr=1e-4)
    criterion = torch.nn.CrossEntropyLoss()
    epochs = 10

    # Loop over the dataset multiple times
    for epoch in range(epochs):
        running_loss = 0.0
        for i, data in enumerate(trainloader, 0):
            # Get the inputs and labels
            inputs, labels = data

            # Zero the parameter gradients
            optimizer.zero_grad()

            # Forward + Backward + Optimize
            outputs = model_torch(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            # Print statistics
            running_loss += loss.detach().item()
            if (i + 1) % 100 == 0:
                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')
                running_loss = 0.0






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [1,   100] loss: 0.072
    [1,   200] loss: 0.023
    [1,   300] loss: 0.017
    [1,   400] loss: 0.014
    [2,   100] loss: 0.011
    [2,   200] loss: 0.009
    [2,   300] loss: 0.008
    [2,   400] loss: 0.007
    [3,   100] loss: 0.006
    [3,   200] loss: 0.006
    [3,   300] loss: 0.006
    [3,   400] loss: 0.006
    [4,   100] loss: 0.005
    [4,   200] loss: 0.005
    [4,   300] loss: 0.005
    [4,   400] loss: 0.005
    [5,   100] loss: 0.004
    [5,   200] loss: 0.004
    [5,   300] loss: 0.004
    [5,   400] loss: 0.004
    [6,   100] loss: 0.004
    [6,   200] loss: 0.004
    [6,   300] loss: 0.004
    [6,   400] loss: 0.003
    [7,   100] loss: 0.003
    [7,   200] loss: 0.003
    [7,   300] loss: 0.003
    [7,   400] loss: 0.003
    [8,   100] loss: 0.003
    [8,   200] loss: 0.003
    [8,   300] loss: 0.003
    [8,   400] loss: 0.003
    [9,   100] loss: 0.003
    [9,   200] loss: 0.003
    [9,   300] loss: 0.003
    [9,   400] loss: 0.003
    [10,   100] loss: 0.003
    [10,   200] loss: 0.002
    [10,   300] loss: 0.003
    [10,   400] loss: 0.002




.. GENERATED FROM PYTHON SOURCE LINES 177-182

1.4. Model testing
^^^^^^^^^^^^^^^^^^

Evaluate the model performance on the test set. It should achieve an accuracy over 98%.


.. GENERATED FROM PYTHON SOURCE LINES 182-198

.. code-block:: Python


    correct = 0
    total = 0
    with torch.no_grad():
        for data in testloader:
            inputs, labels = data
            # Calculate outputs by running images through the network
            outputs = model_torch(inputs)
            # The class with the highest score is the prediction
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    assert correct / total >= 0.98
    print(f'Test accuracy: {100 * correct // total} %')





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Test accuracy: 98 %




.. GENERATED FROM PYTHON SOURCE LINES 199-209

2. Export
~~~~~~~~~

PyTorch models are not directly compatible with the `QuantizeML quantization
tool <../../api_reference/quantizeml_apis.html>`__, it is therefore necessary
to use an intermediate format. Like many other machine learning frameworks,
PyTorch has tools to export modules in the `ONNX <https://onnx.ai>`__ format.

Therefore, the model is exported by the following code:


.. GENERATED FROM PYTHON SOURCE LINES 209-218

.. code-block:: Python


    sample, _ = next(iter(trainloader))
    torch.onnx.export(model_torch,
                      sample,
                      f="mnist_cnn.onnx",
                      input_names=["inputs"],
                      output_names=["outputs"],
                      dynamic_axes={'inputs': {0: 'batch_size'}, 'outputs': {0: 'batch_size'}})








.. GENERATED FROM PYTHON SOURCE LINES 219-223

.. Note::
 Find more information about how to export PyTorch models in ONNX at
 `<https://pytorch.org/docs/stable/onnx.html>`_.


.. GENERATED FROM PYTHON SOURCE LINES 225-246

3. Quantize
~~~~~~~~~~~

An Akida accelerator processes integer activations and weights. Therefore, the floating
point model must be quantized in preparation to run on an Akida accelerator.

The `QuantizeML quantize() <../../api_reference/quantizeml_apis.html#quantizeml.models.quantize>`__
function recognizes `ModelProto <https://onnx.ai/onnx/api/classes.html#modelproto>`__ objects
and can quantize them for Akida. The result is another ``ModelProto``, compatible with the
`CNN2SNN Toolkit <../../user_guide/cnn2snn.html>`__.

.. Warning::
 ONNX and PyTorch offer their own quantization methods. You should not use those when preparing
 your model for Akida. Only the `QuantizeML quantize()
 <../../api_reference/quantizeml_apis.html#quantizeml.models.quantize>`__ function
 can be used to generate a quantized model ready for conversion to Akida.

.. Note::
 For this simple model, using random samples for calibration is sufficient, as
 shown in the following steps.


.. GENERATED FROM PYTHON SOURCE LINES 246-260

.. code-block:: Python


    import onnx
    from quantizeml.models import quantize

    # Read the exported ONNX model
    model_onnx = onnx.load_model("mnist_cnn.onnx")

    # Extract a batch of train samples for calibration
    calib_samples = next(iter(trainloader))[0].numpy()

    # Quantize
    model_quantized = quantize(model_onnx, samples=calib_samples)
    print(onnx.helper.printable_graph(model_quantized.graph))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Applied 1 of general pattern rewrite rules.
    Calibrating with 128/128.0 samples
    /drone/src/examples/general/plot_7_global_pytorch_workflow.py:258: DeprecationWarning: Deprecated since 1.19. Consider using onnx.printer.to_text() instead.
      print(onnx.helper.printable_graph(model_quantized.graph))
    graph main_graph (
      %inputs[FLOAT, Nx1x28x28]
    ) initializers (
      %quantize_scale[FLOAT, 1]
      %quantize_zp[UINT8, 1]
      %/0/Conv_Xpad[UINT8, 1]
      %/0/Conv_Wi[INT8, 32x1x5x5]
      %/0/Conv_B[INT32, 32]
      %/0/Conv_pads[INT64, 8]
      %/0/Conv_max_value[INT32, 1x32x1x1]
      %/0/Conv_M[UINT8, 1x32x1x1]
      %/0/Conv_S_out[FLOAT, 1x32x1x1]
      %/3/Conv_Wi[INT8, 64x32x3x3]
      %/3/Conv_B[INT32, 64]
      %/3/Conv_pads[INT64, 8]
      %/3/Conv_M[UINT8, 1x64x1x1]
      %/3/Conv_S_out[FLOAT, 1x64x1x1]
      %/7/Gemm_Wi[INT8, 512x2304]
      %/7/Gemm_B[INT32, 512]
      %/7/Gemm_M[UINT8, 1x512]
      %/7/Gemm_S_out[FLOAT, 1x512]
      %/10/Gemm_Wi[INT8, 10x512]
      %/10/Gemm_B[INT32, 10]
      %dequantizer_scale[FLOAT, 10]
    ) {
      %quantize = InputQuantizer[perm = [0, 1, 2, 3], scale = <Tensor>](%inputs, %quantize_scale, %quantize_zp)
      %/2/MaxPool_output_0 = QuantizedInputConv2DBiasedMaxPoolReLUClippedScaled[pool_pads = [0, 0, 0, 0], pool_size = [2, 2], pool_strides = [2, 2], scale = <Tensor>, strides = [1, 1]](%quantize, %/0/Conv_Xpad, %/0/Conv_Wi, %/0/Conv_B, %/0/Conv_pads, %/0/Conv_max_value, %/0/Conv_M, %/0/Conv_S_out)
      %/4/Relu_output_0 = QuantizedConv2DBiasedReLUScaled[scale = <Tensor>, strides = [2, 2]](%/2/MaxPool_output_0, %/3/Conv_Wi, %/3/Conv_B, %/3/Conv_pads, %/3/Conv_M, %/3/Conv_S_out)
      %/8/Relu_output_0 = QuantizedDense1DFlattenBiasedReLUScaled[scale = <Tensor>](%/4/Relu_output_0, %/7/Gemm_Wi, %/7/Gemm_B, %/7/Gemm_M, %/7/Gemm_S_out)
      %outputs = QuantizedDense1DBiased[scale = <Tensor>](%/8/Relu_output_0, %/10/Gemm_Wi, %/10/Gemm_B)
      %outputs/dequantize = Dequantizer(%outputs, %dequantizer_scale)
      return %outputs/dequantize
    }




.. GENERATED FROM PYTHON SOURCE LINES 261-264

4. Convert
~~~~~~~~~~


.. GENERATED FROM PYTHON SOURCE LINES 266-273

4.1 Convert to Akida model
^^^^^^^^^^^^^^^^^^^^^^^^^^

The quantized model can now be converted to the native Akida format.
The `convert() <../../api_reference/cnn2snn_apis.html#cnn2snn.convert>`__
function returns a model in Akida format ready for inference.


.. GENERATED FROM PYTHON SOURCE LINES 273-279

.. code-block:: Python


    from cnn2snn import convert

    model_akida = convert(model_quantized)
    model_akida.summary()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

                    Model Summary                 
    ______________________________________________
    Input shape  Output shape  Sequences  Layers
    ==============================================
    [28, 28, 1]  [1, 1, 10]    1          5     
    ______________________________________________

    _________________________________________________________
    Layer (type)               Output shape  Kernel shape  

    =========== SW//0/Conv-dequantizer (Software) ===========

    /0/Conv (InputConv2D)      [14, 14, 32]  (5, 5, 1, 32) 
    _________________________________________________________
    /3/Conv (Conv2D)           [6, 6, 64]    (3, 3, 32, 64)
    _________________________________________________________
    /7/Gemm (Dense1D)          [1, 1, 512]   (2304, 512)   
    _________________________________________________________
    /10/Gemm (Dense1D)         [1, 1, 10]    (512, 10)     
    _________________________________________________________
    dequantizer (Dequantizer)  [1, 1, 10]    N/A           
    _________________________________________________________




.. GENERATED FROM PYTHON SOURCE LINES 280-289

4.2. Check performance
^^^^^^^^^^^^^^^^^^^^^^

Native PyTorch data must be presented in a different format to perform
the evaluation in Akida models. Specifically:

1. images must be numpy-raw, with an 8-bit unsigned integer data type and
2. the channel dimension must be in the last dimension.


.. GENERATED FROM PYTHON SOURCE LINES 289-308

.. code-block:: Python


    import numpy as np

    # Read raw data and convert it into numpy
    x_test = testloader.dataset.data.numpy()
    y_test = testloader.dataset.targets.numpy()

    # Add a channel dimension to the image sets as Akida expects 4-D inputs corresponding to
    # (num_samples, width, height, channels). Note: MNIST is a grayscale dataset and is unusual
    # in this respect - most image data already includes a channel dimension, and this step will
    # not be necessary.
    x_test = x_test[..., None]

    accuracy = model_akida.evaluate(x_test, y_test.astype(np.int32))
    print('Test accuracy after conversion:', accuracy)

    # For non-regression purposes
    assert accuracy > 0.96





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Test accuracy after conversion: 0.9901999831199646




.. GENERATED FROM PYTHON SOURCE LINES 309-315

4.3 Show predictions for a single image
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Display one of the test images, such as the first image in the aforementioned
dataset, to visualize the output of the model.


.. GENERATED FROM PYTHON SOURCE LINES 315-324

.. code-block:: Python


    # Test a single example
    sample_image = 0
    image = x_test[sample_image]
    outputs = model_akida.predict(image.reshape(1, 28, 28, 1))

    plt.imshow(x_test[sample_image].reshape((28, 28)), cmap="Greys")
    print('Input Label:', y_test[sample_image].item())
    print('Prediction Label:', outputs.squeeze().argmax())



.. image-sg:: /examples/general/images/sphx_glr_plot_7_global_pytorch_workflow_002.png
   :alt: plot 7 global pytorch workflow
   :srcset: /examples/general/images/sphx_glr_plot_7_global_pytorch_workflow_002.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Input Label: 7
    Prediction Label: 7





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (3 minutes 11.796 seconds)


.. _sphx_glr_download_examples_general_plot_7_global_pytorch_workflow.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_7_global_pytorch_workflow.ipynb <plot_7_global_pytorch_workflow.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_7_global_pytorch_workflow.py <plot_7_global_pytorch_workflow.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_7_global_pytorch_workflow.zip <plot_7_global_pytorch_workflow.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
