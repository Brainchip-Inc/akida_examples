
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "examples/general/plot_1_akidanet_imagenet.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_examples_general_plot_1_akidanet_imagenet.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_examples_general_plot_1_akidanet_imagenet.py:


AkidaNet/ImageNet inference
============================

This tutorial presents how to convert, map, and capture performance from AKD1000 Hardware using an
AkidaNet model.

AkidaNet architecture is a `MobileNet v1-inspired <https://arxiv.org/abs/1704.04861>`__ architecture
optimized for implementation on Akida 1.0: it exploits the richer expressive power of standard
convolutions in early layers, but uses separable convolutions in later layers where filter memory is
limiting.

As `ImageNet <https://www.image-net.org/>`__ images are not publicly available, performance is
assessed using a set of 10 copyright free images that were found on Google using ImageNet class
names.

.. Note::
    This tutorial uses an Akida 1.0 architecture to show AKD1000 mapping and performance. See the
    `dedicated tutorial <../quantization/plot_1_upgrading_to_2.0.html>`__ for 1.0 and 2.0
    differences.

.. GENERATED FROM PYTHON SOURCE LINES 24-34

1. Dataset preparation
~~~~~~~~~~~~~~~~~~~~~~

Test images all have at least 256 pixels in the smallest dimension. They must
be preprocessed to fit in the model. The ``imagenet.preprocessing.get_preprocessed_samples``
function loads and preprocesses (decodes, crops and extracts a square
224x224x3 patch from an input image) a set of 10 ImageNet-like images.

.. Note:: Input size is here set to 224x224x3 as this is what is used by the
          model presented in the next section.

.. GENERATED FROM PYTHON SOURCE LINES 34-47

.. code-block:: Python


    import akida
    import numpy as np
    from akida_models.imagenet import get_preprocessed_samples

    # Model specification and hyperparameters
    NUM_CHANNELS = 3
    IMAGE_SIZE = 224

    # Load the preprocessed images and their corresponding labels for the test set
    x_test, labels_test = get_preprocessed_samples(IMAGE_SIZE, NUM_CHANNELS)
    print(f'{x_test.shape[0]} images and their labels are loaded and preprocessed.')





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Downloading data from https://data.brainchip.com/dataset-mirror/imagenet_like/imagenet_like.zip.
           0/20418307 [..............................] - ETA: 0s        8192/20418307 [..............................] - ETA: 4:48       24576/20418307 [..............................] - ETA: 2:53       90112/20418307 [..............................] - ETA: 1:09      172032/20418307 [..............................] - ETA: 46s       385024/20418307 [..............................] - ETA: 25s      663552/20418307 [..............................] - ETA: 16s      778240/20418307 [>.............................] - ETA: 15s     1335296/20418307 [>.............................] - ETA: 9s      1482752/20418307 [=>............................] - ETA: 9s     2072576/20418307 [==>...........................] - ETA: 7s     2203648/20418307 [==>...........................] - ETA: 7s     2826240/20418307 [===>..........................] - ETA: 5s     2973696/20418307 [===>..........................] - ETA: 6s     3629056/20418307 [====>.........................] - ETA: 5s     3776512/20418307 [====>.........................] - ETA: 5s     4415488/20418307 [=====>........................] - ETA: 4s     4562944/20418307 [=====>........................] - ETA: 4s     4612096/20418307 [=====>........................] - ETA: 4s     5185536/20418307 [======>.......................] - ETA: 4s     5382144/20418307 [======>.......................] - ETA: 4s     5496832/20418307 [=======>......................] - ETA: 4s     6045696/20418307 [=======>......................] - ETA: 3s     6299648/20418307 [========>.....................] - ETA: 3s     6430720/20418307 [========>.....................] - ETA: 3s     6873088/20418307 [=========>....................] - ETA: 3s     7233536/20418307 [=========>....................] - ETA: 3s     7413760/20418307 [=========>....................] - ETA: 3s     7757824/20418307 [==========>...................] - ETA: 3s     8249344/20418307 [===========>..................] - ETA: 3s     8429568/20418307 [===========>..................] - ETA: 3s     8445952/20418307 [===========>..................] - ETA: 3s     8806400/20418307 [===========>..................] - ETA: 2s     9297920/20418307 [============>.................] - ETA: 2s     9363456/20418307 [============>.................] - ETA: 2s     9527296/20418307 [============>.................] - ETA: 2s     9871360/20418307 [=============>................] - ETA: 2s    10395648/20418307 [==============>...............] - ETA: 2s    10477568/20418307 [==============>...............] - ETA: 2s    10657792/20418307 [==============>...............] - ETA: 2s    11001856/20418307 [===============>..............] - ETA: 2s    11542528/20418307 [===============>..............] - ETA: 2s    11657216/20418307 [================>.............] - ETA: 2s    11837440/20418307 [================>.............] - ETA: 2s    12197888/20418307 [================>.............] - ETA: 2s    12754944/20418307 [=================>............] - ETA: 1s    12902400/20418307 [=================>............] - ETA: 1s    13082624/20418307 [==================>...........] - ETA: 1s    13410304/20418307 [==================>...........] - ETA: 1s    14049280/20418307 [===================>..........] - ETA: 1s    14213120/20418307 [===================>..........] - ETA: 1s    14393344/20418307 [====================>.........] - ETA: 1s    14852096/20418307 [====================>.........] - ETA: 1s    15507456/20418307 [=====================>........] - ETA: 1s    15736832/20418307 [======================>.......] - ETA: 1s    15769600/20418307 [======================>.......] - ETA: 1s    16244736/20418307 [======================>.......] - ETA: 0s    16916480/20418307 [=======================>......] - ETA: 0s    17162240/20418307 [========================>.....] - ETA: 0s    17211392/20418307 [========================>.....] - ETA: 0s    17620992/20418307 [========================>.....] - ETA: 0s    18276352/20418307 [=========================>....] - ETA: 0s    18505728/20418307 [==========================>...] - ETA: 0s    18735104/20418307 [==========================>...] - ETA: 0s    19177472/20418307 [===========================>..] - ETA: 0s    19914752/20418307 [============================>.] - ETA: 0s    20111360/20418307 [============================>.] - ETA: 0s    20324352/20418307 [============================>.] - ETA: 0s    20418307/20418307 [==============================] - 4s 0us/step
    Download complete.
    10 images and their labels are loaded and preprocessed.




.. GENERATED FROM PYTHON SOURCE LINES 48-59

2. Pretrained quantized model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The Akida model zoo contains a `pretrained quantized helper
<../../api_reference/akida_models_apis.html#akida_models.akidanet_imagenet_pretrained>`_.

The quantization scheme for this model is the following:

 * the first layer has 8-bit weights,
 * all other layers have 4-bit weights,
 * all activations are 4-bit.

.. GENERATED FROM PYTHON SOURCE LINES 59-68

.. code-block:: Python


    from cnn2snn import set_akida_version, AkidaVersion
    from akida_models import akidanet_imagenet_pretrained

    # Use a quantized model with pretrained quantized weights
    with set_akida_version(AkidaVersion.v1):
        model_keras_quantized_pretrained = akidanet_imagenet_pretrained(0.5)
    model_keras_quantized_pretrained.summary()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /usr/local/lib/python3.11/dist-packages/akida_models/model_io.py:147: UserWarning: Model akidanet_imagenet_224_alpha_50_iq8_wq4_aq4.h5 has been trained with akida_models 1.1.10 which is the last version supporting 1.0 models training. Continuing execution.
      warnings.warn(f'Model {model_name_v1} has been trained with akida_models 1.1.10 which is '
    Downloading data from https://data.brainchip.com/models/AkidaV1/akidanet/akidanet_imagenet_224_alpha_50_iq8_wq4_aq4.h5.
          0/5589312 [..............................] - ETA: 0s      32768/5589312 [..............................] - ETA: 37s      90112/5589312 [..............................] - ETA: 27s     212992/5589312 [>.............................] - ETA: 16s     335872/5589312 [>.............................] - ETA: 13s     466944/5589312 [=>............................] - ETA: 9s      679936/5589312 [==>...........................] - ETA: 6s     745472/5589312 [===>..........................] - ETA: 6s     909312/5589312 [===>..........................] - ETA: 5s    1056768/5589312 [====>.........................] - ETA: 4s    1236992/5589312 [=====>........................] - ETA: 4s    1417216/5589312 [======>.......................] - ETA: 3s    1597440/5589312 [=======>......................] - ETA: 3s    1794048/5589312 [========>.....................] - ETA: 3s    1990656/5589312 [=========>....................] - ETA: 2s    2203648/5589312 [==========>...................] - ETA: 2s    2433024/5589312 [============>.................] - ETA: 2s    2662400/5589312 [=============>................] - ETA: 2s    2744320/5589312 [=============>................] - ETA: 1s    2924544/5589312 [==============>...............] - ETA: 1s    3186688/5589312 [================>.............] - ETA: 1s    3448832/5589312 [=================>............] - ETA: 1s    4104192/5589312 [=====================>........] - ETA: 0s    4907008/5589312 [=========================>....] - ETA: 0s    5589312/5589312 [==============================] - 2s 0us/step
    Download complete.
    Model: "sequential_2"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     rescaling (Rescaling)       (None, 224, 224, 3)       0         
                                                                 
     conv_0 (QuantizedConv2D)    (None, 112, 112, 16)      448       
                                                                 
     conv_0/relu (QuantizedReLU  (None, 112, 112, 16)      0         
     )                                                               
                                                                 
     conv_1 (QuantizedConv2D)    (None, 112, 112, 32)      4640      
                                                                 
     conv_1/relu (QuantizedReLU  (None, 112, 112, 32)      0         
     )                                                               
                                                                 
     conv_2 (QuantizedConv2D)    (None, 56, 56, 64)        18496     
                                                                 
     conv_2/relu (QuantizedReLU  (None, 56, 56, 64)        0         
     )                                                               
                                                                 
     conv_3 (QuantizedConv2D)    (None, 56, 56, 64)        36928     
                                                                 
     conv_3/relu (QuantizedReLU  (None, 56, 56, 64)        0         
     )                                                               
                                                                 
     separable_4 (QuantizedSepa  (None, 28, 28, 128)       8896      
     rableConv2D)                                                    
                                                                 
     separable_4/relu (Quantize  (None, 28, 28, 128)       0         
     dReLU)                                                          
                                                                 
     separable_5 (QuantizedSepa  (None, 28, 28, 128)       17664     
     rableConv2D)                                                    
                                                                 
     separable_5/relu (Quantize  (None, 28, 28, 128)       0         
     dReLU)                                                          
                                                                 
     separable_6 (QuantizedSepa  (None, 14, 14, 256)       34176     
     rableConv2D)                                                    
                                                                 
     separable_6/relu (Quantize  (None, 14, 14, 256)       0         
     dReLU)                                                          
                                                                 
     separable_7 (QuantizedSepa  (None, 14, 14, 256)       68096     
     rableConv2D)                                                    
                                                                 
     separable_7/relu (Quantize  (None, 14, 14, 256)       0         
     dReLU)                                                          
                                                                 
     separable_8 (QuantizedSepa  (None, 14, 14, 256)       68096     
     rableConv2D)                                                    
                                                                 
     separable_8/relu (Quantize  (None, 14, 14, 256)       0         
     dReLU)                                                          
                                                                 
     separable_9 (QuantizedSepa  (None, 14, 14, 256)       68096     
     rableConv2D)                                                    
                                                                 
     separable_9/relu (Quantize  (None, 14, 14, 256)       0         
     dReLU)                                                          
                                                                 
     separable_10 (QuantizedSep  (None, 14, 14, 256)       68096     
     arableConv2D)                                                   
                                                                 
     separable_10/relu (Quantiz  (None, 14, 14, 256)       0         
     edReLU)                                                         
                                                                 
     separable_11 (QuantizedSep  (None, 14, 14, 256)       68096     
     arableConv2D)                                                   
                                                                 
     separable_11/relu (Quantiz  (None, 14, 14, 256)       0         
     edReLU)                                                         
                                                                 
     separable_12 (QuantizedSep  (None, 7, 7, 512)         133888    
     arableConv2D)                                                   
                                                                 
     separable_12/relu (Quantiz  (None, 7, 7, 512)         0         
     edReLU)                                                         
                                                                 
     separable_13 (QuantizedSep  (None, 7, 7, 512)         267264    
     arableConv2D)                                                   
                                                                 
     separable_13/global_avg (G  (None, 512)               0         
     lobalAveragePooling2D)                                          
                                                                 
     separable_13/relu (Quantiz  (None, 512)               0         
     edReLU)                                                         
                                                                 
     dropout (Dropout)           (None, 512)               0         
                                                                 
     classifier (QuantizedDense  (None, 1000)              513000    
     )                                                               
                                                                 
    =================================================================
    Total params: 1375880 (5.25 MB)
    Trainable params: 1375880 (5.25 MB)
    Non-trainable params: 0 (0.00 Byte)
    _________________________________________________________________




.. GENERATED FROM PYTHON SOURCE LINES 69-70

Check model performance on the 10 images set.

.. GENERATED FROM PYTHON SOURCE LINES 70-84

.. code-block:: Python


    from timeit import default_timer as timer

    num_images = len(x_test)

    start = timer()
    potentials_keras = model_keras_quantized_pretrained.predict(x_test, batch_size=100)
    end = timer()
    print(f'TF-Keras inference on {num_images} images took {end-start:.2f} s.\n')

    preds_keras = np.squeeze(np.argmax(potentials_keras, 1))
    accuracy_keras = np.sum(np.equal(preds_keras, labels_test)) / num_images
    print(f"TF-Keras accuracy: {accuracy_keras*num_images:.0f}/{num_images}.")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    1/1 [==============================] - ETA: 0s    1/1 [==============================] - 1s 749ms/step
    TF-Keras inference on 10 images took 0.77 s.

    TF-Keras accuracy: 9/10.




.. GENERATED FROM PYTHON SOURCE LINES 85-87

3. Conversion to Akida
~~~~~~~~~~~~~~~~~~~~~~

.. GENERATED FROM PYTHON SOURCE LINES 89-96

3.1 Convert to Akida model
^^^^^^^^^^^^^^^^^^^^^^^^^^

Here, the TF-Keras quantized model is converted into a suitable version for
the Akida accelerator. The
`cnn2snn.convert <../../api_reference/cnn2snn_apis.html#cnn2snn.convert>`__ function only needs
the TF-Keras model as argument.

.. GENERATED FROM PYTHON SOURCE LINES 96-101

.. code-block:: Python


    from cnn2snn import convert

    model_akida = convert(model_keras_quantized_pretrained)








.. GENERATED FROM PYTHON SOURCE LINES 102-104

The `Model.summary <../../api_reference/akida_apis.html#akida.Model.summary>`__
method provides a detailed description of the Model layers.

.. GENERATED FROM PYTHON SOURCE LINES 104-107

.. code-block:: Python


    model_akida.summary()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

                     Model Summary                  
    ________________________________________________
    Input shape    Output shape  Sequences  Layers
    ================================================
    [224, 224, 3]  [1, 1, 1000]  1          15    
    ________________________________________________

    _____________________________________________________________
    Layer (type)              Output shape    Kernel shape     

    ============== SW/conv_0-classifier (Software) ==============

    conv_0 (InputConv.)       [112, 112, 16]  (3, 3, 3, 16)    
    _____________________________________________________________
    conv_1 (Conv.)            [112, 112, 32]  (3, 3, 16, 32)   
    _____________________________________________________________
    conv_2 (Conv.)            [56, 56, 64]    (3, 3, 32, 64)   
    _____________________________________________________________
    conv_3 (Conv.)            [56, 56, 64]    (3, 3, 64, 64)   
    _____________________________________________________________
    separable_4 (Sep.Conv.)   [28, 28, 128]   (3, 3, 64, 1)    
    _____________________________________________________________
                                              (1, 1, 64, 128)  
    _____________________________________________________________
    separable_5 (Sep.Conv.)   [28, 28, 128]   (3, 3, 128, 1)   
    _____________________________________________________________
                                              (1, 1, 128, 128) 
    _____________________________________________________________
    separable_6 (Sep.Conv.)   [14, 14, 256]   (3, 3, 128, 1)   
    _____________________________________________________________
                                              (1, 1, 128, 256) 
    _____________________________________________________________
    separable_7 (Sep.Conv.)   [14, 14, 256]   (3, 3, 256, 1)   
    _____________________________________________________________
                                              (1, 1, 256, 256) 
    _____________________________________________________________
    separable_8 (Sep.Conv.)   [14, 14, 256]   (3, 3, 256, 1)   
    _____________________________________________________________
                                              (1, 1, 256, 256) 
    _____________________________________________________________
    separable_9 (Sep.Conv.)   [14, 14, 256]   (3, 3, 256, 1)   
    _____________________________________________________________
                                              (1, 1, 256, 256) 
    _____________________________________________________________
    separable_10 (Sep.Conv.)  [14, 14, 256]   (3, 3, 256, 1)   
    _____________________________________________________________
                                              (1, 1, 256, 256) 
    _____________________________________________________________
    separable_11 (Sep.Conv.)  [14, 14, 256]   (3, 3, 256, 1)   
    _____________________________________________________________
                                              (1, 1, 256, 256) 
    _____________________________________________________________
    separable_12 (Sep.Conv.)  [7, 7, 512]     (3, 3, 256, 1)   
    _____________________________________________________________
                                              (1, 1, 256, 512) 
    _____________________________________________________________
    separable_13 (Sep.Conv.)  [1, 1, 512]     (3, 3, 512, 1)   
    _____________________________________________________________
                                              (1, 1, 512, 512) 
    _____________________________________________________________
    classifier (Fully.)       [1, 1, 1000]    (1, 1, 512, 1000)
    _____________________________________________________________




.. GENERATED FROM PYTHON SOURCE LINES 108-112

3.2 Check performance
^^^^^^^^^^^^^^^^^^^^^

The following will only compute accuracy for the 10 images set.

.. GENERATED FROM PYTHON SOURCE LINES 112-123

.. code-block:: Python


    # Check Model performance
    start = timer()
    accuracy_akida = model_akida.evaluate(x_test, labels_test)
    end = timer()
    print(f'Inference on {num_images} images took {end-start:.2f} s.\n')
    print(f"Accuracy: {accuracy_akida*num_images:.0f}/{num_images}.")

    # For non-regression purposes
    assert accuracy_akida >= 0.8





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Inference on 10 images took 0.27 s.

    Accuracy: 8/10.




.. GENERATED FROM PYTHON SOURCE LINES 124-130

3.3 Show predictions for a random image
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Labels for test images are stored in the akida_models package. The matching
between names (*string*) and labels (*integer*) is given through the
``imagenet.preprocessing.index_to_label`` method.

.. GENERATED FROM PYTHON SOURCE LINES 130-263

.. code-block:: Python



    import matplotlib.pyplot as plt
    import matplotlib.lines as lines
    from akida_models.imagenet import preprocessing


    # Functions used to display the top5 results
    def get_top5(potentials, true_label):
        """
        Returns the top 5 classes from the output potentials
        """
        tmp_pots = potentials.copy()
        top5 = []
        min_val = np.min(tmp_pots)
        for ii in range(5):
            best = np.argmax(tmp_pots)
            top5.append(best)
            tmp_pots[best] = min_val

        vals = np.zeros((6,))
        vals[:5] = potentials[top5]
        if true_label not in top5:
            vals[5] = potentials[true_label]
        else:
            vals[5] = 0
        vals /= np.max(vals)

        class_name = []
        for ii in range(5):
            class_name.append(preprocessing.index_to_label(top5[ii]).split(',')[0])
        if true_label in top5:
            class_name.append('')
        else:
            class_name.append(
                preprocessing.index_to_label(true_label).split(',')[0])

        return top5, vals, class_name


    def adjust_spines(ax, spines):
        for loc, spine in ax.spines.items():
            if loc in spines:
                spine.set_position(('outward', 10))  # outward by 10 points
            else:
                spine.set_color('none')  # don't draw spine
        # Turn off ticks where there is no spine
        if 'left' in spines:
            ax.yaxis.set_ticks_position('left')
        else:
            # No yaxis ticks
            ax.yaxis.set_ticks([])
        if 'bottom' in spines:
            ax.xaxis.set_ticks_position('bottom')
        else:
            # No xaxis ticks
            ax.xaxis.set_ticks([])


    def prepare_plots():
        fig = plt.figure(figsize=(8, 4))
        # Image subplot
        ax0 = plt.subplot(1, 3, 1)
        imgobj = ax0.imshow(np.zeros((IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS), dtype=np.uint8))
        ax0.set_axis_off()
        # Top 5 results subplot
        ax1 = plt.subplot(1, 2, 2)
        bar_positions = (0, 1, 2, 3, 4, 6)
        rects = ax1.barh(bar_positions, np.zeros((6,)), align='center', height=0.5)
        plt.xlim(-0.2, 1.01)
        ax1.set(xlim=(-0.2, 1.15), ylim=(-1.5, 12))
        ax1.set_yticks(bar_positions)
        ax1.invert_yaxis()
        ax1.yaxis.set_ticks_position('left')
        ax1.xaxis.set_ticks([])
        adjust_spines(ax1, 'left')
        ax1.add_line(lines.Line2D((0, 0), (-0.5, 6.5), color=(0.0, 0.0, 0.0)))
        # Adjust Plot Positions
        ax0.set_position([0.05, 0.055, 0.3, 0.9])
        l1, b1, w1, h1 = ax1.get_position().bounds
        ax1.set_position([l1 * 1.05, b1 + 0.09 * h1, w1, 0.8 * h1])
        # Add title box
        plt.figtext(0.5,
                    0.9,
                    "ImageNet Classification by Akida",
                    size=20,
                    ha="center",
                    va="center",
                    bbox=dict(boxstyle="round",
                              ec=(0.5, 0.5, 0.5),
                              fc=(0.9, 0.9, 1.0)))

        return fig, imgobj, ax1, rects


    def update_bars_chart(rects, vals, true_label):
        counter = 0
        for rect, h in zip(rects, yvals):
            rect.set_width(h)
            if counter < 5:
                if top5[counter] == true_label:
                    if counter == 0:
                        rect.set_facecolor((0.0, 1.0, 0.0))
                    else:
                        rect.set_facecolor((0.0, 0.5, 0.0))
                else:
                    rect.set_facecolor('gray')
            elif counter == 5:
                rect.set_facecolor('red')
            counter += 1


    # Prepare plots
    fig, imgobj, ax1, rects = prepare_plots()

    # Get a random image
    rng = np.random.default_rng()
    img = rng.integers(0, num_images)

    # Predict image class
    outputs_akida = model_akida.predict(np.expand_dims(x_test[img], axis=0)).squeeze()

    # Get top 5 prediction labels and associated names
    true_label = labels_test[img]
    top5, yvals, class_name = get_top5(outputs_akida, true_label)

    # Draw Plots
    imgobj.set_data(x_test[img])
    ax1.set_yticklabels(class_name, rotation='horizontal', size=9)
    update_bars_chart(rects, yvals, true_label)
    fig.canvas.draw()
    plt.show()




.. image-sg:: /examples/general/images/sphx_glr_plot_1_akidanet_imagenet_001.png
   :alt: plot 1 akidanet imagenet
   :srcset: /examples/general/images/sphx_glr_plot_1_akidanet_imagenet_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 264-266

4. Hardware mapping and performance
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. GENERATED FROM PYTHON SOURCE LINES 268-275

4.1. Map on hardware
^^^^^^^^^^^^^^^^^^^^

List available Akida devices and check that an NSoC V2, Akida 1.0 production chip is available.

If a device is installed but not detected, reinstalling the driver might help, see the `driver
setup helper <https://github.com/Brainchip-Inc/akida_dw_edma/blob/master/README.md>`__.

.. GENERATED FROM PYTHON SOURCE LINES 275-282

.. code-block:: Python


    devices = akida.devices()
    print(f'Available devices: {[dev.desc for dev in devices]}')
    assert len(devices), "No device found, this example needs an Akida NSoC_v2 device."
    device = devices[0]
    assert device.version == akida.NSoC_v2, "Wrong device found, this example needs an Akida NSoC_v2."





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Available devices: ['PCIe/NSoC_v2/0']




.. GENERATED FROM PYTHON SOURCE LINES 283-284

Map the model on the device

.. GENERATED FROM PYTHON SOURCE LINES 284-290

.. code-block:: Python


    model_akida.map(device)

    # Check model mapping: NP allocation and binary size
    model_akida.summary()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

                                          Model Summary                                      
    _________________________________________________________________________________________
    Input shape    Output shape  Sequences  Layers  NPs  Skip DMAs  External Memory (Bytes)
    =========================================================================================
    [224, 224, 3]  [1, 1, 1000]  1          15      68   0          400000                 
    _________________________________________________________________________________________

    _________________________
    Component (type)  Count
    =========================
    HRC               1    
    _________________________
    CNP1              67   
    _________________________
    FNP2              1    
    _________________________

               External Memory Summary            
    ______________________________________________
    Layer (type)         External Memory (Bytes)
    ==============================================
    classifier (Fully.)  400000                 
    ______________________________________________

    _________________________________________________________________________
    Layer (type)              Output shape    Kernel shape       Components

    ========= HW/conv_0-classifier (Hardware) - size: 1361268 bytes =========

    conv_0 (InputConv.)       [112, 112, 16]  (3, 3, 3, 16)      1 HRC     
    _________________________________________________________________________
    conv_1 (Conv.)            [112, 112, 32]  (3, 3, 16, 32)     4 CNP1    
    _________________________________________________________________________
    conv_2 (Conv.)            [56, 56, 64]    (3, 3, 32, 64)     6 CNP1    
    _________________________________________________________________________
    conv_3 (Conv.)            [56, 56, 64]    (3, 3, 64, 64)     3 CNP1    
    _________________________________________________________________________
    separable_4 (Sep.Conv.)   [28, 28, 128]   (3, 3, 64, 1)      6 CNP1    
    _________________________________________________________________________
                                              (1, 1, 64, 128)              
    _________________________________________________________________________
    separable_5 (Sep.Conv.)   [28, 28, 128]   (3, 3, 128, 1)     4 CNP1    
    _________________________________________________________________________
                                              (1, 1, 128, 128)             
    _________________________________________________________________________
    separable_6 (Sep.Conv.)   [14, 14, 256]   (3, 3, 128, 1)     8 CNP1    
    _________________________________________________________________________
                                              (1, 1, 128, 256)             
    _________________________________________________________________________
    separable_7 (Sep.Conv.)   [14, 14, 256]   (3, 3, 256, 1)     4 CNP1    
    _________________________________________________________________________
                                              (1, 1, 256, 256)             
    _________________________________________________________________________
    separable_8 (Sep.Conv.)   [14, 14, 256]   (3, 3, 256, 1)     4 CNP1    
    _________________________________________________________________________
                                              (1, 1, 256, 256)             
    _________________________________________________________________________
    separable_9 (Sep.Conv.)   [14, 14, 256]   (3, 3, 256, 1)     4 CNP1    
    _________________________________________________________________________
                                              (1, 1, 256, 256)             
    _________________________________________________________________________
    separable_10 (Sep.Conv.)  [14, 14, 256]   (3, 3, 256, 1)     4 CNP1    
    _________________________________________________________________________
                                              (1, 1, 256, 256)             
    _________________________________________________________________________
    separable_11 (Sep.Conv.)  [14, 14, 256]   (3, 3, 256, 1)     4 CNP1    
    _________________________________________________________________________
                                              (1, 1, 256, 256)             
    _________________________________________________________________________
    separable_12 (Sep.Conv.)  [7, 7, 512]     (3, 3, 256, 1)     8 CNP1    
    _________________________________________________________________________
                                              (1, 1, 256, 512)             
    _________________________________________________________________________
    separable_13 (Sep.Conv.)  [1, 1, 512]     (3, 3, 512, 1)     8 CNP1    
    _________________________________________________________________________
                                              (1, 1, 512, 512)             
    _________________________________________________________________________
    classifier (Fully.)       [1, 1, 1000]    (1, 1, 512, 1000)  1 FNP2    
    _________________________________________________________________________




.. GENERATED FROM PYTHON SOURCE LINES 291-297

4.2. Performance measurement
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Power measurement must be enabled on the device' soc (disabled by default).
After sending data for inference, performance measurements are available in
the `model statistics <../../api_reference/akida_apis.html#akida.Model.statistics>`__.

.. GENERATED FROM PYTHON SOURCE LINES 297-310

.. code-block:: Python


    # Enable power measurement
    device.soc.power_measurement_enabled = True

    # Send data for inference
    _ = model_akida.forward(x_test)

    # Display floor current
    floor_power = device.soc.power_meter.floor
    print(f'Floor power: {floor_power:.2f} mW')

    # Retrieve statistics
    print(model_akida.statistics)




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Floor power: 909.58 mW
    Average framerate = 52.63 fps
    Last inference power range (mW):  Avg 1039.50 / Min 909.00 / Max 1170.00 / Std 184.55 
    Last inference energy consumed (mJ/frame): 19.75
    Last inference clock: 43007922
    Last program clock: 999305





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 13.912 seconds)


.. _sphx_glr_download_examples_general_plot_1_akidanet_imagenet.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_1_akidanet_imagenet.ipynb <plot_1_akidanet_imagenet.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_1_akidanet_imagenet.py <plot_1_akidanet_imagenet.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_1_akidanet_imagenet.zip <plot_1_akidanet_imagenet.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
