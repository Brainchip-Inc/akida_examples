{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Advanced QuantizeML tutorial\n\nThis tutorial provides a comprehensive understanding of quantization in [QuantizeML python\npackage](../../user_guide/quantizeml.html#quantizeml-toolkit)_. Refer to [QuantizeML user\nguide](../../user_guide/quantizeml.html)_  and [Global Akida workflow tutorial](../general/plot_0_global_workflow.html)_ for additional resources.\n\n[QuantizeML python package](../../user_guide/quantizeml.html#quantizeml-toolkit)_ provides\na user-friendly collection of functions for obtaining a quantized model. The [quantize](../../api_reference/quantizeml_apis.html#quantizeml.models.quantize)_ function replaces TF-Keras\nlayers with quantized, integer only layers from [QuantizeML](../../user_guide/quantizeml.html)_.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Defining a quantization scheme\n\nThe quantization scheme refers to all the parameters used for quantization, that is the method of\nquantization such as per-axis or per-tensor, and the bitwidth used for inputs, outputs and\nweights.\n\nThe first part in this section explains how to define a quantization scheme using\n[QuantizationParams](../../api_reference/quantizeml_apis.html#quantizeml.models.QuantizationParams)_,\nwhich defines a homogeneous scheme that applies to all layers, and the second part explains how to\nfully customize the quantization scheme using a configuration file.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1. The quantization parameters\n\nThe easiest way to customize quantization is to use the ``qparams`` parameter of the [quantize](../../api_reference/quantizeml_apis.html#quantizeml.models.quantize) function. This is made\npossible by creating a [QuantizationParams](../../api_reference/quantizeml_apis.html#quantizeml.models.QuantizationParams)_ object.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from quantizeml.models import QuantizationParams\n\nqparams = QuantizationParams(input_weight_bits=8, weight_bits=8, activation_bits=8,\n                             per_tensor_activations=False, output_bits=8, buffer_bits=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By default, the quantization scheme adopted is 8-bit with per-axis activations, but it is possible\nto set every parameter with a different value. The following list is a detailed description of the\nparameters with tips on how to set them:\n\n- ``input_weight_bits`` is the bitwidth used to quantize weights of the first layer. It is usually\n  set to 8 which allows to better preserve the overall accuracy.\n- ``weight_bits`` is the bitwidth  used to quantize all other weights. It is usually set to 8\n  (Akida 2.0) or 4 (Akida 1.0).\n- ``activation_bits`` is the bitwidth used to quantize all ReLU activations. It is usually set to\n  8 (Akida 2.0) or 4 (Akida 1.0) but can be lower for edge learning (1-bit).\n- ``per_tensor_activations`` is a boolean that allows to define a per-axis (default) or per-tensor\n  quantization for ReLU activations. Per-axis quantization will usually provide more accurate\n  results (default ``False`` value) but it might be more challenging to [calibrate](./plot_0_advanced_quantizeml.html#calibration)_ the model. Note that Akida 1.0 only supports\n  per-tensor activations.\n- ``output_bits`` is the bitwidth used to quantize intermediate results in\n  [OutputQuantizer](../../api_reference/quantizeml_apis.html#quantizeml.layers.OutputQuantizer)_.\n  Go back to the [user guide quantization flow](../../user_guide/quantizeml.html#quantization-flow)_\n  for details about this process.\n- ``buffer_bits`` is the maximum bitwidth allowed for low-level integer operations (e.g matrix\n  multiplications). It is set to 32 and should not be changed as this is what the Akida hardware\n  target will use.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>It is recommended to quantize a model to 8-bit or 4-bit to ensure it is Akida hardware\n          compatible.</p></div>\n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>``QuantizationParams`` is only applied the first time a model is quantized.\n             If you want to re-quantize a model, you must to provide a complete ``q_config``.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2. Using a configuration file\n\nQuantization can be further customized via a JSON configuration passed to the ``q_config``\nparameter of the [quantize](../../api_reference/quantizeml_apis.html#quantizeml.models.quantize)_\nfunction. This usage should be limited to targeted customization as writing a whole\nconfiguration from scratch is really error prone. An example of targeted customization is to set\nthe quantization bitwidth of the output of a feature extractor to 1 which will allow edge learning\n(1.0 feature only).\n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>When provided, the configuration file has priority over arguments. As a result\n             however, the configuration file therefore must contain all parameters - you cannot\n             rely on argument defaults to set non-specified values.</p></div>\n\nThe following code snippets show what a configuration file looks like and how to edit it to\ncustomize quantization.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import tf_keras as keras\nimport tensorflow as tf\nimport json\nfrom quantizeml.models import quantize, dump_config, QuantizationParams\n\n# Define an example model with few layers to keep what follows readable\ninput = keras.layers.Input((28, 28, 3), dtype=tf.uint8)\nx = keras.layers.Rescaling(scale=1. / 255, name=\"rescale\")(input)\nx = keras.layers.Conv2D(filters=16, kernel_size=3, name=\"input_conv\")(x)\nx = keras.layers.DepthwiseConv2D(kernel_size=3, name=\"dw_conv\")(x)\nx = keras.layers.Conv2D(filters=32, kernel_size=1, name=\"pw_conv\")(x)\nx = keras.layers.ReLU(name=\"relu\")(x)\nx = keras.layers.Dense(units=10, name=\"dense\")(x)\n\nmodel = keras.Model(input, x)\n\n# Define QuantizationParams with specific values just for the sake of understanding the JSON\n# configuration that follows.\nqparams = QuantizationParams(input_weight_bits=16, weight_bits=4, activation_bits=6, output_bits=12,\n                             per_tensor_activations=True, buffer_bits=24)\n\n# Quantize the model\nquantized_model = quantize(model, qparams=qparams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "quantized_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Dump the configuration\nconfig = dump_config(quantized_model)\n\n# Display in a JSON format for readability\nprint(json.dumps(config, indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Explaining the above configuration:\n\n- the layer names are indexing the configuration dictionary.\n- the depthwise layer has an OutputQuantizer set to 12-bit (``output_bits=12``) to reduce\n  intermediate potentials bitwidth before the pointwise layer that follows (automatically added\n  when calling ``quantize``).\n- the depthwise layer weights are quantized to 16-bit because it is the first layer\n  (``input_weight_bits=16``) and are quantized per-axis (default for weights). The given axis is\n  -2 because of TF-Keras depthwise kernel shape that is (Kx, Ky, F, 1), channel dimension is at\n  index -2.\n- the pointwise layer has weights quantized to 4-bit (``weight_bits=4``) but the quantization axis\n  is not specified as it defaults to -1 for a per-axis quantization. One would need to set it to\n  ``None`` for a per-tensor quantization.\n- the ReLU activation is quantized to 6-bit per-tensor (``activation_bits=6,\n  per_tensor_activations=True``)\n- all ``buffer_bitwidth`` are set to 24 (``buffer_bits=24``)\n\nThe configuration will now be edited and used to quantize the float model with ``q_config``\nparameter.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Edit the ReLU activation configuration\nconfig[\"relu\"][\"output_quantizer\"]['bitwidth'] = 1\nconfig[\"relu\"][\"output_quantizer\"]['axis'] = 'per-axis'\nconfig[\"relu\"][\"output_quantizer\"]['buffer_bitwidth'] = 32\nconfig[\"relu\"]['buffer_bitwidth'] = 32\n\n# Drop other layers configurations\ndel config['dw_conv']\ndel config['pw_conv']\ndel config['dense']\n\n# The configuration is now limited to the ReLU activation\nprint(json.dumps(config, indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now quantize with setting both ``qparams`` and ``q_config`` parameters: the activation will be\nquantized using the given configuration and the other layers will use what is provided in\n``qparams``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "new_quantized_model = quantize(model, q_config=config, qparams=qparams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Dump the new configuration\nnew_config = dump_config(new_quantized_model)\n\n# Display in a JSON format for readability\nprint(json.dumps(new_config, indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The new configuration contains both the manually set configuration in the activation and the\nparameters defined configuration for other layers.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Calibration\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1. Why is calibration required?\n\n[OutputQuantizer](../../api_reference/quantizeml_apis.html#quantizeml.layers.OutputQuantizer)_\nare added between layer blocks during quantization in order to decrease intermediate potential\nbitwidth and prevent saturation. Calibration is the process of defining the best quantization\nrange possible for the OutputQuantizer.\n\nCalibration will statistically determine the quantization range by passing samples into the float\nmodel and observing the intermediate output values. The quantization range is stored in\n``range_max`` variable. The calibration algorithm used in QuantizeML is based on a moving maximum:\n``range_max`` is initialized with the maximum value of the first batch of samples (per-axis or\nper-tensor depending on the quantization scheme) and the following batches will update\n``range_max`` with a moving momentum strategy (momentum is set to 0.9). Refer to the following\npseudo code:\n\n```python\nsamples_max = reduce_max(samples)\ndelta = previous_range_max - new_range_max * (1 - momentum)\nnew_range_max = previous_range_max - delta\n```\nIn QuantizeML like in other frameworks, the calibration process happens simultaneously\nwith quantization and the [quantize](../../api_reference/quantizeml_apis.html#quantizeml.models.quantize)_ function thus comes with\ncalibration parameters: ``samples``, ``num_samples``, ``batch_size`` and ``epochs``. Sections\nbelow describe how to set these parameters.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Calibration does not require any label or sample annotation and is therefore different\n          from training.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2. The samples\n\nThere are two types of calibration samples: randomly generated samples or real samples.\n\nWhen the ``samples`` parameter of ``quantize`` is left to the default ``None`` value, random\nsamples will be generated using the ``num_samples`` value (default is 1024). When the model input\nshape has 1 or 3 channels, which corresponds to an image, the random samples value are unsigned\n8-bit integers in the [0, 255] range. If the channel dimension is not 1 or 3, the generated\nsamples are 8-bit signed integers in the [-128, 127] range.\nIf that does not correspond to the range expected by your model, either add a [Rescaling](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Rescaling)_ layer to your model\nusing the [insert_rescaling helper](../../api_reference/quantizeml_apis.html#quantizeml.models.transforms.insert_rescaling)_ or\nprovide real samples.\n\nReal samples are often (but not necessarily) taken from the training dataset and should be the\npreferred option for calibration as it will always lead to better results.\n\nSamples are batched before being passed to the model for calibration. It is recommended to use at\nleast 1024 samples for calibration. When providing samples, ``num_samples`` is only used to\ncompute the number of steps during calibration.\n\n```python\nif batch_size is None:\n    steps = num_samples\nelse:\n    steps = np.ceil(num_samples / batch_size)\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3. Other calibration parameters\n\n#### ``batch_size``\nSetting a large enough ``batch_size`` is important as it will impact ``range_max`` initialization\nthat is made on the first batch of samples. The recommended value is 100.\n\n#### ``epochs``\nIt is the number of iterations over the calibration samples. Increasing the value will allow for\nmore updates of the ``range_max`` variables thanks to the momentum policy without requiring a huge\namount of samples. The recommended value is 2.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Handling input types\n\nIn standard machine learning frameworks such as TF-Keras or PyTorch, models usually expect\nfloating-point inputs. In an embedded software and deployment context, floating-points might\nhowever not be handled. That is the case for Akida hardware that only accepts integer inputs.\n\nQuantizeML provides an [InputQuantizer](../../api_reference/quantizeml_apis.html#quantizeml.layers.InputQuantizer)_ layer that can be\nadded at the model input in order to convert floating-point inputs to integer inputs expected by\nAkida. The InputQuantizer layer performs input quantization by applying a scale and an offset\nto the inputs. These values are computed during calibration by observing the input samples\nstatistics and the quantization range is determined by the quantization dtype given to the\nquantization parameters, see [QuantizationParams.input_dtype](../../api_reference/quantizeml_apis.html#quantizeml.models.QuantizationParams)_.\nBecause Akida only supports a channel last data format, the InputQuantizer layer can also convert\ndata format from channel first to channel last. This only applies to models coming from PyTorch\nthrough ONNX.\n\nThe InputQuantizer layer added during quantization is later converted to an [Akida.Quantizer](../../api_reference/akida_apis.html#akida.Quantizer)_ layer.\n\nWhile this allows to quickly prototype models that will be deployed on Akida hardware, it is\noften preferable to handle input quantization and data format conversion natively to prevent the\nextra scaling, offset and transpose. It is also key to train a model with the same data type as\nthe target application expects.\n\n\n### 3.1. InputQuantizer for floating-point inputs\n\nTo illustrate this, let's consider image classification models. While usually trained with float32\ndata, for deployment, sensors will provide unsigned 8-bit integer images in a channel last format.\nIn that case, it is better to define the model with a uint8 input type (passing the right dtype to\nthe Input layer), quantize with a uint8 dtype and avoid adding an InputQuantizer layer altogether.\n\nLet's first look at what happens without explicitely setting the Input dtype.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Define an example model with few layers that could be used for image classification\ninput = keras.layers.Input((28, 28, 3))\nx = keras.layers.Rescaling(scale=1. / 255, name=\"rescale\")(input)\nx = keras.layers.Conv2D(16, 3, strides=2, padding=\"same\", name=\"input_conv\")(x)\nx = keras.layers.ReLU(name=\"relu_0\")(x)\nx = keras.layers.DepthwiseConv2D(3, strides=2, padding=\"same\", name=\"dw_conv\")(x)\nx = keras.layers.Conv2D(32, 1, name=\"pw_conv\")(x)\nx = keras.layers.ReLU(name=\"relu_1\")(x)\nx = keras.layers.Flatten()(x)\nx = keras.layers.Dense(units=10, name=\"dense\")(x)\n\nmodel = keras.Model(input, x)\n\n# Define QuantizationParams with explicit dtype uint8 (which is the default)\nqparams = QuantizationParams(input_dtype='uint8')\n\n# Define random calibration samples in range [0, 255] as float32 (it could be any range but this is\n# kept simple for the sake of the example)\nimport numpy as np\n\ncalibration_samples = np.random.randint(0, 256, size=(256, 28, 28, 3)).astype(np.float32)\n\n# Quantize the model\nquantized_model = quantize(model, qparams=qparams, num_samples=256,\n                           samples=calibration_samples, batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As the model Input Layer is not typed (defaulting to float32), an InputQuantizer layer has been\nadded in second position:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "quantized_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's take a look at what the InputQuantizer layer does during the conversion process.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from quantizeml.models import record_quantization_variables\n\n\ndef print_input_quantizer_params(q_model):\n    # Record variables to display them below.\n    # Note this is only needed for tutorial purposes and handled automatically during standard\n    # conversion process.\n    record_quantization_variables(q_model)\n\n    print('InputQuantizer parameters computed during calibration:')\n    print(f'  Bitwidth: {q_model.layers[1].bitwidth}')\n    print(f'  Signedness: {q_model.layers[1].signed}')\n    print(f'  Scale (per-channel): {2 ** q_model.layers[1].frac_bits.value.numpy()}')\n    if hasattr(q_model.layers[1], 'zero_points'):\n        print(f'  Offset (per-channel): {q_model.layers[1].zero_points.value.values.numpy()}')\n\n\nprint_input_quantizer_params(quantized_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since inputs were created in the [0, 255] range, the InputQuantizer layer has learned to\nquantize inputs to uint8 with a scale of 1 and no offset as expected.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2. Conversion to Akida with floating-point data\n\nHere is another example where the model naturally takes float32 input data (e.g. a time frequency\nmap). With Akida, this will be quantized to int8.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "float_input = keras.layers.Input((10, 25, 2))\ny = keras.layers.Conv2D(16, 3, strides=2, padding=\"same\", name=\"input_conv\")(float_input)\ny = keras.layers.ReLU(name=\"relu_0\")(y)\ny = keras.layers.DepthwiseConv2D(3, strides=2, padding=\"same\", name=\"dw_conv\")(y)\ny = keras.layers.Conv2D(32, 1, name=\"pw_conv\")(y)\ny = keras.layers.ReLU(name=\"relu_1\")(y)\ny = keras.layers.Flatten()(y)\ny = keras.layers.Dense(units=10, name=\"dense\")(y)\n\nmodel = keras.Model(float_input, y)\n\n# Explicitly set input dtype to int8 and recompute calibration samples in the [-1, 1) range as an\n# example.\nqparams = QuantizationParams(input_dtype='int8')\ncalibration_samples_int8 = np.random.uniform(-1.0, 1.0, size=(256, 10, 25, 2)).astype(np.float32)\n\n# Quantize the model\nquantized_model = quantize(model, qparams=qparams, num_samples=256,\n                           samples=calibration_samples_int8, batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Take a look at the InputQuantizer:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "quantized_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print_input_quantizer_params(quantized_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Proceed with conversion to an Akida model:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from cnn2snn import convert\n\nakida_model = convert(quantized_model)\nakida_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model can be deployed on Akida hardware, with the extra scaling and offset natively\nhandled.\n\n### 3.3. Preventing the InputQuantizer\n\nWhen using images, it makes more sense to avoid the unecessary Inputquantizer layer (as we saw\nabove). Here, we will show you how to define a model with uint8 typed input to avoid adding this\nextra layer. Notice in the Input layer below the added dtype.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "typed_input = keras.layers.Input((28, 28, 3), dtype=tf.uint8)\nz = keras.layers.Rescaling(scale=1. / 255, name=\"rescale\")(typed_input)\nz = keras.layers.Conv2D(16, 3, strides=2, padding=\"same\", name=\"input_conv\")(z)\nz = keras.layers.ReLU(name=\"relu_0\")(z)\nz = keras.layers.DepthwiseConv2D(3, strides=2, padding=\"same\", name=\"dw_conv\")(z)\nz = keras.layers.Conv2D(32, 1, name=\"pw_conv\")(z)\nz = keras.layers.ReLU(name=\"relu_1\")(z)\nz = keras.layers.Flatten()(z)\nz = keras.layers.Dense(units=10, name=\"dense\")(z)\n\nmodel = keras.Model(typed_input, z)\n\nquantized_model = quantize(model, num_samples=256, samples=calibration_samples, batch_size=64)\n\nquantized_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As expected, the model does not contain any InputQuantizer layer since both the model input and\nquantization are typed as uint8. The quantization algorithm recognizes that inputs are\nalready in the right format, and so it does not need to quantize them.\nThus, conversion to an Akida model will not add any Akida.Quantizer layer, allowing for a more\nefficient deployment.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}